<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLE HA 15 SP1 | Geo Clustering Guide</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Geo Clustering Guide | SLE HA 15 SP1"/>
<meta name="description" content="This document covers the setup options and parameters …"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="15 SP1"/>
<meta name="book-title" content="Geo Clustering Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 15 SP1"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Geo Clustering Guide | SLE HA 15 SP1"/>
<meta property="og:description" content="This document covers the setup options and parameters for Geo clusters and their components, such as booth ticket manager, t…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Geo Clustering Guide | SLE HA 15 SP1"/>
<meta name="twitter:description" content="This document covers the setup options and parameters for Geo clusters and their components, such as booth ticket manager, t…"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/book_sle_ha_geo.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-sleha-geo">Geo Clustering Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-sleha-geo" data-id-title="Geo Clustering Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">15 SP1</span></div><div class="title-container"><div class="title-container"><h1 class="title">Geo Clustering Guide <a title="Permalink" class="permalink" href="#book-sleha-geo">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/book_sle_ha_geo.xml" title="Edit source document"> </a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/book_sle_ha_geo.xml" title="Edit source document"> </a></div></div><div class="abstract"><p>
      This document covers the setup options and parameters for Geo clusters and
   their components, such as booth ticket manager, the specific Csync2 setup, and the
   configuration of the required cluster resources (and how to transfer them to
   other sites in case of changes). Learn how to monitor and manage Geo clusters
   from command line or with the Hawk2 Web interface.
    </p></div><div class="date"><span class="imprint-label">Publication Date: </span>February 05, 2024
</div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-geo-challenges"><span class="title-number">1 </span><span class="title-name">Challenges for Geo Clusters</span></a></span></li><li><span class="chapter"><a href="#cha-ha-geo-concept"><span class="title-number">2 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="chapter"><a href="#cha-ha-geo-req"><span class="title-number">3 </span><span class="title-name">Requirements</span></a></span></li><li><span class="chapter"><a href="#cha-ha-geo-booth"><span class="title-number">4 </span><span class="title-name">Setting Up the Booth Services</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-geo-booth-basic"><span class="title-number">4.1 </span><span class="title-name">Booth Configuration and Setup Options</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-booth-ticket-types"><span class="title-number">4.2 </span><span class="title-name">Automatic versus Manual Tickets</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-booth-default"><span class="title-number">4.3 </span><span class="title-name">Using the Default Booth Setup</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-booth-multi"><span class="title-number">4.4 </span><span class="title-name">Using a Multi-Tenant Booth Setup</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-booth-sync"><span class="title-number">4.5 </span><span class="title-name">Synchronizing the Booth Configuration to All Sites and Arbitrators</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-setup-booth-service"><span class="title-number">4.6 </span><span class="title-name">Enabling and Starting the Booth Services</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-setup-booth-reconfig"><span class="title-number">4.7 </span><span class="title-name">Reconfiguring Booth While Running</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo-sync"><span class="title-number">5 </span><span class="title-name">Synchronizing Configuration Files Across All Sites and Arbitrators</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-geo-booth-sync-csync2-setup"><span class="title-number">5.1 </span><span class="title-name">Csync2 Setup for Geo Clusters</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-booth-sync-csync2-start"><span class="title-number">5.2 </span><span class="title-name">Synchronizing Changes with Csync2</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo-rsc"><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources and Constraints</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-geo-rsc-ticket-dep"><span class="title-number">6.1 </span><span class="title-name">Configuring Ticket Dependencies of Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-rsc-boothd"><span class="title-number">6.2 </span><span class="title-name">Configuring a Resource Group for <code class="systemitem">boothd</code></span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-rsc-order"><span class="title-number">6.3 </span><span class="title-name">Adding an Order Constraint</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-rsc-sync-cib"><span class="title-number">6.4 </span><span class="title-name">Transferring the Resource Configuration to Other Cluster Sites</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo-ip-relocation"><span class="title-number">7 </span><span class="title-name">Setting Up IP Relocation via DNS Update</span></a></span></li><li><span class="chapter"><a href="#cha-ha-geo-manage"><span class="title-number">8 </span><span class="title-name">Managing Geo Clusters</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-geo-manage-cli"><span class="title-number">8.1 </span><span class="title-name">Managing Tickets From Command Line</span></a></span></li><li><span class="sect1"><a href="#sec-ha-geo-manage-hawk2"><span class="title-number">8.2 </span><span class="title-name">Managing Tickets with Hawk2</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo-trouble"><span class="title-number">9 </span><span class="title-name">Troubleshooting</span></a></span></li><li><span class="chapter"><a href="#cha-ha-geo-upgrade"><span class="title-number">10 </span><span class="title-name">Upgrading to the Latest Product Version</span></a></span></li><li><span class="chapter"><a href="#sec-ha-geo-more"><span class="title-number">11 </span><span class="title-name">For More Information</span></a></span></li><li><span class="appendix"><a href="#id-1.5.14"><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="#id-1.5.14.4"><span class="title-number">A.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#fig-ha-geo-example-setup"><span class="number">2.1 </span><span class="name">Two-Site Cluster—2x2 Nodes + Arbitrator (Optional)</span></a></span></li><li><span class="figure"><a href="#fig-yast-ha-geo-booth"><span class="number">4.1 </span><span class="name">Example Ticket Dependency</span></a></span></li><li><span class="figure"><a href="#fig-ha-geo-csync-config"><span class="number">5.1 </span><span class="name">Example Csync2 Setup for Geo Clusters</span></a></span></li><li><span class="figure"><a href="#id-1.5.10.4.6.4.2"><span class="number">8.1 </span><span class="name">Hawk2—Ticket Details</span></a></span></li><li><span class="figure"><a href="#id-1.5.10.4.8"><span class="number">8.2 </span><span class="name">Hawk2 Simulator—Tickets</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-ha-booth-conf-default"><span class="number">4.1 </span><span class="name">A Booth Configuration File</span></a></span></li><li><span class="example"><a href="#ex-ha-conf-booth-multi-1"><span class="number">4.2 </span><span class="name"><code class="filename">/etc/booth/apac.conf</code></span></a></span></li><li><span class="example"><a href="#ex-ha-conf-booth-multi-2"><span class="number">4.3 </span><span class="name"><code class="filename">/etc/booth/emea.conf</code></span></a></span></li><li><span class="example"><a href="#ex-ha-geo-dyn-dns-rsc-config"><span class="number">7.1 </span><span class="name">Resource Configuration for Dynamic DNS Update</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.5.2.5"><p>
  Copyright © 2006–2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="chapter" id="cha-ha-geo-challenges" data-id-title="Challenges for Geo Clusters"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Challenges for Geo Clusters</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-challenges">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_challenges_i.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Typically, Geo environments are too far apart to support synchronous
    communication between the sites. That leads to the following challenges:
   </p></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    How to make sure that a cluster site is up and running?
   </p></li><li class="listitem"><p>
    How to make sure that resources are only started once?
   </p></li><li class="listitem"><p>
    How to make sure that quorum can be reached between the different sites and
    a split-brain scenario can be avoided?
   </p></li><li class="listitem"><p>
    How to keep the CIB up to date on all nodes and sites?
   </p></li><li class="listitem"><p>
    How to manage failover between the sites?
   </p></li><li class="listitem"><p>
    How to deal with high latency in case of resources that need to be stopped?
   </p></li></ul></div><p>
  In the following sections, learn how to meet these challenges with
  SUSE Linux Enterprise High Availability.
 </p></section><section class="chapter" id="cha-ha-geo-concept" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-concept">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_concept_i.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Geo clusters based on SUSE® Linux Enterprise High Availability can be considered
    <span class="quote">“<span class="quote">overlay</span>”</span> clusters where each cluster site corresponds to a
    cluster node in a traditional cluster. The overlay cluster is managed by
    the booth cluster ticket manager (in the following called booth).
   </p></div></div></div></div><p>
  Each of the parties involved in a Geo cluster runs a service, the <code class="systemitem">boothd</code>.
  It connects to the booth daemons running at the other sites and exchanges
  connectivity details. For making cluster resources highly available across
  sites, booth relies on cluster objects called tickets. A ticket grants the
  right to run certain resources on a specific cluster site. Booth guarantees
  that every ticket is granted to no more than one site at a time.
 </p><p>
  If the communication between two booth instances breaks down, it might be
  because of a network breakdown between the cluster sites
  <span class="emphasis"><em>or</em></span> because of
  an outage of one cluster site. In this case, you need an additional instance
  (a third cluster site or an
  <code class="literal">arbitrator</code>) to
  reach consensus about decisions (such as failover of resources across sites).
  Arbitrators are single machines (outside of the clusters) that run a booth
  instance in a special mode. Each Geo cluster can have one or multiple
  arbitrators.
 </p><p>
  The most common scenario probably is a Geo cluster with two sites and a
  single arbitrator on a third site. This requires three booth instances.
 </p><p>
   It is also possible to run a two-site Geo cluster <span class="emphasis"><em>without</em></span>
   an arbitrator. In this case, a Geo cluster administrator needs to manually manage
   the tickets. If a ticket should be granted to more than one site at the same time,
   booth displays a warning.
  </p><div class="figure" id="fig-ha-geo-example-setup"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_geocluster.png"><img src="images/ha_geocluster.png" width="85%" alt="Two-Site Cluster—2x2 Nodes + Arbitrator (Optional)" title="Two-Site Cluster—2x2 Nodes + Arbitrator (Optional)"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 2.1: </span><span class="title-name">Two-Site Cluster—2x2 Nodes + Arbitrator (Optional) </span></span><a title="Permalink" class="permalink" href="#fig-ha-geo-example-setup">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_concept_i.xml" title="Edit source document"> </a></div></div></div><p>
  The following list explains the components and mechanisms for Geo clusters
  in more detail.
 </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-geo-components-arbitrator"><span class="term">Arbitrator</span></dt><dd><p>
     Each site runs one booth instance that is responsible for communicating
     with the other sites. If you have a setup with an even number of sites,
     it is useful to have an additional instance to reach consensus about decisions such as
     failover of resources across sites. In this case, add one or more
     arbitrators running at additional sites. Arbitrators are single machines
     that run a booth instance in a special mode. As all booth instances
     communicate with each other, arbitrators help to make more reliable
     decisions about granting or revoking tickets. Arbitrators cannot hold any
     tickets.
    </p><p>
     An arbitrator is especially important for a two-site scenario: For
     example, if site <code class="literal">A</code> can no longer communicate with site
     <code class="literal">B</code>, there are two possible causes for that:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A network failure between <code class="literal">A</code> and <code class="literal">B</code>.
      </p></li><li class="listitem"><p>
       Site <code class="literal">B</code> is down.
      </p></li></ul></div><p>
     However, if site <code class="literal">C</code> (the arbitrator) can still
     communicate with site <code class="literal">B</code>, site <code class="literal">B</code> must
     still be up and running.
    </p></dd><dt id="vle-ha-geo-components-booth"><span class="term">Booth Cluster Ticket Manager</span></dt><dd><p>
     Booth is the instance managing the ticket distribution, and thus, the
     failover process between the sites of a Geo cluster. Each of the
     participating clusters and arbitrators runs a service, the <code class="systemitem">boothd</code>. It
     connects to the booth daemons running at the other sites and exchanges
     connectivity details. After a ticket has been granted to a site, the booth
     mechanism can manage the ticket automatically: If the site that holds the
     ticket is out of service, the booth daemons will vote which of the other
     sites will get the ticket. To protect against brief connection failures,
     sites that lose the vote (either explicitly or implicitly by being
     disconnected from the voting body) need to relinquish the ticket after a
     time-out. Thus, it is made sure that a ticket will only be redistributed
     after it has been relinquished by the previous site. See also
     <a class="xref" href="#vle-ha-geo-components-deadman">Dead Man Dependency (<code class="literal">loss-policy="fence"</code>)</a>.
    </p><p>
     For a Geo cluster with two sites and arbitrator, you need 3 booth
     instances: one instance per site plus the instance running on the
     arbitrator.
    </p><div id="id-1.5.4.9.2.2.3" data-id-title="Limited Number of Booth Instances" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Limited Number of Booth Instances</div><p>
      The upper limit is (currently) 16 booth instances.
     </p></div></dd><dt id="vle-ha-geo-components-deadman"><span class="term">Dead Man Dependency (<code class="literal">loss-policy="fence"</code>)</span></dt><dd><p>
     After a ticket is revoked, it can take a long time until all resources
     depending on that ticket are stopped, especially in case of cascaded
     resources. To cut that process short, the cluster administrator can
     configure a <code class="literal">loss-policy</code> (together with the ticket
     dependencies) for the case that a ticket gets revoked from a site. If the
     loss-policy is set to <code class="literal">fence</code>, the nodes that are hosting
     dependent resources are fenced.
    </p><div id="id-1.5.4.9.3.2.2" data-id-title="Potential Loss of Data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Potential Loss of Data</div><p>
      On the one hand, <code class="literal">loss-policy="fence"</code> considerably
      speeds up the recovery process of the cluster and makes sure that
      resources can be migrated more quickly.
     </p><p>
      On the other hand, it can lead to loss of all unwritten data, such as:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Data lying on shared storage.
       </p></li><li class="listitem"><p>
        Data in a replicating database (for example, MariaDB or PostgreSQL)
        or on a replicating device (DRBD), where the data has not yet reached
        the other site because of a slow network link.
       </p></li></ul></div></div></dd><dt id="vle-ha-geo-components-ticket"><span class="term">Ticket</span></dt><dd><p>
     A ticket grants the right to run certain resources on a specific cluster
     site. A ticket can only be owned by one site at a time. Initially, none of
     the sites has a ticket—each ticket must be granted once by the
     cluster administrator. After that, tickets are managed by the booth for
     automatic failover of resources. But administrators may also intervene and
     grant or revoke tickets manually.
    </p><p>
     After a ticket is administratively revoked, it is not managed by booth
     anymore. For booth to start managing the ticket again, the ticket must be
     again granted to a site.
    </p><p>
     Resources can be bound to a certain ticket by dependencies. Only if the
     defined ticket is available at a site, the respective resources are
     started. Vice versa, if the ticket is removed, the resources depending on
     that ticket are automatically stopped.
    </p><p>
     The presence or absence of tickets for a site is stored in the CIB as a
     cluster status. With regard to a certain ticket, there are only two states
     for a site: <code class="literal">true</code> (the site has the ticket) or
     <code class="literal">false</code> (the site does not have the ticket). The absence
     of a certain ticket (during the initial state of the Geo cluster) is not
     treated differently from the situation after the ticket has been revoked.
     Both are reflected by the value <code class="literal">false</code>.
    </p><p>
     A ticket within an overlay cluster is similar to a resource in a
     traditional cluster. But in contrast to traditional clusters, tickets are
     the only type of resource in an overlay cluster. They are primitive
     resources that do not need to be configured or cloned.
    </p></dd><dt id="id-1.5.4.9.5"><span class="term">Ticket Failover</span></dt><dd><p>
     After you have initially granted an automatic ticket to a site, booth
     will manage this ticket automatically.
     If the site holding a ticket should be out of service, the ticket is
     automatically revoked after the expiry time.
     If the remaining sites have quorum, the ticket will be granted to another
     site (fail over). The resources that depend on that ticket fail over to
     the new site that holds the ticket. The <code class="literal">loss-policy</code>
     (which is defined within the constraint) specifies what happens to the nodes
     that have run the resources before.
    </p><p>
      If automatic failover in case of a split brain scenario is
      <span class="emphasis"><em>not</em></span> required, administrators can also grant manual
      tickets to the healthy site. How to manage tickets from command line is
      described in <a class="xref" href="#sec-ha-geo-manage-cli" title="8.1. Managing Tickets From Command Line">Section 8.1</a>.
    </p></dd></dl></div></section><section class="chapter" id="cha-ha-geo-req" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-req">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_requirements_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Software Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.5.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_requirements_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
    All machines (cluster nodes and arbitrators) that will be part of the cluster
    need at least the following modules and extensions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Base System Module 15 SP1</p></li><li class="listitem"><p>Server Applications Module 15 SP1</p></li><li class="listitem"><p>SUSE Linux Enterprise High Availability 15 SP1</p></li></ul></div></li><li class="listitem"><p>
     When installing the machines, select <code class="literal">HA GEO Node</code> as 
     <code class="systemitem">system role</code>. This leads to the installation of a
     minimal system where the packages from the pattern <code class="literal">Geo Clustering
     for High Availability (ha_geo)</code> are installed by default.
   </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Network Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.5.5.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_requirements_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
    The virtual IPs to be used for each cluster site must be accessible across
    the Geo cluster.
   </p></li><li class="listitem"><p>
     The sites must be reachable on one UDP and TCP port per booth instance.
     That means any firewalls or IPsec tunnels in between must be configured
     accordingly.
    </p></li><li class="listitem"><p>
     Other setup decisions may require to open more ports (for example, for DRBD
     or database replication).
    </p></li></ul></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Other Requirements and Recommendations </span></span><a title="Permalink" class="permalink" href="#id-1.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_requirements_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
    All cluster nodes on all sites should synchronize to an NTP server outside
    the cluster. For more information, see the
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html" target="_blank">
     Administration Guide
     for SUSE Linux Enterprise Server 15 SP1</a>.
   </p><p>
    If nodes are not synchronized, log files and cluster reports are very hard
    to analyze.
   </p></li><li class="listitem"><p>
    Use an <span class="emphasis"><em>uneven</em></span> number of sites in your Geo
    cluster. In case the network connection breaks down, this makes sure that
    there still is a majority of sites (to avoid a split brain scenario). In
    case you have an even number of cluster sites, use an arbitrator for
    handling automatic failover of tickets. If you do not use an arbitrator,
    you need to handle ticket failover manually.
   </p></li><li class="listitem"><p>
    The cluster on each site has a meaningful name, for example:
    <code class="literal">amsterdam</code> and <code class="literal">berlin</code>.
   </p><p>
    The cluster names for each site are defined in the respective
    <code class="filename">/etc/corosync/corosync.conf</code> files:
   </p><div class="verbatim-wrap"><pre class="screen">totem {
    [...]
    cluster_name: amsterdam
    }</pre></div><p>
    Change the name with following crmsh command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster rename <em class="replaceable">NEW_NAME</em></pre></div><p>
    Stop and start the cluster services for the changes to take effect:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster restart</pre></div></li><li class="listitem"><p>
     Mixed architectures within one cluster are not supported. However, for
     Geo clusters, each member of the Geo cluster can have a different
     architecture—be it a cluster site or an arbitrator. For example,
     you can run a Geo cluster with three members (two cluster sites and an
     arbitrator), where one cluster site runs on IBM Z, the other
     cluster site runs on x86, and the arbitrator runs on POWER.
   </p></li></ul></div></section><section class="chapter" id="cha-ha-geo-booth" data-id-title="Setting Up the Booth Services"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Setting Up the Booth Services</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-booth">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter describes the setup and configuration options for booth, how
    to synchronize the booth configuration to all sites and arbitrators, how to
    enable and start the booth services, and how to reconfigure booth while its
    services are running.
   </p></div></div></div></div><section class="sect1" id="sec-ha-geo-booth-basic" data-id-title="Booth Configuration and Setup Options"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Booth Configuration and Setup Options</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-basic">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The default booth configuration is <code class="filename">/etc/booth/booth.conf</code>. This file must be the same
   on all sites of your Geo cluster, including the arbitrator or arbitrators.
   To keep the booth configuration synchronous across all sites and
   arbitrators, use Csync2, as described in
   <a class="xref" href="#sec-ha-geo-booth-sync" title="4.5. Synchronizing the Booth Configuration to All Sites and Arbitrators">Section 4.5, “Synchronizing the Booth Configuration to All Sites and Arbitrators”</a>.
  </p><div id="note-ha-geo-booth-ownership" data-id-title="Ownership of /etc/booth and Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Ownership of <code class="filename">/etc/booth</code> and Files</div><p>
    The directory <code class="filename">/etc/booth</code> and all
    files therein need to belong to the user
    <code class="systemitem">hacluster</code> and the group
    <code class="systemitem">haclient</code>. Whenever you copy
    a new file from this directory, use the option <code class="option">-p</code> for the
    <code class="command">cp</code> command to preserve the ownership. Alternatively,
    when you create a new file, set the user and group afterward with
    <code class="command">chown</code> <code class="option">hacluster:haclient
    <em class="replaceable">FILE</em></code>.
   </p></div><p>
    For setups including multiple Geo clusters, it is possible to <span class="quote">“<span class="quote">share</span>”</span> the same arbitrator (as
 of SUSE Linux Enterprise High Availability 12). By providing several booth configuration files, you can
 start multiple booth instances on the same arbitrator, with each booth
 instance running on a different port. That way, you can use <span class="emphasis"><em>one</em></span> machine to serve as
 arbitrator for <span class="emphasis"><em>different</em></span> Geo clusters. For details on how to configure booth for multiple
   Geo clusters, refer to <a class="xref" href="#sec-ha-geo-booth-multi" title="4.4. Using a Multi-Tenant Booth Setup">Section 4.4, “Using a Multi-Tenant Booth Setup”</a>.
  </p><p>
   To prevent malicious parties from disrupting the booth service, you can
   configure authentication for talking to booth, based on a shared key. For
   details, see <a class="xref" href="#co-ha-geo-booth-config-auth"><span class="callout">5</span></a> in
   <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>. All hosts that communicate with
   various booth servers need this key. Therefore make sure to include the key
   file in the Csync2 configuration or to synchronize it manually across all
   parties.
  </p></section><section class="sect1" id="sec-ha-geo-booth-ticket-types" data-id-title="Automatic versus Manual Tickets"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Automatic versus Manual Tickets</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-ticket-types">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A ticket grants the right to run certain resources on a specific
   cluster site. Two types of tickets are supported:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Automatic tickets are controlled by the <code class="systemitem">boothd</code> daemon.</p></li><li class="listitem"><p>Manual tickets are managed by the cluster administrator only.</p></li></ul></div><p>Automatic and manual tickets have the following properties:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Automatic and manual tickets can be defined together. </span>You can define and use both automatic and manual tickets within the
      same Geo cluster.</p></li><li class="listitem"><p><span class="formalpara-title">Manual ticket management remains manual. </span>The automatic ticket management is not applied to manually controlled
      tickets.
      Manual tickets do not require any quorum elections, cannot fail over
      automatically, and do not have an expiry time.</p></li><li class="listitem"><p><span class="formalpara-title">Manual tickets will not be moved automatically. </span>Tickets which were manually granted to a site will remain there
      until they are manually revoked. Even if a site goes offline, the
      ticket will not be moved to another site. This behavior ensures that
      the services that depend on a ticket remain on a particular site and
      are not moved to another site.
     </p></li><li class="listitem"><p><span class="formalpara-title">Same commands for managing both types of tickets. </span>The manual tickets are managed by the same commands as automatic
     tickets (<code class="command">grant</code> or <code class="command">revoke</code>, for example).</p></li><li class="listitem"><p><span class="formalpara-title">Arbitrators are not needed if only manual tickets are used. </span>
      If you configure only manual tickets in a Geo cluster, arbitrators are
      not necessary, because manual ticket management does not require quorum
      decisions.</p></li></ul></div><p>
   To configure tickets, use the <code class="filename">/etc/booth/booth.conf</code>
   configuration file (see <a class="xref" href="#sec-ha-geo-booth-default" title="4.3. Using the Default Booth Setup">Section 4.3, “Using the Default Booth Setup”</a> for
   further information).</p></section><section class="sect1" id="sec-ha-geo-booth-default" data-id-title="Using the Default Booth Setup"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Using the Default Booth Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-default">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have set up your basic Geo cluster with the
   <code class="systemitem">ha-cluster-bootstrap</code>
   scripts as described in the Geo Clustering Quick Start, the scripts have created a default
   booth configuration on all sites with a minimal set of parameters. To extend
   or fine-tune the minimal booth configuration, have a look at
   <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1</a> or at
   the examples in <a class="xref" href="#sec-ha-geo-booth-multi" title="4.4. Using a Multi-Tenant Booth Setup">Section 4.4, “Using a Multi-Tenant Booth Setup”</a>.
  </p><p>
   To add or change parameters needed for booth, either edit the booth
   configuration files manually or use the YaST <span class="guimenu">Geo
   Cluster</span> module. To access the YaST module, start it from command
   line with <code class="command">yast2 geo-cluster</code> (or start YaST and select
   <span class="guimenu">High Availability</span> › <span class="guimenu">Geo
   Cluster</span>).
  </p><div class="example" id="ex-ha-booth-conf-default" data-id-title="A Booth Configuration File"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 4.1: </span><span class="title-name">A Booth Configuration File </span></span><a title="Permalink" class="permalink" href="#ex-ha-booth-conf-default">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">transport = UDP <span class="callout" id="co-ha-geo-booth-config-transport">1</span>
port = 9929 <span class="callout" id="co-ha-geo-booth-config-port">2</span>
arbitrator = 192.168.203.100 <span class="callout" id="co-ha-geo-booth-config-arbitrator">3</span>
site =  192.168.201.100 <span class="callout" id="co-ha-geo-booth-config-site">4</span>
site =  192.168.202.100 <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>
authfile = /etc/booth/authkey <span class="callout" id="co-ha-geo-booth-config-auth">5</span>
ticket = "ticket-nfs" <span class="callout" id="co-ha-geo-booth-config-ticket">6</span>
     mode = MANUAL <span class="callout" id="co-ha-geo-booth-mode">7</span>
ticket = "ticketA" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     expire = 600 <span class="callout" id="co-ha-geo-booth-config-expiry">8</span>
     timeout = 10 <span class="callout" id="co-ha-geo-booth-config-timeout">9</span>
     retries = 5 <span class="callout" id="co-ha-geo-booth-config-retries">10</span>
     renewal-freq = 30 <span class="callout" id="co-ha-geo-booth-config-renewal">11</span>
     before-acquire-handler<span class="callout" id="co-ha-geo-booth-config-handler">12</span> = /etc/booth/ticket-A<span class="callout" id="co-ha-geo-booth-config-script">13</span> db-1 <span class="callout" id="co-ha-geo-booth-config-rsc">14</span>
     acquire-after = 60 <span class="callout" id="co-ha-geo-booth-config-acquire-after">15</span>
ticket = "ticketB" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     expire = 600 <a class="xref" href="#co-ha-geo-booth-config-expiry"><span class="callout">8</span></a>
     timeout = 10 <a class="xref" href="#co-ha-geo-booth-config-timeout"><span class="callout">9</span></a>
     retries = 5 <a class="xref" href="#co-ha-geo-booth-config-retries"><span class="callout">10</span></a>
     renewal-freq = 30 <a class="xref" href="#co-ha-geo-booth-config-renewal"><span class="callout">11</span></a>
     before-acquire-handler<a class="xref" href="#co-ha-geo-booth-config-handler"><span class="callout">12</span></a> = /etc/booth/ticket-B<a class="xref" href="#co-ha-geo-booth-config-script"><span class="callout">13</span></a> db-8 <a class="xref" href="#co-ha-geo-booth-config-rsc"><span class="callout">14</span></a>
     acquire-after = 60 <a class="xref" href="#co-ha-geo-booth-config-acquire-after"><span class="callout">15</span></a></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-transport"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The transport protocol used for communication between the sites.
   Only UDP is supported, but other transport layers will follow in
   the future. Currently, this parameter can therefore be omitted.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The port to be used for communication between the booth instances at each
 site. When not using the default port (<code class="literal">9929</code>),
      choose a port that is not already used for different services. Make sure
      to open the port in the nodes' and arbitrators' firewalls. The
      booth clients use TCP to communicate with the <code class="systemitem">boothd</code>. Booth will always
      bind and listen to both UDP and TCP ports.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-arbitrator"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The IP address of the machine to use as arbitrator.  Add an entry for each arbitrator you use in your Geo
      cluster setup.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The IP address used for the <code class="systemitem">boothd</code> on a site.  Add an entry for each site you use in your Geo cluster
      setup. Make sure to insert the correct virtual IP addresses
      (<code class="systemitem">IPaddr2</code>) for each site, otherwise the booth
      mechanism will not work correctly. Booth works with both IPv4 and IPv6
      addresses.
     </p><p>
      If you have set up booth with the
      <code class="systemitem">ha-cluster-bootstrap</code>
      scripts, the virtual IPs you have specified during setup have been
      written to the booth configuration already (and have been added to the
      cluster configuration, too). To set up the cluster resources manually,
      see <a class="xref" href="#sec-ha-geo-rsc-boothd" title="6.2. Configuring a Resource Group for boothd">Section 6.2, “Configuring a Resource Group for <code class="systemitem">boothd</code>”</a>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-auth"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. Enables booth authentication for clients and servers on the basis
  of a shared key. This parameter specifies the path to the
  key file.
     </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Key Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.5.6.5.4.3.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>The key can be either binary or text.</p><p>If it is text, the following characters are ignored: leading and trailing white space,
        new lines.</p></li><li class="listitem"><p>The key must be between 8 and 64 characters long.</p></li><li class="listitem"><p>The key must belong to the user <code class="systemitem">hacluster</code> and the group <code class="systemitem">haclient</code>.
        </p></li><li class="listitem"><p>The key must be readable only by the file owner.</p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The tickets to be managed by booth or a cluster administrator. For each ticket, add a <code class="literal">ticket</code> entry.
      For example, the ticket <code class="literal">ticket-nfs</code> specified here can
      be used for failover of NFS and DRBD as explained in
      <a class="link" href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html" target="_blank">https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html</a>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-mode"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Optional parameter. Defines the ticket mode. By default, all
      tickets are managed by booth. To define tickets which are managed by
      the administrator (<span class="emphasis"><em>manual tickets</em></span>), set the
      <em class="parameter">mode</em> parameter to
      <code class="literal">MANUAL</code> or <code class="literal">manual</code>.
     </p><p>Manual tickets do not have <em class="parameter">expire</em>,
      <em class="parameter">renewal-freq</em>, and <em class="parameter">retries</em>
      parameters.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-expiry"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. Defines the ticket's expiry time in seconds. A
      site that has been granted a ticket will renew the ticket regularly. If
      booth does not receive any information about renewal of the ticket within
      the defined expiry time, the ticket will be revoked and granted to
      another site. If no expiry time is specified, the ticket will expire
      after <code class="literal">600</code> seconds by default. The parameter should not
      be set to a value less than 120 seconds. The default value set by the
      <code class="systemitem">ha-cluster-init</code> scripts is
      <code class="literal">600</code>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-timeout"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. Defines a timeout period in seconds. After that time,
      booth will resend packets if it did not receive a reply within this
      period. The timeout defined should be long enough to allow packets to
      reach other booth members (all arbitrators and sites).
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-retries"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. Defines how many times booth retries sending packets
      before giving up waiting for confirmation by other sites. Values smaller
      than <code class="literal">3</code> are invalid and will prevent booth from
      starting.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-renewal"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. Sets the ticket renewal frequency period. Ticket
      renewal occurs every half expiry time by default. If the network
      reliability is often reduced over prolonged periods, it is advisable to
      renew more often. Before every renewal the
      <code class="literal">before-acquire-handler</code> is run.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-handler"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. It supports one or more scripts. To use more than one
      script, each script can be responsible for different checks, like cluster
      state, data center connectivity, environment health sensors, and more.
      Store all scripts in the directory
      <code class="filename">/etc/booth.d/<em class="replaceable">TICKET_NAME</em></code>
      and make sure they have the correct ownership (user
      <code class="systemitem">hacluster</code> and group
      <code class="systemitem">haclient</code>). Assign the
      directory name as a value to the parameter
      <em class="parameter">before-acquire-handler</em>.
     </p><p>
      The scripts in this directory are executed in alphabetical order. All
      scripts will be called before <code class="systemitem">boothd</code> tries to acquire or renew a
      ticket. For the ticket to be granted or renewed, <span class="emphasis"><em>all</em></span>
      scripts must succeed. The semantics are the same as for a single script:
      On exit code other than <code class="literal">0</code>, <code class="systemitem">boothd</code> relinquishes the
      ticket.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-script"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The <code class="filename">/usr/share/booth/service-runnable</code> script is
      included in the product as an example. To use it, link it into the
      respective <span class="quote">“<span class="quote">ticket</span>”</span> directory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ln</code> -s /usr/share/booth/service-runnable /etc/booth.d/<em class="replaceable">TICKET_NAME</em></pre></div><p>
      Assume that the
      <code class="filename">/etc/booth.d<em class="replaceable">TICKET_NAME</em></code>
      directory contains the <code class="command">service-runnable</code> script. This
      simple script is based on <code class="command">crm_simulate</code>. It can be used
      to test whether a particular cluster resource <span class="emphasis"><em>can</em></span> be
      run on the current cluster site. That means, it checks if the cluster is
      healthy enough to run the resource (all resource dependencies are
      fulfilled, the cluster partition has quorum, no dirty nodes, etc.). For
      example, if a service in the dependency-chain has a fail count of
      <code class="literal">INFINITY</code> on all available nodes, the service cannot be
      run on that site. In that case, it is of no use to claim the ticket.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-rsc"><span class="callout">14</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The resource to be tested by the
      <code class="literal">before-acquire-handler</code> (in this case, by the
      <code class="filename">service-runnable</code> script). You need to reference the
      resource that is protected by the respective ticket. In this example,
      resource <code class="literal">db-1</code> is protected by
      <code class="literal">ticketA</code> whereas <code class="literal">db-8</code> is protected
      by <code class="literal">ticketB</code>. The resource for DRBD
      (<code class="literal">ms_drbd_nfs</code>) is protected by the ticket
      <code class="literal">ticket-nfs</code>.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-acquire-after"><span class="callout">15</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Optional parameter. After a ticket is lost, booth will wait this time in
      addition before acquiring the ticket. This is to allow for the site that
      lost the ticket to relinquish the resources, by either stopping them or
      fencing a node. A typical delay might be <code class="literal">60</code> seconds,
      but ultimately it depends on the protected resources and the fencing
      configuration. The default value is <code class="literal">0</code>.
     </p><p>
      If you are unsure how long stopping or demoting the resources or fencing
      a node may take (depending on the <code class="literal">loss-policy</code>), use
      this parameter to prevent resources from running on two sites at the same
      time.
     </p></td></tr></table></div></div></div><section class="sect2" id="pro-ha-geo-setup-booth-config-edit" data-id-title="Manually Editing The Booth Configuration File"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.1 </span><span class="title-name">Manually Editing The Booth Configuration File</span></span> <a title="Permalink" class="permalink" href="#pro-ha-geo-setup-booth-config-edit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to a cluster node as <code class="systemitem">root</code> or equivalent.
     </p></li><li class="step"><p>
      If <code class="filename">/etc/booth/booth.conf</code> does not exist yet, copy the example booth configuration
      file <code class="filename">/etc/booth/booth.conf.example</code> to <code class="filename">/etc/booth/booth.conf</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">cp</code> -p /etc/booth/booth.conf.example /etc/booth/booth.conf</pre></div></li><li class="step"><p>
      Edit <code class="filename">/etc/booth/booth.conf</code> according to
      <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>.
     </p></li><li class="step"><p>
      Verify your changes and save the file.
     </p></li><li class="step"><p>
      On all cluster nodes and arbitrators, open the port in the firewall that
      you have configured for booth. See
      <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>, position
      <a class="xref" href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a>.
     </p></li></ol></div></div></section><section class="sect2" id="pro-ha-geo-setup-booth-yast" data-id-title="Setting Up Booth with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3.2 </span><span class="title-name">Setting Up Booth with YaST</span></span> <a title="Permalink" class="permalink" href="#pro-ha-geo-setup-booth-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to a cluster node as <code class="systemitem">root</code> or equivalent.
     </p></li><li class="step"><p>
      Start the YaST <span class="guimenu">Geo Cluster</span> module.
     </p></li><li class="step"><p>
      Choose to <span class="guimenu">Edit</span> an existing booth configuration file or
      click <span class="guimenu">Add</span> to create a new booth configuration file:
     </p><ol type="a" class="substeps"><li class="step" id="step-ha-booth-conf-params"><p>
        In the screen that appears configure the following parameters:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Configuration File. </span>
           A name for the booth configuration file. YaST suggests
           <code class="literal">booth</code> by default. This results in the booth
           configuration being written to <code class="filename">/etc/booth/booth.conf</code>. Only change this value
           if you need to set up multiple booth instances for different Geo
           clusters as described in <a class="xref" href="#sec-ha-geo-booth-multi" title="4.4. Using a Multi-Tenant Booth Setup">Section 4.4, “Using a Multi-Tenant Booth Setup”</a>.
          </p></li><li class="listitem"><p><span class="formalpara-title">Transport. </span>
           The transport protocol used for communication between the sites.
   Only UDP is supported, but other transport layers will follow in
   the future. See also
           <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>, position
           <a class="xref" href="#co-ha-geo-booth-config-transport"><span class="callout">1</span></a>.
          </p></li><li class="listitem"><p><span class="formalpara-title">Port. </span>
           The port to be used for communication between the booth instances at each
 site. See also <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>,
           position <a class="xref" href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a>.
          </p></li><li class="listitem"><p><span class="formalpara-title">Arbitrator. </span>
            The IP address of the machine to use as arbitrator.  See also
           <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>, position
           <a class="xref" href="#co-ha-geo-booth-config-arbitrator"><span class="callout">3</span></a>.
          </p><p>
          To specify an <span class="guimenu">Arbitrator</span>, click
          <span class="guimenu">Add</span>. In the dialog that opens, enter the IP
          address of your arbitrator and click <span class="guimenu">OK</span>.
         </p></li><li class="listitem"><p><span class="formalpara-title">Site. </span>
           The IP address used for the <code class="systemitem">boothd</code> on a site.  See also <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>,
           position <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>.
          </p><p>
          To specify a <span class="guimenu">Site</span> of your Geo cluster, click
          <span class="guimenu">Add</span>. In the dialog that opens, enter the IP
          address of one site and click <span class="guimenu">OK</span>.
         </p></li><li class="listitem"><p><span class="formalpara-title">Ticket. </span>
           The tickets to be managed by booth or a cluster administrator. See also <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>,
           position <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>.
          </p><p>
          To specify a <span class="guimenu">Ticket</span>, click <span class="guimenu">Add</span>.
          In the dialog that opens, enter a unique <span class="guimenu">Ticket</span>
          name. If you need to define multiple tickets with the same parameters
          and values, save configuration effort by creating a <span class="quote">“<span class="quote">ticket
          template</span>”</span> that specifies the default parameters and values for
          all tickets. To do so, use <code class="literal">__default__</code> as
          <span class="guimenu">Ticket</span> name.
         </p></li><li class="listitem"><p><span class="formalpara-title">Authentication. </span>
           To enable authentication for booth, click
           <span class="guimenu">Authentication</span> and in the dialog that opens,
           activate <span class="guimenu">Enable Security Auth</span>. If you already
           have an existing key, specify the path and file name in
           <span class="guimenu">Authentication file</span>. To generate a key file for a
           new Geo cluster, click <span class="guimenu">Generate Authentication Key
           File</span>. The key will be created and written to the location
           specified in <span class="guimenu">Authentication file</span>.
          </p><p>
          Additionally, you can specify optional parameters for your ticket.
          For an overview, see <a class="xref" href="#ex-ha-booth-conf-default" title="A Booth Configuration File">Example 4.1, “A Booth Configuration File”</a>,
          positions <a class="xref" href="#co-ha-geo-booth-mode"><span class="callout">7</span></a> to
          <a class="xref" href="#co-ha-geo-booth-config-acquire-after"><span class="callout">15</span></a>.
         </p><p>
          Click <span class="guimenu">OK</span> to confirm your changes.
         </p></li></ul></div><div class="figure" id="fig-yast-ha-geo-booth"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_geo_cluster_booth.png"><img src="images/yast_geo_cluster_booth.png" width="50%" alt="Example Ticket Dependency" title="Example Ticket Dependency"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.1: </span><span class="title-name">Example Ticket Dependency </span></span><a title="Permalink" class="permalink" href="#fig-yast-ha-geo-booth">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
        Click <span class="guimenu">OK</span> to close the current booth configuration
        screen. YaST shows the name of the booth configuration file that you
        have defined.
       </p></li></ol></li><li class="step"><p>
      Before closing the YaST module, switch to the <span class="guimenu">Firewall
      Configuration</span> category.
     </p></li><li class="step"><p>
      To open the port you have configured for booth, enable <span class="guimenu">Open Port
      in Firewall</span>.
     </p><div id="id-1.5.6.5.6.2.5.2" data-id-title="Firewall Setting for Local Machine Only" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Firewall Setting for Local Machine Only</div><p>
       The firewall setting is only applied to the current machine. It will
       open the UDP/TCP ports for all ports that have been specified in
       <code class="filename">/etc/booth/booth.conf</code> or any other booth configuration files (see
       <a class="xref" href="#sec-ha-geo-booth-multi" title="4.4. Using a Multi-Tenant Booth Setup">Section 4.4, “Using a Multi-Tenant Booth Setup”</a>).
      </p><p>
       Make sure to open the respective ports on all other cluster nodes and
       arbitrators of your Geo cluster setup, too. Do so either manually or
       by synchronizing the following files with Csync2:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="filename">/usr/lib/firewalld</code>
        </p></li><li class="listitem"><p>
         <code class="filename">/usr/lib/firewalld/services/booth.xml</code>
        </p></li></ul></div></div></li><li class="step"><p>
      Click <span class="guimenu">Finish</span> to confirm all settings and close the
      YaST module. Depending on the <em class="replaceable">NAME</em> of the
      <span class="guimenu">Configuration File </span> specified in
      <a class="xref" href="#step-ha-booth-conf-params" title="Step 3.a">Step 3.a</a>, the configuration is written
      to <code class="filename">/etc/booth/<em class="replaceable">NAME</em>.conf</code>.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-geo-booth-multi" data-id-title="Using a Multi-Tenant Booth Setup"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Using a Multi-Tenant Booth Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-multi">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For setups including multiple Geo clusters, it is possible to <span class="quote">“<span class="quote">share</span>”</span> the same arbitrator (as
 of SUSE Linux Enterprise High Availability 12). By providing several booth configuration files, you can
 start multiple booth instances on the same arbitrator, with each booth
 instance running on a different port. That way, you can use <span class="emphasis"><em>one</em></span> machine to serve as
 arbitrator for <span class="emphasis"><em>different</em></span> Geo clusters.
  </p><p>
   Let us assume you have two Geo clusters, one in EMEA (Europe, the Middle
   East and Africa), and one in the Asia-Pacific region (APAC).
  </p><p>
   To use the same arbitrator for both Geo clusters, create two configuration
   files in the <code class="filename">/etc/booth</code> directory:
   <code class="filename">/etc/booth/emea.conf</code> and
   <code class="filename">/etc/booth/apac.conf</code>. Both must minimally differ in the
   following parameters:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The port used for the communication of the booth instances.
    </p></li><li class="listitem"><p>
     The sites belonging to the different Geo clusters that the arbitrator is
     used for.
    </p></li></ul></div><div class="example" id="ex-ha-conf-booth-multi-1" data-id-title="/etc/booth/apac.conf"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 4.2: </span><span class="title-name"><code class="filename">/etc/booth/apac.conf</code> </span></span><a title="Permalink" class="permalink" href="#ex-ha-conf-booth-multi-1">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">transport = UDP <a class="xref" href="#co-ha-geo-booth-config-transport"><span class="callout">1</span></a>
port = 9133 <a class="xref" href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a>
arbitrator = 192.168.203.100 <a class="xref" href="#co-ha-geo-booth-config-arbitrator"><span class="callout">3</span></a>
site = 192.168.2.254 <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>
site = 192.168.1.112 <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>
authfile = /etc/booth/authkey-apac <a class="xref" href="#co-ha-geo-booth-config-auth"><span class="callout">5</span></a>
ticket ="tkt-db-apac-intern" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler<a class="xref" href="#co-ha-geo-booth-config-handler"><span class="callout">12</span></a> = /usr/share/booth/service-runnable<a class="xref" href="#co-ha-geo-booth-config-script"><span class="callout">13</span></a> db-apac-intern <a class="xref" href="#co-ha-geo-booth-config-rsc"><span class="callout">14</span></a> 
ticket = "tkt-db-apac-cust" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler = /usr/share/booth/service-runnable db-apac-cust</pre></div></div></div><div class="example" id="ex-ha-conf-booth-multi-2" data-id-title="/etc/booth/emea.conf"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 4.3: </span><span class="title-name"><code class="filename">/etc/booth/emea.conf</code> </span></span><a title="Permalink" class="permalink" href="#ex-ha-conf-booth-multi-2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">transport = UDP <a class="xref" href="#co-ha-geo-booth-config-transport"><span class="callout">1</span></a>
port = 9150 <a class="xref" href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a>
arbitrator = 192.168.203.100 <a class="xref" href="#co-ha-geo-booth-config-arbitrator"><span class="callout">3</span></a>
site = 192.168.201.100 <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>
site = 192.168.202.100 <a class="xref" href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a>
authfile = /etc/booth/authkey-emea <a class="xref" href="#co-ha-geo-booth-config-auth"><span class="callout">5</span></a>
ticket = "tkt-sap-crm" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     expire = 900 
     renewal-freq = 60 
     before-acquire-handler<a class="xref" href="#co-ha-geo-booth-config-handler"><span class="callout">12</span></a> = /usr/share/booth/service-runnable<a class="xref" href="#co-ha-geo-booth-config-script"><span class="callout">13</span></a> sap-crm <a class="xref" href="#co-ha-geo-booth-config-rsc"><span class="callout">14</span></a>
ticket = "tkt-sap-prod" <a class="xref" href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a>
     expire = 600 
     renewal-freq = 60 
     before-acquire-handler = /usr/share/booth/service-runnable sap-prod</pre></div></div></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-transport"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The transport protocol used for communication between the sites.
   Only UDP is supported, but other transport layers will follow in
   the future. Currently, this parameter can therefore be omitted.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-port"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The port to be used for communication between the booth instances at each
 site. The configuration files use different ports to allow for
     start of multiple booth instances on the same arbitrator.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-arbitrator"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      The IP address of the machine to use as arbitrator.  In the examples above, we use the same arbitrator for
     different Geo clusters.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-site"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The IP address used for the <code class="systemitem">boothd</code> on a site.  The sites defined in both booth configuration files are
     different, because they belong to two different Geo clusters.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-auth"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Optional parameter. Enables booth authentication for clients and servers on the basis
  of a shared key. This parameter specifies the path to the
  key file. Use different key files for different
     tenants.
    </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Key Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.5.6.6.8.5.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>The key can be either binary or text.</p><p>If it is text, the following characters are ignored: leading and trailing white space,
        new lines.</p></li><li class="listitem"><p>The key must be between 8 and 64 characters long.</p></li><li class="listitem"><p>The key must belong to the user <code class="systemitem">hacluster</code> and the group <code class="systemitem">haclient</code>.
        </p></li><li class="listitem"><p>The key must be readable only by the file owner.</p></li></ul></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-ticket"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The tickets to be managed by booth or a cluster administrator. Theoretically the same ticket names can be defined in
     different booth configuration files—the tickets will not interfere
     because they are part of different Geo clusters that are managed by
     different booth instances. However, (for better overview) we advise to use
     distinct ticket names for each Geo cluster as shown in the examples
     above.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-handler"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Optional parameter. If set, the specified command will be called before
     <code class="systemitem">boothd</code> tries to acquire or renew a ticket. On exit code other than
     <code class="literal">0</code>, <code class="systemitem">boothd</code> relinquishes the ticket.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-script"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The <code class="filename">service-runnable</code> script referenced here is
     included in the product as an example. It is a simple script based on
     <code class="command">crm_simulate</code>. It can be used to test whether a
     particular cluster resource <span class="emphasis"><em>can</em></span> be run on the current
     cluster site. That means, it checks if the cluster is healthy enough to
     run the resource (all resource dependencies are fulfilled, the cluster
     partition has quorum, no dirty nodes, etc.). For example, if a service in
     the dependency chain has a fail count of <code class="literal">INFINITY</code> on all
     available nodes, the service cannot be run on that site. In that case, it
     is of no use to claim the ticket.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-geo-booth-config-rsc"><span class="callout">14</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     The resource to be tested by the <code class="literal">before-acquire-handler</code>
     (in this case, by the <code class="filename">service-runnable</code> script). You
     need to reference the resource that is protected by the respective ticket.
    </p></td></tr></table></div><div class="procedure" id="id-1.5.6.6.9" data-id-title="Using the Same Arbitrator for Different Geo Clusters"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.1: </span><span class="title-name">Using the Same Arbitrator for Different Geo Clusters </span></span><a title="Permalink" class="permalink" href="#id-1.5.6.6.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create different booth configuration files in
     <code class="filename">/etc/booth</code> as shown in
     <a class="xref" href="#ex-ha-conf-booth-multi-1" title="/etc/booth/apac.conf">Example 4.2, “<code class="filename">/etc/booth/apac.conf</code>”</a> and
     <a class="xref" href="#ex-ha-conf-booth-multi-2" title="/etc/booth/emea.conf">Example 4.3, “<code class="filename">/etc/booth/emea.conf</code>”</a>. Do so either manually or with
     YaST, as outlined in <a class="xref" href="#pro-ha-geo-setup-booth-yast" title="4.3.2. Setting Up Booth with YaST">Section 4.3.2, “Setting Up Booth with YaST”</a>.
    </p></li><li class="step"><p>
     On the arbitrator, open the ports that are defined in any of the booth
     configuration files in <code class="filename">/etc/booth</code>.
    </p></li><li class="step"><p>
     On the nodes belonging to the individual Geo clusters that the
     arbitrator is used for, open the port that is used for the respective
     booth instance.
    </p></li><li class="step"><p>
     Synchronize the respective booth configuration files across all cluster
     nodes and arbitrators that use the same booth configuration. For details,
     see <a class="xref" href="#sec-ha-geo-booth-sync" title="4.5. Synchronizing the Booth Configuration to All Sites and Arbitrators">Section 4.5, “Synchronizing the Booth Configuration to All Sites and Arbitrators”</a>.
    </p></li><li class="step"><p>
     On the arbitrator, start the individual booth instances as described in
     <a class="xref" href="#vle-ha-geo-setup-booth-service-arbitrator">Starting the Booth Services on Arbitrators</a> for
     multi-tenancy setups.
    </p></li><li class="step"><p>
     On the individual Geo clusters, start the booth service as described in
     <a class="xref" href="#vle-ha-geo-setup-booth-service-sites">Starting the Booth Services on Cluster Sites</a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-geo-booth-sync" data-id-title="Synchronizing the Booth Configuration to All Sites and Arbitrators"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">Synchronizing the Booth Configuration to All Sites and Arbitrators</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-sync">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.5.6.7.2" data-id-title="Use the Same Booth Configuration On All Sites and Arbitrators" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use the Same Booth Configuration On All Sites and Arbitrators</div><p>
    To make booth work correctly, all cluster nodes and arbitrators within one
    Geo cluster must use the same booth configuration.
   </p><p>
    You can use Csync2 to synchronize the booth configuration. For details,
    see <a class="xref" href="#sec-ha-geo-booth-sync-csync2-setup" title="5.1. Csync2 Setup for Geo Clusters">Section 5.1, “Csync2 Setup for Geo Clusters”</a> and
    <a class="xref" href="#sec-ha-geo-booth-sync-csync2-start" title="5.2. Synchronizing Changes with Csync2">Section 5.2, “Synchronizing Changes with Csync2”</a>.
   </p><p>
    In case of any booth configuration changes, make sure to update the
    configuration files accordingly on all parties and to restart the booth
    services as described in <a class="xref" href="#sec-ha-geo-setup-booth-reconfig" title="4.7. Reconfiguring Booth While Running">Section 4.7, “Reconfiguring Booth While Running”</a>.
   </p></div></section><section class="sect1" id="sec-ha-geo-setup-booth-service" data-id-title="Enabling and Starting the Booth Services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">Enabling and Starting the Booth Services</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-setup-booth-service">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-geo-setup-booth-service-sites"><span class="term">Starting the Booth Services on Cluster Sites</span></dt><dd><p>
      The booth service for each cluster site is managed by the booth resource
      group (that has either been configured automatically if you used the
      <code class="systemitem">ha-cluster-init</code> scripts for Geo cluster setup,
      or manually as described in <a class="xref" href="#sec-ha-geo-rsc-boothd" title="6.2. Configuring a Resource Group for boothd">Section 6.2, “Configuring a Resource Group for <code class="systemitem">boothd</code>”</a>). To
      start one instance of the booth service per site, start the respective
      booth resource group on each cluster site.
     </p></dd><dt id="vle-ha-geo-setup-booth-service-arbitrator"><span class="term">Starting the Booth Services on Arbitrators</span></dt><dd><p>
      Starting with SUSE Linux Enterprise 12, booth arbitrators are managed with systemd. The
      unit file is named <code class="filename">booth@.service</code>. The
      <code class="literal">@</code> denotes the possibility to run the service with a
      parameter, which is in this case the name of the configuration file.
     </p><p>
      To <span class="emphasis"><em>enable</em></span> the booth service on an arbitrator, use
      the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable booth@booth</pre></div><p>
      After the service has been enabled from command line, YaST
      Services Manager can be used to manage the service (as long as the service
      is not disabled). In that case, it will disappear from the service list
      in YaST the next time systemd is restarted.
     </p><p>
      The command to <span class="emphasis"><em>start</em></span> the booth service depends on
      your booth setup:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If you are using the default setup as described in
        <a class="xref" href="#sec-ha-geo-booth-default" title="4.3. Using the Default Booth Setup">Section 4.3</a>,
        only <code class="filename">/etc/booth/booth.conf</code> is configured. In that
        case, log in to each arbitrator and use the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start booth@booth</pre></div></li><li class="listitem"><p>
        If you are running booth in multi-tenancy mode as described in
        <a class="xref" href="#sec-ha-geo-booth-multi" title="4.4. Using a Multi-Tenant Booth Setup">Section 4.4</a>, you
        have configured multiple booth configuration files in
        <code class="filename">/etc/booth</code>. To start the services for the
        individual booth instances, use
        <code class="command">systemctl start booth@</code>
        <em class="replaceable">NAME</em>, where <em class="replaceable">NAME</em>
        stands for the name of the respective configuration file
        <code class="filename">/etc/booth/<em class="replaceable">NAME</em>.conf</code>.
       </p><p>
        For example, if you have the booth configuration files
        <code class="filename">/etc/booth/emea.conf</code> and
        <code class="filename">/etc/booth/apac.conf</code>, log in to your arbitrator
        and execute the following commands:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start booth@emea
<code class="prompt root"># </code><code class="command">systemctl</code> start booth@apac</pre></div></li></ul></div><p>
      This starts the booth service in arbitrator mode. It can communicate with
      all other booth daemons but in contrast to the booth daemons running on
      the cluster sites, it cannot be granted a ticket. Booth arbitrators take
      part in elections only. Otherwise, they are dormant.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-geo-setup-booth-reconfig" data-id-title="Reconfiguring Booth While Running"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.7 </span><span class="title-name">Reconfiguring Booth While Running</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-setup-booth-reconfig">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_booth_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case you need to change the booth configuration while the booth services
   are already running, proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Adjust the booth configuration files as desired.
    </p></li><li class="step"><p>
     Synchronize the updated booth configuration files to all cluster nodes and
     arbitrators that are part of your Geo cluster. For details, see
     <a class="xref" href="#cha-ha-geo-sync" title="Chapter 5. Synchronizing Configuration Files Across All Sites and Arbitrators">Chapter 5, <em>Synchronizing Configuration Files Across All Sites and Arbitrators</em></a>.
    </p></li><li class="step"><p>
     Restart the booth services on the arbitrators and cluster sites as
     described in <a class="xref" href="#sec-ha-geo-setup-booth-service" title="4.6. Enabling and Starting the Booth Services">Section 4.6, “Enabling and Starting the Booth Services”</a>. This does
     not have any effect on tickets that have already been granted to sites.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-geo-sync" data-id-title="Synchronizing Configuration Files Across All Sites and Arbitrators"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Synchronizing Configuration Files Across All Sites and Arbitrators</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-sync">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To replicate important configuration files across all nodes in the cluster
  and across Geo clusters, use Csync2.

  Csync2 can handle any number of hosts, sorted into synchronization groups.
  Each synchronization group has its own list of member hosts and its
  include/exclude patterns that define which ﬁles should be synchronized in
  the synchronization group. The groups, the host names belonging to each
  group, and the include/exclude rules for each group are specified in the
  Csync2 configuration file, <code class="filename">/etc/csync2/csync2.cfg</code>.
 </p><p>
  For authentication, Csync2 uses the IP addresses and pre-shared keys within
  a synchronization group. You need to generate one key file for each
  synchronization group and copy it to all group members.
 </p><p>
  Csync2 will contact other servers via a TCP port (by default
  <code class="literal">30865</code>), and start remote Csync2 instances. For detailed
  information about Csync2, refer to
  <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>
 </p><section class="sect1" id="sec-ha-geo-booth-sync-csync2-setup" data-id-title="Csync2 Setup for Geo Clusters"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Csync2 Setup for Geo Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-sync-csync2-setup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   How to set up Csync2 for individual clusters with YaST is explained in
   <span class="intraxref">Book “Administration Guide”, Chapter 4 “Using the YaST Cluster Module”, Section 4.7 “Transferring the configuration to all nodes”</span>. However,
   YaST cannot handle more complex Csync2 setups, like those that are needed
   for Geo clusters. For the following setup, as shown in
   <a class="xref" href="#fig-ha-geo-csync-config" title="Example Csync2 Setup for Geo Clusters">Figure 5.1, “Example Csync2 Setup for Geo Clusters”</a>, configure Csync2 manually by
   editing the configuration files.
  </p><p>
   To adjust Csync2 for synchronizing files not only within local clusters but
   also across geographically dispersed sites, you need to define two synchronization
   groups in the Csync2 configuration:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A global group <code class="literal">ha_global</code> (for the files that need to be
     synchronized globally, across all sites and arbitrators belonging to a
     Geo cluster).
    </p></li><li class="listitem"><p>
     A group for the local cluster site <code class="literal">ha_local</code> (for the
     files that need to be synchronized within the local cluster).
    </p></li></ul></div><p>
   For an overview of the multiple Csync2 configuration files for the two
   synchronization groups, see <a class="xref" href="#fig-ha-geo-csync-config" title="Example Csync2 Setup for Geo Clusters">Figure 5.1, “Example Csync2 Setup for Geo Clusters”</a>.
  </p><div class="figure" id="fig-ha-geo-csync-config"><div class="figure-contents"><div class="mediaobject"><a href="images/multi-site-csync2.png"><img src="images/multi-site-csync2.png" width="100%" alt="Example Csync2 Setup for Geo Clusters" title="Example Csync2 Setup for Geo Clusters"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.1: </span><span class="title-name">Example Csync2 Setup for Geo Clusters </span></span><a title="Permalink" class="permalink" href="#fig-ha-geo-csync-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div></div><p>
   Authentication key files and their references are displayed in red. The
   names of Csync2 configuration files are displayed in blue, and their
   references are displayed in green. For details, refer to
   <a class="xref" href="#vl-fig-ha-geo-csync-config-files" title="Example Csync2 Setup: Configuration Files">Example Csync2 Setup: Configuration Files</a>.
  </p><div class="variablelist" id="vl-fig-ha-geo-csync-config-files" data-id-title="Example Csync2 Setup: Configuration Files"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Example Csync2 Setup: Configuration Files </span></span><a title="Permalink" class="permalink" href="#vl-fig-ha-geo-csync-config-files">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="vle-ha-geo-csync-csync2-cfg"><span class="term"><code class="filename">/etc/csync2/csync2.cfg</code>
    </span></dt><dd><p>
      The main Csync2 configuration file. It is kept short and simple on
      purpose and only contains the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        The definition of the synchronization group
        <code class="literal">ha_local</code>. The group consists of two nodes
        (<code class="literal">this-site-host-1</code> and
        <code class="literal">this-site-host-2</code>) and uses
        <code class="filename">/etc/csync2/ha_local.key</code> for authentication. A
        list of files to be synchronized for this group only is defined in
        another Csync2 configuration file,
        <code class="filename">/etc/csync2/ha_local.cfg</code>. It is included with the
        <code class="literal">config</code> statement.
       </p></li><li class="listitem"><p>
        A reference to another Csync2 configuration file,
        <code class="filename">/etc/csync2.cfg/ha_global.cfg</code>, included with the
        <code class="literal">config</code> statement.
       </p></li></ul></div></dd><dt id="vle-ha-geo-csync-ha-local-cfg"><span class="term"><code class="filename">/etc/csync2/ha_local.cfg</code>
    </span></dt><dd><p>
      This file concerns only the local cluster. It specifies a list of files
      to be synchronized only within the <code class="literal">ha_local</code>
      synchronization group, as these files are specific per cluster. The most
      important ones are the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="filename">/etc/csync2/csync2.cfg</code>, as this file contains the
        list of the local cluster nodes.
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/csync2/ha_local.key</code>, the authentication key
        to be used for Csync2 synchronization within the local cluster.
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/corosync/corosync.conf</code>, as this file defines the communication channels
        between the local cluster nodes.
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/corosync/authkey</code>, the Corosync
        authentication key.
       </p></li></ul></div><p>
      The rest of the file list depends on your specific cluster setup. The
      files listed in <a class="xref" href="#fig-ha-geo-csync-config" title="Example Csync2 Setup for Geo Clusters">Figure 5.1, “Example Csync2 Setup for Geo Clusters”</a> are only
      examples. If you also want to synchronize files for any site-specific
      applications, include them in <code class="filename">ha_local.cfg</code>, too.
      Even though <code class="filename">ha_local.cfg</code> is targeted at the nodes
      belonging to one site of your Geo cluster, the content may be identical
      on all sites. If you need different sets of hosts or different keys,
      adding extra groups may be necessary.
     </p></dd><dt id="vle-ha-geo-csync-ha-global-cfg"><span class="term"><code class="filename">/etc/csync2.cfg/ha_global.cfg</code>
    </span></dt><dd><p>
      This file defines the Csync2 synchronization group
      <code class="literal">ha_global</code>. The group spans <span class="emphasis"><em>all</em></span>
      cluster nodes across multiple sites, including the arbitrator. As it is
      recommended to use a separate key for each Csync2 synchronization group,
      this group uses <code class="filename">/etc/csync2/ha_global.key</code> for
      authentication. The <code class="literal">include</code> statements define the list
      of files to be synchronized within the <code class="literal">ha_global</code>
      synchronization group. The most important ones are the following:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="filename">/etc/csync2/ha_global.cfg</code> and
        <code class="filename">/etc/csync2/ha_global.key</code> (the configuration file
        for the <code class="literal">ha_global</code>synchronization group and the
        authentication key used for synchronization within the group).
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/booth/</code>, the default directory holding the
        booth configuration. In case you are using a booth setup for multiple
        tenants, it contains more than one booth configuration file. If you use
        authentication for booth, it is useful to place the key file in this
        directory, too.
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/drbd.conf</code> and
        <code class="filename">/etc/drbd.d</code> (if you are using DRBD within your
        cluster setup). The DRBD configuration can be globally synchronized, as
        it derives the configuration from the host names contained in the
        resource configuration file.
       </p></li><li class="listitem"><p>
        <code class="filename">/etc/zypp/repos.de</code>. The package repositories are
        likely to be the same on all cluster nodes.
       </p></li></ul></div><p>
      The other files shown
      (<code class="filename">/etc/root/<em class="replaceable">*</em></code>) are
      examples that may be included for reasons of convenience (to make a
      cluster administrator's life easier).
     </p></dd></dl></div><div id="id-1.5.7.5.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The files <code class="filename">csync2.cfg</code> and
    <code class="filename">ha_local.key</code> are site-specific, which means you need
    to create different ones for each cluster site. The files are identical on
    the nodes belonging to the same cluster but different on another cluster.
    Each <code class="filename">csync2.cfg</code> file needs to contain a lists of hosts
    (cluster nodes) belonging to the site, plus a site-specific authentication
    key.
   </p><p>
    The arbitrator needs a <code class="filename">csync2.cfg</code> file, too. It only
    needs to reference <code class="filename">ha_global.cfg</code> though.
   </p></div></section><section class="sect1" id="sec-ha-geo-booth-sync-csync2-start" data-id-title="Synchronizing Changes with Csync2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Synchronizing Changes with Csync2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-booth-sync-csync2-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To successfully synchronize the files with Csync2, the following
   prerequisites must be met:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The same Csync2 configuration is available on all machines that belong to
     the same synchronization group.
    </p></li><li class="listitem"><p>
     The Csync2 authentication key for each synchronization group must be
     available on all members of that group.
    </p></li><li class="listitem"><p>
     Csync2 must be running on <span class="emphasis"><em>all</em></span> nodes and the
     arbitrator.
    </p></li></ul></div><p>
   Before the first Csync2 run, you therefore need to make the following
   preparations:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to one machine per synchronization group and generate an
     authentication key for the respective group:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -k <em class="replaceable">NAME_OF_KEYFILE</em></pre></div><p>
     However, do <span class="emphasis"><em>not</em></span> regenerate the key file on any other
     member of the same group.
    </p><p>
     With regard to <a class="xref" href="#fig-ha-geo-csync-config" title="Example Csync2 Setup for Geo Clusters">Figure 5.1, “Example Csync2 Setup for Geo Clusters”</a>, this would
     result in the following key files:
     <code class="filename">/etc/csync2/ha_global.key</code> and one local key
     (<code class="filename">/etc/csync2/ha_local.key</code>) per site.
    </p></li><li class="step"><p>
     Copy each key file to <span class="emphasis"><em>all</em></span> members of the respective
     synchronization group. With regard to
     <a class="xref" href="#fig-ha-geo-csync-config" title="Example Csync2 Setup for Geo Clusters">Figure 5.1, “Example Csync2 Setup for Geo Clusters”</a>:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Copy <code class="filename">/etc/csync2/ha_global.key</code> to
       <span class="emphasis"><em>all</em></span> parties (the arbitrator and all cluster nodes
       on all sites of your Geo cluster). The key file needs to be available
       on all hosts listed within the <code class="literal">ha_global</code> group that
       is defined in <code class="filename">ha_global.cfg</code>.
      </p></li><li class="step"><p>
       Copy the local key file for each site
       (<code class="filename">/etc/csync2/ha_local.key</code>) to all cluster nodes
       belonging to the respective site of your Geo cluster.
      </p></li></ol></li><li class="step"><p>
     Copy the site-specific <code class="filename">/etc/csync2/csync2.cfg</code>
     configuration file to all cluster nodes belonging to the respective site
     of your Geo cluster and to the arbitrator.
    </p></li><li class="step"><p>
     Execute the following command on all nodes and the arbitrator to make the
     csync2 service start automatically at boot time:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable csync2.socket</pre></div></li><li class="step"><p>
     Execute the following command on all nodes and the arbitrator to start the
     service now:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start csync2.socket</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-geo-setup-csync2-start" data-id-title="Synchronizing Files with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Synchronizing Files with Csync2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-geo-setup-csync2-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_sync_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     To initially synchronize all files once, execute the following command on
     the machine that you want to copy the configuration
     <span class="emphasis"><em>from</em></span>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p>
     This will synchronize all the files once by pushing them to the other
     members of the synchronization groups. If all files are synchronized
     successfully, Csync2 will finish with no errors.
    </p><p>
     If one or several files that are to be synchronized have been modified on
     other machines (not only on the current one), Csync2 will report a
     conflict. You will get an output similar to the one below:
    </p><div class="verbatim-wrap"><pre class="screen">While syncing file /etc/corosync/corosync.conf:
ERROR from peer site-2-host-1: File is also marked dirty here!
Finished with 1 errors.</pre></div></li><li class="step"><p>
     If you are sure that the file version on the current machine is the
     <span class="quote">“<span class="quote">best</span>”</span> one, you can resolve the conflict by forcing this file
     and re-synchronizing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-f</code> /etc/corosync/corosync.conf
<code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-x</code></pre></div></li></ol></div></div><p>
   For more information on the Csync2 options, run
   <code class="command">csync2 </code> <code class="option">-help</code>.
  </p><div id="id-1.5.7.6.8" data-id-title="Pushing Synchronization After Any Changes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Pushing Synchronization After Any Changes</div><p>
    Csync2 only pushes changes. It does <span class="emphasis"><em>not</em></span> continuously
    synchronize files between the machines.
   </p><p>
    Each time you update files that need to be synchronized, you need to push
    the changes to the other machines of the same synchronization group: Run
    <code class="command">csync2 </code> <code class="option">-xv</code> on the machine where
    you did the changes. If you run the command on any of the other machines
    with unchanged files, nothing will happen.
   </p></div></section></section><section class="chapter" id="cha-ha-geo-rsc" data-id-title="Configuring Cluster Resources and Constraints"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources and Constraints</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-rsc">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Apart from the resources and constraints that you need to define for your
  specific cluster setup, Geo clusters require additional resources and
  constraints as described below. You can either configure them with the
  crm shell (crmsh) as demonstrated in the examples below, or with
  Hawk2.
 </p><p>
  This chapter focuses on tasks specific to Geo clusters. For an introduction
  to your preferred cluster management tool and general instructions on how to
  configure resources and constraints with it, refer to one of the following
  chapters:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="intraxref">Book “Administration Guide”, Chapter 5 “Configuration and Administration Basics”, Section 5.4 “Introduction to Hawk2”</span>
   </p></li><li class="listitem"><p>
    <span class="intraxref">Book “Administration Guide”, Chapter 5 “Configuration and Administration Basics”, Section 5.5 “Introduction to crmsh”</span>
   </p></li></ul></div><p>
  If you have set up your Geo cluster with the bootstrap scripts, the cluster
  resources needed for booth have been configured already (including a resource
  group for boothd). In this case, you can skip
  <a class="xref" href="#sec-ha-geo-rsc-boothd" title="6.2. Configuring a Resource Group for boothd">Section 6.2</a> and only
  need to execute the remaining steps below to complete the cluster resource
  configuration.
 </p><p>
  If you are setting up your Geo cluster manually, you need to execute all of
  the following steps:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>

    <a class="xref" href="#sec-ha-geo-rsc-ticket-dep" title="6.1. Configuring Ticket Dependencies of Resources">Section 6.1, “Configuring Ticket Dependencies of Resources”</a>
   </p></li><li class="listitem"><p>

    <a class="xref" href="#sec-ha-geo-rsc-boothd" title="6.2. Configuring a Resource Group for boothd">Section 6.2, “Configuring a Resource Group for <code class="systemitem">boothd</code>”</a>
   </p></li><li class="listitem"><p>

    <a class="xref" href="#sec-ha-geo-rsc-order" title="6.3. Adding an Order Constraint">Section 6.3, “Adding an Order Constraint”</a>
   </p></li><li class="listitem"><p>

    <a class="xref" href="#sec-ha-geo-rsc-sync-cib" title="6.4. Transferring the Resource Configuration to Other Cluster Sites">Section 6.4, “Transferring the Resource Configuration to Other Cluster Sites”</a>
   </p></li></ul></div><div id="id-1.5.8.8" data-id-title="No CIB Synchronization Across Sites" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No CIB Synchronization Across Sites</div><p>
   The CIB is <span class="emphasis"><em>not</em></span> automatically synchronized across
   cluster sites of a Geo cluster. All resources that must be highly
   available across the Geo cluster need to be configured for each site
   accordingly or need to be transferred to the other site or sites.
  </p><p>
   To simplify transfer, any resources with site-specific parameters can be
   configured in such a way that the parameters' values depend on the name of
   the cluster site where the resource is running (see also
   <a class="xref" href="#cha-ha-geo-req" title="Chapter 3. Requirements">Chapter 3, <em>Requirements</em></a>, <em class="citetitle">Other Requirements and
   Recommendations</em>).
  </p><p>
   After you have configured the resources on one site, you can tag the
   resources that are needed on all cluster sites, export them from the current
   CIB, and import them into the CIB of another cluster site. For details, see
   <a class="xref" href="#sec-ha-geo-rsc-sync-cib" title="6.4. Transferring the Resource Configuration to Other Cluster Sites">Section 6.4, “Transferring the Resource Configuration to Other Cluster Sites”</a>.
  </p></div><section class="sect1" id="sec-ha-geo-rsc-ticket-dep" data-id-title="Configuring Ticket Dependencies of Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Configuring Ticket Dependencies of Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-rsc-ticket-dep">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     For Geo clusters, you can specify which resources depend on a
     certain ticket. Together with this special type of constraint, you can
     set a <code class="literal">loss-policy</code> that defines what should happen to
     the respective resources if the ticket is revoked. The attribute
     <code class="literal">loss-policy</code> can have the following values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">fence</code>: Fence the nodes that are running the
       relevant resources.
      </p></li><li class="listitem"><p>
       <code class="literal">stop</code>: Stop the relevant resources.
      </p></li><li class="listitem"><p>
       <code class="literal">freeze</code>: Do nothing to the relevant resources.
      </p></li><li class="listitem"><p>
       <code class="literal">demote</code>: Demote relevant resources that are running
       in active mode to passive mode.
      </p></li></ul></div><div class="procedure" id="pro-ha-geo-setup-rsc-constraints" data-id-title="Configuring Ticket Dependencies of Resources with crmsh"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.1: </span><span class="title-name">Configuring Ticket Dependencies of Resources with crmsh </span></span><a title="Permalink" class="permalink" href="#pro-ha-geo-setup-rsc-constraints">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On one of the nodes of cluster amsterdam, start a shell and log in as
     <code class="systemitem">root</code> or equivalent.

    </p></li><li class="step"><p>
     Enter <code class="command">crm configure</code> to switch to the interactive
     crm shell.
    </p></li><li class="step" id="step-ha-geo-setup-rsc-constraints"><p>
     Configure constraints that define which resources depend on a certain
     ticket. For example, to make a primitive resource <code class="literal">rsc1</code>
     depend on <code class="literal">ticketA</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">rsc_ticket</code> rsc1-req-ticketA ticketA: \
  rsc1 loss-policy="fence"</pre></div><p>
     In case <code class="literal">ticketA</code> is revoked, the node running the
     resource should be fenced.
    </p></li><li class="step"><p>
     If you want other resources to depend on further tickets, create as many
     constraints as necessary with <code class="command">rsc_ticket</code>.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p><p>
     The configuration is saved to the CIB.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-geo-rsc-boothd" data-id-title="Configuring a Resource Group for boothd"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Configuring a Resource Group for <code class="systemitem">boothd</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-rsc-boothd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have set up your Geo cluster with the
   <code class="systemitem">ha-cluster-init</code> bootstrap scripts, you can skip the
   following procedure as the resources and the resource group for boothd have
   already been configured in this case.
  </p><p>
      Each site needs to run one instance of
      <code class="systemitem">boothd</code> that communicates
      with the other booth daemons. The daemon can be started on any node,
      therefore it should be configured as primitive resource. To make the
      <code class="systemitem">boothd</code> resource stay on the same node, if
      possible, add resource stickiness to the configuration. As each daemon
      needs a persistent IP address, configure another primitive with a
      virtual IP address. Group both primitives:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On one of the nodes of cluster <code class="literal">amsterdam</code>, start a
     shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Enter <code class="command">crm configure</code> to switch to the interactive
     crm shell.
    </p></li><li class="step"><p>
     Enter the following to create both primitive resources and to add them to
     one group, <code class="literal">g-booth</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ip-booth ocf:heartbeat:IPaddr2 \
  params iflabel="ha" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq amsterdam ip="192.168.201.100" \
  params rule #cluster-name eq berlin ip="192.168.202.100"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> booth-site ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  params config="nfs" op monitor interval="10s"
<code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-booth ip-booth booth-site</pre></div><p>
     With this configuration, each booth daemon will be available at its
     individual IP address, independent of the node the daemon is running on.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p><p>
     The configuration is saved to the CIB.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-geo-rsc-order" data-id-title="Adding an Order Constraint"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Adding an Order Constraint</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-rsc-order">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      If a ticket has been granted to a site but all nodes of that site
      should fail to host the <code class="systemitem">boothd</code>
      resource group for any reason, a <span class="quote">“<span class="quote">split-brain</span>”</span> situation
      among the geographically dispersed sites may occur. In that case, no
      <code class="systemitem">boothd</code> instance would be available to safely manage failover of the
      ticket to another site. To avoid a potential concurrency violation of the
      ticket (the ticket is granted to multiple sites simultaneously), add an
      order constraint:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On one of the nodes of cluster amsterdam, start a shell and log in as
     <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Enter <code class="command">crm configure</code> to switch to the interactive
     crm shell.
    </p></li><li class="step"><p>
     Create an order constraint, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-booth-before-rsc1 Mandatory: g-booth rsc1</pre></div><p>
     It defines that <code class="literal">rsc1</code> (which depends on
     <code class="literal">ticketA</code>) can only be started after the
     <code class="literal">g-booth</code> resource group.
    </p></li><li class="step"><p>
     For any other resources that depend on a certain ticket, define further
     order constraints.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p><p>
     The configuration is saved to the CIB.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-geo-rsc-sync-cib" data-id-title="Transferring the Resource Configuration to Other Cluster Sites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Transferring the Resource Configuration to Other Cluster Sites</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-rsc-sync-cib">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After having completed or changed your resource configuration for one
   cluster site, transfer it to the other sites of your Geo cluster.
  </p><p>
   To simplify the transfer, you can tag any resources that are needed on all
   cluster sites, export them from the current CIB, and import them into the
   CIB of another cluster site. Tagging does not create any colocation or
   ordering relationship between the resources.
  </p><p>
   <a class="xref" href="#pro-ha-geo-rsc-sync-cib-export" title="Tagging and Exporting a Resource Configuration">Procedure 6.2, “Tagging and Exporting a Resource Configuration”</a> and
   <a class="xref" href="#pro-ha-geo-rsc-sync-cib-import" title="Importing a Tagged Resource Configuration">Procedure 6.3, “Importing a Tagged Resource Configuration”</a> give an example of how
   to do so. They are based on the following prerequisites:
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.5.8.12.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     You have a Geo cluster with two sites: cluster
     <code class="literal">amsterdam</code> and cluster <code class="literal">berlin</code>.
    </p></li><li class="listitem"><p>
     The cluster names for each site are defined in the respective
     <code class="filename">/etc/corosync/corosync.conf</code> files:
    </p><div class="verbatim-wrap"><pre class="screen">totem {
     [...]
     cluster_name: amsterdam
     }</pre></div><p>
     This can either be done manually (by editing <code class="filename">/etc/corosync/corosync.conf</code>) or with the
     YaST cluster module (by switching to the <span class="guimenu">Communication
     Channels</span> category and defining a <span class="guimenu">Cluster
     Name</span>). Afterward, stop and start the cluster services
     for the changes to take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster restart</pre></div></li><li class="listitem"><p>
     The necessary resources for booth and for all services that should be
     highly available across your Geo cluster have been configured in the CIB
     on site <code class="literal">amsterdam</code>. They will be imported to the CIB on
     site <code class="literal">berlin</code>.
    </p></li></ul></div><div class="procedure" id="pro-ha-geo-rsc-sync-cib-export" data-id-title="Tagging and Exporting a Resource Configuration"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.2: </span><span class="title-name">Tagging and Exporting a Resource Configuration </span></span><a title="Permalink" class="permalink" href="#pro-ha-geo-rsc-sync-cib-export">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to one of the nodes of cluster <code class="literal">amsterdam</code>.
    </p></li><li class="step"><p>
     Start the cluster with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
     Enter <code class="command">crm configure</code> to switch to the interactive
     crm shell.
    </p></li><li class="step"><p>
     Review the current CIB configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code>show</pre></div></li><li class="step"><p>
     Mark the resources and constraints that are needed across the Geo
     cluster with the tag <code class="literal">geo_resources</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">tag</code> geo_resources: \
  <em class="replaceable">LIST_OF_RESOURCES_and_CONSTRAINTS_FOR_REQUIRED_SERVICES</em> <span class="callout" id="co-geo-rsc-any">1</span>\
  rsc1-req-ticketA ip-booth booth-site g-booth o-booth-before-rsc1 <span class="callout" id="co-geo-rsc-booth">2</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-geo-rsc-any"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Any resources and constraints of your specific setup that you need on
       all sites of the Geo cluster (for example, resources for DRBD as
       described in the
       <a class="link" href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html" target="_blank"><em class="citetitle">SUSE Best Practices</em>
       document</a>).
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-geo-rsc-booth"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Resources and constraints for boothd (primitives, booth resource group,
       ticket dependency, additional order constraint), see
       <a class="xref" href="#sec-ha-geo-rsc-ticket-dep" title="6.1. Configuring Ticket Dependencies of Resources">Section 6.1</a>
       to <a class="xref" href="#sec-ha-geo-rsc-order" title="6.3. Adding an Order Constraint">Section 6.3</a>.
      </p></td></tr></table></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If the configuration is according to your wishes, submit your changes with
     <code class="command">submit</code> and leave the crm live shell with
     <code class="command">quit</code>.
    </p></li><li class="step" id="st-ha-geo-rsc-sync-cib-export-start"><p>
     Export the tagged resources and constraints to a file named
     <code class="filename">exported.cib</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show</code> tag:geo_resources geo_resources &gt; exported.cib</pre></div><p>
     The command <code class="command">crm configure show
     tag:</code><em class="replaceable">TAGNAME</em> shows all resources that
     belong to the tag <em class="replaceable">TAGNAME</em>.
    </p></li></ol></div></div><div class="procedure" id="pro-ha-geo-rsc-sync-cib-import" data-id-title="Importing a Tagged Resource Configuration"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.3: </span><span class="title-name">Importing a Tagged Resource Configuration </span></span><a title="Permalink" class="permalink" href="#pro-ha-geo-rsc-sync-cib-import">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_resources_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To import the saved configuration file into the CIB of the second cluster
    site, proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to one of the nodes of cluster <code class="literal">berlin</code>.
    </p></li><li class="step"><p>
     Start the cluster with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
     Copy the file <code class="filename">exported.cib</code> from cluster
     <code class="literal">amsterdam</code> to this node.
     
    </p></li><li class="step"><p>
     Import the tagged resources and constraints from the file
     <code class="filename">exported.cib</code> into the CIB of cluster
     <code class="literal">berlin</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure load</code> update <em class="replaceable">PATH_TO_FILE/exported.cib</em></pre></div><p>
     When using the <code class="option">update</code> parameter for the <code class="command">crm
     configure load</code> command, crmsh tries to integrate the contents
     of the file into the current CIB configuration (instead of replacing the
     current CIB with the file contents).
    </p></li><li class="step" id="st-ha-geo-rsc-sync-cib-import-stop"><p>
     View the updated CIB configuration with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show</code></pre></div><p>
     The imported resources and constraints will appear in the CIB.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-geo-ip-relocation" data-id-title="Setting Up IP Relocation via DNS Update"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Setting Up IP Relocation via DNS Update</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-ip-relocation">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_ip_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In case one site of your Geo cluster is down and a ticket failover appears,
  you usually need to adjust the network routing accordingly (or you need to
  have configured a network failover for each ticket). Depending on the kind of
  service that is bound to a ticket, there is an alternative solution to
  reconfiguring the routing: You can use dynamic DNS update and instead change
  the IP address for a service.
 </p><p>
  The following prerequisites must be fulfilled for this scenario:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The service that needs to fail over is bound to a host name.
   </p></li><li class="listitem"><p>
    Your DNS server must be configured for dynamic DNS updates. For information
    on how to do so with BIND/named, see the <code class="literal">named</code>
    documentation, or refer to
    <a class="link" href="http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" target="_blank">http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/</a>.
    
    More information on how to set up DNS, including dynamic update of zone
    data, can be found in the <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html" target="_blank">
     Administration Guide for SUSE Linux Enterprise Server 15 SP1</a>.
   </p></li><li class="listitem"><p>
    The following example assumes that the DNS updates are protected by a
    shared key (TSIG key) for the zone to be updated. The key can be created
    using <code class="command">dnssec-keygen</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">dnssec-keygen</code> -a hmac-md5 -b 128 -n USER geo-update</pre></div><p>
    For more information, see the <code class="command">dnssec-keygen</code> man page or the
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html#sec-dns-tsig" target="_blank">
     Administration Guide for SUSE Linux Enterprise Server 15 SP1</a>.
   </p></li></ul></div><p>
  <a class="xref" href="#ex-ha-geo-dyn-dns-rsc-config" title="Resource Configuration for Dynamic DNS Update">Example 7.1, “Resource Configuration for Dynamic DNS Update”</a> illustrates how to use the
  <code class="systemitem">ocf:heartbeat:dnsupdate</code> resource agent to manage the
  <code class="command">nsupdate</code>

  command.
  
  The resource agent supports both IPv4 and IPv6.
 </p><div class="complex-example"><div class="example" id="ex-ha-geo-dyn-dns-rsc-config" data-id-title="Resource Configuration for Dynamic DNS Update"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.1: </span><span class="title-name">Resource Configuration for Dynamic DNS Update </span></span><a title="Permalink" class="permalink" href="#ex-ha-geo-dyn-dns-rsc-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_ip_i.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dns-update-ip ocf:heartbeat:dnsupdate params \
  hostname="www.domain.com"<span class="callout" id="co-dyn-dns-hostname">1</span> ip="192.168.3.4"<span class="callout" id="co-dyn-dns-ip">2</span>\
  keyfile="<em class="replaceable">/etc/whereever/Kgeo-update*</em>.key"<span class="callout" id="co-dyn-dns-key">3</span>\
  server="192.168.1.1"<span class="callout" id="co-dyn-dns-srv">4</span> serverport="53"<span class="callout" id="co-dyn-dns-srv-port">5</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-dyn-dns-hostname"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Host name bound to the service that needs to fail over together with the
     ticket. The IP address of this host name needs to be updated via dynamic
     DNS.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-dyn-dns-ip"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     IP address of the server hosting the service to be migrated.
     
     The IP address specified here can be under cluster control, too. This does
     not handle local failover, but it ensures that outside parties will be
     directed to the right site after a ticket failover.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-dyn-dns-key"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Path to the public key file generated with
     <code class="command">dnssec-keygen</code>.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-dyn-dns-srv"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     IP address of the DNS server to send the updates to. If no server is
     provided, this defaults to the primary server for the correct zone.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-dyn-dns-srv-port"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
     Port to use for communication with the DNS server. This option will only
     take effect if a DNS server is specified.
    </p></td></tr></table></div><p>
   With the resource configuration above, the resource agent takes care of
   removing the failed Geo cluster site from the DNS record and changing the
   IP for a service via dynamic DNS update.
  </p></div></div></div></section><section class="chapter" id="cha-ha-geo-manage" data-id-title="Managing Geo Clusters"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Managing Geo Clusters</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-manage">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Before booth can manage a certain ticket within the Geo cluster, you
  initially need to grant it to a site manually—either with the booth
  command line client or with Hawk2.
 </p><section class="sect1" id="sec-ha-geo-manage-cli" data-id-title="Managing Tickets From Command Line"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Managing Tickets From Command Line</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-cli">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p> Use the <code class="command">booth</code> command line tool to grant, list, or
   revoke tickets as described in <a class="xref" href="#sec-ha-geo-manage-cli-booth" title="8.1.1. Overview of booth Commands">Section 8.1.1, “Overview of <code class="command">booth</code> Commands”</a>.
  </p><div id="id-1.5.10.3.3" data-id-title="crm_ticket and crm site ticket" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: <code class="command">crm_ticket</code> and
    <code class="command">crm site ticket</code></div><p> If the booth service is not running for any reasons, you can also
    manage tickets fully manually with <code class="command">crm_ticket</code> or
    <code class="command">crm site ticket</code>. Both commands are only
    available on cluster nodes. Use them with great care as they
     <span class="emphasis"><em>cannot</em></span> verify if the same ticket is already granted
    elsewhere. For more information, read the man pages. </p><p> As long as booth is up and running, only use the
    <code class="command">booth</code> for manual intervention. </p></div><section class="sect2" id="sec-ha-geo-manage-cli-booth" data-id-title="Overview of booth Commands"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1.1 </span><span class="title-name">Overview of <code class="command">booth</code> Commands</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-cli-booth">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">booth</code> commands can be run on any machine in the cluster,
   not only the ones having the <code class="systemitem">boothd</code> running. The
   <code class="command">booth</code> commands try to find the <span class="quote">“<span class="quote">local</span>”</span>
   cluster by looking at the booth configuration file and the locally defined IP
   addresses. If you do not specify a site which <code class="command">booth</code> should
   connect to (using the <code class="option">-s</code> option), it will always connect to
   the local site.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.10.3.4.3.1"><span class="term">Listing All Tickets</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> list
ticket: ticketA, leader: none
ticket: ticketB, leader: 10.2.12.101, expires: 2014-08-13 10:28:57</pre></div><p> If you do not specify a certain site with <code class="option">-s</code>, the
      information about the tickets will be requested from the local booth
      instance. </p></dd><dt id="id-1.5.10.3.4.3.2"><span class="term">Granting a Ticket to a Site</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> grant -s 192.168.201.100 ticketA
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</pre></div><p> In this case, <code class="literal">ticketA</code> will be granted to
      the site <code class="literal">192.168.201.100</code>. Without the
       <code class="option">-s</code> option, booth would automatically connect to the
      current site (the site you are running the booth client on) and would
      request the <code class="command">grant</code> operation. </p><p> Before granting a ticket, the command executes a sanity check. If
      the same ticket is already granted to another site, you are warned about
      that and are prompted to revoke the ticket from the current site first.
     </p></dd><dt id="id-1.5.10.3.4.3.3"><span class="term">Revoking a Ticket From a Site</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> revoke ticketA
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</pre></div><p> Booth checks to which site the ticket is currently granted and
      requests the <code class="command">revoke</code> operation for
       <code class="literal">ticketA</code>. The revoke operation will be executed
      immediately. </p><p>The <code class="command">grant</code> and (under certain circumstances),
      <code class="command">revoke</code> operations may take a while to return a definite
      operation's outcome. The client waits for the result up to the ticket's
       <code class="varname">timeout</code> value before it gives up waiting. If the
       <code class="option">-w</code> option was used, the client will wait indefinitely
      instead. Find the exact status in the log files or with the
      <code class="command">crm_ticket -L</code> command.</p></dd><dt id="id-1.5.10.3.4.3.4"><span class="term">Forcing a Grant Operation</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> grant -F ticketA</pre></div><p>The result of this command depends on whether you use automatic or
      manual tickets.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Automatic Tickets. </span>As long as booth can make sure a ticket is granted to one site,
         you cannot grant the same ticket to another site, not even by using
         the <code class="option">-F</code> option. However, in case of a split brain
         situation, booth might not be able to check if an automatic ticket is
         granted somewhere else. In that case, the Geo cluster
         administrator can override the automatic process and manually grant
         the ticket to the site that is still up and running. In this
         situation, the <code class="option">-F</code> options tells booth
          <span class="emphasis"><em>not</em></span> to wait for a response from other,
         unreachable sites (so ignoring the parameters
         <em class="parameter">expire</em> and <em class="parameter">acquire-after</em>,
         if defined for this ticket). Instead, booth will immediately grant the
         ticket to the specified site.</p></li><li class="listitem"><p><span class="formalpara-title">Manual Tickets. </span>When using <span class="emphasis"><em>manual</em></span> tickets, <code class="command">booth
         grant -F</code> makes booth grant the ticket immediately to the
         specified site.</p></li></ul></div><div id="id-1.5.10.3.4.3.4.2.4" data-id-title="Potential Loss of Data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Potential Loss of Data</div><p>Before using <code class="command">booth grant -F</code>, make sure that no
       other site (which is online) owns the same ticket. If the same ticket is
       granted to multiple sites, resources depending on the ticket might start
       on several sites in parallel. This results in concurrency violation and
       potential data corruption.</p><p>As Geo cluster administrator, you need to resolve a conflict
       between tickets once the other site is reachable again.</p></div></dd></dl></div><p>In the following sections, find some examples for managing tickets in different
   scenarios.</p></section><section class="sect2" id="sec-ha-geo-manage-cli-booth-auto-ticket-move" data-id-title="Manually Moving an Automatic Ticket"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1.2 </span><span class="title-name">Manually Moving an Automatic Ticket</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-cli-booth-auto-ticket-move">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Assuming that you want to manually move <code class="literal">ticketA</code> from
    site <code class="literal">amsterdam</code> (with the virtual IP
    <code class="literal">192.168.201.100</code>) to site <code class="literal">berlin</code>
    (with the virtual IP <code class="literal">192.168.202.100</code>), proceed as
    follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Log in to <code class="literal">amsterdam</code>.</p></li><li class="step"><p>
     Set <code class="literal">ticketA</code> to standby:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_ticket</code> -t ticketA -s</pre></div></li><li class="step"><p>
     Wait for any resources that depend on <code class="literal">ticketA</code> to be
     stopped or demoted cleanly.
    </p></li><li class="step"><p>
     Revoke <code class="literal">ticketA</code> from site
     <code class="literal">amsterdam</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> revoke -s 192.168.201.100 ticketA</pre></div></li><li class="step"><p>
     After the ticket has been revoked from its original site, grant
     <code class="literal">ticketA</code> to the site <code class="literal">berlin</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> grant -s 192.168.202.100 ticketA</pre></div><p>This enables the resources which depend on this ticket to start on site
    <code class="literal">berlin</code>.</p></li><li class="step"><p>
     Remove the standby mode for <code class="literal">ticketA</code> on site <code class="literal">amsterdam</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_ticket</code> -t ticketA -a</pre></div><p>In case <code class="literal">berlin</code> fails, resources depending on
       <code class="literal">ticketA</code> will automatically fail over to site
       <code class="literal">amsterdam</code>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-geo-manage-cli-booth-man-ticket-move" data-id-title="Moving a Manual Ticket"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1.3 </span><span class="title-name">Moving a Manual Ticket</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-cli-booth-man-ticket-move">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Assuming that you want to move the manual ticket <code class="literal">ticket-nfs</code> from
    site <code class="literal">amsterdam</code> (with the virtual IP
    <code class="literal">192.168.201.100</code>) to site <code class="literal">berlin</code>
    (with the virtual IP <code class="literal">192.168.202.100</code>), proceed as
    follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Log in to <code class="literal">amsterdam</code>.</p></li><li class="step"><p>
     Set <code class="literal">ticket-nfs</code> to standby:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_ticket</code> -t ticket-nfs -s</pre></div></li><li class="step"><p>
     Wait for any resources that depend on <code class="literal">ticket-nfs</code> to be
     stopped or demoted cleanly.
    </p></li><li class="step"><p>
     Revoke <code class="literal">ticket-nfs</code> from site <code class="literal">amsterdam</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> revoke -s 192.168.201.100 ticket-nfs</pre></div></li><li class="step"><p>
     After the ticket has been revoked from its original site, grant
     <code class="literal">ticket-nfs</code> to the site <code class="literal">berlin</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> grant -s 192.168.202.100 ticket-nfs</pre></div><p>This enables the resources which depend on this ticket to start on site
     <code class="literal">berlin</code>.</p></li><li class="step"><p>
     If you want to move the resources back to site <code class="literal">amsterdam</code> at any point in
     time, remove the standby mode for <code class="literal">ticket-nfs</code> on site <code class="literal">amsterdam</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_ticket</code> -t ticket-nfs -a</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-geo-manage-cli-booth-man-ticket-failover" data-id-title="Failing Over a Manual Ticket"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1.4 </span><span class="title-name">Failing Over a Manual Ticket</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-cli-booth-man-ticket-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p> Let us assume that the (manually managed) ticket
     <code class="literal">ticket-nfs</code> had been granted to site
     <code class="literal">amsterdam</code> (with the virtual IP
     <code class="literal">192.168.201.100</code>. This site cannot be reached at the
    moment. Site <code class="literal">berlin</code> (with the virtual IP
     <code class="literal">192.168.202.100</code>) is still available. </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Try to contact a local administrator on site <code class="literal">amsterdam</code> and
      check if the site is down.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If yes, proceed with <a class="xref" href="#step-login" title="Step 2">Step 2</a>.</p></li><li class="listitem"><p>If <code class="literal">amsterdam</code> cannot be reached because of a
        connectivity problem, but the nodes are still running, ask the local
        cluster administrator to put <code class="literal">ticket-nfs</code> into standby
        mode on site <code class="literal">amsterdam</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_ticket</code> -t ticket-nfs -s</pre></div><p>This will relinquish the resources which depend on <code class="literal">ticket-nfs</code>.
        Now the ticket can safely be granted to the other site.
       </p></li></ul></div></li><li class="step" id="step-login"><p> Log in to <code class="literal">berlin</code>.</p></li><li class="step"><p> Grant <code class="literal">ticket-nfs</code>to site
      <code class="literal">berlin</code> using the <code class="option">-F</code> option: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> grant -F ticket-nfs</pre></div><p>You will see a warning that the same ticket might be granted to
     another site, but the command will be executed.</p></li><li class="step"><p>Check the result with:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> list</pre></div><p>It should show <code class="literal">berlin</code> as ticket owner for
      <code class="literal">ticket-nfs</code> now. All resources that depend on this
     ticket will be started on <code class="literal">berlin</code>.</p></li><li class="step"><p>Before trying to bring back <code class="literal">amsterdam</code> into the
     Geo cluster again, make sure to revoke <code class="literal">ticket-nfs</code>
     on <code class="literal">amsterdam</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">booth</code> revoke -s 192.168.201.100 ticket-nfs</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-geo-manage-hawk2" data-id-title="Managing Tickets with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Managing Tickets with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-manage-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_hawk2_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Tickets can be viewed in both the <span class="guimenu">Dashboard</span> and the
  <span class="guimenu">Status</span> view. Hawk2 displays the following ticket
  statuses:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="guimenu">Granted</span>: Tickets that are granted to the current site.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Elsewhere</span>: Tickets that are granted to another site.
   </p></li><li class="listitem"><p>
    <span class="guimenu">Revoked</span>: Tickets that have been revoked. Additionally,
    Hawk2 also displays tickets as revoked if they are referenced in a ticket
    dependency, but have not been granted to any site yet.
   </p></li></ul></div><div id="id-1.5.10.4.5" data-id-title="Granting Tickets to Current Site and Revoking Tickets" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Granting Tickets to Current Site and Revoking Tickets</div><p>
   Though you can view tickets for all sites with Hawk2, any grant or revoke
   operations triggered by Hawk2 only apply to the current site (that you are
   currently connected to with Hawk2). To grant a ticket to another site of
   your Geo cluster, start Hawk2 on one of the cluster nodes belonging to
   the respective site.
  </p><p>
   You can only grant tickets that are not already given to any site.
  </p></div><div class="procedure" id="pro-ha-config-hawk2-viewtickets" data-id-title="Viewing, Granting and Revoking Tickets with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.1: </span><span class="title-name">Viewing, Granting and Revoking Tickets with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-config-hawk2-viewtickets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_hawk2_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Start a Web browser and log in to Hawk2.
   </p></li><li class="step"><p>
    In the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>.
   </p><p>
    Along with information about cluster nodes and resources, Hawk2 also
    displays a <span class="guimenu">Tickets</span> category. It lists the ticket status,
    the ticket name and when the ticket was last granted. From the
    <span class="guimenu">Granted</span> column you can manage the tickets.
   </p></li><li class="step"><p>
    To show further information about the ticket, along with information about
    the cluster sites and arbitrators, click the <span class="guimenu">Details</span>
    icon next to the ticket.
   </p><div class="figure" id="id-1.5.10.4.6.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-geo-ticket-details.png"><img src="images/hawk2-geo-ticket-details.png" width="80%" alt="Hawk2—Ticket Details" title="Hawk2—Ticket Details"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.1: </span><span class="title-name">Hawk2—Ticket Details </span></span><a title="Permalink" class="permalink" href="#id-1.5.10.4.6.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_hawk2_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
    To revoke a granted ticket from the current site or to grant a ticket to
    the current site, click the switch in the <span class="guimenu">Granted</span> column
    next to the ticket. On clicking, it shows the available action. Confirm
    your choice when Hawk2 prompts for a confirmation.
   </p><p>
    If the ticket cannot be granted or revoked for any reason, Hawk2 shows an
    error message. If the ticket has been successfully granted or revoked,
    Hawk2 will update the ticket <span class="guimenu">Status</span>.
   </p></li></ol></div></div><div class="procedure" id="pro-ha-config-hawk2-geo-simulator" data-id-title="Simulating Granting and Revoking Tickets"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.2: </span><span class="title-name">Simulating Granting and Revoking Tickets </span></span><a title="Permalink" class="permalink" href="#pro-ha-config-hawk2-geo-simulator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_hawk2_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
   Hawk2's <span class="guimenu">Batch Mode</span> allows you to explore failure
   scenarios before they happen. To explore whether your resources that depend
   on a certain ticket behave as expected, you can also test the impact of
   granting or revoking tickets.
  </p><ol class="procedure" type="1"><li class="step"><p>
    Start a Web browser and log in to Hawk2.
   </p></li><li class="step"><p>
    From the top-level row, select <span class="guimenu">Batch Mode</span>.
   </p></li><li class="step"><p>
    In the batch mode bar, click <span class="guimenu">Show</span> to open the
    <span class="guimenu">Batch Mode</span> window.
   </p></li><li class="step"><p>
    To simulate a status change of a ticket:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Ticket
      Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Ticket</span> you want to manipulate and select
      the <span class="guimenu">Action </span> you want to simulate.
     </p></li><li class="step"><p>
      Confirm your changes. Your event is added to the queue of events listed
      in the <span class="guimenu">Batch Mode</span> dialog. Any event listed here is
      simulated immediately and is reflected on the <span class="guimenu">Status</span>
      screen.
     </p></li><li class="step"><p>
      Close the <span class="guimenu">Batch Mode</span> dialog and review the simulated
      changes.
     </p></li></ol></li><li class="step"><p>
    To leave the batch mode, either <span class="guimenu">Apply</span> or
    <span class="guimenu">Discard</span> the simulated changes.
   </p></li></ol></div></div><div class="figure" id="id-1.5.10.4.8"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-geo-batch-tickets.png"><img src="images/hawk2-geo-batch-tickets.png" width="80%" alt="Hawk2 Simulator—Tickets" title="Hawk2 Simulator—Tickets"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.2: </span><span class="title-name">Hawk2 Simulator—Tickets </span></span><a title="Permalink" class="permalink" href="#id-1.5.10.4.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_manage_hawk2_i.xml" title="Edit source document"> </a></div></div></div><p>
  For more information about Hawk2's <span class="guimenu">Batch Mode</span> (and which
  other scenarios can be explored with it), refer to <span class="intraxref">Book “Administration Guide”, Chapter 5 “Configuration and Administration Basics”, Section 5.4.7 “Using the Batch Mode”</span>.
 </p></section></section><section class="chapter" id="cha-ha-geo-trouble" data-id-title="Troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Troubleshooting</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-trouble">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_trouble_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Booth

  uses the same logging mechanism as the CRM. Thus, changing the log level will
  also take effect on booth logging. The booth log messages also contain
  information about any tickets.
 </p><p>
  Both the booth log messages and the booth configuration file are included in
  the <code class="command">crm report</code>.
 </p><p>
  In case of unexpected booth behavior or any problems, check the logging data
  with <code class="command">sudo journalctl -n</code> or create a detailed cluster
  report with <code class="command">crm report</code>.
 </p><p>
  In case you can access the cluster nodes on all sites (plus the arbitrators)
  from one single host via SSH, it is possible to collect log files from all of
  them within the same <code class="literal">crm report</code>. When calling <code class="command">crm
  report</code> with the <code class="option">-n</code> option, it gets the log files
  from all hosts that you specify with <code class="option">-n</code>. (Without
  <code class="option">-n</code>, it would try to obtain the list of nodes from the
  respective cluster). For example, to create a single <code class="literal">crm
  report</code> that includes the log files from two two-node clusters
  (<code class="literal">192.168.201.111</code>|<code class="literal">192.168.201.112</code> and
  <code class="literal">192.168.202.111</code>|<code class="literal">192.168.202.112</code>) plus
  an arbitrator (<code class="literal">147.2.207.14</code>), use the following command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> crm report -n "147.2.207.14 192.168.201.111 192.168.201.112 \
 192.168.202.111 192.168.202.112"  -f 10:00 -t 11:00 db-incident</pre></div><p>
  If the issue is about booth only and you know on which cluster nodes (within
  a site) booth is running, then specify only those two nodes plus the
  arbitrator.
 </p><p>
  If there is no way to access all sites from one host, run <code class="command">crm
  report</code> individually on the arbitrator, and on the cluster nodes of
  the individual sites, specifying the same period of time. To collect the log
  files on an arbitrator, you must use the <code class="option">-S</code> option for
  single node operation:
 </p><div class="verbatim-wrap"><pre class="screen">amsterdam # crm report -f 10:00 -t 11:00 db-incident-amsterdam
berlin # crm report -f 10:00 -t 11:00 db-incident-berlin
arbitrator # crm report -S -f 10:00 -t 11:00 db-incident-arbitrator</pre></div><p>
  However, it is preferable to produce one single <code class="literal">crm report</code>
  for all machines that you need log files from.
 </p></section><section class="chapter" id="cha-ha-geo-upgrade" data-id-title="Upgrading to the Latest Product Version"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Upgrading to the Latest Product Version</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo-upgrade">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_upgrade_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For instructions on how to upgrade the cluster nodes, refer to <span class="intraxref">Book “Administration Guide”, Chapter 28 “Upgrading Your Cluster and Updating Software Packages”</span>. The chapter also describes which preparations
  to take before starting the upgrade process. It provides an overview of
  the supported upgrade paths and where to find the details for each step.
 </p><p>
  If you use an arbitrator outside of the cluster sites, upgrade the arbitrator
  as described in <a class="xref" href="#pro-ha-geo-upgrade-arbitrators" title="Upgrading an Arbitrator">Procedure 10.1</a>.
 </p><div class="procedure" id="pro-ha-geo-upgrade-arbitrators" data-id-title="Upgrading an Arbitrator"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.1: </span><span class="title-name">Upgrading an Arbitrator </span></span><a title="Permalink" class="permalink" href="#pro-ha-geo-upgrade-arbitrators">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_upgrade_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Perform an upgrade to the desired target version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability
       as described in <span class="intraxref">Book “Administration Guide”, Chapter 28 “Upgrading Your Cluster and Updating Software Packages”</span>.
      </p></li><li class="step"><p>
       Check if you have enabled the modules and extensions mentioned in
       <span class="intraxref">Article “Geo Clustering Quick Start”, Section 3 “Requirements”</span>.
      </p></li><li class="step"><p>
       Check if the <span class="package">booth</span> package is installed:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper</code> pa | grep booth</pre></div></li><li class="step"><p>
       If not, install it with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper</code> install booth</pre></div></li></ol></div></div></section><section class="chapter" id="sec-ha-geo-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-geo-more">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/geo_more_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     More documentation for this product is available at
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
     For example, the Geo Clustering Quick Start guides you through
     the basic setup of a Geo cluster, using the Geo bootstrap scripts
     provided by the <code class="systemitem">ha-cluster-bootstrap</code>
     package.
    </p></li><li class="listitem"><p>
     Find information about data replication across Geo clusters via DRBD in the following
      <a class="link" href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html" target="_blank"><em class="citetitle">SUSE Best Practices</em>
       document</a>.
     </p></li></ul></div></section><div class="legal-section"><section class="appendix" id="id-1.5.14" data-id-title="GNU licenses"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">GNU licenses</span></span> <a title="Permalink" class="permalink" href="#id-1.5.14">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/common_legal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This appendix contains the GNU Free Documentation License version 1.2.
 </p><section class="sect1" id="id-1.5.14.4" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title legal"><span class="title-number-name"><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.5.14.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP1/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="http://www.gnu.org/copyleft/" target="_blank">http://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.5.14.4.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.5.14.4.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>