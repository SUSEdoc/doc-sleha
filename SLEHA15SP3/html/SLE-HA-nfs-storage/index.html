<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 15 SP3 | Highly Available NFS Storage with DRBD and Pacemaker</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Highly Available NFS Storage with DRBD and Pacemaker | SLE …"/>
<meta name="description" content="This document describes how to set up highly available…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="15 SP3"/>
<meta name="book-title" content="Highly Available NFS Storage with DRBD and Pacemaker"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise High Availability 15 SP3"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Highly Available NFS Storage with DRBD and Pacemaker |…"/>
<meta property="og:description" content="This document describes how to set up highly available NFS storage in a two-node cluster, using the following components: DR…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Highly Available NFS Storage with DRBD and Pacemaker |…"/>
<meta name="twitter:description" content="This document describes how to set up highly available NFS storage in a two-node cluster, using the following components: DR…"/>
<link rel="prev" href="bk01ar02apc.html" title="A. GNU licenses"/><link rel="next" href="bk01ar03apd.html" title="A. GNU licenses"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Highly Available NFS Storage with DRBD and Pacemaker</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Highly Available NFS Storage with DRBD and Pacemaker</div> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="article" id="article-nfs-storage" data-id-title="Highly Available NFS Storage with DRBD and Pacemaker"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">15 SP3</span></div><div><div class="title-container"><h1 class="title">Highly Available NFS Storage with DRBD and Pacemaker <a title="Permalink" class="permalink" href="article-nfs-storage.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div><div class="date"><span class="imprint-label">Publication Date: </span>March 04, 2024
</div><div><div class="abstract"><p>
     This document describes how to set up highly available NFS storage in a
    two-node cluster, using the following components:
    DRBD* (Distributed Replicated Block Device), LVM (Logical Volume Manager),
    and Pacemaker as cluster resource manager.
   </p></div></div></div></div><div><div xml:lang="en" class="legalnotice" id="id-1.3.4.2.4"><p>
  Copyright © 2006–2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="sect1" id="sec-ha-quick-nfs-usagescenario" data-id-title="Usage scenario"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Usage scenario</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-usagescenario">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This document helps you set up a highly available NFS server.
   The cluster used for the highly available NFS storage has the
   following properties:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Two nodes: <code class="systemitem">alice</code> (IP: <code class="systemitem">192.168.1.1</code>)
     and <code class="systemitem">bob</code> (IP: <code class="systemitem">192.168.1.2</code>),
     connected to each other via network.
    </p></li><li class="listitem"><p>
     Two floating, virtual IP addresses (<code class="systemitem">192.168.1.10</code> and <code class="systemitem">192.168.1.11</code>), allowing clients to connect to
     a service no matter which physical node it is running on.
     One IP address is used for cluster administration with Hawk2, and the other
     IP address is used exclusively for the NFS exports.
    </p></li><li class="listitem"><p>
     SBD used as a STONITH fencing device to avoid split-brain scenarios.
     STONITH is mandatory for the HA cluster.
    </p></li><li class="listitem"><p>
     Failover of resources from one node to the other if the active host breaks
     down (<span class="emphasis"><em>active/passive</em></span> setup).
    </p></li><li class="listitem"><p>
     Local storage on each node. The data is synchronized between the
     nodes using DRBD on top of LVM.
    </p></li><li class="listitem"><p>
      A file system exported through NFS and a separate file system used to track
      the NFS client states.
    </p></li></ul></div><p>
   After installing and setting up the basic two-node cluster, and extending it
   with storage and cluster resources for NFS, you will have a highly
   available NFS storage server.
  </p></section><section class="sect1" id="sec-ha-quick-nfs-installation" data-id-title="Preparing a two-node cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Preparing a two-node cluster</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can set up highly available NFS storage, you must prepare a High Availability cluster:
  </p><div class="procedure" id="pro-ha-nfs-prepare-cluster" data-id-title="Preparing a two-node cluster for NFS storage"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 1: </span><span class="title-name">Preparing a two-node cluster for NFS storage </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-prepare-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install and set up a basic two-node cluster as described in
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/article-installation.html" target="_blank">Installation and Setup Quick Start</a>.

    </p></li><li class="step"><p>
     On <span class="emphasis"><em>both</em></span> nodes, install the package <span class="package">nfs-kernel-server</span>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper install nfs-kernel-server</code></pre></div></li><li class="step"><p>
     On <span class="emphasis"><em>both</em></span> nodes, set the NFS server scope:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Create a new directory named <code class="filename">nfs-server.service.d</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir -p /etc/systemd/system/nfs-server.service.d</code></pre></div></li><li class="step"><p>
       Create the file <code class="filename">/etc/systemd/system/nfs-server.service.d/scope.conf</code>
       and add the following content:
      </p><div class="verbatim-wrap"><pre class="screen">[Service]
ExecStart= <span class="callout" id="co-nfs-server-empty-exec">1</span>
ExecStart=/usr/bin/unshare -u /bin/sh -c "hostname SUSE; /usr/sbin/rpc.nfsd" <span class="callout" id="co-nfs-server-scope">2</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-server-empty-exec"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         A service can only have one <code class="literal">ExecStart</code> setting, so the empty
         <code class="literal">ExecStart</code> line in this override file is used to undo
         any existing <code class="literal">ExecStart</code> setting in the NFS service file.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nfs-server-scope"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The scope must be the same on all nodes in the cluster that run the NFS server.
         All clusters using SUSE software can use the same scope, so we recommend setting
         the value to <code class="literal">SUSE</code>.
        </p></td></tr></table></div></li><li class="step"><p>
       Reload the <code class="systemitem">systemd</code> files:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl daemon-reload</code></pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="sec-ha-quick-nfs-lvm" data-id-title="Creating LVM devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Creating LVM devices</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-lvm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    LVM (Logical Volume Manager) enables flexible distribution of storage space
    across several file systems.
   </p><p>
    Use <code class="command">crm cluster run</code> to run these commands on both nodes at once.
   </p><div class="procedure" id="pro-ha-nfs-create-lvm-devices" data-id-title="Creating LVM devices for DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 2: </span><span class="title-name">Creating LVM devices for DRBD </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-lvm-devices">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create an LVM physical volume, replacing
      <code class="filename">/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code>
      with your corresponding device for LVM:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "pvcreate /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>"</code></pre></div></li><li class="step"><p>Create an LVM volume group <code class="systemitem">nfs</code>
            that includes this physical volume: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "vgcreate nfs /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>"</code></pre></div></li><li class="step"><p>
       Create a logical volume named <code class="systemitem">share</code> in the
       volume group <code class="systemitem">nfs</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "lvcreate -n share -L 20G nfs"</code></pre></div><p>
       This volume is for the NFS exports.
      </p></li><li class="step"><p>
       Create a logical volume named <code class="systemitem">state</code> in the
       volume group <code class="systemitem">nfs</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "lvcreate -n state -L 8G nfs"</code></pre></div><p>
       This volume is for the NFS client states. The 8 GB volume size
       used in this example should support several thousand concurrent NFS clients.
      </p></li><li class="step"><p>
       Activate the volume group: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "vgchange -ay nfs"</code></pre></div></li></ol></div></div><p>
    You should now see the following devices on the system:
    <code class="filename">/dev/nfs/share</code> and <code class="filename">/dev/nfs/state</code>.
   </p></section><section class="sect1" id="sec-ha-quick-nfs-drbd-device" data-id-title="Creating DRBD devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Creating DRBD devices</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-drbd-device">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section describes how to set up DRBD devices on top of LVM.
    Using LVM as a back-end of DRBD has the following benefits:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Easier setup than with LVM on top of DRBD.</p></li><li class="listitem"><p>Easier administration in case the LVM disks need to be resized or
     more disks are added to the volume group.
    </p></li></ul></div><p>
    The following procedures result in two DRBD devices: one device for the
    NFS exports, and a second device to track the NFS client states.
   </p><section class="sect2" id="sec-ha-quick-nfs-drbd-config" data-id-title="Creating the DRBD configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Creating the DRBD configuration</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-drbd-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     DRBD configuration files are kept in the <code class="filename">/etc/drbd.d/</code>
     directory and must end with a <code class="filename">.res</code>
     extension. In this procedure, the configuration file is named
     <code class="filename">/etc/drbd.d/nfs.res</code>.
    </p><div class="procedure" id="pro-ha-nfs-create-drbd-config" data-id-title="Creating a DRBD configuration"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3: </span><span class="title-name">Creating a DRBD configuration </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-drbd-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the file <code class="filename">/etc/drbd.d/nfs.res</code> with the
        following contents:
      </p><div class="verbatim-wrap"><pre class="screen">resource nfs {
   volume 0 { <span class="callout" id="co-ha-quick-nfs-drbd-volume">1</span>
      device           /dev/drbd0; <span class="callout" id="co-ha-quick-nfs-drbd-device">2</span>
      disk             /dev/nfs/state; <span class="callout" id="co-ha-quick-nfs-drbd-disk">3</span>
      meta-disk        internal; <span class="callout" id="co-ha-quick-nfs-drbd-metadisk">4</span>
   }
   volume 1 {
      device           /dev/drbd1;
      disk             /dev/nfs/share;
      meta-disk        internal;
   }

   net {
      protocol  C; <span class="callout" id="co-ha-quick-nfs-drbd-protocol">5</span>
      fencing resource-and-stonith; <span class="callout" id="co-ha-quick-nfs-fencing-policy">6</span>
   }

   handlers { <span class="callout" id="co-ha-quick-nfs-fencing-handlers">7</span>
      fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
      after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
   }

   connection-mesh { <span class="callout" id="co-ha-quick-nfs-connectionmesh">8</span>
      hosts     alice bob;
   }
   on alice { <span class="callout" id="co-ha-quick-nfs-drbd-on">9</span>
      address   192.168.1.1:7790;
      node-id   0;
   }
   on bob { <a class="xref" href="article-nfs-storage.html#co-ha-quick-nfs-drbd-on"><span class="callout">9</span></a>
      address   192.168.1.2:7790;
      node-id   1;
   }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-volume"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The volume number for each DRBD device you want to create.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The DRBD device that applications will access.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-disk"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <a class="xref" href="article-nfs-storage.html#sec-ha-quick-nfs-lvm" title="3. Creating LVM devices">Section 3, “Creating LVM devices”</a>.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-metadisk"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Where the metadata is stored. Using
         <code class="literal">internal</code>, the metadata is stored together with
         the user data on the same device. See the man page for further
         information.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-protocol"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The protocol to use for this connection. Protocol <code class="literal">C</code>
         provides better data availability and does not consider a write to be
         complete until it has reached all local and remote disks.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-fencing-policy"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Specifies the fencing policy. For clusters with a STONITH device
         configured, use <code class="literal">resource-and-stonith</code>.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-fencing-handlers"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         Enables resource-level fencing. If the DRBD replication link
         becomes disconnected, Pacemaker tries to promote the DRBD resource
         to another node. For more information, see <span class="intraxref">Book “Administration Guide”, Chapter 22 “DRBD”, Section 22.6 “Using resource-level fencing with STONITH”</span>.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-connectionmesh"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines all nodes of a mesh.
       The <code class="option">hosts</code> parameter contains all host names that
       share the same DRBD setup.
    </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-quick-nfs-drbd-on"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Contains the IP address and a unique identifier for each node.</p></td></tr></table></div></li><li class="step"><p>
       Open <code class="filename">/etc/csync2/csync2.cfg</code> and check whether the
       following two lines exist:
      </p><div class="verbatim-wrap"><pre class="screen">include /etc/drbd.conf;
include /etc/drbd.d;</pre></div><p>
       If not, add them to the file.
      </p></li><li class="step"><p>
       Copy the file to the other nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -xv</pre></div><p>
       For information about Csync2, see
       <span class="intraxref">Book “Administration Guide”, Chapter 4 “Using the YaST cluster module”, Section 4.7 “Transferring the configuration to all nodes”</span>.
      </p></li></ol></div></div></section><section class="sect2" id="sec-ha-quick-nfs-drbd-activate" data-id-title="Activating the DRBD devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Activating the DRBD devices</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-drbd-activate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     After preparing the DRBD configuration, activate the devices:
    </p><div class="procedure" id="pro-ha-nfs-activate-drbd-devices" data-id-title="Activating DRBD devices"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4: </span><span class="title-name">Activating DRBD devices </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-activate-drbd-devices">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       If you use a firewall in the cluster, open port
       <code class="systemitem">7790</code> in the firewall configuration.
      </p></li><li class="step"><p>
       Initialize the metadata storage:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "drbdadm create-md nfs"</code></pre></div></li><li class="step"><p>
       Create the DRBD devices:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "drbdadm up nfs"</code></pre></div></li><li class="step"><p>
       The devices do not have data yet, so you can run these commands to
       skip the initial synchronization:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm new-current-uuid --clear-bitmap nfs/0</code>
<code class="prompt root"># </code><code class="command">drbdadm new-current-uuid --clear-bitmap nfs/1</code></pre></div></li><li class="step"><p>Make <code class="systemitem">alice</code> primary:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm primary --force nfs</code></pre></div></li><li class="step"><p>Check the DRBD status of <code class="literal">nfs</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm status nfs</code></pre></div><p>This returns the following message:</p><div class="verbatim-wrap"><pre class="screen">nfs role:Primary
  volume:0 disk:UpToDate
  volume:1 disk:UpToDate
  bob role:Secondary
    volume:0 peer-disk:UpToDate
    volume:1 peer-disk:UpToDate</pre></div></li></ol></div></div><p>
     You can access the DRBD resources on the block devices
     <code class="filename">/dev/drbd0</code> and <code class="filename">/dev/drbd1</code>.
     </p></section><section class="sect2" id="sec-ha-quick-nfs-drbd-createfs" data-id-title="Creating the file systems"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Creating the file systems</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-drbd-createfs">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     After activating the DRBD devices, create file systems on them:

    </p><div class="procedure" id="pro-ha-nfs-create-drbd-file-systems" data-id-title="Creating file systems for DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5: </span><span class="title-name">Creating file systems for DRBD </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-drbd-file-systems">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create an <code class="literal">ext4</code> file system on <code class="filename">/dev/drbd0</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ext4 /dev/drbd0</code></pre></div></li><li class="step"><p>
       Create an <code class="literal">ext4</code> file system on <code class="filename">/dev/drbd1</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ext4 /dev/drbd1</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-quick-nfs-resources" data-id-title="Creating cluster resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Creating cluster resources</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following procedures describe how to configure the resources required
   for a highly available NFS cluster.
  </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Overview of cluster resources </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#id-1.3.4.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.4.7.3.2"><span class="term">DRBD primitive and promotable clone resources</span></dt><dd><p>
      These resources are used to replicate data. The promotable clone resource
      is switched to and from the primary and secondary roles as deemed necessary
      by the cluster resource manager.
     </p></dd><dt id="id-1.3.4.7.3.3"><span class="term">File system resources</span></dt><dd><p>
      These resources manage the file system that will be exported, and the
      file system that will track NFS client states.
     </p></dd><dt id="id-1.3.4.7.3.4"><span class="term">NFS kernel server resource</span></dt><dd><p>
      This resource manages the NFS server daemon.
     </p></dd><dt id="id-1.3.4.7.3.5"><span class="term">NFS exports</span></dt><dd><p>
      This resource is used to export the directory <code class="filename">/srv/nfs/share</code>
      to clients.
     </p></dd><dt id="id-1.3.4.7.3.6"><span class="term">Virtual IP address</span></dt><dd><p>
      The initial installation creates an administrative virtual IP address for Hawk2.
      Create another virtual IP address exclusively for NFS exports. This makes it
      easier to apply security restrictions later.
     </p></dd></dl></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Example NFS scenario </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#id-1.3.4.7.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>The following configuration examples assume that
         <code class="systemitem">192.168.1.11</code> is the virtual
         IP address to use for an NFS server which serves clients in the
         <code class="systemitem">192.168.1.x/24</code> subnet.</p></li><li class="listitem"><p>The service exports data served from
         <code class="literal">/srv/nfs/share</code>. </p></li><li class="listitem"><p>Into this export directory, the cluster mounts an
            <code class="literal">ext4</code> file system from the DRBD device
         <code class="filename">/dev/drbd1</code>.
         This DRBD device sits on top of an LVM logical volume named
         <code class="literal">/dev/nfs/share</code>.
        </p></li><li class="listitem"><p>
      The DRBD device <code class="literal">/dev/drbd0</code> is used to share the
      NFS client states from <code class="filename">/var/lib/nfs</code>. This DRBD device
      sits on top of an LVM logical volume named <code class="literal">/dev/nfs/state</code>.
     </p></li></ul></div><section class="sect2" id="sec-ha-quick-nfs-resources-drbd" data-id-title="Creating DRBD primitive and promotable clone resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Creating DRBD primitive and promotable clone resources</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources-drbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Create a cluster resource to manage the DRBD devices, and a promotable
    clone to allow this resource to run on both nodes:
   </p><div class="procedure" id="pro-ha-nfs-create-drbd-resource" data-id-title="Creating a DRBD resource for NFS"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6: </span><span class="title-name">Creating a DRBD resource for NFS </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-drbd-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start the <code class="command">crm</code> interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
      Create a primitive for the DRBD configuration <code class="literal">nfs</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive drbd-nfs ocf:linbit:drbd \
  params drbd_resource="nfs" \
  op monitor interval=15 role=Master \
  op monitor interval=30 role=Slave</code></pre></div></li><li class="step"><p>
      Create a promotable clone for the <code class="literal">drbd-nfs</code> primitive:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms ms-drbd-nfs drbd-nfs \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    Pacemaker activates the DRBD resources on both nodes and promotes
    them to the primary role on one of the nodes. Check the state of the
    cluster with the <code class="command">crm status</code> command, or run
    <code class="command">drbdadm status</code>.
   </p></section><section class="sect2" id="sec-ha-quick-nfs-resources-lvm" data-id-title="Creating file system resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Creating file system resources</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources-lvm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Create cluster resources to manage the file systems for export and state tracking:
   </p><div class="procedure" id="pro-ha-nfs-create-fs-resource" data-id-title="Creating file system resources for NFS"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7: </span><span class="title-name">Creating file system resources for NFS </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-fs-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primitive for the NFS client states on <code class="literal">/dev/drbd0</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive fs-nfs-state Filesystem \
  params device=/dev/drbd0 directory=/var/lib/nfs fstype=ext4</code></pre></div></li><li class="step"><p>
      Create a primitive for the file system to be exported on
      <code class="literal">/dev/drbd1</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive fs-nfs-share Filesystem \
  params device=/dev/drbd1 directory=/srv/nfs/share fstype=ext4</code></pre></div><p>
      <span class="emphasis"><em>Do not</em></span> commit this configuration until
    after you add the colocation and order constraints.
     </p></li><li class="step"><p>
      Add both of these resources to a resource group named <code class="literal">g-nfs</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group g-nfs fs-nfs-state fs-nfs-share</code></pre></div><p>
      Resources start in the order they are added to the group and stop in reverse order.
     </p></li><li class="step"><p>
      Add a colocation constraint to make sure that the resource group always
      starts on the node where the DRBD promotable clone is in the primary role:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation col-nfs-on-drbd inf: g-nfs ms-drbd-nfs:Master</code></pre></div></li><li class="step"><p>
      Add an order constraint to make sure the DRBD promotable clone always
      starts before the resource group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order o-drbd-before-nfs Mandatory: ms-drbd-nfs:promote g-nfs:start</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    Pacemaker mounts <code class="literal">/dev/drbd0</code> to <code class="filename">/var/lib/nfs</code>,
    and <code class="literal">/dev/drbd1</code> to <code class="filename">srv/nfs/share</code>. Confirm this
    with <code class="command">mount</code>, or by looking at <code class="filename">/proc/mounts</code>.
   </p></section><section class="sect2" id="sec-ha-quick-nfs-resources-nfsserver" data-id-title="Creating an NFS kernel server resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Creating an NFS kernel server resource</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources-nfsserver">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Create a cluster resource to manage the NFS server daemon:
   </p><div class="procedure" id="pro-ha-nfs-create-nfs-server-resource" data-id-title="Creating an NFS kernel server resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8: </span><span class="title-name">Creating an NFS kernel server resource </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-nfs-server-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primitive to manage the NFS server daemon:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive nfsserver nfsserver \
  params nfs_shared_infodir="/var/lib/nfs"</code></pre></div><div id="id-1.3.4.7.7.3.2.3" data-id-title="Low lease time can cause loss of file state" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Low lease time can cause loss of file state</div><p>
       NFS clients regularly renew their state with the NFS server. If the lease time
       is too low, system or network delays can cause the timer to expire before the
       renewal is complete. This can lead to I/O errors and loss of file state.
      </p><p>
       <code class="literal">NFSV4LEASETIME</code> is set on the NFS server in the file
       <code class="filename">/etc/sysconfig/nfs</code>. The default is 90 seconds.
       If lowering the lease time is necessary, we recommend a value of 60 or
       higher. We strongly discourage values lower than 30.
      </p></div></li><li class="step"><p>
      Append this resource to the existing <code class="literal">g-nfs</code> resource group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-nfs add nfsserver</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-quick-nfs-resources-nfsexport" data-id-title="Creating an NFS export resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Creating an NFS export resource</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources-nfsexport">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Create a cluster resource to manage the NFS exports:
   </p><div class="procedure" id="pro-ha-nfs-create-nfs-export-resource" data-id-title="Creating an NFS export resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 9: </span><span class="title-name">Creating an NFS export resource </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-nfs-export-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a primitive for the NFS exports:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive exportfs-nfs exportfs \
  params directory="/srv/nfs/share" \
  options="rw,mountpoint" clientspec="192.168.1.0/24" fsid=101 \</code><span class="callout" id="co-exportfs-fsid">1</span>
  <code class="command">op monitor interval=30s timeout=90s</code><span class="callout" id="co-exportfs-monitor-timeout">2</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-exportfs-fsid"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The <code class="literal">fsid</code> must be unique for each NFS export resource.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-exportfs-monitor-timeout"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The value of <code class="literal">op monitor timeout</code> must be higher
         than the value of <code class="literal">stonith-timeout</code>. To find the
         <code class="literal">stonith-timeout</code> value, run <code class="command">crm configure show</code>
         and look under the <code class="literal">property</code> section.
        </p></td></tr></table></div><div id="id-1.3.4.7.8.3.2.4" data-id-title="Do not set wait_for_leasetime_on_stop=true" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Do not set <code class="literal">wait_for_leasetime_on_stop=true</code></div><p>
        Setting this option to <code class="literal">true</code> in a highly available
        NFS setup can cause unnecessary delays and loss of locks.
       </p><p>
        The default value for <code class="literal">wait_for_leasetime_on_stop</code> is
        <code class="literal">false</code>. There is no need to set it to <code class="literal">true</code>
        when <code class="filename">/var/lib/nfs</code> and <code class="literal">nfsserver</code>
        are configured as described in this guide.
       </p></div></li><li class="step"><p>
       Append this resource to the existing <code class="literal">g-nfs</code> resource group:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-nfs add exportfs-nfs</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
      Confirm that the NFS exports are set up properly:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">exportfs -v</code>
/srv/nfs/share   <em class="replaceable">IP_ADDRESS_OF_CLIENT</em>(<em class="replaceable">OPTIONS</em>)</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-quick-nfs-resources-vip" data-id-title="Creating a virtual IP address for NFS exports"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5 </span><span class="title-name">Creating a virtual IP address for NFS exports</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-resources-vip">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Create a cluster resource to manage the virtual IP address for the NFS exports:
   </p><div class="procedure" id="pro-ha-nfs-create-vip-resource" data-id-title="Creating a virtual IP address for NFS exports"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10: </span><span class="title-name">Creating a virtual IP address for NFS exports </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-create-vip-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primitive for the virtual IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive vip-nfs IPaddr2 params ip=192.168.1.11</code></pre></div></li><li class="step"><p>
      Append this resource to the existing <code class="literal">g-nfs</code> resource group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-nfs add vip-nfs</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
      Leave the <code class="command">crm</code> interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">quit</code></pre></div></li><li class="step"><p>
      Check the status of the cluster. The resources in the <code class="literal">g-nfs</code>
      group should appear in the following order:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm status</code>
  [...]
  Full List of Resources
    [...]
    * Resource Group: g-nfs:
      * fs-nfs-state    (ocf:heartbeat:Filesystem):   Started alice
      * fs-nfs-share    (ocf:heartbeat:Filesystem):   Started alice
      * nfsserver       (ocf:heartbeat:nfsserver):    Started alice
      * exportfs-nfs    (ocf:heartbeat:exportfs):     Started alice
      * vip-nfs         (ocf:heartbeat:IPaddr2):      Started alice</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-quick-nfs-use" data-id-title="Using the NFS service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Using the NFS service</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-use">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section outlines how to use the highly available NFS service from an
   NFS client.
  </p><p>
   To connect to the NFS service, make sure to use the <span class="emphasis"><em>virtual IP
   address</em></span> to connect to the cluster rather than a physical IP
   configured on one of the cluster nodes' network interfaces. For compatibility
   reasons, use the <span class="emphasis"><em>full</em></span> path of the NFS export on the server.
  </p><p>
   The command to mount the NFS export looks like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount 192.168.1.11:/srv/nfs/share /home/share</code></pre></div><p>
   If you need to configure other mount options, such as a specific transport protocol
   (<code class="option">proto</code>), maximum read and write request sizes (<code class="option">rsize</code>
   and <code class="option">wsize</code>), or a specific NFS version (<code class="option">vers</code>),
   use the <code class="option">-o</code> option. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount -o proto=tcp,rsize=32768,wsize=32768,vers=3 \
192.168.1.11:/srv/nfs/share /home/share</code></pre></div><p>
   For further NFS mount options, see the <code class="command">nfs</code> man page.
  </p><div id="id-1.3.4.8.9" data-id-title="Loopback mounts" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Loopback mounts</div><p>
    Loopback mounts are only supported for NFS version 3, <span class="emphasis"><em>not</em></span>
    NFS version 4. For more information, see
    <a class="link" href="https://www.suse.com/support/kb/doc/?id=000018709" target="_blank">https://www.suse.com/support/kb/doc/?id=000018709</a>.
   </p></div></section><section class="sect1" id="sec-ha-quick-nfs-add-filesystems" data-id-title="Adding more NFS shares to the cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Adding more NFS shares to the cluster</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-add-filesystems">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  If you need to increase the available storage, you can add more NFS shares
  to the cluster.
 </p><p>
  In this example, a new DRBD device named <code class="literal">/dev/drbd2</code> sits
  on top of an LVM logical volume named <code class="literal">/dev/nfs/share2</code>.
 </p><div class="procedure" id="pro-ha-nfs-add-nfs-shares" data-id-title="Adding more NFS shares to the cluster"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11: </span><span class="title-name">Adding more NFS shares to the cluster </span></span><a title="Permalink" class="permalink" href="article-nfs-storage.html#pro-ha-nfs-add-nfs-shares">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Create an LVM logical volume for the new share:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "lvcreate -n share2 -L 20G nfs"</code></pre></div></li><li class="step"><p>
    Update the file <code class="filename">/etc/drbd.d/nfs.res</code> to add the new volume
    under the existing volumes:
   </p><div class="verbatim-wrap"><pre class="screen">   volume 2 {
      device           /dev/drbd2;
      disk             /dev/nfs/share2;
      meta-disk        internal;
   }</pre></div></li><li class="step"><p>
    Copy the updated file to the other nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2 -xv</code></pre></div></li><li class="step"><p>
    Initialize the metadata storage for the new volume:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "drbdadm create-md nfs/2 --force"</code></pre></div></li><li class="step"><p>
    Update the <code class="literal">nfs</code> configuration to create the new device:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "drbdadm adjust nfs"</code></pre></div></li><li class="step"><p>
    Skip the initial synchronization for the new device:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm new-current-uuid --clear-bitmap nfs/2</code></pre></div></li><li class="step"><p>
    The NFS cluster resources might have moved to another node since they were created.
    Check the DRBD status with <code class="command">drbdadm status nfs</code>, and make a
    note of which node is in the <code class="literal">Primary</code> role.
   </p></li><li class="step"><p>
    On the node that is in the <code class="literal">Primary</code> role, create an
    <code class="literal">ext4</code> file system on <code class="literal">/dev/drbd2</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ext4 /dev/drbd2</code></pre></div></li><li class="step"><p>
    Start the <code class="command">crm</code> interactive shell:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
    Create a primitive for the file system to be exported on <code class="literal">/dev/drbd2</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive fs-nfs-share2 Filesystem \
  params device="/dev/drbd2" directory="/srv/nfs/share2" fstype=ext4</code></pre></div></li><li class="step"><p>
    Add the new file system resource to the <code class="literal">g-nfs</code> group
    <span class="emphasis"><em>before</em></span> the <code class="literal">nfsserver</code> resource:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-nfs add fs-nfs-share2 before nfsserver</code></pre></div></li><li class="step"><p>
    Create a primitive for NFS exports from the new share:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive exportfs-nfs2 exportfs \
  params directory="/srv/nfs/share2" \
  options="rw,mountpoint" clientspec="192.168.1.0/24" fsid=102 \
  op monitor interval=30s timeout=90s</code></pre></div></li><li class="step"><p>
    Add the new NFS export resource to the <code class="literal">g-nfs</code> group
    <span class="emphasis"><em>before</em></span> the <code class="literal">vip-nfs</code> resource:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-nfs add exportfs-nfs2 before vip-nfs</code></pre></div></li><li class="step"><p>
    Commit this configuration:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
    Leave the <code class="command">crm</code> interactive shell:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">quit</code></pre></div></li><li class="step"><p>
    Check the status of the cluster. The resources in the <code class="literal">g-nfs</code>
    group should appear in the following order:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm status</code>
[...]
Full List of Resources
  [...]
  * Resource Group: g-nfs:
    * fs-nfs-state    (ocf:heartbeat:Filesystem):   Started alice
    * fs-nfs-share    (ocf:heartbeat:Filesystem):   Started alice
    * fs-nfs-share2   (ocf:heartbeat:Filesystem):   Started alice
    * nfsserver       (ocf:heartbeat:nfsserver):    Started alice
    * exportfs-nfs    (ocf:heartbeat:exportfs):     Started alice
    * exportfs-nfs2   (ocf:heartbeat:exportfs):     Started alice
    * vip-nfs         (ocf:heartbeat:IPaddr2):      Started alice</pre></div></li><li class="step"><p>
    Confirm that the NFS exports are set up properly:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">exportfs -v</code>
/srv/nfs/share   <em class="replaceable">IP_ADDRESS_OF_CLIENT</em>(<em class="replaceable">OPTIONS</em>)
/srv/nfs/share2  <em class="replaceable">IP_ADDRESS_OF_CLIENT</em>(<em class="replaceable">OPTIONS</em>)</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-quick-nfs-more-info" data-id-title="For more information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">For more information</span></span> <a title="Permalink" class="permalink" href="article-nfs-storage.html#sec-ha-quick-nfs-more-info">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/article_nfs_storage.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     For more details about the steps in this guide, see
     <a class="link" href="https://www.suse.com/support/kb/doc/?id=000020396" target="_blank">https://www.suse.com/support/kb/doc/?id=000020396</a>.
    </p></li><li class="listitem"><p>
     For more information about NFS and LVM, see
     <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html" target="_blank">
     Storage Administration Guide for SUSE Linux Enterprise Server</a>.
    </p></li><li class="listitem"><p>
     For more information about DRBD, see <span class="intraxref">Book “Administration Guide”, Chapter 22 “DRBD”</span>.
    </p></li><li class="listitem"><p>
     For more information about cluster resources, see
     <span class="intraxref">Book “Administration Guide”, Chapter 6 “Configuring cluster resources”</span>.
    </p></li></ul></div></section></section><nav class="bottom-pagination"><div> </div><div><a class="pagination-link next" href="bk01ar03apd.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Appendix A </span>GNU licenses</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-usagescenario"><span class="title-number">1 </span><span class="title-name">Usage scenario</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-installation"><span class="title-number">2 </span><span class="title-name">Preparing a two-node cluster</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-lvm"><span class="title-number">3 </span><span class="title-name">Creating LVM devices</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-drbd-device"><span class="title-number">4 </span><span class="title-name">Creating DRBD devices</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-resources"><span class="title-number">5 </span><span class="title-name">Creating cluster resources</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-use"><span class="title-number">6 </span><span class="title-name">Using the NFS service</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-add-filesystems"><span class="title-number">7 </span><span class="title-name">Adding more NFS shares to the cluster</span></a></span></li><li><span class="sect1"><a href="article-nfs-storage.html#sec-ha-quick-nfs-more-info"><span class="title-number">8 </span><span class="title-name">For more information</span></a></span></li><li><span class="appendix"><a href="bk01ar03apd.html"><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="bk01ar03apd.html#id-1.3.4.11.4"><span class="title-number">A.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>