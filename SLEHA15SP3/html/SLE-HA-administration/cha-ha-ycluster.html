<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 15 SP3 | Administration Guide | Using the YaST cluster module</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Using the YaST cluster module | SLE HA 15 SP3"/>
<meta name="description" content="The YaST cluster module allows you to set up a cluster…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="15 SP3"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 4. Using the YaST cluster module"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise High Availability 15 SP3"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Using the YaST cluster module | SLE HA 15 SP3"/>
<meta property="og:description" content="The YaST cluster module allows you to set up a cluster manually (from scratch) or to modify options for an existing cluster.…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using the YaST cluster module | SLE HA 15 SP3"/>
<meta name="twitter:description" content="The YaST cluster module allows you to set up a cluster manually (from scratch) or to modify options for an existing cluster.…"/>
<link rel="prev" href="cha-ha-install.html" title="Chapter 3. Installing SUSE Linux Enterprise High Availability"/><link rel="next" href="part-config.html" title="Part II. Configuration and administration"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-install.html">Installation and setup</a><span> / </span><a class="crumb" href="cha-ha-ycluster.html">Using the YaST cluster module</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="pre-ha.html" class=" "><span class="title-number"> </span><span class="title-name">Preface</span></a></li><li class="active"><a href="part-install.html" class="has-children you-are-here"><span class="title-number">I </span><span class="title-name">Installation and setup</span></a><ol><li><a href="cha-ha-concepts.html" class=" "><span class="title-number">1 </span><span class="title-name">Product overview</span></a></li><li><a href="cha-ha-requirements.html" class=" "><span class="title-number">2 </span><span class="title-name">System requirements and recommendations</span></a></li><li><a href="cha-ha-install.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></li><li><a href="cha-ha-ycluster.html" class=" you-are-here"><span class="title-number">4 </span><span class="title-name">Using the YaST cluster module</span></a></li></ol></li><li><a href="part-config.html" class="has-children "><span class="title-number">II </span><span class="title-name">Configuration and administration</span></a><ol><li><a href="cha-ha-config-basics.html" class=" "><span class="title-number">5 </span><span class="title-name">Configuration and administration basics</span></a></li><li><a href="sec-ha-config-basics-resources.html" class=" "><span class="title-number">6 </span><span class="title-name">Configuring cluster resources</span></a></li><li><a href="sec-ha-config-basics-constraints.html" class=" "><span class="title-number">7 </span><span class="title-name">Configuring resource constraints</span></a></li><li><a href="cha-ha-manage-resources.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing cluster resources</span></a></li><li><a href="sec-ha-config-basics-remote.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing services on remote hosts</span></a></li><li><a href="cha-ha-agents.html" class=" "><span class="title-number">10 </span><span class="title-name">Adding or modifying resource agents</span></a></li><li><a href="cha-ha-monitor-clusters.html" class=" "><span class="title-number">11 </span><span class="title-name">Monitoring clusters</span></a></li><li><a href="cha-ha-fencing.html" class=" "><span class="title-number">12 </span><span class="title-name">Fencing and STONITH</span></a></li><li><a href="cha-ha-storage-protect.html" class=" "><span class="title-number">13 </span><span class="title-name">Storage protection and SBD</span></a></li><li><a href="cha-ha-qdevice.html" class=" "><span class="title-number">14 </span><span class="title-name">QDevice and QNetd</span></a></li><li><a href="cha-ha-acl.html" class=" "><span class="title-number">15 </span><span class="title-name">Access control lists</span></a></li><li><a href="cha-ha-netbonding.html" class=" "><span class="title-number">16 </span><span class="title-name">Network device bonding</span></a></li><li><a href="cha-ha-lb.html" class=" "><span class="title-number">17 </span><span class="title-name">Load balancing</span></a></li><li><a href="cha-ha-geo.html" class=" "><span class="title-number">18 </span><span class="title-name">Geo clusters (multi-site clusters)</span></a></li></ol></li><li><a href="part-storage.html" class="has-children "><span class="title-number">III </span><span class="title-name">Storage and data replication</span></a><ol><li><a href="cha-ha-storage-dlm.html" class=" "><span class="title-number">19 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></li><li><a href="cha-ha-ocfs2.html" class=" "><span class="title-number">20 </span><span class="title-name">OCFS2</span></a></li><li><a href="cha-ha-gfs2.html" class=" "><span class="title-number">21 </span><span class="title-name">GFS2</span></a></li><li><a href="cha-ha-drbd.html" class=" "><span class="title-number">22 </span><span class="title-name">DRBD</span></a></li><li><a href="cha-ha-clvm.html" class=" "><span class="title-number">23 </span><span class="title-name">Cluster logical volume manager (Cluster LVM)</span></a></li><li><a href="cha-ha-cluster-md.html" class=" "><span class="title-number">24 </span><span class="title-name">Cluster multi-device (Cluster MD)</span></a></li><li><a href="cha-ha-samba.html" class=" "><span class="title-number">25 </span><span class="title-name">Samba clustering</span></a></li><li><a href="cha-ha-rear.html" class=" "><span class="title-number">26 </span><span class="title-name">Disaster recovery with ReaR (Relax-and-Recover)</span></a></li></ol></li><li><a href="part-maintenance.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Maintenance and upgrade</span></a><ol><li><a href="cha-ha-maintenance.html" class=" "><span class="title-number">27 </span><span class="title-name">Executing maintenance tasks</span></a></li><li><a href="cha-ha-migration.html" class=" "><span class="title-number">28 </span><span class="title-name">Upgrading your cluster and updating software packages</span></a></li></ol></li><li><a href="part-appendix.html" class="has-children "><span class="title-number">V </span><span class="title-name">Appendix</span></a><ol><li><a href="app-ha-troubleshooting.html" class=" "><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></li><li><a href="app-naming.html" class=" "><span class="title-number">B </span><span class="title-name">Naming conventions</span></a></li><li><a href="app-ha-management.html" class=" "><span class="title-number">C </span><span class="title-name">Cluster management tools (command line)</span></a></li><li><a href="app-crmreport-nonroot.html" class=" "><span class="title-number">D </span><span class="title-name">Running cluster reports without <code class="systemitem">root</code> access</span></a></li></ol></li><li><a href="gl-heartb.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="bk02ape.html" class=" "><span class="title-number">E </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ha-ycluster" data-id-title="Using the YaST cluster module"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">15 SP3</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Using the YaST cluster module</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>The YaST cluster module allows you to set up a cluster manually
    (from scratch) or to modify options for an existing cluster.
   </p><p>
    However, if you prefer an automated approach for setting up a cluster,
    refer to <span class="intraxref">Article “Installation and Setup Quick Start”</span>. It describes how to install the
    needed packages and leads you to a basic two-node cluster, which is
    set up with the <code class="systemitem">ha-cluster-bootstrap</code> scripts.
   </p><p>
    You can also use a combination of both setup methods, for example: set up
    one node with YaST cluster and then use one of the bootstrap scripts
    to integrate more nodes (or vice versa).
   </p></div></div></div></div><section class="sect1" id="sec-ha-installation-terms" data-id-title="Definition of terms"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Definition of terms</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-terms">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Several key terms used in the YaST cluster module and in this chapter are
   defined below.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.6.3.3.1"><span class="term">Bind network address (<code class="systemitem">bindnetaddr</code>)
    </span></dt><dd><p>
      The network address the Corosync executive should bind to.  To simplify sharing configuration files across
      the cluster, Corosync uses network interface netmask to mask only
      the address bits that are used for routing the network. For example,
      if the local interface is <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.0</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.0</code>. If the local interface is
      <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.192</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.64</code>.
     </p><p> If <code class="systemitem">nodelist</code> with
       <code class="systemitem">ringX_addr</code> is explicitly configured in
       <code class="filename">/etc/corosync/corosync.conf</code>,
       <code class="systemitem">bindnetaddr</code> is not strictly required. </p><div id="id-1.4.3.6.3.3.1.2.3" data-id-title="Network address for all nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Network address for all nodes</div><p>
       As the same Corosync configuration will be used on all nodes,
       make sure to use a network address as
       <code class="systemitem">bindnetaddr</code>, not the address of a specific
       network interface.
      </p></div></dd><dt id="id-1.4.3.6.3.3.2"><span class="term"><code class="systemitem">conntrack</code> Tools</span></dt><dd><p>
      Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes. For detailed information, refer to
      <a class="link" href="http://conntrack-tools.netfilter.org/" target="_blank">http://conntrack-tools.netfilter.org/</a>.
     </p></dd><dt id="id-1.4.3.6.3.3.3"><span class="term">Csync2</span></dt><dd><p>
      A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters. Csync2 can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which ﬁles
      should be synchronized in the synchronization group. The groups, the
      host names belonging to each group, and the include/exclude rules for
      each group are specified in the Csync2 configuration file,
      <code class="filename">/etc/csync2/csync2.cfg</code>.
     </p><p>
      For authentication, Csync2 uses the IP addresses and pre-shared
      keys within a synchronization group. You need to generate one key file
      for each synchronization group and copy it to all group members.
     </p><p>
      For more information about Csync2, refer to
      <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>
     </p></dd><dt id="id-1.4.3.6.3.3.4"><span class="term">Existing cluster</span></dt><dd><p>
        The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
     </p></dd><dt id="id-1.4.3.6.3.3.5"><span class="term">Multicast</span></dt><dd><p>
        A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
     </p><div id="id-1.4.3.6.3.3.5.2.2" data-id-title="Switches and multicast" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Switches and multicast</div><p>
       To use multicast for cluster communication, make sure
       your switches support multicast.
      </p></div></dd><dt id="vle-ha-mcastaddr"><span class="term">Multicast address (<code class="systemitem">mcastaddr</code>)
   </span></dt><dd><p>
        IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6.  If IPv6 networking is used, node IDs must be
      specified. You can use any multicast address in your private network.
     </p></dd><dt id="id-1.4.3.6.3.3.7"><span class="term">Multicast port (<code class="systemitem">mcastport</code>)</span></dt><dd><p>
        The port to use for cluster communication. Corosync uses two ports: the specified
      <code class="literal">mcastport</code> for receiving multicast, and
      <code class="literal">mcastport -1</code> for sending multicast.
     </p></dd><dt id="vle-ha-rrp"><span class="term">Redundant Ring Protocol (RRP)</span></dt><dd><p>
       Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol. A logical token-passing ring is imposed on all
      participating nodes to deliver messages in a reliable and sorted
      manner. A node is allowed to broadcast a message only if it holds the
      token.
     </p><p>
      When having defined redundant communication channels in Corosync,
      use RRP to tell the cluster how to use these interfaces. RRP can have
      three modes (<code class="literal">rrp_mode</code>):
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If set to <code class="literal">active</code>, Corosync uses both
        interfaces actively. However, this mode is deprecated.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">passive</code>, Corosync sends messages
        alternatively over the available networks.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">none</code>, RRP is disabled.
       </p></li></ul></div></dd><dt id="id-1.4.3.6.3.3.9"><span class="term">Unicast</span></dt><dd><p>
        A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync, unicast
    is implemented as UDP-unicast (UDPU).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-setup-yast-overview" data-id-title="YaST Cluster module"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">YaST <span class="guimenu">Cluster</span> module</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-setup-yast-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Start YaST and select <span class="guimenu">High Availability</span> › <span class="guimenu">Cluster</span>. Alternatively, start the
    module from command line:
   </p><div class="verbatim-wrap"><pre class="screen">sudo yast2 cluster</pre></div><p>
   The following list shows an overview of the available screens in the
   YaST cluster module. It also mentions whether the screen contains parameters that
   are <span class="emphasis"><em>required</em></span> for successful cluster setup or whether its
   parameters are <span class="emphasis"><em>optional</em></span>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.6.4.5.1"><span class="term">Communication channels (required)</span></dt><dd><p> Allows you to define one or two communication channels for
      communication between the cluster nodes. As transport protocol,
      either use multicast (UDP) or unicast (UDPU). For details, see
      <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-channels" title="4.3. Defining the communication channels">Section 4.3, “Defining the communication channels”</a>.</p><div id="id-1.4.3.6.4.5.1.2.2" data-id-title="Redundant communication paths" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant communication paths</div><p>For a supported cluster setup two or more redundant communication
       paths are required. The preferred way is to use network device bonding as
       described in <a class="xref" href="cha-ha-netbonding.html" title="Chapter 16. Network device bonding">Chapter 16, <em>Network device bonding</em></a>.</p><p>If this is impossible, you need to define a second communication
       channel in Corosync.</p></div></dd><dt id="id-1.4.3.6.4.5.2"><span class="term">Security (optional but recommended)</span></dt><dd><p>Allows you to define the authentication settings for the cluster.
        HMAC/SHA1 authentication requires a shared secret used
        to protect and authenticate messages. For details, see
        <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-security" title="4.4. Defining authentication settings">Section 4.4, “Defining authentication settings”</a>.
       </p></dd><dt id="id-1.4.3.6.4.5.3"><span class="term">Configure Csync2 (optional but recommended)</span></dt><dd><p>
      Csync2 helps you to keep track of configuration changes and to
      keep files synchronized across the cluster nodes. For details, see
      <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
     </p></dd><dt id="id-1.4.3.6.4.5.4"><span class="term">Configure conntrackd (optional)</span></dt><dd><p>
          Allows you to configure the user space
          <code class="systemitem">conntrackd</code>. Use the conntrack
          tools for <span class="emphasis"><em>stateful</em></span> packet inspection for iptables.
          For details, see <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-conntrackd" title="4.5. Synchronizing connection status between cluster nodes">Section 4.5, “Synchronizing connection status between cluster nodes”</a>.
         </p></dd><dt id="id-1.4.3.6.4.5.5"><span class="term">Service (required)</span></dt><dd><p>
      Allows you to configure the service for bringing the cluster node online.
      Define whether to start the cluster services at boot time and whether to open the
      ports in the firewall that are needed for communication between the nodes.
      For details, see <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-services" title="4.6. Configuring services">Section 4.6, “Configuring services”</a>.
     </p></dd></dl></div><p>
    If you start the cluster module for the first time, it appears as a
    wizard, guiding you through all the steps necessary for basic setup.
    Otherwise, click the categories on the left panel to access the
    configuration options for each step.
   </p><div id="id-1.4.3.6.4.7" data-id-title="Settings in the YaST Cluster module" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings in the YaST <span class="guimenu">Cluster</span> module</div><p>Some settings in the YaST cluster module apply only to the
      current node. Other settings may automatically be transferred to all nodes
      with Csync2. Find detailed information about this in the following
      sections.
    </p></div></section><section class="sect1" id="sec-ha-installation-setup-channels" data-id-title="Defining the communication channels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Defining the communication channels</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-setup-channels">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For successful communication between the cluster nodes, define at least
    one communication channel. As transport protocol, either use multicast (UDP)
    or unicast (UDPU) as described in <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel1-udp" title="Defining the first communication channel (multicast)">Procedure 4.1</a>
    or <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel1-udpu" title="Defining the first communication channel (unicast)">Procedure 4.2</a>, respectively.
    If you want to define a second, redundant channel
    (<a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel2" title="Defining a redundant communication channel">Procedure 4.3</a>),
    both communication channels must use the <span class="emphasis"><em>same</em></span> protocol.
   </p><div id="id-1.4.3.6.5.3" data-id-title="Public clouds: use unicast" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Public clouds: use unicast</div><p>
     For deploying SUSE Linux Enterprise High Availability in public cloud platforms, use unicast as
     transport protocol. Multicast is generally not supported by the cloud
     platforms themselves.
    </p></div><p>All settings defined in the YaST
    <span class="guimenu">Communication Channels</span>
    screen are written to <code class="filename">/etc/corosync/corosync.conf</code>. Find example
    files for a multicast and a unicast setup in
    <code class="filename">/usr/share/doc/packages/corosync/</code>.
   </p><p>If you are using IPv4 addresses, node IDs are optional. If you are using
    IPv6 addresses, node IDs are required. Instead of specifying IDs manually
    for each node, the YaST cluster module contains an option to automatically
    generate a unique ID for every cluster node.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udp" data-id-title="Defining the first communication channel (multicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.1: </span><span class="title-name">Defining the first communication channel (multicast) </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel1-udp">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     When using multicast, the same <code class="systemitem">bindnetaddr</code>,
    <code class="systemitem">mcastaddr</code>, and <code class="systemitem">mcastport</code>
     will be used for all cluster nodes. All nodes in the cluster will know each
     other by using the same multicast address. For different clusters, use
     different multicast addresses.
    
    </p><ol class="procedure" type="1"><li class="step"><p>
       Start the YaST cluster module and switch to the <span class="guimenu">Communication
        Channels</span> category.
      </p></li><li class="step"><p>
      Set the <span class="guimenu">Transport</span> protocol to
      <code class="literal">Multicast</code>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Bind Network Address</span>. Set the value to
      the subnet you will use for cluster multicast.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Port</span>.
     </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="gl-heartb.html#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel2" title="Defining a redundant communication channel">Procedure 4.3, “Defining a redundant communication channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.5.7"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_comm_mcast.png"><img src="images/yast_cluster_comm_mcast.png" width="75%" alt="YaST Cluster—multicast configuration" title="YaST Cluster—multicast configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.1: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—multicast configuration </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.5.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>If you want to use unicast instead of multicast for cluster
   communication, proceed as follows.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udpu" data-id-title="Defining the first communication channel (unicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.2: </span><span class="title-name">Defining the first communication channel (unicast) </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel1-udpu">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Communication
       Channels</span> category.
     </p></li><li class="step"><p>
        Set the <span class="guimenu">Transport</span> protocol to
        <code class="literal">Unicast</code>.
       </p></li><li class="step"><p>
        Define the <span class="guimenu">Port</span>.
       </p></li><li class="step"><p>
        For unicast communication, Corosync needs to know the IP
        addresses of all nodes in the cluster. For each node that will be
        part of the cluster, click <span class="guimenu">Add</span> and enter the
        following details:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <span class="guimenu">IP Address</span>
         </p></li><li class="listitem"><p>
          <span class="guimenu">Redundant IP Address</span> (only required if you use
          a second communication channel in Corosync)
         </p></li><li class="listitem"><p>
          <span class="guimenu">Node ID</span> (only required if the option
          <span class="guimenu">Auto Generate Node ID</span> is disabled)
         </p></li></ul></div><p>
        To modify or remove any addresses of cluster members, use the
        <span class="guimenu">Edit</span> or <span class="guimenu">Del</span> buttons.
       </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="gl-heartb.html#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel2" title="Defining a redundant communication channel">Procedure 4.3, “Defining a redundant communication channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.5.10"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_comm_ucast.png"><img src="images/yast_cluster_comm_ucast.png" width="75%" alt="YaST Cluster—unicast configuration" title="YaST Cluster—unicast configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.2: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—unicast configuration </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.5.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>
    If network device bonding cannot be used for any reason, the second
    best choice is to define a redundant communication channel (a second
    ring) in Corosync. That way, two physically separate networks can
    be used for communication. If one network fails, the cluster nodes
    can still communicate via the other network.
   </p><p>The additional communication channel in
    Corosync will form a second token-passing ring. In
    <code class="filename">/etc/corosync/corosync.conf</code>, the first channel you
    configured is the primary ring and gets the ring number
   <code class="literal">0</code>. The second ring (redundant channel) gets the ring number
    <code class="literal">1</code>.
   </p><p>When having defined redundant communication channels in Corosync,
    use RRP to tell the cluster how to use these interfaces. With RRP, two
    physically separate networks are used for communication. If one
    network fails, the cluster nodes can still communicate via the other
    network.</p><p>RRP can have three modes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If set to <code class="literal">active</code>, Corosync uses both
      interfaces actively. However, this mode is deprecated.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">passive</code>, Corosync sends messages
      alternatively over the available networks.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">none</code>, RRP is disabled.
     </p></li></ul></div><div class="procedure" id="pro-ha-installation-setup-channel2" data-id-title="Defining a redundant communication channel"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.3: </span><span class="title-name">Defining a redundant communication channel </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-channel2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.4.3.6.5.16.2" data-id-title="Redundant rings and /etc/hosts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant rings and <code class="filename">/etc/hosts</code></div><p> If multiple rings are configured in Corosync, each node can
     have multiple IP addresses. This needs to be reflected in the
      <code class="filename">/etc/hosts</code> file of all nodes. </p></div><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Communication Channels</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Redundant Channel</span>. The redundant channel
     must use the same protocol as the first communication channel you defined.
    </p></li><li class="step"><p> If you use multicast, enter the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, the <span class="guimenu">Multicast
      Address</span> and the <span class="guimenu">Port</span> for the
     redundant channel. </p><p> If you use unicast, define the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, and the <span class="guimenu">Port</span>.
     Enter the IP addresses of all nodes that will be part of
     the cluster. </p></li><li class="step"><p>To tell Corosync how and when to use the different channels,
     select the <span class="guimenu">rrp_mode</span> to use:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> If only one communication channel is defined,
        <span class="guimenu">rrp_mode</span> is automatically disabled (value
        <code class="literal">none</code>).</p></li><li class="listitem"><p> If set to <code class="literal">active</code>, Corosync uses both
       interfaces actively. However, this mode is deprecated.</p></li><li class="listitem"><p> If set to <code class="literal">passive</code>, Corosync sends messages
       alternatively over the available networks. </p></li></ul></div><p>When RRP is used, SUSE Linux Enterprise High Availability monitors the status of the current
     rings and automatically re-enables redundant rings after faults.</p><p>Alternatively, check the ring status manually with
     <code class="command">corosync-cfgtool</code>. View the available options with
      <code class="option">-h</code>. </p></li><li class="step"><p> Confirm your changes. </p></li></ol></div></div></section><section class="sect1" id="sec-ha-installation-setup-security" data-id-title="Defining authentication settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Defining authentication settings</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-setup-security">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To define the authentication settings for the cluster, you can use HMAC/SHA1
    authentication. This requires a shared secret used
    to protect and authenticate messages. The authentication key (password)
    you specify will be used on all nodes in the cluster.
   </p><div class="procedure" id="pro-ha-installation-setup-security" data-id-title="Enabling secure authentication"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.4: </span><span class="title-name">Enabling secure authentication </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-security">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Security</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Enable Security Auth</span>. </p></li><li class="step"><p> For a newly created cluster, click <span class="guimenu">Generate Auth Key
      File</span>. An authentication key is created and written to
      <code class="filename">/etc/corosync/authkey</code>. </p><p> If you want the current machine to join an existing cluster, do not
     generate a new key file. Instead, copy the
      <code class="filename">/etc/corosync/authkey</code> from one of the nodes to the
     current machine (either manually or with Csync2). </p></li><li class="step"><p> Confirm your changes. YaST writes the configuration to
      <code class="filename">/etc/corosync/corosync.conf</code>. </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_security.png"><img src="images/yast_cluster_security.png" width="75%" alt="YaST Cluster—security" title="YaST Cluster—security"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.3: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—security </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-conntrackd" data-id-title="Synchronizing connection status between cluster nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">Synchronizing connection status between cluster nodes</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-setup-conntrackd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To enable <span class="emphasis"><em>stateful</em></span> packet inspection for iptables,
    configure and use the conntrack tools. This requires the following basic
    steps:
   </p><div class="procedure" id="pro-ha-installation-setup-conntrackd" data-id-title="Configuring the conntrackd with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.5: </span><span class="title-name">Configuring the <code class="systemitem">conntrackd</code> with YaST </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     Use the YaST cluster module to configure the user space
     <code class="systemitem">conntrackd</code> (see <a class="xref" href="cha-ha-ycluster.html#fig-ha-installation-setup-conntrackd" title="YaST Cluster—conntrackd">Figure 4.4, “YaST <span class="guimenu">Cluster</span>—<code class="systemitem">conntrackd</code>”</a>).  It needs a
     dedicated network interface that is not used for other communication
     channels. The daemon can be started via a resource agent afterward.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Configure
      conntrackd</span> category.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span> to be used for
      synchronizing the connection status.
     </p></li><li class="step"><p>
      In <span class="guimenu">Group Number</span>, define a numeric ID for the group
      to synchronize the connection status to.
      
     </p></li><li class="step"><p>
      Click <span class="guimenu">Generate /etc/conntrackd/conntrackd.conf</span> to
      create the configuration file for
      <code class="systemitem">conntrackd</code>.
     </p></li><li class="step"><p>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module.
     </p></li><li class="step"><p>
      For further cluster configuration, click <span class="guimenu">Next</span> and
      proceed with <a class="xref" href="cha-ha-ycluster.html#sec-ha-installation-setup-services" title="4.6. Configuring services">Section 4.6, “Configuring services”</a>.
     </p></li><li class="step"><p>
      Select a <span class="guimenu">Dedicated Interface</span> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in YaST. It must already be
      configured and it must support multicast.
      
     </p></li></ol></div></div><div class="figure" id="fig-ha-installation-setup-conntrackd"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_conntrackd.png"><img src="images/yast_cluster_conntrackd.png" width="75%" alt="YaST Cluster—conntrackd" title="YaST Cluster—conntrackd"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.4: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—<code class="systemitem">conntrackd</code> </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#fig-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>
    After having configured the conntrack tools, you can use them for Linux Virtual Server
    (see <a class="xref" href="cha-ha-lb.html" title="Chapter 17. Load balancing"><em>Load balancing</em></a>).
   </p></section><section class="sect1" id="sec-ha-installation-setup-services" data-id-title="Configuring services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">Configuring services</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-setup-services">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In the YaST cluster module define whether to start certain services
    on a node at boot time. You can also use the module to start and stop
    the services manually. To bring the cluster nodes online and start the
    cluster resource manager, Pacemaker must be running as a service.
   </p><div class="procedure" id="pro-ha-installation-setup-services" data-id-title="Enabling the cluster services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.6: </span><span class="title-name">Enabling the cluster services </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-services">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      In the YaST cluster module, switch to the
      <span class="guimenu">Service</span> category.
     </p></li><li class="step"><p>
      To start the cluster services each time this cluster node is booted, select the
      respective option in the <span class="guimenu">Booting</span> group. If you
      select <span class="guimenu">Off</span> in the <span class="guimenu">Booting</span> group,
      you must start the cluster services manually each time this node is booted. To
      start the cluster services manually, use the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      To start or stop the cluster services immediately, click the respective button.
     </p></li><li class="step"><p>
      To open the ports in the firewall that are needed for cluster
      communication on the current machine, activate <span class="guimenu">Open Port in
      Firewall</span>.
     </p></li><li class="step"><p>
      Confirm your changes. Note that the configuration only
      applies to the current machine, not to all cluster nodes.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_services.png"><img src="images/yast_cluster_services.png" width="75%" alt="YaST Cluster—services" title="YaST Cluster—services"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.5: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—services </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-csync2" data-id-title="Transferring the configuration to all nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.7 </span><span class="title-name">Transferring the configuration to all nodes</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-setup-csync2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Instead of copying the resulting configuration files to all nodes
    manually, use the <code class="command">csync2</code> tool for replication across
    all nodes in the cluster.
   </p><p>
    This requires the following basic steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Configuring Csync2 with YaST</a>.
     </p></li><li class="step"><p>
      <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-start" title="Synchronizing the configuration files with Csync2">Synchronizing the configuration files with Csync2</a>.
     </p></li></ol></div></div><p>
    Csync2 helps you to keep track of configuration changes and to keep
    files synchronized across the cluster nodes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can define a list of files that are important for operation.
     </p></li><li class="listitem"><p>
      You can show changes to these files (against the other cluster nodes).
     </p></li><li class="listitem"><p>
      You can synchronize the configured files with a single command.
     </p></li><li class="listitem"><p>
      With a simple shell script in <code class="filename">~/.bash_logout</code>, you
      can be reminded about unsynchronized changes before logging out of the
      system.
     </p></li></ul></div><p>
    Find detailed information about Csync2 at
    <a class="link" href="http://oss.linbit.com/csync2/" target="_blank">http://oss.linbit.com/csync2/</a> and
    <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>.
   </p><section class="sect2" id="pro-ha-installation-setup-csync2-yast" data-id-title="Configuring Csync2 with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.1 </span><span class="title-name">Configuring Csync2 with YaST</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.3.6.9.8.2" data-id-title="Configuring Csync2 with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.7: </span><span class="title-name">Configuring Csync2 with YaST </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.9.8.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
       <span class="guimenu">Csync2</span> category. </p></li><li class="step"><p> To specify the synchronization group, click <span class="guimenu">Add</span>
      in the <span class="guimenu">Sync Host</span> group and enter the local host names
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <code class="command">hostname</code> command. </p><div id="id-1.4.3.6.9.8.2.3.2" data-id-title="Host name resolution" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Host name resolution</div><p> If host name resolution does not work properly in your
       network, you can also specify a combination of host name and IP address
       for each cluster node. To do so, use the string
        <em class="replaceable">HOSTNAME@IP</em> such as
        <code class="literal">alice@192.168.2.100</code>, for example. Csync2
       will then use the IP addresses when connecting. </p></div></li><li class="step" id="step-csync2-generate-key"><p> Click <span class="guimenu">Generate Pre-Shared-Keys</span> to create a key
      file for the synchronization group. The key file is written to
       <code class="filename">/etc/csync2/key_hagroup</code>. After it has been created,
      it must be copied manually to all members of the cluster. </p></li><li class="step"><p> To populate the <span class="guimenu">Sync File</span> list with the files
      that usually need to be synchronized among all nodes, click <span class="guimenu">Add
       Suggested Files</span>. </p></li><li class="step"><p> To <span class="guimenu">Edit</span>, <span class="guimenu">Add</span> or
       <span class="guimenu">Remove</span> files from the list of files to be synchronized
      use the respective buttons. You must enter the absolute path name for each
      file. </p></li><li class="step"><p> Activate Csync2 by clicking <span class="guimenu">Turn Csync2
       ON</span>. This will execute the following command to start
      Csync2 automatically at boot time: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable csync2.socket</pre></div></li><li class="step"><p>Click <span class="guimenu">Finish</span>. YaST writes the Csync2
      configuration to <code class="filename">/etc/csync2/csync2.cfg</code>.</p></li></ol></div></div><div class="figure" id="id-1.4.3.6.9.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_sync.png"><img src="images/yast_cluster_sync.png" width="75%" alt="YaST Cluster—Csync2" title="YaST Cluster—Csync2"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.6: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—Csync2 </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.9.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-setup-yast-csync2-sync" data-id-title="Synchronizing changes with Csync2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.2 </span><span class="title-name">Synchronizing changes with Csync2</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-setup-yast-csync2-sync">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p> Before running Csync2 for the first time, you need to make the
    following preparations: </p><div class="procedure" id="id-1.4.3.6.9.9.3" data-id-title="Preparing for initial synchronization with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.8: </span><span class="title-name">Preparing for initial synchronization with Csync2 </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.9.9.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Copy the file <code class="filename">/etc/csync2/csync2.cfg</code>
      manually to all nodes after you have configured it as described in <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1, “Configuring Csync2 with YaST”</a>. </p></li><li class="step"><p> Copy the file <code class="filename">/etc/csync2/key_hagroup</code> that you
      have generated on one node in <a class="xref" href="cha-ha-ycluster.html#step-csync2-generate-key" title="Step 3">Step 3</a>
      of <a class="xref" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1</a>
      to <span class="emphasis"><em>all</em></span> nodes in the cluster. It is needed for
      authentication by Csync2. However, do <span class="emphasis"><em>not</em></span>
      regenerate the file on the other nodes—it needs to be the same
      file on all nodes. </p></li><li class="step"><p>Execute the following command on all nodes to start the service now: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start csync2.socket</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-installation-setup-csync2-start" data-id-title="Synchronizing the configuration files with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.9: </span><span class="title-name">Synchronizing the configuration files with Csync2 </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#pro-ha-installation-setup-csync2-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>To initially synchronize all files once, execute the following
      command on the machine that you want to copy the configuration
       <span class="emphasis"><em>from</em></span>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p> This will synchronize all the files once by pushing them to the
      other nodes. If all files are synchronized successfully, Csync2 will
      finish with no errors. </p><p> If one or several files that are to be synchronized have been
      modified on other nodes (not only on the current one), Csync2
      reports a conflict. You will get an output similar to the one below: </p><div class="verbatim-wrap"><pre class="screen">While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</pre></div></li><li class="step"><p> If you are sure that the file version on the current node is the
       <span class="quote">“<span class="quote">best</span>”</span> one, you can resolve the conflict by forcing this
      file and resynchronizing: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-f</code> /etc/corosync/corosync.conf
<code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-x</code></pre></div></li></ol></div></div><p> For more information on the Csync2 options, run</p><div class="verbatim-wrap"><pre class="screen">csync2 -help</pre></div><div id="id-1.4.3.6.9.9.7" data-id-title="Pushing synchronization after any changes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Pushing synchronization after any changes</div><p> Csync2 only pushes changes. It does <span class="emphasis"><em>not</em></span>
     continuously synchronize files between the machines. </p><p> Each time you update files that need to be synchronized, you need to
     push the changes to the other machines: Run <code class="command">csync2 </code>
     <code class="option">-xv</code> on the machine where you did the changes. If you run
     the command on any of the other machines with unchanged files, nothing will
     happen. </p></div></section></section><section class="sect1" id="sec-ha-installation-start" data-id-title="Bringing the cluster online node by node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.8 </span><span class="title-name">Bringing the cluster online node by node</span></span> <a title="Permalink" class="permalink" href="cha-ha-ycluster.html#sec-ha-installation-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After the initial cluster configuration is done, start the cluster
    services on <span class="emphasis"><em>each</em></span> cluster node to bring the stack
    online:
   </p><div class="procedure" id="id-1.4.3.6.10.3" data-id-title="Starting cluster services and checking the status"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.10: </span><span class="title-name">Starting cluster services and checking the status </span></span><a title="Permalink" class="permalink" href="cha-ha-ycluster.html#id-1.4.3.6.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to an existing node.
     </p></li><li class="step"><p>
      Check if the service is already running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster status</pre></div><p>
      If not, start the cluster services now:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      Repeat the steps above for each of the cluster nodes.
     </p></li><li class="step"><p>
      On one of the nodes, check the cluster status with the
      <code class="command">crm status</code> command. If all nodes are
      online, the output should be similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status
Cluster Summary:
  * Stack: corosync
  * Current DC: alice (version ...) - partition with quorum
  * Last updated: ...
  * Last change:  ... by hacluster via crmd on bob
  * 2 nodes configured
  * 1 resource instance configured

Node List:
  * Online: [ alice bob ]
...</pre></div><p>
      This output indicates that the cluster resource manager is started and
      is ready to manage resources.
     </p></li></ol></div></div><p>
    After the basic configuration is done and the nodes are online, you can
    start to configure cluster resources. Use one of the cluster management
    tools like the crm shell (crmsh) or Hawk2. For more
    information, see <a class="xref" href="cha-ha-config-basics.html#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a> or
    <a class="xref" href="cha-ha-config-basics.html#cha-conf-hawk2" title="5.4. Introduction to Hawk2">Section 5.4, “Introduction to Hawk2”</a>.
   </p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ha-install.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 3 </span>Installing SUSE Linux Enterprise High Availability</span></a> </div><div><a class="pagination-link next" href="part-config.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Part II </span>Configuration and administration</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-terms"><span class="title-number">4.1 </span><span class="title-name">Definition of terms</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-setup-yast-overview"><span class="title-number">4.2 </span><span class="title-name">YaST <span class="guimenu">Cluster</span> module</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-setup-channels"><span class="title-number">4.3 </span><span class="title-name">Defining the communication channels</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-setup-security"><span class="title-number">4.4 </span><span class="title-name">Defining authentication settings</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-setup-conntrackd"><span class="title-number">4.5 </span><span class="title-name">Synchronizing connection status between cluster nodes</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-setup-services"><span class="title-number">4.6 </span><span class="title-name">Configuring services</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-setup-csync2"><span class="title-number">4.7 </span><span class="title-name">Transferring the configuration to all nodes</span></a></span></li><li><span class="sect1"><a href="cha-ha-ycluster.html#sec-ha-installation-start"><span class="title-number">4.8 </span><span class="title-name">Bringing the cluster online node by node</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>