<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 15 SP4 | Administration Guide | Glossary</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Glossary | SLE HA 15 SP4"/>
<meta name="description" content="A concept of how services are running on nodes. An act…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="15 SP4"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Glossary"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="PUBLIC SUSE Linux Enterprise High Availability Extension 15 SP4"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Glossary | SLE HA 15 SP4"/>
<meta property="og:description" content="A concept of how services are running on nodes. An active-passive scenario means that one or more services are running on th…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Glossary | SLE HA 15 SP4"/>
<meta name="twitter:description" content="A concept of how services are running on nodes. An active-passive scenario means that one or more services are running on th…"/>
<link rel="prev" href="app-crmreport-nonroot.html" title="Appendix D. Running cluster reports without root access"/><link rel="next" href="bk02ape.html" title="Appendix E. GNU licenses"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_glossary.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="gl-heartb.html">Glossary</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="pre-ha.html" class=" "><span class="title-number"> </span><span class="title-name">Preface</span></a></li><li><a href="part-install.html" class="has-children "><span class="title-number">I </span><span class="title-name">Installation and setup</span></a><ol><li><a href="cha-ha-concepts.html" class=" "><span class="title-number">1 </span><span class="title-name">Product overview</span></a></li><li><a href="cha-ha-requirements.html" class=" "><span class="title-number">2 </span><span class="title-name">System requirements and recommendations</span></a></li><li><a href="cha-ha-install.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></li><li><a href="cha-ha-ycluster.html" class=" "><span class="title-number">4 </span><span class="title-name">Using the YaST cluster module</span></a></li></ol></li><li><a href="part-config.html" class="has-children "><span class="title-number">II </span><span class="title-name">Configuration and administration</span></a><ol><li><a href="cha-ha-config-basics.html" class=" "><span class="title-number">5 </span><span class="title-name">Configuration and administration basics</span></a></li><li><a href="sec-ha-config-basics-resources.html" class=" "><span class="title-number">6 </span><span class="title-name">Configuring cluster resources</span></a></li><li><a href="sec-ha-config-basics-constraints.html" class=" "><span class="title-number">7 </span><span class="title-name">Configuring resource constraints</span></a></li><li><a href="cha-ha-manage-resources.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing cluster resources</span></a></li><li><a href="sec-ha-config-basics-remote.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing services on remote hosts</span></a></li><li><a href="cha-ha-agents.html" class=" "><span class="title-number">10 </span><span class="title-name">Adding or modifying resource agents</span></a></li><li><a href="cha-ha-monitor-clusters.html" class=" "><span class="title-number">11 </span><span class="title-name">Monitoring clusters</span></a></li><li><a href="cha-ha-fencing.html" class=" "><span class="title-number">12 </span><span class="title-name">Fencing and STONITH</span></a></li><li><a href="cha-ha-storage-protect.html" class=" "><span class="title-number">13 </span><span class="title-name">Storage protection and SBD</span></a></li><li><a href="cha-ha-qdevice.html" class=" "><span class="title-number">14 </span><span class="title-name">QDevice and QNetd</span></a></li><li><a href="cha-ha-acl.html" class=" "><span class="title-number">15 </span><span class="title-name">Access control lists</span></a></li><li><a href="cha-ha-netbonding.html" class=" "><span class="title-number">16 </span><span class="title-name">Network device bonding</span></a></li><li><a href="cha-ha-lb.html" class=" "><span class="title-number">17 </span><span class="title-name">Load balancing</span></a></li><li><a href="cha-ha-virtualization.html" class=" "><span class="title-number">18 </span><span class="title-name">High Availability for virtualization</span></a></li><li><a href="cha-ha-geo.html" class=" "><span class="title-number">19 </span><span class="title-name">Geo clusters (multi-site clusters)</span></a></li></ol></li><li><a href="part-storage.html" class="has-children "><span class="title-number">III </span><span class="title-name">Storage and data replication</span></a><ol><li><a href="cha-ha-storage-dlm.html" class=" "><span class="title-number">20 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></li><li><a href="cha-ha-ocfs2.html" class=" "><span class="title-number">21 </span><span class="title-name">OCFS2</span></a></li><li><a href="cha-ha-gfs2.html" class=" "><span class="title-number">22 </span><span class="title-name">GFS2</span></a></li><li><a href="cha-ha-drbd.html" class=" "><span class="title-number">23 </span><span class="title-name">DRBD</span></a></li><li><a href="cha-ha-clvm.html" class=" "><span class="title-number">24 </span><span class="title-name">Cluster logical volume manager (Cluster LVM)</span></a></li><li><a href="cha-ha-cluster-md.html" class=" "><span class="title-number">25 </span><span class="title-name">Cluster multi-device (Cluster MD)</span></a></li><li><a href="cha-ha-samba.html" class=" "><span class="title-number">26 </span><span class="title-name">Samba clustering</span></a></li><li><a href="cha-ha-rear.html" class=" "><span class="title-number">27 </span><span class="title-name">Disaster recovery with ReaR (Relax-and-Recover)</span></a></li></ol></li><li><a href="part-maintenance.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Maintenance and upgrade</span></a><ol><li><a href="cha-ha-maintenance.html" class=" "><span class="title-number">28 </span><span class="title-name">Executing maintenance tasks</span></a></li><li><a href="cha-ha-migration.html" class=" "><span class="title-number">29 </span><span class="title-name">Upgrading your cluster and updating software packages</span></a></li></ol></li><li><a href="part-appendix.html" class="has-children "><span class="title-number">V </span><span class="title-name">Appendix</span></a><ol><li><a href="app-ha-troubleshooting.html" class=" "><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></li><li><a href="app-naming.html" class=" "><span class="title-number">B </span><span class="title-name">Naming conventions</span></a></li><li><a href="app-ha-management.html" class=" "><span class="title-number">C </span><span class="title-name">Cluster management tools (command line)</span></a></li><li><a href="app-crmreport-nonroot.html" class=" "><span class="title-number">D </span><span class="title-name">Running cluster reports without <code class="systemitem">root</code> access</span></a></li></ol></li><li><a href="gl-heartb.html" class=" you-are-here"><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="bk02ape.html" class=" "><span class="title-number">E </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="glossary"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">15 SP4</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Glossary</span></span> <a title="Permalink" class="permalink" href="gl-heartb.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/main/xml/ha_glossary.xml" title="Edit source document"> </a></div></div></div></div></div><div class="line"/><dl><dt id="id-1.4.8.3"><span><span class="glossterm">active/active, active/passive</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.3">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time. For
    example, it has <span class="emphasis"><em>some</em></span> services running, but can take
    over other services from the other node. Compare with primary/secondary
    and dual-primary in DRBD speak.
   </p></dd><dt id="id-1.4.8.4"><span><span class="glossterm">arbitrator</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.4">#</a></span></dt><dd class="glossdef"><p>
    Additional instance in a Geo cluster that helps to reach consensus
    about decisions such as failover of resources across sites. Arbitrators
    are single machines that run one or more booth instances in a special
    mode.
   </p></dd><dt id="id-1.4.8.5"><span><span class="glossterm">AutoYaST</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.5">#</a></span></dt><dd class="glossdef"><p>
    AutoYaST is a system for installing one or more SUSE Linux Enterprise systems automatically
    and without user intervention. 
   </p></dd><dt id="id-1.4.8.6"><span><span class="glossterm">bindnetaddr (bind network address)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.6">#</a></span></dt><dd class="glossdef"><p>
    The network address the Corosync executive should bind to. 
   </p></dd><dt id="glos-booth"><span><span class="glossterm">booth</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glos-booth">#</a></span></dt><dd class="glossdef"><p>
    The instance that manages the failover process between the sites of a
    Geo cluster. It aims to get multi-site resources active on one and
    only one site. This is achieved by using so-called tickets that are
    treated as failover domain between cluster sites, in case a site should
    be down.
   </p></dd><dt id="id-1.4.8.8"><span><span class="glossterm">boothd (booth
  daemon)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.8">#</a></span></dt><dd class="glossdef"><p>
    Each of the participating clusters and arbitrators in a Geo cluster
    runs a service, the <code class="systemitem">boothd</code>. It
    connects to the booth daemons running at the other sites and exchanges
    connectivity details.
   </p></dd><dt id="id-1.4.8.9"><span><span class="glossterm">CCM (consensus cluster membership)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.9">#</a></span></dt><dd class="glossdef"><p>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </p></dd><dt id="id-1.4.8.10"><span><span class="glossterm">CIB (cluster information base)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.10">#</a></span></dt><dd class="glossdef"><p>
    A representation of the whole cluster configuration and status (cluster
    options, nodes, resources, constraints and the relationship to each
    other). It is written in XML and resides in memory. A primary CIB is kept
    and maintained on the
    <a class="xref" href="gl-heartb.html#glos-dc" title="DC (designated coordinator)">DC (designated coordinator)</a> and replicated to
    the other nodes. Normal read and write operations on the CIB are
    serialized through the primary CIB.
   </p></dd><dt id="id-1.4.8.11"><span><span class="glossterm">cluster</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.11">#</a></span></dt><dd class="glossdef"><p>
    A <span class="emphasis"><em>high-performance</em></span> cluster is a group of computers
    (real or virtual) sharing the application load to achieve faster
    results. A <span class="emphasis"><em>high-availability</em></span> cluster is designed
    primarily to secure the highest possible availability of services.
   </p></dd><dt id="id-1.4.8.12"><span><span class="glossterm">cluster partition</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.12">#</a></span></dt><dd class="glossdef"><p>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    split brain scenario develops (see also
    <a class="xref" href="gl-heartb.html#glos-splitbrain" title="split brain">split brain</a>).
   </p></dd><dt id="id-1.4.8.13"><span><span class="glossterm">cluster site</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.13">#</a></span></dt><dd class="glossdef"><p>
    In Geo clustering, a cluster site (or just <span class="quote">“<span class="quote">site</span>”</span>) is a group of
    nodes in the same physical location, managed by <a class="xref" href="gl-heartb.html#glos-booth" title="booth">booth</a>.
   </p></dd><dt id="id-1.4.8.14"><span><span class="glossterm">cluster stack</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.14">#</a></span></dt><dd class="glossdef"><p>
    The ensemble of software technologies and components that compose a cluster.
   </p></dd><dt id="id-1.4.8.15"><span><span class="glossterm">concurrency violation</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.15">#</a></span></dt><dd class="glossdef"><p>
    A resource that should be running on only one node in the cluster is
    running on several nodes.
   </p></dd><dt id="id-1.4.8.16"><span><span class="glossterm">conntrack tools</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.16">#</a></span></dt><dd class="glossdef"><p>
    Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes.
   </p></dd><dt id="id-1.4.8.17"><span><span class="glossterm">CRM (cluster resource manager)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.17">#</a></span></dt><dd class="glossdef"><p>
    The management entity responsible for coordinating all non-local
    interactions in a High Availability cluster. SUSE Linux Enterprise High Availability uses Pacemaker as CRM.
    The CRM is implemented as <code class="systemitem">pacemaker-controld</code>. It interacts with several
    components: local resource managers, both on its own node and on the other nodes,
    non-local CRMs, administrative commands, the fencing functionality, and the membership
    layer.
   </p></dd><dt id="id-1.4.8.18"><span><span class="glossterm">crmsh</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.18">#</a></span></dt><dd class="glossdef"><p>
    The command line utility crmsh manages your cluster, nodes, and
    resources.
   </p><p>
    See <a class="xref" href="cha-ha-config-basics.html#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a> for more information.
   </p></dd><dt id="id-1.4.8.19"><span><span class="glossterm">Csync2</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.19">#</a></span></dt><dd class="glossdef"><p>
    A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters.
   </p></dd><dt id="glos-dc"><span><span class="glossterm">DC (designated coordinator)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glos-dc">#</a></span></dt><dd class="glossdef"><p>
    The DC is elected from all nodes in the cluster. This happens if there
    is no DC yet or if the current DC leaves the cluster for any reason.
    The DC is the only entity
    in the cluster that can decide that a cluster-wide change needs to
    be performed, such as fencing a node or moving resources around. All
    other nodes get their configuration and resource allocation
    information from the current DC.
   </p></dd><dt id="id-1.4.8.21"><span><span class="glossterm">Disaster</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.21">#</a></span></dt><dd class="glossdef"><p>
    Unexpected interruption of critical infrastructure induced by nature,
    humans, hardware failure, or software bugs.
   </p></dd><dt id="id-1.4.8.23"><span><span class="glossterm">Disaster Recover Plan</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.23">#</a></span></dt><dd class="glossdef"><p>
    A strategy to recover from a disaster with minimum impact on IT
    infrastructure.
   </p></dd><dt id="id-1.4.8.22"><span><span class="glossterm">Disaster Recovery</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.22">#</a></span></dt><dd class="glossdef"><p>
    Disaster recovery is the process by which a business function is
    restored to the normal, steady state after a disaster.
   </p></dd><dt id="id-1.4.8.24"><span><span class="glossterm">DLM (distributed lock manager)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.24">#</a></span></dt><dd class="glossdef"><p>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </p></dd><dt id="id-1.4.8.25"><span><span class="glossterm">DRBD</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.25">#</a></span></dt><dd class="glossdef"><p>
    <span class="trademark">DRBD</span>® is a block device
    designed for building high availability clusters. The whole block device
    is mirrored via a dedicated network and is seen as a network RAID-1.
   </p></dd><dt id="id-1.4.8.26"><span><span class="glossterm">existing cluster</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.26">#</a></span></dt><dd class="glossdef"><p>
      The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
   </p></dd><dt id="glo-failover"><span><span class="glossterm">failover</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glo-failover">#</a></span></dt><dd class="glossdef"><p>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </p></dd><dt id="id-1.4.8.28"><span><span class="glossterm">failover domain</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.28">#</a></span></dt><dd class="glossdef"><p>
    A named subset of cluster nodes that are eligible to run a cluster
    service if a node fails.
   </p></dd><dt id="id-1.4.8.29"><span><span class="glossterm">fencing</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.29">#</a></span></dt><dd class="glossdef"><p>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. There are two classes of fencing:
    resource level fencing and node level fencing. Resource level fencing ensures
    exclusive access to a given resource. Node level fencing prevents a failed
    node from accessing shared resources entirely and prevents resources from running
    on a node whose status is uncertain. This is usually done in a simple and
    abrupt way: reset or power off the node.
   </p></dd><dt id="id-1.4.8.30"><span><span class="glossterm">Geo cluster (geographically dispersed cluster)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.30">#</a></span></dt><dd class="glossdef"><p>
    Consists of multiple, geographically dispersed sites with a local cluster
    each. The sites communicate via IP. Failover across the sites is
    coordinated by a higher-level entity, the booth. Geo clusters need
    to cope with limited network bandwidth and high latency. Storage is
    replicated asynchronously.
   </p></dd><dt id="glos-lb"><span><span class="glossterm">load balancing</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glos-lb">#</a></span></dt><dd class="glossdef"><p>
    The ability to make several servers participate in the same service and
    do the same work.
   </p></dd><dt id="id-1.4.8.32"><span><span class="glossterm">local cluster</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.32">#</a></span></dt><dd class="glossdef"><p>
    A single cluster in one location (for example, all nodes are located in
    one data center). Network latency can be neglected. Storage is typically
    accessed synchronously by all nodes.
   </p></dd><dt id="id-1.4.8.33"><span><span class="glossterm">location</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.33">#</a></span></dt><dd class="glossdef"><p>
    In the context of a <span class="emphasis"><em>location constraint</em></span>, <span class="quote">“<span class="quote">location</span>”</span>
    refers to the nodes on which a resource can or cannot run.
   </p></dd><dt id="id-1.4.8.34"><span><span class="glossterm">LRM (local resource manager)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.34">#</a></span></dt><dd class="glossdef"><p>
    The local resource manager is located between the Pacemaker layer and the
    resources layer on each node. It is implemented as <code class="systemitem">pacemaker-execd</code> daemon. Through this daemon,
    Pacemaker can start, stop, and monitor resources.
   </p></dd><dt id="id-1.4.8.35"><span><span class="glossterm">mcastaddr (multicast address)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.35">#</a></span></dt><dd class="glossdef"><p>
      IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6. 
   </p></dd><dt id="id-1.4.8.36"><span><span class="glossterm">mcastport (multicast port)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.36">#</a></span></dt><dd class="glossdef"><p>
       The port to use for cluster communication.
   </p></dd><dt id="id-1.4.8.37"><span><span class="glossterm">metro cluster</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.37">#</a></span></dt><dd class="glossdef"><p>
    A single cluster that can stretch over multiple buildings or data
    centers, with all sites connected by fibre channel. Network latency is
    usually low (&lt;5 ms for distances of approximately
    20 miles). Storage is frequently replicated (mirroring or
    synchronous replication).
   </p></dd><dt id="id-1.4.8.38"><span><span class="glossterm">multicast</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.38">#</a></span></dt><dd class="glossdef"><p>
      A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
   </p></dd><dt id="id-1.4.8.39"><span><span class="glossterm">node</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.39">#</a></span></dt><dd class="glossdef"><p>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </p></dd><dt id="id-1.4.8.40"><span><span class="glossterm">pacemaker-controld (cluster controller daemon)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.40">#</a></span></dt><dd class="glossdef"><p>
    The CRM is implemented as daemon, pacemaker-controld. It has an instance on each
    cluster node. All cluster decision-making is centralized by electing one
    of the pacemaker-controld instances to act as a primary. If the elected pacemaker-controld process
    fails (or the node it ran on), a new one is established.
   </p></dd><dt id="id-1.4.8.41"><span><span class="glossterm">PE (policy engine)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.41">#</a></span></dt><dd class="glossdef"><p>
    The policy engine is implemented as
    <code class="systemitem">pacemaker-schedulerd</code> daemon.
    When a cluster transition is needed, based on the current state and
    configuration, <code class="systemitem">pacemaker-schedulerd</code>
    calculates the expected next state of the cluster. It determines what
    actions need to be scheduled to achieve the next state.
   </p></dd><dt id="gloss-quorum"><span><span class="glossterm">quorum</span> <a title="Permalink" class="permalink" href="gl-heartb.html#gloss-quorum">#</a></span></dt><dd class="glossdef"><p>
    In a cluster, a cluster partition is defined to have quorum (be
    <span class="quote">“<span class="quote">quorate</span>”</span>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </p></dd><dt id="id-1.4.8.43"><span><span class="glossterm">RA (resource agent)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.43">#</a></span></dt><dd class="glossdef"><p>
    A script acting as a proxy to manage a resource (for example, to start,
    stop, or monitor a resource). SUSE Linux Enterprise High Availability supports different
    kinds of resource agents. For details, see
    <a class="xref" href="sec-ha-config-basics-resources.html#sec-ha-config-basics-raclasses" title="6.2. Supported resource agent classes">Section 6.2, “Supported resource agent classes”</a>.
   </p></dd><dt id="id-1.4.8.44"><span><span class="glossterm">ReaR (Relax and Recover)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.44">#</a></span></dt><dd class="glossdef"><p>
    An administrator tool set for creating disaster recovery images.
   </p></dd><dt id="id-1.4.8.45"><span><span class="glossterm">resource</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.45">#</a></span></dt><dd class="glossdef"><p>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </p><p>
    The term <span class="quote">“<span class="quote">resource</span>”</span> is also used for DRBD, where it names a
    set of block devices that are using a common connection for replication.
   </p></dd><dt id="id-1.4.8.46"><span><span class="glossterm">RRP (redundant ring protocol)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.46">#</a></span></dt><dd class="glossdef"><p>
     Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol.
   </p></dd><dt id="id-1.4.8.47"><span><span class="glossterm">SBD (STONITH Block Device)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.47">#</a></span></dt><dd class="glossdef"><p>
    Provides a node fencing mechanism through the exchange of messages via shared
    block storage (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode.
    Needs a hardware or software watchdog on each node to ensure that misbehaving
    nodes are really stopped.
   </p></dd><dt id="id-1.4.8.48"><span><span class="glossterm">SFEX (shared disk file exclusiveness)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.48">#</a></span></dt><dd class="glossdef"><p>
    SFEX provides storage protection over SAN.
   </p></dd><dt id="glos-splitbrain"><span><span class="glossterm">split brain</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glos-splitbrain">#</a></span></dt><dd class="glossdef"><p>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). STONITH prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <span class="quote">“<span class="quote">partitioned
    cluster</span>”</span> scenario.
   </p><p>
    The term split brain is also used in DRBD but means that the two nodes
    contain different data.
   </p></dd><dt id="id-1.4.8.50"><span><span class="glossterm">SPOF (single point of failure)</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.50">#</a></span></dt><dd class="glossdef"><p>
    Any component of a cluster that, should it fail, triggers the failure of
    the entire cluster.
   </p></dd><dt id="glo-stonith"><span><span class="glossterm">STONITH</span> <a title="Permalink" class="permalink" href="gl-heartb.html#glo-stonith">#</a></span></dt><dd class="glossdef"><p>
    The acronym for <span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>. It refers
    to the fencing mechanism that shuts down a misbehaving node to prevent it
    from causing trouble in a cluster. In a Pacemaker cluster, the implementation
    of node level fencing is STONITH. For this, Pacemaker comes with a fencing
    subsystem, <code class="systemitem">pacemaker-fenced</code>.
   </p></dd><dt id="id-1.4.8.52"><span><span class="glossterm">switchover</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.52">#</a></span></dt><dd class="glossdef"><p>
    Planned, on-demand moving of services to other nodes in a cluster. See
    <a class="xref" href="gl-heartb.html#glo-failover" title="failover">failover</a>.
   </p></dd><dt id="id-1.4.8.53"><span><span class="glossterm">ticket</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.53">#</a></span></dt><dd class="glossdef"><p>
    A component used in Geo clusters. A ticket grants the right to run
    certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by
    dependencies. Only if the defined ticket is available at a site, the
    respective resources are started. Vice versa, if the ticket is removed,
    the resources depending on that ticket are automatically stopped.
   </p></dd><dt id="id-1.4.8.54"><span><span class="glossterm">unicast</span> <a title="Permalink" class="permalink" href="gl-heartb.html#id-1.4.8.54">#</a></span></dt><dd class="glossdef"><p>
    A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync,
    unicast is implemented as UDP-unicast (UDPU).
   </p></dd></dl></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="app-crmreport-nonroot.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Appendix D </span>Running cluster reports without <code class="systemitem">root</code> access</span></a> </div><div><a class="pagination-link next" href="bk02ape.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Appendix E </span>GNU licenses</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>