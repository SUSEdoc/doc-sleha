<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLE HA 12 SP4 | Administration Guide</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Administration Guide | SLE HA 12 SP4"/>
<meta name="description" content="This guide is intended for administrators who need to …"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="12 SP4"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 12 SP4"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Administration Guide | SLE HA 12 SP4"/>
<meta property="og:description" content="This guide is intended for administrators who need to set up, configure, and maintain clusters with SUSE® Linux Enterprise H…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration Guide | SLE HA 12 SP4"/>
<meta name="twitter:description" content="This guide is intended for administrators who need to set up, configure, and maintain clusters with SUSE® Linux Enterprise H…"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-sleha">Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-sleha" data-id-title="Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">12 SP4</span></div><div class="title-container"><h1 class="title">Administration Guide</h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div><div class="abstract"><p> This guide is intended for administrators who need to set up, configure,
  and maintain clusters with SUSE® Linux Enterprise High Availability. For quick and efficient
  configuration and administration, SUSE Linux Enterprise High Availability includes both a graphical user
  interface (GUI) and a command line interface (CLI). For performing key tasks,
  both approaches (GUI and CLI) are covered in detail in this guide. Thus,
  administrators can choose the appropriate tool that matches their needs.</p></div><div class="date"><span class="imprint-label">Publication Date: </span>
        November 08, 2023

      </div></div></div><div class="toc"><ul><li><span class="preface"><a href="#pre-ha"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.2.7"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.8"><span class="title-name">Improving the documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.9"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.10"><span class="title-name">Support</span></a></span></li></ul></li><li><span class="part"><a href="#part-install"><span class="title-number">I </span><span class="title-name">Installation and Setup</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-concepts"><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-availability"><span class="title-number">1.1 </span><span class="title-name">Availability as a Module or Extension</span></a></span></li><li><span class="sect1"><a href="#sec-ha-features"><span class="title-number">1.2 </span><span class="title-name">Key Features</span></a></span></li><li><span class="sect1"><a href="#sec-ha-benefits"><span class="title-number">1.3 </span><span class="title-name">Benefits</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clusterconfig"><span class="title-number">1.4 </span><span class="title-name">Cluster Configurations: Storage</span></a></span></li><li><span class="sect1"><a href="#sec-ha-architecture"><span class="title-number">1.5 </span><span class="title-name">Architecture</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-requirements"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-requirements-hw"><span class="title-number">2.1 </span><span class="title-name">Hardware Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-sw"><span class="title-number">2.2 </span><span class="title-name">Software Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-disk"><span class="title-number">2.3 </span><span class="title-name">Storage Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-other"><span class="title-number">2.4 </span><span class="title-name">Other Requirements and Recommendations</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-install"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-install-manual"><span class="title-number">3.1 </span><span class="title-name">Manual Installation</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-autoyast"><span class="title-number">3.2 </span><span class="title-name">Mass Installation and Deployment with AutoYaST</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-setup"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-installation-terms"><span class="title-number">4.1 </span><span class="title-name">Definition of Terms</span></a></span></li><li><span class="sect1"><a href="#sec-ha-setup-yast-overview"><span class="title-number">4.2 </span><span class="title-name">YaST Cluster Module</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-channels"><span class="title-number">4.3 </span><span class="title-name">Defining the Communication Channels</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-security"><span class="title-number">4.4 </span><span class="title-name">Defining Authentication Settings</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-conntrackd"><span class="title-number">4.5 </span><span class="title-name">Synchronizing Connection Status Between Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-services"><span class="title-number">4.6 </span><span class="title-name">Configuring Services</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-csync2"><span class="title-number">4.7 </span><span class="title-name">Transferring the Configuration to All Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-start"><span class="title-number">4.8 </span><span class="title-name">Bringing the Cluster Online</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-config"><span class="title-number">II </span><span class="title-name">Configuration and Administration</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-config-basics"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-config-basics-scenarios"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-global"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources"><span class="title-number">5.3 </span><span class="title-name">Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-monitoring"><span class="title-number">5.4 </span><span class="title-name">Resource Monitoring</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-constraints"><span class="title-number">5.5 </span><span class="title-name">Resource Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-remote"><span class="title-number">5.6 </span><span class="title-name">Managing Services on Remote Hosts</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-monitor-health"><span class="title-number">5.7 </span><span class="title-name">Monitoring System Health</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-more"><span class="title-number">5.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-conf-hawk2"><span class="title-number">6 </span><span class="title-name">Configuring and Managing Cluster Resources with Hawk2</span></a></span><ul><li><span class="sect1"><a href="#sec-conf-hawk2-req"><span class="title-number">6.1 </span><span class="title-name">Hawk2 Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-login"><span class="title-number">6.2 </span><span class="title-name">Logging In</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-overview"><span class="title-number">6.3 </span><span class="title-name">Hawk2 Overview: Main Elements</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-cluster-config"><span class="title-number">6.4 </span><span class="title-name">Configuring Global Cluster Options</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-rsc"><span class="title-number">6.5 </span><span class="title-name">Configuring Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-cons"><span class="title-number">6.6 </span><span class="title-name">Configuring Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-manage"><span class="title-number">6.7 </span><span class="title-name">Managing Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-monitor"><span class="title-number">6.8 </span><span class="title-name">Monitoring Clusters</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-batch"><span class="title-number">6.9 </span><span class="title-name">Using the Batch Mode</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-history"><span class="title-number">6.10 </span><span class="title-name">Viewing the Cluster History</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-health"><span class="title-number">6.11 </span><span class="title-name">Verifying Cluster Health</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-manual-config"><span class="title-number">7 </span><span class="title-name">Configuring and Managing Cluster Resources (Command Line)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-manual-config-crm"><span class="title-number">7.1 </span><span class="title-name">crmsh—Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-manual-config-crm-corosync"><span class="title-number">7.2 </span><span class="title-name">Managing Corosync Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm-global"><span class="title-number">7.3 </span><span class="title-name">Configuring Global Cluster Options</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm-resources"><span class="title-number">7.4 </span><span class="title-name">Configuring Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm"><span class="title-number">7.5 </span><span class="title-name">Managing Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm-setpwd"><span class="title-number">7.6 </span><span class="title-name">Setting Passwords Independent of <code class="filename">cib.xml</code></span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm-history"><span class="title-number">7.7 </span><span class="title-name">Retrieving History Information</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-crm-more"><span class="title-number">7.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-agents"><span class="title-number">8 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-stonithagents"><span class="title-number">8.1 </span><span class="title-name">STONITH Agents</span></a></span></li><li><span class="sect1"><a href="#sec-ha-writingresourceagents"><span class="title-number">8.2 </span><span class="title-name">Writing OCF Resource Agents</span></a></span></li><li><span class="sect1"><a href="#sec-ha-errorcodes"><span class="title-number">8.3 </span><span class="title-name">OCF Return Codes and Failure Recovery</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-fencing"><span class="title-number">9 </span><span class="title-name">Fencing and STONITH</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-fencing-classes"><span class="title-number">9.1 </span><span class="title-name">Classes of Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-nodes"><span class="title-number">9.2 </span><span class="title-name">Node Level Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-config"><span class="title-number">9.3 </span><span class="title-name">STONITH Resources and Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-monitor"><span class="title-number">9.4 </span><span class="title-name">Monitoring Fencing Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-special"><span class="title-number">9.5 </span><span class="title-name">Special Fencing Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-recommend"><span class="title-number">9.6 </span><span class="title-name">Basic Recommendations</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-more"><span class="title-number">9.7 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-storage-protect"><span class="title-number">10 </span><span class="title-name">Storage Protection and SBD</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-storage-protect-overview"><span class="title-number">10.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-steps"><span class="title-number">10.2 </span><span class="title-name">Overview of Manually Setting Up SBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-req"><span class="title-number">10.3 </span><span class="title-name">Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-fencing-number"><span class="title-number">10.4 </span><span class="title-name">Number of SBD Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-watchdog-timings"><span class="title-number">10.5 </span><span class="title-name">Calculation of Timeouts</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-watchdog"><span class="title-number">10.6 </span><span class="title-name">Setting Up the Watchdog</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-fencing-setup"><span class="title-number">10.7 </span><span class="title-name">Setting Up SBD with Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-diskless-sbd"><span class="title-number">10.8 </span><span class="title-name">Setting Up Diskless SBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-test"><span class="title-number">10.9 </span><span class="title-name">Testing SBD and Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-rsc-fencing"><span class="title-number">10.10 </span><span class="title-name">Additional Mechanisms for Storage Protection</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-moreinfo"><span class="title-number">10.11 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-acl"><span class="title-number">11 </span><span class="title-name">Access Control Lists</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-acl-require"><span class="title-number">11.1 </span><span class="title-name">Requirements and Prerequisites</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-enable"><span class="title-number">11.2 </span><span class="title-name">Enabling Use of ACLs in Your Cluster</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-basics"><span class="title-number">11.3 </span><span class="title-name">The Basics of ACLs</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-config-hawk2"><span class="title-number">11.4 </span><span class="title-name">Configuring ACLs with Hawk2</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-config-crm"><span class="title-number">11.5 </span><span class="title-name">Configuring ACLs with crmsh</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-netbonding"><span class="title-number">12 </span><span class="title-name">Network Device Bonding</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-netbond-yast"><span class="title-number">12.1 </span><span class="title-name">Configuring Bonding Devices with YaST</span></a></span></li><li><span class="sect1"><a href="#sec-ha-netbond-hotpug-yast"><span class="title-number">12.2 </span><span class="title-name">Hotplugging of Bonding Slaves</span></a></span></li><li><span class="sect1"><a href="#sec-ha-netbonding-more"><span class="title-number">12.3 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-lb"><span class="title-number">13 </span><span class="title-name">Load Balancing</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-lb-overview"><span class="title-number">13.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-lvs"><span class="title-number">13.2 </span><span class="title-name">Configuring Load Balancing with Linux Virtual Server</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-haproxy"><span class="title-number">13.3 </span><span class="title-name">Configuring Load Balancing with HAProxy</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-more"><span class="title-number">13.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo"><span class="title-number">14 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></span></li></ul></li><li><span class="part"><a href="#part-storage"><span class="title-number">III </span><span class="title-name">Storage and Data Replication</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-storage-dlm"><span class="title-number">15 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-storage-dlm-protocol"><span class="title-number">15.1 </span><span class="title-name">Protocols for DLM Communication</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-generic-dlm-config"><span class="title-number">15.2 </span><span class="title-name">Configuring DLM Cluster Resources</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-ocfs2"><span class="title-number">16 </span><span class="title-name">OCFS2</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-ocfs2-features"><span class="title-number">16.1 </span><span class="title-name">Features and Benefits</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-utils"><span class="title-number">16.2 </span><span class="title-name">OCFS2 Packages and Management Utilities</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-create-service"><span class="title-number">16.3 </span><span class="title-name">Configuring OCFS2 Services and a STONITH Resource</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-create"><span class="title-number">16.4 </span><span class="title-name">Creating OCFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-mount"><span class="title-number">16.5 </span><span class="title-name">Mounting OCFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-rsc-hawk2"><span class="title-number">16.6 </span><span class="title-name">Configuring OCFS2 Resources With Hawk2</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-quota"><span class="title-number">16.7 </span><span class="title-name">Using Quotas on OCFS2 File Systems</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-more"><span class="title-number">16.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-gfs2"><span class="title-number">17 </span><span class="title-name">GFS2</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-gfs2-utils"><span class="title-number">17.1 </span><span class="title-name">GFS2 Packages and Management Utilities</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-create-service"><span class="title-number">17.2 </span><span class="title-name">Configuring GFS2 Services and a STONITH Resource</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-create"><span class="title-number">17.3 </span><span class="title-name">Creating GFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-mount"><span class="title-number">17.4 </span><span class="title-name">Mounting GFS2 Volumes</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-drbd"><span class="title-number">18 </span><span class="title-name">DRBD</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-drbd-overview"><span class="title-number">18.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-install"><span class="title-number">18.2 </span><span class="title-name">Installing DRBD Services</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-configure"><span class="title-number">18.3 </span><span class="title-name">Setting Up DRBD Service</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-migrate"><span class="title-number">18.4 </span><span class="title-name">Migrating from DRBD 8 to DRBD 9</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-resource-stacking"><span class="title-number">18.5 </span><span class="title-name">Creating a Stacked DRBD Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-fencing"><span class="title-number">18.6 </span><span class="title-name">Using Resource-Level Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-test"><span class="title-number">18.7 </span><span class="title-name">Testing the DRBD Service</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-tuning"><span class="title-number">18.8 </span><span class="title-name">Tuning DRBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-trouble"><span class="title-number">18.9 </span><span class="title-name">Troubleshooting DRBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-more"><span class="title-number">18.10 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-clvm"><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-clvm-overview"><span class="title-number">19.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-config"><span class="title-number">19.2 </span><span class="title-name">Configuration of cLVM</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-drbd"><span class="title-number">19.3 </span><span class="title-name">Configuring Eligible LVM2 Devices Explicitly</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-more"><span class="title-number">19.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-cluster-md"><span class="title-number">20 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-cluster-md-overview"><span class="title-number">20.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-create"><span class="title-number">20.2 </span><span class="title-name">Creating a Clustered MD RAID Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-ra"><span class="title-number">20.3 </span><span class="title-name">Configuring a Resource Agent</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-add"><span class="title-number">20.4 </span><span class="title-name">Adding a Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-readd"><span class="title-number">20.5 </span><span class="title-name">Re-adding a Temporarily Failed Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-remove"><span class="title-number">20.6 </span><span class="title-name">Removing a Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-convert-raid"><span class="title-number">20.7 </span><span class="title-name">Assembling Cluster MD as normal RAID at the disaster recovery site</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-samba"><span class="title-number">21 </span><span class="title-name">Samba Clustering</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-samba-overview"><span class="title-number">21.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-basicconf"><span class="title-number">21.2 </span><span class="title-name">Basic Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-ad"><span class="title-number">21.3 </span><span class="title-name">Joining an Active Directory Domain</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-testing"><span class="title-number">21.4 </span><span class="title-name">Debugging and Testing Clustered Samba</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-moreinfo"><span class="title-number">21.5 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-rear"><span class="title-number">22 </span><span class="title-name">Disaster Recovery with Relax-and-Recover (ReaR)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-rear-concept"><span class="title-number">22.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-config"><span class="title-number">22.2 </span><span class="title-name">Setting Up ReaR and Your Backup Solution</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-mkbackup"><span class="title-number">22.3 </span><span class="title-name">Creating the Recovery Installation System</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-testing"><span class="title-number">22.4 </span><span class="title-name">Testing the Recovery Process</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-recover"><span class="title-number">22.5 </span><span class="title-name">Recovering from Disaster</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-more"><span class="title-number">22.6 </span><span class="title-name">For More Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-maintenance"><span class="title-number">IV </span><span class="title-name">Maintenance and Upgrade</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-maintenance"><span class="title-number">23 </span><span class="title-name">Executing Maintenance Tasks</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-maint-shutdown-node"><span class="title-number">23.1 </span><span class="title-name">Implications of Taking Down a Cluster Node</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-overview"><span class="title-number">23.2 </span><span class="title-name">Different Options for Maintenance Tasks</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-outline"><span class="title-number">23.3 </span><span class="title-name">Preparing and Finishing Maintenance Work</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-cluster"><span class="title-number">23.4 </span><span class="title-name">Putting the Cluster into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-node"><span class="title-number">23.5 </span><span class="title-name">Putting a Node into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-node-standby"><span class="title-number">23.6 </span><span class="title-name">Putting a Node into Standby Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-rsc"><span class="title-number">23.7 </span><span class="title-name">Putting a Resource into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-rsc-unmanaged"><span class="title-number">23.8 </span><span class="title-name">Putting a Resource into Unmanaged Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-shutdown-node-maint-mode"><span class="title-number">23.9 </span><span class="title-name">Rebooting a Cluster Node While In Maintenance Mode</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-migration"><span class="title-number">24 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-migration-terminology"><span class="title-number">24.1 </span><span class="title-name">Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-upgrade"><span class="title-number">24.2 </span><span class="title-name">Upgrading your Cluster to the Latest Product Version</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-update"><span class="title-number">24.3 </span><span class="title-name">Updating Software Packages on Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-more"><span class="title-number">24.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-appendix"><span class="title-number">V </span><span class="title-name">Appendix</span></a></span><ul><li><span class="appendix"><a href="#app-ha-troubleshooting"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-troubleshooting-install"><span class="title-number">A.1 </span><span class="title-name">Installation and First Steps</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-log"><span class="title-number">A.2 </span><span class="title-name">Logging</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-resource"><span class="title-number">A.3 </span><span class="title-name">Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-stonith"><span class="title-number">A.4 </span><span class="title-name">STONITH and Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-history"><span class="title-number">A.5 </span><span class="title-name">History</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-hawk2"><span class="title-number">A.6 </span><span class="title-name">Hawk2</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-misc"><span class="title-number">A.7 </span><span class="title-name">Miscellaneous</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-moreinfo"><span class="title-number">A.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="appendix"><a href="#app-naming"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></span></li><li><span class="appendix"><a href="#app-ha-management"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></span></li><li><span class="appendix"><a href="#app-crmreport-nonroot"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></span><ul><li><span class="sect1"><a href="#sec-crmreport-nonroot-user"><span class="title-number">D.1 </span><span class="title-name">Creating a Local User Account</span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-ssh"><span class="title-number">D.2 </span><span class="title-name">Configuring a Passwordless SSH Account</span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-sudo"><span class="title-number">D.3 </span><span class="title-name">Configuring <code class="command">sudo</code></span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-execute"><span class="title-number">D.4 </span><span class="title-name">Generating a Cluster Report</span></a></span></li></ul></li><li><span class="appendix"><a href="#app-ha-docupdates"><span class="title-number">E </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp4-maint-2"><span class="title-number">E.1 </span><span class="title-name">2020 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp4-maint-1"><span class="title-number">E.2 </span><span class="title-name">2019 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp4"><span class="title-number">E.3 </span><span class="title-name">December 2018 (Initial Release of SUSE Linux Enterprise High Availability 12 SP4)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp3-maint-2"><span class="title-number">E.4 </span><span class="title-name">October 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp3-maint-1"><span class="title-number">E.5 </span><span class="title-name">July 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp3"><span class="title-number">E.6 </span><span class="title-name">September 2017 (Initial Release of SUSE Linux Enterprise High Availability 12 SP3)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp2-maint-3"><span class="title-number">E.7 </span><span class="title-name">April 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp2-maint-2"><span class="title-number">E.8 </span><span class="title-name">March 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp2-maint-1"><span class="title-number">E.9 </span><span class="title-name">November 2016 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp2"><span class="title-number">E.10 </span><span class="title-name">November 2016 (Initial Release of SUSE Linux Enterprise High Availability 12 SP2)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp1-maint-1"><span class="title-number">E.11 </span><span class="title-name">December 2015 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP1)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-sp1"><span class="title-number">E.12 </span><span class="title-name">December 2015 (Initial Release of SUSE Linux Enterprise High Availability 12 SP1)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-docupdates-sle12-ga"><span class="title-number">E.13 </span><span class="title-name">October 2014 (Initial Release of SUSE Linux Enterprise High Availability 12)</span></a></span></li></ul></li></ul></li><li><span class="glossary"><a href="#gl-heartb"><span class="title-name">Glossary</span></a></span></li><li><span class="appendix"><a href="#id-1.3.9"><span class="title-number">F </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.9.4"><span class="title-number">F.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.3.3.3.5.11"><span class="number">1.1 </span><span class="name">Three-Server Cluster</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.3.5.14"><span class="number">1.2 </span><span class="name">Three-Server Cluster after One Server Fails</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.3.6.4"><span class="number">1.3 </span><span class="name">Typical Fibre Channel Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.3.6.6"><span class="number">1.4 </span><span class="name">Typical iSCSI Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.3.6.8"><span class="number">1.5 </span><span class="name">Typical Cluster Configuration Without Shared Storage</span></a></span></li><li><span class="figure"><a href="#fig-ha-architecture"><span class="number">1.6 </span><span class="name">Architecture</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.6.5.6"><span class="number">4.1 </span><span class="name">YaST Cluster—Multicast Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.6.5.9"><span class="number">4.2 </span><span class="name">YaST Cluster—Unicast Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.6.6.4"><span class="number">4.3 </span><span class="name">YaST Cluster—Security</span></a></span></li><li><span class="figure"><a href="#fig-ha-installation-setup-conntrackd"><span class="number">4.4 </span><span class="name">YaST Cluster—<code class="systemitem">conntrackd</code></span></a></span></li><li><span class="figure"><a href="#id-1.3.3.6.8.4"><span class="number">4.5 </span><span class="name">YaST Cluster—Services</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.6.9.8.3"><span class="number">4.6 </span><span class="name">YaST <span class="guimenu">Cluster</span>—Csync2</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.3.5.7.3.4"><span class="number">5.1 </span><span class="name">Group Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.6.4.3.4"><span class="number">6.1 </span><span class="name">Hawk2—Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.8.4"><span class="number">6.2 </span><span class="name">Hawk2—Wizard for Apache Web Server</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.9.3.8.4"><span class="number">6.3 </span><span class="name">Hawk2—Primitive Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.11.3.4.3"><span class="number">6.4 </span><span class="name">Hawk2—Editing A Primitive Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.12.4.9.2"><span class="number">6.5 </span><span class="name">Hawk2—STONITH Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.13.6"><span class="number">6.6 </span><span class="name">Hawk2—Resource Group</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.14.6"><span class="number">6.7 </span><span class="name">Hawk2—Clone Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.15.6"><span class="number">6.8 </span><span class="name">Hawk2—Multi-state Resource</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.16.5"><span class="number">6.9 </span><span class="name">Hawk2—Tag</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.7.17.6"><span class="number">6.10 </span><span class="name">Hawk2—Resource Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.8.5.4"><span class="number">6.11 </span><span class="name">Hawk2—Location Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.8.6.4"><span class="number">6.12 </span><span class="name">Hawk2—Colocation Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.8.7.4"><span class="number">6.13 </span><span class="name">Hawk2—Order Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.8.8.3.3.2.4.2"><span class="number">6.14 </span><span class="name">Hawk2—Two Resource Sets in a Colocation Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.10.4.4"><span class="number">6.15 </span><span class="name">Hawk2—Cluster Status</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.10.5.5.3.3"><span class="number">6.16 </span><span class="name">Hawk2 Dashboard with One Cluster Site (<code class="literal">amsterdam</code>)</span></a></span></li><li><span class="figure"><a href="#fig-hawk2-batch"><span class="number">6.17 </span><span class="name">Hawk2 Batch Mode Activated</span></a></span></li><li><span class="figure"><a href="#fig-hawk2-batch-show"><span class="number">6.18 </span><span class="name">Hawk2 Batch Mode—Injected Invents and Configuration Changes</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.12.6.5"><span class="number">6.19 </span><span class="name">Hawk2—History Explorer Main View</span></a></span></li><li><span class="figure"><a href="#fig-ha-lvs-yast-global"><span class="number">13.1 </span><span class="name">YaST IP Load Balancing—Global Parameters</span></a></span></li><li><span class="figure"><a href="#fig-ha-lvs-yast-virtual"><span class="number">13.2 </span><span class="name">YaST IP Load Balancing—Virtual Services</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-concept"><span class="number">18.1 </span><span class="name">Position of DRBD within Linux</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-yast-resconfig"><span class="number">18.2 </span><span class="name">Resource Configuration</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-resource-stacking"><span class="number">18.3 </span><span class="name">Resource Stacking</span></a></span></li><li><span class="figure"><a href="#fig-ha-clvm-scenario-iscsi"><span class="number">19.1 </span><span class="name">Setup of iSCSI with cLVM</span></a></span></li><li><span class="figure"><a href="#fig-ha-samba-overview"><span class="number">21.1 </span><span class="name">Structure of a CTDB Cluster</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#id-1.3.4.3.5.10.3"><span class="number">5.1 </span><span class="name">Resource Operation Properties</span></a></span></li><li><span class="table"><a href="#id-1.3.4.5.4.8.8.9"><span class="number">7.1 </span><span class="name">Common Parameters</span></a></span></li><li><span class="table"><a href="#id-1.3.4.6.5.4"><span class="number">8.1 </span><span class="name">Failure Recovery Types</span></a></span></li><li><span class="table"><a href="#id-1.3.4.6.5.7"><span class="number">8.2 </span><span class="name">OCF Return Codes</span></a></span></li><li><span class="table"><a href="#tab-ha-storage-protect-watchdog-drivers"><span class="number">10.1 </span><span class="name">Commonly Used Watchdog Drivers</span></a></span></li><li><span class="table"><a href="#tab-ha-acl-operator"><span class="number">11.1 </span><span class="name">Operator Role—Access Types and XPath Expressions</span></a></span></li><li><span class="table"><a href="#id-1.3.5.4.4.4"><span class="number">16.1 </span><span class="name">OCFS2 Utilities</span></a></span></li><li><span class="table"><a href="#tab-ha-ofcs2-mkfs-ocfs2-params"><span class="number">16.2 </span><span class="name">Important OCFS2 Parameters</span></a></span></li><li><span class="table"><a href="#id-1.3.5.5.4.4"><span class="number">17.1 </span><span class="name">GFS2 Utilities</span></a></span></li><li><span class="table"><a href="#tab-ha-gfs2-mkfs-gfs2-params"><span class="number">17.2 </span><span class="name">Important GFS2 Parameters</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-ha-config-basics-corosync-quorum"><span class="number">5.1 </span><span class="name">Excerpt of Corosync Configuration for a Two-Node Cluster</span></a></span></li><li><span class="example"><a href="#id-1.3.4.3.4.10.5"><span class="number">5.2 </span><span class="name">Excerpt of Corosync Configuration for an N-Node Cluster</span></a></span></li><li><span class="example"><a href="#ex-ha-config-resource-group"><span class="number">5.3 </span><span class="name">Resource Group for a Web Server</span></a></span></li><li><span class="example"><a href="#ex-config-basic-resourceset-loc"><span class="number">5.4 </span><span class="name">A Resource Set for Location Constraints</span></a></span></li><li><span class="example"><a href="#id-1.3.4.3.7.3.5.3.5"><span class="number">5.5 </span><span class="name">A Chain of Colocated Resources</span></a></span></li><li><span class="example"><a href="#id-1.3.4.3.7.3.5.3.9"><span class="number">5.6 </span><span class="name">A Chain of Ordered Resources</span></a></span></li><li><span class="example"><a href="#id-1.3.4.3.7.3.5.3.11"><span class="number">5.7 </span><span class="name">A Chain of Ordered Resources Expressed as Resource Set</span></a></span></li><li><span class="example"><a href="#ex-ha-config-basics-failover"><span class="number">5.8 </span><span class="name">Migration Threshold—Process Flow</span></a></span></li><li><span class="example"><a href="#ex-ha-config-basics-utilization"><span class="number">5.9 </span><span class="name">Example Configuration for Load-Balanced Placing</span></a></span></li><li><span class="example"><a href="#ex-ha-nagios-config"><span class="number">5.10 </span><span class="name">Configuring Resources for Monitoring Plug-ins</span></a></span></li><li><span class="example"><a href="#ex-ha-manual-config-crmshellscripts"><span class="number">7.1 </span><span class="name">A Simple crmsh Shell Script</span></a></span></li><li><span class="example"><a href="#id-1.3.4.7.5.11.5"><span class="number">9.1 </span><span class="name">Configuration of an IBM RSA Lights-out Device</span></a></span></li><li><span class="example"><a href="#id-1.3.4.7.5.11.6"><span class="number">9.2 </span><span class="name">Configuration of a UPS Fencing Device</span></a></span></li><li><span class="example"><a href="#ex-ha-fencing-kdump"><span class="number">9.3 </span><span class="name">Configuration of a Kdump Device</span></a></span></li><li><span class="example"><a href="#ex-ha-storage-protect-sbd-timings"><span class="number">10.1 </span><span class="name">Formula for Timeout Calculation</span></a></span></li><li><span class="example"><a href="#ex-ha-acl-excerpt"><span class="number">11.1 </span><span class="name">Excerpt of a Cluster Configuration in XML</span></a></span></li><li><span class="example"><a href="#ex-ha-lvs-ldirectord"><span class="number">13.1 </span><span class="name">Simple ldirectord Configuration</span></a></span></li><li><span class="example"><a href="#exa-ha-drbd-stacked-drbd"><span class="number">18.1 </span><span class="name">Configuration of a Three-Node Stacked DRBD Resource</span></a></span></li><li><span class="example"><a href="#ex-ha-drbd-fencing"><span class="number">18.2 </span><span class="name">Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB)</span></a></span></li><li><span class="example"><a href="#ex-ha-rear-nfs-server-backup"><span class="number">22.1 </span><span class="name">Using an NFS Server to Store the File Backup</span></a></span></li><li><span class="example"><a href="#ex-ha-rear-config-EMC"><span class="number">22.2 </span><span class="name">Using Third-Party Backup Tools Like EMC NetWorker</span></a></span></li><li><span class="example"><a href="#id-1.3.7.2.4.2.4.2.4"><span class="number">A.1 </span><span class="name">Stopped Resources</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.3.1.6"><p>
  Copyright © 2006–2023

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="pre-ha" data-id-title="About This Guide"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">About This Guide</span></span> <a title="Permalink" class="permalink" href="#pre-ha">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_intro.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p> This guide is intended for administrators who need to set up, configure,
  and maintain clusters with SUSE® Linux Enterprise High Availability. For quick and efficient
  configuration and administration, SUSE Linux Enterprise High Availability includes both a graphical user
  interface (GUI) and a command line interface (CLI). For performing key tasks,
  both approaches (GUI and CLI) are covered in detail in this guide. Thus,
  administrators can choose the appropriate tool that matches their needs.</p></div></div></div></div><p>
  This guide is divided into the following parts:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.4.1"><span class="term"><a class="xref" href="#part-install" title="Part I. Installation and Setup">Installation and Setup</a>
   </span></dt><dd><p>
     Before starting to install and configure your cluster, make yourself
     familiar with cluster fundamentals and architecture, get an overview of
     the key features and benefits. Learn which hardware and software
     requirements must be met and what preparations to take before executing
     the next steps. Perform the installation and basic setup of your HA
     cluster using YaST. Learn how to upgrade your cluster to the most recent
     release version or how to update individual packages.
    </p></dd><dt id="id-1.3.2.4.2"><span class="term"><a class="xref" href="#part-config" title="Part II. Configuration and Administration">Configuration and Administration</a>
   </span></dt><dd><p>
     Add, configure and manage cluster resources with either the Web
     interface (Hawk2), or the command line interface (crmsh). To
     avoid unauthorized access to the cluster configuration, define roles
     and assign them to certain users for fine-grained control. Learn how to
     use load balancing and fencing. If you consider writing your own
     resource agents or modifying existing ones, get some background
     information on how to create different types of resource agents.
    </p></dd><dt id="id-1.3.2.4.3"><span class="term"><a class="xref" href="#part-storage" title="Part III. Storage and Data Replication">Storage and Data Replication</a>
   </span></dt><dd><p>
     SUSE Linux Enterprise High Availability ships with the cluster-aware file systems OCFS2 and
     GFS2, and the clustered Logical Volume Manager (cLVM). For replication
     of your data, use DRBD*. It lets you mirror the data of a High Availability
     service from the active node of a cluster to its standby node.
     Furthermore, a clustered Samba server also provides a High Availability solution
     for heterogeneous environments.
    </p></dd><dt id="id-1.3.2.4.4"><span class="term"><a class="xref" href="#part-appendix" title="Part V. Appendix">Appendix</a>
   </span></dt><dd><p>Contains an overview of common problems and their solution. Presents the
     naming conventions used in this documentation with regard to clusters,
     resources and constraints. Shows the published documentation updates
     including a detailed list of the content changes for each update.
     Contains a glossary with HA-specific terminology.</p></dd></dl></div><p>
  Many chapters in this manual contain links to additional documentation
  resources, either on the system or available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="http://www.suse.com/doc/" target="_blank">http://www.suse.com/doc/</a>.
 </p><section xml:lang="en" class="sect1" id="id-1.3.2.7" data-id-title="Available documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Available documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_available_doc.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.7.3.1"><span class="term">Online documentation</span></dt><dd><p>
     Our documentation is available online at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
     Browse or download the documentation in various formats.
    </p><div id="id-1.3.2.7.3.1.2.2" data-id-title="Latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Latest updates</div><p>
      The latest updates are usually available in the English-language version of this documentation.
     </p></div></dd><dt id="id-1.3.2.7.3.2"><span class="term">Release notes</span></dt><dd><p>
     For release notes, see
     <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
    </p></dd><dt id="id-1.3.2.7.3.3"><span class="term">In your system</span></dt><dd><p>
      For offline use, the release notes are also available under
      <code class="filename">/usr/share/doc/release-notes</code> on your system.
      The documentation for individual packages is available at
      <code class="filename">/usr/share/doc/packages</code>.
    </p><p>
      Many commands are also described in their <span class="emphasis"><em>manual
      pages</em></span>. To view them, run <code class="command">man</code>, followed
      by a specific command name. If the <code class="command">man</code> command is
      not installed on your system, install it with <code class="command">sudo zypper
      install man</code>.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.3.2.8" data-id-title="Improving the documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Improving the documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_feedback.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Your feedback and contributions to this documentation are welcome.
  The following channels for giving feedback are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.8.4.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.3.2.8.4.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
    </p><p>
     To simplify this process, click the <span class="guimenu">Report
     an issue</span> icon next to a headline in the HTML
     version of this document. This preselects the right product and
     category in Bugzilla and adds a link to the current section.
     You can start typing your bug report right away.
    </p><p>
     A Bugzilla account is required.
    </p></dd><dt id="id-1.3.2.8.4.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, click the <span class="guimenu">Edit source
     document</span> icon next to a headline in the HTML version of
     this document. This will take you to the source code on GitHub, where you
     can open a pull request.</p><p>
     A GitHub account is required.
    </p><div id="id-1.3.2.8.4.3.2.3" data-id-title="Edit source document only available for English" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="guimenu">Edit source document</span> only available for English</div><p>
      The <span class="guimenu">Edit source document</span> icons are only available for the
      English version of each document. For all other languages, use the
      <span class="guimenu">Report an issue</span> icons instead.
     </p></div><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README at
     <a class="link" href="https://github.com/SUSE/doc-sleha" target="_blank">https://github.com/SUSE/doc-sleha</a>.
    </p></dd><dt id="id-1.3.2.8.4.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.3.2.9" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.9">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_convention_ha.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as non-privileged user.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands executed in the interactive crm shell.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.3.2.9.4.14.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Warning notice</div><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.3.2.9.4.14.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Important notice</div><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.3.2.9.4.14.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Note notice</div><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.3.2.9.4.14.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Tip notice</div><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.3.2.9.4.15.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.3.2.9.4.15.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div><p>
  For an overview of naming conventions for cluster nodes and names,
  resources, and constraints, see <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>.
 </p></section><section xml:lang="en" class="sect1" id="id-1.3.2.10" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.10">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Find the support statement for SUSE Linux Enterprise High Availability and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.3.2.10.5" data-id-title="Support statement for SUSE Linux Enterprise High Availability"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Linux Enterprise High Availability</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offers available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.10.5.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.3.2.10.5.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate a problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.3.2.10.5.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Linux Enterprise High Availability is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.3.2.10.6" data-id-title="Technology previews"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span></span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback.
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or otherwise
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a>.
  </p></section></section></section><div class="part" id="part-install" data-id-title="Installation and Setup"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part I </span><span class="title-name">Installation and Setup </span></span><a title="Permalink" class="permalink" href="#part-install">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-concepts"><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></span></li><dd class="toc-abstract"><p>
    SUSE® Linux Enterprise High Availability is an integrated suite of open source clustering
    technologies that enables you to implement highly available physical and
    virtual Linux clusters, and to eliminate single points of failure. It
    ensures the high availability and manageability of critical
    resources including data, applications, and services. Thus, it helps you
    maintain business continuity, protect data integrity, and reduce
    unplanned downtime for your mission-critical Linux workloads.
   </p><p>
    It ships with essential monitoring, messaging, and cluster resource
    management functionality (supporting failover, failback, and migration
    (load balancing) of individually managed cluster resources).
   </p><p>
    This chapter introduces the main product features and benefits of SUSE Linux Enterprise High Availability.
    Inside you will find several example clusters and learn about
    the components making up a cluster. The last section provides an
    overview of the architecture, describing the individual architecture
    layers and processes within the cluster.
   </p><p>
    For explanations of some common terms used in the context of High Availability
    clusters, refer to <a class="xref" href="#gl-heartb" title="Glossary">Glossary</a>.
   </p></dd><li><span class="chapter"><a href="#cha-ha-requirements"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></span></li><dd class="toc-abstract"><p>
    The following section informs you about system requirements, and some
    prerequisites for SUSE® Linux Enterprise High Availability. It also includes recommendations
    for cluster setup.
   </p></dd><li><span class="chapter"><a href="#cha-ha-install"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></span></li><dd class="toc-abstract"><p>If you are setting up a High Availability cluster with SUSE® Linux Enterprise High Availability for the first time, the
    easiest way is to start with a basic two-node cluster. You can also use the
    two-node cluster to run some tests. Afterward, you can add more
    nodes by cloning existing cluster nodes with AutoYaST. The cloned nodes will
    have the same packages installed and the same system configuration as the
    original ones.
   </p><p>
    If you want to upgrade an existing cluster that runs an older version of
    SUSE Linux Enterprise High Availability, refer to <a class="xref" href="#cha-ha-migration" title="Chapter 24. Upgrading Your Cluster and Updating Software Packages">Chapter 24, <em>Upgrading Your Cluster and Updating Software Packages</em></a>.
   </p></dd><li><span class="chapter"><a href="#cha-ha-setup"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></span></li><dd class="toc-abstract"><p>The YaST cluster module allows you to set up a cluster manually
    (from scratch) or to modify options for an existing cluster.
   </p><p>
    However, if you prefer an automated approach for setting up a cluster,
    see the Installation and Setup Quick Start, available from
    <a class="link" href="https://www.suse.com/documentation/" target="_blank">https://www.suse.com/documentation/</a>. It describes how to install the
    needed packages and leads you to a basic two-node cluster, which is
    set up with the <code class="systemitem">ha-cluster-bootstrap</code> scripts.
   </p><p>
    You can also use a combination of both setup methods, for example: set up
    one node with YaST cluster and then use one of the bootstrap scripts
    to integrate more nodes (or vice versa).
   </p></dd></ul></div><section class="chapter" id="cha-ha-concepts" data-id-title="Product Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Product Overview</span></span> <a title="Permalink" class="permalink" href="#cha-ha-concepts">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    SUSE® Linux Enterprise High Availability is an integrated suite of open source clustering
    technologies that enables you to implement highly available physical and
    virtual Linux clusters, and to eliminate single points of failure. It
    ensures the high availability and manageability of critical
    resources including data, applications, and services. Thus, it helps you
    maintain business continuity, protect data integrity, and reduce
    unplanned downtime for your mission-critical Linux workloads.
   </p><p>
    It ships with essential monitoring, messaging, and cluster resource
    management functionality (supporting failover, failback, and migration
    (load balancing) of individually managed cluster resources).
   </p><p>
    This chapter introduces the main product features and benefits of SUSE Linux Enterprise High Availability.
    Inside you will find several example clusters and learn about
    the components making up a cluster. The last section provides an
    overview of the architecture, describing the individual architecture
    layers and processes within the cluster.
   </p><p>
    For explanations of some common terms used in the context of High Availability
    clusters, refer to <a class="xref" href="#gl-heartb" title="Glossary">Glossary</a>.
   </p></div></div></div></div><section class="sect1" id="sec-ha-availability" data-id-title="Availability as a Module or Extension"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">Availability as a Module or Extension</span></span> <a title="Permalink" class="permalink" href="#sec-ha-availability">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   High Availability is available for several products. Support for geographically dispersed clusters
   (Geo clusters) is available as a separate extension called Geo Clustering for SUSE Linux Enterprise High Availability. For details, see
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/html/SLES-all/cha-add-ons.html#sec-add-ons-extensions" target="_blank">https://documentation.suse.com/sles/12-SP5/html/SLES-all/cha-add-ons.html#sec-add-ons-extensions</a>.
  </p></section><section class="sect1" id="sec-ha-features" data-id-title="Key Features"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">Key Features</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE® Linux Enterprise High Availability helps you ensure and manage the availability of your
   network resources. The following sections highlight some of the key
   features:
  </p><section class="sect2" id="sec-ha-features-scenarios" data-id-title="Wide Range of Clustering Scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.1 </span><span class="title-name">Wide Range of Clustering Scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability supports the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Active/active configurations
     </p></li><li class="listitem"><p>
      
      Active/passive configurations: N+1, N+M, N to 1, N to M
     </p></li><li class="listitem"><p>
      Hybrid physical and virtual clusters, allowing virtual servers to be
      clustered with physical servers. This improves service availability
      and resource usage.
     </p></li><li class="listitem"><p>
      Local clusters
     </p></li><li class="listitem"><p>
      Metro clusters (<span class="quote">“<span class="quote">stretched</span>”</span> local clusters)
     </p></li><li class="listitem"><p>
      Geo clusters (geographically dispersed clusters) are supported with the
      additional Geo extension, see <a class="xref" href="#sec-ha-features-geo" title="1.2.5. Support of Local, Metro, and Geo Clusters">Section 1.2.5, “Support of Local, Metro, and Geo Clusters”</a>.
     </p></li></ul></div><div id="id-1.3.3.3.4.3.4" data-id-title="No support for mixed architectures" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No support for mixed architectures</div><p>
     All nodes belonging to a cluster should have the same processor platform:
     x86, IBM Z, or POWER. Clusters of mixed architectures are
     <span class="emphasis"><em>not</em></span> supported.
    </p></div><p>
    Your cluster can contain up to 32 Linux servers. Using 
    <code class="literal">pacemaker_remote</code>, the cluster can be extended to include
    additional Linux servers beyond this limit.
    Any server in the cluster can restart resources (applications, services, IP
    addresses, and file systems) from a failed server in the cluster.
   </p></section><section class="sect2" id="sec-ha-features-flexibility" data-id-title="Flexibility"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.2 </span><span class="title-name">Flexibility</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-flexibility">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability ships with Corosync messaging and membership layer
    and Pacemaker Cluster Resource Manager. Using Pacemaker, administrators
    can continually monitor the health and status of their resources, manage
    dependencies, and automatically stop and start services based on highly
    configurable rules and policies. SUSE Linux Enterprise High Availability allows you to tailor a
    cluster to the specific applications and hardware infrastructure that
    fit your organization. Time-dependent configuration enables services to
    automatically migrate back to repaired nodes at specified times.
   </p></section><section class="sect2" id="sec-ha-features-storage" data-id-title="Storage and Data Replication"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.3 </span><span class="title-name">Storage and Data Replication</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-storage">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With SUSE Linux Enterprise High Availability you can dynamically assign and reassign server
    storage as needed. It supports Fibre Channel or iSCSI storage area
    networks (SANs). Shared disk systems are also supported, but they are
    not a requirement. SUSE Linux Enterprise High Availability also comes with a cluster-aware file
    system (OCFS2) and the cluster Logical Volume Manager (Cluster LVM).
    For replication of your data, use DRBD* to mirror the data of
    a High Availability service from the active node of a cluster to its standby node.
    Furthermore, SUSE Linux Enterprise High Availability also supports CTDB (Cluster Trivial Database),
    a technology for Samba clustering.
   </p></section><section class="sect2" id="sec-ha-features-virtualized" data-id-title="Support for Virtualized Environments"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.4 </span><span class="title-name">Support for Virtualized Environments</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-virtualized">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability supports the clustering of both physical and
    virtual Linux servers. Mixing both types of servers is supported as well.
    SUSE Linux Enterprise Server 12 SP4 ships with Xen and KVM (Kernel-based Virtual Machine).
    Both are open source virtualization hypervisors. Virtualization guest
    systems (also known as VMs) can be managed as services by the cluster.
   </p></section><section class="sect2" id="sec-ha-features-geo" data-id-title="Support of Local, Metro, and Geo Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.5 </span><span class="title-name">Support of Local, Metro, and Geo Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-geo">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability has been extended to support different geographical
    scenarios. Support for geographically dispersed clusters (Geo clusters)
    is available as a separate extension called Geo Clustering for SUSE Linux Enterprise High Availability.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.3.4.7.3.1"><span class="term">Local Clusters</span></dt><dd><p>
       A single cluster in one location (for example, all nodes are located
       in one data center). The cluster uses multicast or unicast for
       communication between the nodes and manages failover internally.
       Network latency can be neglected. Storage is typically accessed
       synchronously by all nodes.
      </p></dd><dt id="id-1.3.3.3.4.7.3.2"><span class="term">Metro Clusters</span></dt><dd><p>
       A single cluster that can stretch over multiple buildings or data
       centers. The cluster usually uses unicast for communication between
       the nodes and manages failover internally. Network latency is usually
       low (&lt;5 ms for distances of approximately 20 miles). Storage
       is preferably connected by fibre channel. Data replication is done by
       storage internally, or by host based mirror under control of the cluster.
      </p></dd><dt id="id-1.3.3.3.4.7.3.3"><span class="term">Geo Clusters (Multi-Site Clusters)</span></dt><dd><p>
       Multiple, geographically dispersed sites with a local cluster each. The
       sites communicate via IP. Failover across the sites is coordinated by
       a higher-level entity. Geo clusters need to cope with limited
       network bandwidth and high latency. Storage is replicated
       asynchronously.
      </p><div id="id-1.3.3.3.4.7.3.3.2.2" data-id-title="Geo clustering and SAP workloads" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Geo clustering and SAP workloads</div><p>
        Currently Geo clusters do neither support SAP HANA system replication
        nor SAP S/4HANA and SAP NetWeaver enqueue replication setups.
       </p></div></dd></dl></div><p>
    The greater the geographical distance between individual cluster nodes,
    the more factors may potentially disturb the high availability of
    services the cluster provides. Network latency, limited bandwidth and
    access to storage are the main challenges for long-distance clusters.
   </p></section><section class="sect2" id="sec-ha-features-ra" data-id-title="Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.6 </span><span class="title-name">Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-ra">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability includes a huge number of resource agents to manage
    resources such as Apache, IPv4, IPv6 and many more. It also ships with
    resource agents for popular third party applications such as IBM
    WebSphere Application Server. For an overview of Open Cluster Framework
    (OCF) resource agents included with your product, use the <code class="command">crm
    ra</code> command as described in
    <a class="xref" href="#sec-ha-manual-config-ocf" title="7.1.3. Displaying Information about OCF Resource Agents">Section 7.1.3, “Displaying Information about OCF Resource Agents”</a>.
   </p></section><section class="sect2" id="sec-ha-features-tools" data-id-title="User-friendly Administration Tools"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.7 </span><span class="title-name">User-friendly Administration Tools</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-tools">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability ships with a set of powerful tools. Use them for basic installation
    and setup of your cluster and for effective configuration and
    administration:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.3.4.9.3.1"><span class="term">YaST </span></dt><dd><p>
       A graphical user interface for general system installation and
       administration. Use it to install SUSE Linux Enterprise High Availability on top of SUSE Linux Enterprise Server as
       described in the Installation and Setup Quick Start. YaST
       also provides the following modules in the High Availability category to help
       configure your cluster or individual components:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Cluster: Basic cluster setup. For details, refer to
         <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
        </p></li><li class="listitem"><p>
         DRBD: Configuration of a Distributed Replicated Block Device.
        </p></li><li class="listitem"><p>
         IP Load Balancing: Configuration of load balancing with Linux Virtual Server or
         HAProxy. For details, refer to <a class="xref" href="#cha-ha-lb" title="Chapter 13. Load Balancing">Chapter 13, <em>Load Balancing</em></a>.
        </p></li></ul></div></dd><dt id="id-1.3.3.3.4.9.3.2"><span class="term">HA Web Console (Hawk2)</span></dt><dd><p>
       A Web-based user interface with which you can administer your Linux
       cluster from non-Linux machines. It is also an ideal solution in case
       your system does not provide a graphical user interface. It guides
       you through the creation and configuration of resources and lets you
       execute management tasks like starting, stopping or migrating
       resources. For details, refer to
       <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>.
      </p></dd><dt id="id-1.3.3.3.4.9.3.3"><span class="term"><code class="command">crm</code> Shell
     </span></dt><dd><p>
       A powerful unified command line interface to configure resources and
       execute all monitoring or administration tasks. For details, refer to
       <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-benefits" data-id-title="Benefits"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">Benefits</span></span> <a title="Permalink" class="permalink" href="#sec-ha-benefits">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE Linux Enterprise High Availability allows you to configure up to 32 Linux servers into a
   high-availability cluster (HA cluster). Resources can be
   dynamically switched or moved to any node in the cluster. Resources can
   be configured to automatically migrate if a node fails, or they can be
   moved manually to troubleshoot hardware or balance the workload.
  </p><p>
   SUSE Linux Enterprise High Availability provides high availability from commodity components. Lower
   costs are obtained through the consolidation of applications and
   operations onto a cluster. SUSE Linux Enterprise High Availability also allows you to centrally
   manage the complete cluster. You can adjust resources to meet changing
   workload requirements (thus, manually <span class="quote">“<span class="quote">load balance</span>”</span> the
   cluster). Allowing clusters of more than two nodes also provides savings
   by allowing several nodes to share a <span class="quote">“<span class="quote">hot spare</span>”</span>.
  </p><p>
   An equally important benefit is the potential reduction of unplanned
   service outages as well as planned outages for software and hardware
   maintenance and upgrades.
  </p><p>
   Reasons that you would want to implement a cluster include:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Increased availability
    </p></li><li class="listitem"><p>
     Improved performance
    </p></li><li class="listitem"><p>
     Low cost of operation
    </p></li><li class="listitem"><p>
     Scalability
    </p></li><li class="listitem"><p>
     Disaster recovery
    </p></li><li class="listitem"><p>
     Data protection
    </p></li><li class="listitem"><p>
     Server consolidation
    </p></li><li class="listitem"><p>
     Storage consolidation
    </p></li></ul></div><p>
   Shared disk fault tolerance can be obtained by implementing RAID on the
   shared disk subsystem.
  </p><p>
   The following scenario illustrates some benefits SUSE Linux Enterprise High Availability can
   provide.
  </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.3.3.5.9"><span class="name">Example Cluster Scenario</span><a title="Permalink" class="permalink" href="#id-1.3.3.3.5.9">#</a></h2></div><p>
   Suppose you have configured a three-server cluster, with a Web server
   installed on each of the three servers in the cluster. Each of the
   servers in the cluster hosts two Web sites. All the data, graphics, and
   Web page content for each Web site are stored on a shared disk subsystem
   connected to each of the servers in the cluster. The following figure
   depicts how this setup might look.
  </p><div class="figure" id="id-1.3.3.3.5.11"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example1.png"><img src="images/ha_cluster_example1.png" width="85%" alt="Three-Server Cluster" title="Three-Server Cluster"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.1: </span><span class="title-name">Three-Server Cluster </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.3.5.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   During normal cluster operation, each server is in constant communication
   with the other servers in the cluster and performs periodic polling of
   all registered resources to detect failure.
  </p><p>
   Suppose Web Server 1 experiences hardware or software problems and the
   users depending on Web Server 1 for Internet access, e-mail, and
   information lose their connections. The following figure shows how
   resources are moved when Web Server 1 fails.
  </p><div class="figure" id="id-1.3.3.3.5.14"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example2.png"><img src="images/ha_cluster_example2.png" width="75%" alt="Three-Server Cluster after One Server Fails" title="Three-Server Cluster after One Server Fails"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.2: </span><span class="title-name">Three-Server Cluster after One Server Fails </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.3.5.14">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Web Site A moves to Web Server 2 and Web Site B moves to Web Server 3. IP
   addresses and certificates also move to Web Server 2 and Web Server 3.
  </p><p>
   When you configured the cluster, you decided where the Web sites hosted
   on each Web server would go should a failure occur. In the previous
   example, you configured Web Site A to move to Web Server 2 and Web Site B
   to move to Web Server 3. This way, the workload once handled by Web
   Server 1 continues to be available and is evenly distributed between any
   surviving cluster members.
  </p><p>
   When Web Server 1 failed, the High Availability software did the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Detected a failure and verified with STONITH that Web Server 1 was
     really dead. STONITH is an acronym for <span class="quote">“<span class="quote">Shoot The Other Node
     In The Head</span>”</span> and is a means of bringing down misbehaving nodes
     to prevent them from causing trouble in the cluster.
    </p></li><li class="listitem"><p>
     Remounted the shared data directories that were formerly mounted on Web
     server 1 on Web Server 2 and Web Server 3.
    </p></li><li class="listitem"><p>
     Restarted applications that were running on Web Server 1 on Web Server
     2 and Web Server 3.
    </p></li><li class="listitem"><p>
     Transferred IP addresses to Web Server 2 and Web Server 3.
    </p></li></ul></div><p>
   In this example, the failover process happened quickly and users regained
   access to Web site information within seconds, usually without needing to
   log in again.
  </p><p>
   Now suppose the problems with Web Server 1 are resolved, and Web Server 1
   is returned to a normal operating state. Web Site A and Web Site B can
   either automatically fail back (move back) to Web Server 1, or they can
   stay where they are. This depends on how you configured the resources for
   them. Migrating the services back to Web Server 1 will incur some
   down-time. Therefore SUSE Linux Enterprise High Availability also allows you to defer the migration until
   a period when it will cause little or no service interruption. There are
   advantages and disadvantages to both alternatives.
  </p><p>
   SUSE Linux Enterprise High Availability also provides resource migration capabilities. You can move
   applications, Web sites, etc. to other servers in your cluster as
   required for system management.
  </p><p>
   For example, you could have manually moved Web Site A or Web Site B from
   Web Server 1 to either of the other servers in the cluster. Use cases for
   this are upgrading or performing scheduled maintenance on Web Server 1,
   or increasing performance or accessibility of the Web sites.
  </p></section><section class="sect1" id="sec-ha-clusterconfig" data-id-title="Cluster Configurations: Storage"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">Cluster Configurations: Storage</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clusterconfig">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Cluster configurations with SUSE Linux Enterprise High Availability might or might not include a
   shared disk subsystem. The shared disk subsystem can be connected via
   high-speed Fibre Channel cards, cables, and switches, or it can be
   configured to use iSCSI. If a server fails, another designated server in
   the cluster automatically mounts the shared disk directories that were
   previously mounted on the failed server. This gives network users
   continuous access to the directories on the shared disk subsystem.
  </p><p>
   Typical resources might include data, applications, and services. The
   following figure shows how a typical Fibre Channel cluster configuration
   might look.
  </p><div class="figure" id="id-1.3.3.3.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example3.png"><img src="images/ha_cluster_example3.png" width="85%" alt="Typical Fibre Channel Cluster Configuration" title="Typical Fibre Channel Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.3: </span><span class="title-name">Typical Fibre Channel Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.3.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Although Fibre Channel provides the best performance, you can also
   configure your cluster to use iSCSI. iSCSI is an alternative to Fibre
   Channel that can be used to create a low-cost Storage Area Network (SAN).
   The following figure shows how a typical iSCSI cluster configuration
   might look.
  </p><div class="figure" id="id-1.3.3.3.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example4.png"><img src="images/ha_cluster_example4.png" width="100%" alt="Typical iSCSI Cluster Configuration" title="Typical iSCSI Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.4: </span><span class="title-name">Typical iSCSI Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.3.6.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Although most clusters include a shared disk subsystem, it is also
   possible to create a cluster without a shared disk subsystem. The
   following figure shows how a cluster without a shared disk subsystem
   might look.
  </p><div class="figure" id="id-1.3.3.3.6.8"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example5.png"><img src="images/ha_cluster_example5.png" width="100%" alt="Typical Cluster Configuration Without Shared Storage" title="Typical Cluster Configuration Without Shared Storage"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.5: </span><span class="title-name">Typical Cluster Configuration Without Shared Storage </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.3.6.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-architecture" data-id-title="Architecture"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.5 </span><span class="title-name">Architecture</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section provides a brief overview of SUSE Linux Enterprise High Availability architecture. It
   identifies and provides information on the architectural components, and
   describes how those components interoperate.
  </p><section class="sect2" id="sec-ha-architecture-layers" data-id-title="Architecture Layers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.1 </span><span class="title-name">Architecture Layers</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability has a layered architecture.
    <a class="xref" href="#fig-ha-architecture" title="Architecture">Figure 1.6, “Architecture”</a> illustrates
    the different layers and their associated components.
   </p><div class="figure" id="fig-ha-architecture"><div class="figure-contents"><div class="mediaobject"><a href="images/cluster_stack_arch.png"><img src="images/cluster_stack_arch.png" width="100%" alt="Architecture" title="Architecture"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.6: </span><span class="title-name">Architecture </span></span><a title="Permalink" class="permalink" href="#fig-ha-architecture">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><section class="sect3" id="sec-ha-architecture-layers-messaging" data-id-title="Messaging and Infrastructure Layer"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.1 </span><span class="title-name">Messaging and Infrastructure Layer</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-messaging">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The primary or first layer is the messaging/infrastructure layer, also
     known as the Corosync layer. This layer contains components that
     send out the messages containing <span class="quote">“<span class="quote">I am alive</span>”</span> signals, as
     well as other information.
    </p></section><section class="sect3" id="sec-ha-architecture-layers-allocation" data-id-title="Resource Allocation Layer"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.2 </span><span class="title-name">Resource Allocation Layer</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-allocation">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The next layer is the resource allocation layer. This layer is the most
     complex, and consists of the following components:
    </p><div class="variablelist"><dl class="variablelist"><dt id="vle-crm"><span class="term">Cluster Resource Manager (CRM)</span></dt><dd><p>
        Every action taken in the resource allocation layer passes through
        the Cluster Resource Manager. If other components of the resource
        allocation layer (or components which are in a higher layer) need to
        communicate, they do so through the local CRM. On every node, the
        CRM maintains the
        <a class="xref" href="#vle-cib">Cluster Information Base (CIB)</a>.
       </p></dd><dt id="vle-cib"><span class="term">Cluster Information Base (CIB)</span></dt><dd><p>
        The Cluster Information Base is an in-memory XML representation of
        the entire cluster configuration and current status. It contains
        definitions of all cluster options, nodes, resources, constraints
        and the relationship to each other. The CIB also synchronizes
        updates to all cluster nodes. There is one master CIB in the
        cluster, maintained by the
        <a class="xref" href="#vle-dc">Designated Coordinator (DC)</a>. All other nodes
        contain a CIB replica.
       </p></dd><dt id="vle-dc"><span class="term">Designated Coordinator (DC)</span></dt><dd><p>
        One CRM in the cluster is elected as DC. The DC is the only entity
        in the cluster that can decide that a cluster-wide change needs to
        be performed, such as fencing a node or moving resources around. The
        DC is also the node where the master copy of the CIB is kept. All
        other nodes get their configuration and resource allocation
        information from the current DC. The DC is elected from all nodes in
        the cluster after a membership change.
       </p></dd><dt id="vle-pe-and-te"><span class="term">Policy Engine (PE)</span></dt><dd><p>
        Whenever the Designated Coordinator needs to make a cluster-wide
        change (react to a new CIB), the Policy Engine calculates the next
        state of the cluster based on the current state and configuration.
        The PE also produces a transition graph containing a list of
        (resource) actions and dependencies to achieve the next cluster
        state. The PE always runs on the DC.
       </p></dd><dt id="vle-lrm"><span class="term">Local Resource Manager (LRM)</span></dt><dd><p>
        The LRM calls the local Resource Agents (see
        <a class="xref" href="#sec-ha-architecture-layers-resources" title="1.5.1.3. Resource Layer">Section 1.5.1.3, “Resource Layer”</a>) on behalf of
        the CRM. It can thus perform start / stop / monitor operations and
        report the result to the CRM. The LRM is the authoritative source
        for all resource-related information on its local node.

       </p></dd></dl></div></section><section class="sect3" id="sec-ha-architecture-layers-resources" data-id-title="Resource Layer"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.3 </span><span class="title-name">Resource Layer</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-resources">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The highest layer is the Resource Layer. The Resource Layer includes
     one or more Resource Agents (RA). Resource Agents are programs (usually
     shell scripts) that have been written to start, stop, and monitor a
     certain kind of service (a resource). Resource Agents are called only
     by the LRM. Third parties can include their own agents in a defined
     location in the file system and thus provide out-of-the-box cluster
     integration for their own software.
    </p></section></section><section class="sect2" id="sec-ha-architecture-processflow" data-id-title="Process Flow"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.2 </span><span class="title-name">Process Flow</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-processflow">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability uses Pacemaker as CRM. The CRM is implemented as daemon
    (<code class="literal">crmd</code>) that has an instance on each cluster node.
    Pacemaker centralizes all cluster decision-making by electing one of the
    crmd instances to act as a master. Should the elected crmd process (or
    the node it is on) fail, a new one is established.
   </p><p>
    A CIB, reflecting the cluster’s configuration and current state of all
    resources in the cluster is kept on each node. The contents of the CIB
    are automatically kept synchronous across the entire cluster.
   </p><p>
    Many actions performed in the cluster will cause a cluster-wide change.
    These actions can include things like adding or removing a cluster
    resource or changing resource constraints. It is important to understand
    what happens in the cluster when you perform such an action.
   </p><p>
    For example, suppose you want to add a cluster IP address resource. To
    do this, you can use one of the command line tools or the Web interface
    to modify the CIB. It is not required to perform the actions on the DC,
    you can use either tool on any node in the cluster and they will be
    relayed to the DC. The DC will then replicate the CIB change to all
    cluster nodes.
   </p><p>
    Based on the information in the CIB, the PE then computes the ideal
    state of the cluster and how it should be achieved and feeds a list of
    instructions to the DC. The DC sends commands via the
    messaging/infrastructure layer which are received by the crmd peers on
    other nodes. Each crmd uses its LRM (implemented as lrmd) to perform
    resource modifications. The lrmd is not cluster-aware and interacts
    directly with resource agents (scripts).
   </p><p>
    All peer nodes report the results of their operations back to the DC.
    After the DC concludes that all necessary operations are successfully
    performed in the cluster, the cluster will go back to the idle state and
    wait for further events. If any operation was not carried out as
    planned, the PE is invoked again with the new information recorded in
    the CIB.
   </p><p>
    In some cases, it may be necessary to power off nodes to protect shared
    data or complete resource recovery. For this Pacemaker comes with a
    fencing subsystem, stonithd. STONITH is an acronym for <span class="quote">“<span class="quote">Shoot
    The Other Node In The Head</span>”</span>.
    It is usually implemented with a STONITH shared block device, remote
    management boards, or remote power switches. In Pacemaker, STONITH
    devices are modeled as resources (and configured in the CIB) to
    enable them to be easily used.
    However, stonithd takes care of understanding the
    STONITH topology such that its clients request a node be
    fenced and it does the rest.
   </p></section></section></section><section class="chapter" id="cha-ha-requirements" data-id-title="System Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="#cha-ha-requirements">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The following section informs you about system requirements, and some
    prerequisites for SUSE® Linux Enterprise High Availability. It also includes recommendations
    for cluster setup.
   </p></div></div></div></div><section class="sect1" id="sec-ha-requirements-hw" data-id-title="Hardware Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Hardware Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-hw">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following list specifies hardware requirements for a cluster based on
   SUSE® Linux Enterprise High Availability. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.4.3.3.1"><span class="term">Servers</span></dt><dd><p> 1 to 32 Linux servers with software as specified in <a class="xref" href="#sec-ha-requirements-sw" title="2.2. Software Requirements">Section 2.2, “Software Requirements”</a>. </p><p>
      The servers can be bare metal or virtual machines. They do not require
      identical hardware (memory, disk space, etc.), but they must have the
      same architecture. Cross-platform clusters are not supported.
     </p><p> Using <code class="literal">pacemaker_remote</code>, the cluster can be
      extended to include additional Linux servers beyond the 32-node limit.
     </p></dd><dt id="id-1.3.3.4.3.3.2"><span class="term">Communication Channels</span></dt><dd><p>
       At least two TCP/IP communication media per cluster node.
       The network equipment must support the communication means you want to use
       for cluster communication: multicast or unicast. The communication
       media should support a data rate of 100 Mbit/s or higher.
       For a supported cluster setup two or more redundant communication paths
       are required. This can be done via:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Network Device Bonding (to be preferred).
         </p></li><li class="listitem"><p>
          A second communication channel in Corosync.
         </p></li><li class="listitem"><p>
            Network fault tolerance on infrastructure layer (for example, hypervisor).
          </p></li></ul></div><p>For details, refer to <a class="xref" href="#cha-ha-netbonding" title="Chapter 12. Network Device Bonding">Chapter 12, <em>Network Device Bonding</em></a> and <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>, respectively.
      
     </p></dd><dt id="id-1.3.3.4.3.3.3"><span class="term">Node Fencing/STONITH</span></dt><dd><p>
      To avoid a <span class="quote">“<span class="quote">split brain</span>”</span> scenario,
      clusters need a node fencing mechanism. In a split brain scenario, cluster
      nodes are divided into two or more groups that do not know about each other
      (because of a hardware or software failure or because of a cut network
      connection). A fencing mechanism isolates the node in question
      (usually by resetting or powering off the node). This is also called
      STONITH (<span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>). A node fencing
      mechanism can be either a physical device (a power  switch) or a mechanism
      like SBD (STONITH by disk) in combination with a watchdog. Using SBD
      requires shared storage.
     </p><p>Unless SBD is used, each node in the High Availability cluster must have at least
      one STONITH device. We strongly recommend multiple
      STONITH devices per node.</p><div id="id-1.3.3.4.3.3.3.2.3" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div></dd></dl></div></section><section class="sect1" id="sec-ha-requirements-sw" data-id-title="Software Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Software Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-sw">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   On all nodes that will be part of the cluster the following software
   must be installed.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
   SUSE® Linux Enterprise Server 12 SP4 (with all available online updates)
  </p></li><li class="listitem"><p>
   SUSE Linux Enterprise High Availability 12 SP4 (with all available online updates)
  </p></li><li class="listitem"><p>
     (Optional) For Geo clusters: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP4
     (with all available online updates)
    </p></li></ul></div></section><section class="sect1" id="sec-ha-requirements-disk" data-id-title="Storage Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">Storage Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-disk">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Some services require shared storage. If using an external NFS share, it must
   be reliably accessible from all cluster nodes via redundant communication
   paths.</p><p>To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The shared disk system is properly set up and functional according to
     the manufacturer’s instructions.
    </p></li><li class="listitem"><p>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
    </p></li><li class="listitem"><p>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </p></li><li class="listitem"><p>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD—never the backing device. Use bonded NICs. To leverage the
     redundancy it is possible to use the same NICs as the rest of the cluster.
    </p></li></ul></div><p>
   When using SBD as STONITH mechanism, additional requirements apply
   for the shared storage. For details, see <a class="xref" href="#sec-ha-storage-protect-req" title="10.3. Requirements">Section 10.3, “Requirements”</a>.
   </p></section><section class="sect1" id="sec-ha-requirements-other" data-id-title="Other Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.4 </span><span class="title-name">Other Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-other">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For a supported and useful High Availability setup, consider the following
   recommendations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-req-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>For clusters with more than two nodes, it is strongly recommended to use
       an odd number of cluster nodes to have quorum. For more information
       about quorum, see <a class="xref" href="#sec-ha-config-basics-global" title="5.2. Quorum Determination">Section 5.2, “Quorum Determination”</a>.
     </p></dd><dt id="id-1.3.3.4.6.3.2"><span class="term">Time Synchronization</span></dt><dd><p>
     Cluster nodes must synchronize to an NTP server outside the cluster.
     For more information, see the Administration Guide for
     SUSE Linux Enterprise Server 12 SP4:
     <a class="link" href="http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html" target="_blank">http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html</a>.
    </p><p>
     If nodes are not synchronized, the cluster may not work properly.
     In addition, log files and cluster reports are very hard to analyze
     without synchronization.
     If you use the bootstrap scripts, you will be
     warned if NTP is not configured yet.
    </p></dd><dt id="id-1.3.3.4.6.3.3"><span class="term">Network Interface Card (NIC) Names</span></dt><dd><p>
      Must be identical on all nodes.
     </p></dd><dt id="id-1.3.3.4.6.3.4"><span class="term">Host Name and IP Address</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Use static IP addresses.
       </p></li><li class="listitem"><p>
     List all cluster nodes in the <code class="filename">/etc/hosts</code> file
     with their fully qualified host name and short host name. It is essential that
     members of the cluster can find each other by name. If the names are not
     available, internal cluster communication will fail.
   </p><p>
        For details on how Pacemaker gets the node names, see also
        <a class="link" href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html" target="_blank">http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html</a>.
       </p></li></ul></div></dd><dt id="vle-ha-req-ssh"><span class="term">SSH</span></dt><dd><p>
    All cluster nodes must be able to access each other via SSH. Tools
    like <code class="command">crm report</code> (for troubleshooting) and
    Hawk2's <span class="guimenu">History Explorer</span> require passwordless
    SSH access between the nodes,
    otherwise they can only collect data from the current node.
  </p><div id="id-1.3.3.4.6.3.5.2.2" data-id-title="Regulatory Requirements" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Regulatory Requirements</div><p>
       If passwordless SSH access does not comply with regulatory
       requirements, you can use the work-around described in
       <a class="xref" href="#app-crmreport-nonroot" title="Appendix D. Running Cluster Reports Without root Access">Appendix D, <em>Running Cluster Reports Without <code class="systemitem">root</code> Access</em></a> for running 
       <code class="command">crm report</code>.
      </p><p>
       For the <span class="guimenu">History Explorer</span> there is currently no
       alternative for passwordless login.
      </p></div></dd></dl></div></section></section><section class="chapter" id="cha-ha-install" data-id-title="Installing SUSE Linux Enterprise High Availability"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></span> <a title="Permalink" class="permalink" href="#cha-ha-install">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_install.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>If you are setting up a High Availability cluster with SUSE® Linux Enterprise High Availability for the first time, the
    easiest way is to start with a basic two-node cluster. You can also use the
    two-node cluster to run some tests. Afterward, you can add more
    nodes by cloning existing cluster nodes with AutoYaST. The cloned nodes will
    have the same packages installed and the same system configuration as the
    original ones.
   </p><p>
    If you want to upgrade an existing cluster that runs an older version of
    SUSE Linux Enterprise High Availability, refer to <a class="xref" href="#cha-ha-migration" title="Chapter 24. Upgrading Your Cluster and Updating Software Packages">Chapter 24, <em>Upgrading Your Cluster and Updating Software Packages</em></a>.
   </p></div></div></div></div><section class="sect1" id="sec-ha-install-manual" data-id-title="Manual Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Manual Installation</span></span> <a title="Permalink" class="permalink" href="#sec-ha-install-manual">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For the manual installation of the packages for High Availability refer to
   <a class="link" href="https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-install-quick/art-ha-install-quick.html" target="_blank">https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-install-quick/art-ha-install-quick.html</a>. It leads you through the setup of a
   basic two-node cluster.
  </p></section><section class="sect1" id="sec-ha-installation-autoyast" data-id-title="Mass Installation and Deployment with AutoYaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">Mass Installation and Deployment with AutoYaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-autoyast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have installed and set up a two-node cluster, you can extend the
   cluster by cloning existing nodes with AutoYaST and adding the clones to the cluster.
  </p><p>AutoYaST uses profiles that contains installation and configuration data.
   A profile tells AutoYaST what to install and how to configure the installed system to
   get a ready-to-use system in the end. This profile can then be used
   for mass deployment in different ways (for example, to clone existing
   cluster nodes).</p><p>
    For detailed instructions on how to use AutoYaST in various scenarios,
    see the <em class="citetitle">SUSE Linux Enterprise 12 SP4 Deployment Guide</em>, available at
    <a class="link" href="http://www.suse.com/documentation/sles-12" target="_blank">http://www.suse.com/documentation/sles-12</a>. Refer to chapter
    <em class="citetitle">Automated Installation</em>.
   </p><div id="id-1.3.3.5.4.5" data-id-title="Identical Hardware" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Identical Hardware</div><p>
     <a class="xref" href="#pro-ha-installation-clone-node" title="Cloning a Cluster Node with AutoYaST">Procedure 3.1, “Cloning a Cluster Node with AutoYaST”</a> assumes you are rolling
     out SUSE Linux Enterprise High Availability 12 SP4 to a set of machines with identical hardware
     configurations.
    </p><p>
     If you need to deploy cluster nodes on non-identical hardware, refer to the
     Deployment Guide for SUSE Linux Enterprise Server 12 SP4,
     chapter <em class="citetitle">Automated Installation</em>, section
     <em class="citetitle">Rule-Based Autoinstallation</em>.
    </p></div><div class="procedure" id="pro-ha-installation-clone-node" data-id-title="Cloning a Cluster Node with AutoYaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3.1: </span><span class="title-name">Cloning a Cluster Node with AutoYaST </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-clone-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_install.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the node you want to clone is correctly installed and
     configured. For details, see the Installation and Setup Quick Start or
     <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
    </p></li><li class="step"><p>
     Follow the description outlined in the SUSE Linux Enterprise
     12 SP4 Deployment Guide for simple mass
     installation. This includes the following basic steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Creating an AutoYaST profile. Use the AutoYaST GUI to create and modify
       a profile based on the existing system configuration. In AutoYaST,
       choose the <span class="guimenu">High Availability</span> module and click the
       <span class="guimenu">Clone</span> button. If needed, adjust the configuration
       in the other modules and save the resulting control file as XML.
      </p><p>
       If you have configured DRBD, you can select and clone this module in
       the AutoYaST GUI, too.
      </p></li><li class="step"><p>
       Determining the source of the AutoYaST profile and the parameter to
       pass to the installation routines for the other nodes.
      </p></li><li class="step"><p>
       Determining the source of the SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability
       installation data.
      </p></li><li class="step"><p>
       Determining and setting up the boot scenario for autoinstallation.
      </p></li><li class="step"><p>
       Passing the command line to the installation routines, either by
       adding the parameters manually or by creating an
       <code class="filename">info</code> file.
      </p></li><li class="step"><p>
       Starting and monitoring the autoinstallation process.
      </p></li></ol></li></ol></div></div><p>
   After the clone has been successfully installed, execute the following
   steps to make the cloned node join the cluster:
  </p><div class="procedure" id="pro-ha-installation-clone-start" data-id-title="Bringing the Cloned Node Online"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3.2: </span><span class="title-name">Bringing the Cloned Node Online </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-clone-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_install.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Transfer the key configuration files from the already configured nodes
     to the cloned node with Csync2 as described in
     <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a>.
    </p></li><li class="step"><p>
     To bring the node online, start the Pacemaker service on the cloned
     node as described in <a class="xref" href="#sec-ha-installation-start" title="4.8. Bringing the Cluster Online">Section 4.8, “Bringing the Cluster Online”</a>.
    </p></li></ol></div></div><p>
   The cloned node will now join the cluster because the
   <code class="filename">/etc/corosync/corosync.conf</code> file has been applied to
   the cloned node via Csync2. The CIB is automatically synchronized
   among the cluster nodes.
  </p></section></section><section class="chapter" id="cha-ha-setup" data-id-title="Using the YaST Cluster Module"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></span> <a title="Permalink" class="permalink" href="#cha-ha-setup">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>The YaST cluster module allows you to set up a cluster manually
    (from scratch) or to modify options for an existing cluster.
   </p><p>
    However, if you prefer an automated approach for setting up a cluster,
    see the Installation and Setup Quick Start, available from
    <a class="link" href="https://www.suse.com/documentation/" target="_blank">https://www.suse.com/documentation/</a>. It describes how to install the
    needed packages and leads you to a basic two-node cluster, which is
    set up with the <code class="systemitem">ha-cluster-bootstrap</code> scripts.
   </p><p>
    You can also use a combination of both setup methods, for example: set up
    one node with YaST cluster and then use one of the bootstrap scripts
    to integrate more nodes (or vice versa).
   </p></div></div></div></div><section class="sect1" id="sec-ha-installation-terms" data-id-title="Definition of Terms"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Definition of Terms</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-terms">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Several key terms used in the YaST cluster module and in this chapter are
   defined below.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.6.3.3.1"><span class="term">Bind Network Address (<code class="systemitem">bindnetaddr</code>)
    </span></dt><dd><p>
      The network address the Corosync executive should bind to.  To simplify sharing configuration files across
      the cluster, Corosync uses network interface netmask to mask only
      the address bits that are used for routing the network. For example,
      if the local interface is <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.0</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.0</code>. If the local interface is
      <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.192</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.64</code>.
     </p><div id="id-1.3.3.6.3.3.1.2.2" data-id-title="Network Address for All Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Network Address for All Nodes</div><p>
       As the same Corosync configuration will be used on all nodes,
       make sure to use a network address as
       <code class="systemitem">bindnetaddr</code>, not the address of a specific
       network interface.
      </p></div></dd><dt id="id-1.3.3.6.3.3.2"><span class="term"><code class="systemitem">conntrack</code> Tools</span></dt><dd><p>
      Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes. For detailed information, refer to
      <a class="link" href="http://conntrack-tools.netfilter.org/" target="_blank">http://conntrack-tools.netfilter.org/</a>.
     </p></dd><dt id="id-1.3.3.6.3.3.3"><span class="term">Csync2</span></dt><dd><p>
      A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters. Csync2 can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which ﬁles
      should be synchronized in the synchronization group. The groups, the
      host names belonging to each group, and the include/exclude rules for
      each group are specified in the Csync2 configuration file,
      <code class="filename">/etc/csync2/csync2.cfg</code>.
     </p><p>
      For authentication, Csync2 uses the IP addresses and pre-shared
      keys within a synchronization group. You need to generate one key file
      for each synchronization group and copy it to all group members.
     </p><p>
      For more information about Csync2, refer to
      <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>
     </p></dd><dt id="id-1.3.3.6.3.3.4"><span class="term">Existing Cluster</span></dt><dd><p>
        The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
     </p></dd><dt id="id-1.3.3.6.3.3.5"><span class="term">Multicast</span></dt><dd><p>
        A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast. If multicast does not comply with your corporate IT
      policy, use unicast instead.
     </p><div id="id-1.3.3.6.3.3.5.2.2" data-id-title="Switches and Multicast" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Switches and Multicast</div><p>
       To use multicast for cluster communication, make sure
       your switches support multicast.
      </p></div></dd><dt id="vle-ha-mcastaddr"><span class="term">Multicast Address (<code class="systemitem">mcastaddr</code>)
   </span></dt><dd><p>
        IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6.  If IPv6 networking is used, node IDs must be
      specified. You can use any multicast address in your private network.
     </p></dd><dt id="id-1.3.3.6.3.3.7"><span class="term">Multicast Port (<code class="systemitem">mcastport</code>)</span></dt><dd><p>
        The port to use for cluster communication. Corosync uses two ports: the specified
      <code class="literal">mcastport</code> for receiving multicast, and
      <code class="literal">mcastport -1</code> for sending multicast.
     </p></dd><dt id="vle-ha-rrp"><span class="term">Redundant Ring Protocol (RRP)</span></dt><dd><p>
       Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol. A logical token-passing ring is imposed on all
      participating nodes to deliver messages in a reliable and sorted
      manner. A node is allowed to broadcast a message only if it holds the
      token.
     </p><p>
      When having defined redundant communication channels in Corosync,
      use RRP to tell the cluster how to use these interfaces. RRP can have
      three modes (<code class="literal">rrp_mode</code>):
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If set to <code class="literal">active</code>, Corosync uses both
        interfaces actively. However, this mode is deprecated.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">passive</code>, Corosync sends messages
        alternatively over the available networks.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">none</code>, RRP is disabled.
       </p></li></ul></div></dd><dt id="id-1.3.3.6.3.3.9"><span class="term">Unicast</span></dt><dd><p>
        A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync, unicast
    is implemented as UDP-unicast (UDPU).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-setup-yast-overview" data-id-title="YaST Cluster Module"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">YaST Cluster Module</span></span> <a title="Permalink" class="permalink" href="#sec-ha-setup-yast-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Start YaST and select <span class="guimenu">High Availability</span> › <span class="guimenu">Cluster</span>. Alternatively, start the
    module from command line:
   </p><div class="verbatim-wrap"><pre class="screen">sudo yast2 cluster</pre></div><p>
   The following list shows an overview of the available screens in the
   YaST cluster module. It also mentions whether the screen contains parameters that
   are <span class="emphasis"><em>required</em></span> for successful cluster setup or whether its
   parameters are <span class="emphasis"><em>optional</em></span>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.6.4.5.1"><span class="term">Communication Channels (required)</span></dt><dd><p> Allows you to define one or two communication channels for
      communication between the cluster nodes. As transport protocol,
      either use multicast (UDP) or unicast (UDPU). For details, see
      <a class="xref" href="#sec-ha-installation-setup-channels" title="4.3. Defining the Communication Channels">Section 4.3, “Defining the Communication Channels”</a>.</p><div id="id-1.3.3.6.4.5.1.2.2" data-id-title="Redundant Communication Paths" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant Communication Paths</div><p>For a supported cluster setup two or more redundant communication
       paths are required. The preferred way is to use network device bonding as
       described in <a class="xref" href="#cha-ha-netbonding" title="Chapter 12. Network Device Bonding">Chapter 12, <em>Network Device Bonding</em></a>.</p><p>If this is impossible, you need to define a second communication
       channel in Corosync.</p></div></dd><dt id="id-1.3.3.6.4.5.2"><span class="term">Security (optional but recommended)</span></dt><dd><p>Allows you to define the authentication settings for the cluster.
        HMAC/SHA1 authentication requires a shared secret used
        to protect and authenticate messages. For details, see
        <a class="xref" href="#sec-ha-installation-setup-security" title="4.4. Defining Authentication Settings">Section 4.4, “Defining Authentication Settings”</a>.
       </p></dd><dt id="id-1.3.3.6.4.5.3"><span class="term">Configure Csync2 (optional but recommended)</span></dt><dd><p>
      Csync2 helps you to keep track of configuration changes and to
      keep files synchronized across the cluster nodes. For details, see
      <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a>.
     </p></dd><dt id="id-1.3.3.6.4.5.4"><span class="term">Configure conntrackd (optional)</span></dt><dd><p>
          Allows you to configure the user space
          <code class="systemitem">conntrackd</code>. Use the conntrack
          tools for <span class="emphasis"><em>stateful</em></span> packet inspection for iptables.
          For details, see <a class="xref" href="#sec-ha-installation-setup-conntrackd" title="4.5. Synchronizing Connection Status Between Cluster Nodes">Section 4.5, “Synchronizing Connection Status Between Cluster Nodes”</a>.
         </p></dd><dt id="id-1.3.3.6.4.5.5"><span class="term">Service (required)</span></dt><dd><p>
      Allows you to configure the service for bringing the cluster node online.
      Define whether to start the Pacemaker service at boot time and whether to open the
      ports in the firewall that are needed for communication between the nodes.
      For details, see <a class="xref" href="#sec-ha-installation-setup-services" title="4.6. Configuring Services">Section 4.6, “Configuring Services”</a>.
     </p></dd></dl></div><p>
    If you start the cluster module for the first time, it appears as a
    wizard, guiding you through all the steps necessary for basic setup.
    Otherwise, click the categories on the left panel to access the
    configuration options for each step.
   </p><div id="id-1.3.3.6.4.7" data-id-title="Settings in the YaST Cluster Module" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings in the YaST Cluster Module</div><p>Some settings in the YaST cluster module apply only to the
      current node. Other settings may automatically be transferred to all nodes
      with Csync2. Find detailed information about this in the following
      sections.
    </p></div></section><section class="sect1" id="sec-ha-installation-setup-channels" data-id-title="Defining the Communication Channels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Defining the Communication Channels</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-channels">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For successful communication between the cluster nodes, define at least
    one communication channel. As transport protocol, either use multicast (UDP)
    or unicast (UDPU) as described in <a class="xref" href="#pro-ha-installation-setup-channel1-udp" title="Defining the First Communication Channel (Multicast)">Procedure 4.1</a>
    or <a class="xref" href="#pro-ha-installation-setup-channel1-udpu" title="Defining the First Communication Channel (Unicast)">Procedure 4.2</a>, respectively.
    If you want to define a second, redundant channel
    (<a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3</a>),
    both communication channels must use the <span class="emphasis"><em>same</em></span> protocol.
   </p><p>All settings defined in the YaST
    <span class="guimenu">Communication Channels</span>
    screen are written to <code class="filename">/etc/corosync/corosync.conf</code>. Find example
    files for a multicast and a unicast setup in
    <code class="filename">/usr/share/doc/packages/corosync/</code>.
   </p><p>If you are using IPv4 addresses, node IDs are optional. If you are using
    IPv6 addresses, node IDs are required. Instead of specifying IDs manually
    for each node, the YaST cluster module contains an option to automatically
    generate a unique ID for every cluster node.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udp" data-id-title="Defining the First Communication Channel (Multicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.1: </span><span class="title-name">Defining the First Communication Channel (Multicast) </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel1-udp">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     When using multicast, the same <code class="systemitem">bindnetaddr</code>,
    <code class="systemitem">mcastaddr</code>, and <code class="systemitem">mcastport</code>
     will be used for all cluster nodes. All nodes in the cluster will know each
     other by using the same multicast address. For different clusters, use
     different multicast addresses.
    
    </p><ol class="procedure" type="1"><li class="step"><p>
       Start the YaST cluster module and switch to the <span class="guimenu">Communication
        Channels</span> category.
      </p></li><li class="step"><p>
      Set the <span class="guimenu">Transport</span> protocol to
      <code class="literal">Multicast</code>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Bind Network Address</span>. Set the value to
      the subnet you will use for cluster multicast.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Port</span>.
     </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.3.3.6.5.6"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_comm_multicast.png"><img src="images/yast2_cluster_comm_multicast.png" width="75%" alt="YaST Cluster—Multicast Configuration" title="YaST Cluster—Multicast Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.1: </span><span class="title-name">YaST Cluster—Multicast Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.5.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div><p>If you want to use unicast instead of multicast for cluster
   communication, proceed as follows.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udpu" data-id-title="Defining the First Communication Channel (Unicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.2: </span><span class="title-name">Defining the First Communication Channel (Unicast) </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel1-udpu">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Communication
       Channels</span> category.
     </p></li><li class="step"><p>
        Set the <span class="guimenu">Transport</span> protocol to
        <code class="literal">Unicast</code>.
       </p></li><li class="step"><p>
        Define the <span class="guimenu">Port</span>.
       </p></li><li class="step"><p>
        For unicast communication, Corosync needs to know the IP
        addresses of all nodes in the cluster. For each node that will be
        part of the cluster, click <span class="guimenu">Add</span> and enter the
        following details:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <span class="guimenu">IP Address</span>
         </p></li><li class="listitem"><p>
          <span class="guimenu">Redundant IP Address</span> (only required if you use
          a second communication channel in Corosync)
         </p></li><li class="listitem"><p>
          <span class="guimenu">Node ID</span> (only required if the option
          <span class="guimenu">Auto Generate Node ID</span> is disabled)
         </p></li></ul></div><p>
        To modify or remove any addresses of cluster members, use the
        <span class="guimenu">Edit</span> or <span class="guimenu">Del</span> buttons.
       </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.3.3.6.5.9"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_comm_unicast.png"><img src="images/yast2_cluster_comm_unicast.png" width="75%" alt="YaST Cluster—Unicast Configuration" title="YaST Cluster—Unicast Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.2: </span><span class="title-name">YaST Cluster—Unicast Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.5.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div><p>
    If network device bonding cannot be used for any reason, the second
    best choice is to define a redundant communication channel (a second
    ring) in Corosync. That way, two physically separate networks can
    be used for communication. If one network fails, the cluster nodes
    can still communicate via the other network.
   </p><p>The additional communication channel in
    Corosync will form a second token-passing ring. In
    <code class="filename">/etc/corosync/corosync.conf</code>, the first channel you
    configured is the primary ring and gets the ringnumber
   <code class="literal">0</code>. The second ring (redundant channel) gets the ringnumber
    <code class="literal">1</code>.
   </p><p>When having defined redundant communication channels in Corosync,
    use RRP to tell the cluster how to use these interfaces. With RRP, two
    physically separate networks are used for communication. If one
    network fails, the cluster nodes can still communicate via the other
    network.</p><p>RRP can have three modes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If set to <code class="literal">active</code>, Corosync uses both
      interfaces actively. However, this mode is deprecated.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">passive</code>, Corosync sends messages
      alternatively over the available networks.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">none</code>, RRP is disabled.
     </p></li></ul></div><div class="procedure" id="pro-ha-installation-setup-channel2" data-id-title="Defining a Redundant Communication Channel"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.3: </span><span class="title-name">Defining a Redundant Communication Channel </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.3.3.6.5.15.2" data-id-title="Redundant Rings and /etc/hosts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant Rings and <code class="filename">/etc/hosts</code></div><p> If multiple rings are configured in Corosync, each node can
     have multiple IP addresses. This needs to be reflected in the
      <code class="filename">/etc/hosts</code> file of all nodes. </p></div><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Communication Channels</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Redundant Channel</span>. The redundant channel
     must use the same protocol as the first communication channel you defined.
    </p></li><li class="step"><p> If you use multicast, enter the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, the <span class="guimenu">Multicast
      Address</span> and the <span class="guimenu">Port</span> for the
     redundant channel. </p><p> If you use unicast, define the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, and the
     <span class="guimenu">Port</span>. Enter the IP addresses of all nodes that will be part of
     the cluster. </p></li><li class="step"><p>To tell Corosync how and when to use the different channels,
     select the <span class="guimenu">rrp_mode</span> to use:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> If only one communication channel is defined,
        <span class="guimenu">rrp_mode</span> is automatically disabled (value
        <code class="literal">none</code>).</p></li><li class="listitem"><p> If set to <code class="literal">active</code>, Corosync uses both
       interfaces actively. However, this mode is deprecated.</p></li><li class="listitem"><p> If set to <code class="literal">passive</code>, Corosync sends messages
       alternatively over the available networks. </p></li></ul></div><p>When RRP is used, SUSE Linux Enterprise High Availability monitors the status of the current
     rings and automatically re-enables redundant rings after faults.</p><p>Alternatively, check the ring status manually with
     <code class="command">corosync-cfgtool</code>. View the available options with
      <code class="option">-h</code>. </p></li><li class="step"><p> Confirm your changes. </p></li></ol></div></div></section><section class="sect1" id="sec-ha-installation-setup-security" data-id-title="Defining Authentication Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Defining Authentication Settings</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-security">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To define the authentication settings for the cluster, you can use HMAC/SHA1
    authentication. This requires a shared secret used
    to protect and authenticate messages. The authentication key (password)
    you specify will be used on all nodes in the cluster.
   </p><div class="procedure" id="pro-ha-installation-setup-security" data-id-title="Enabling Secure Authentication"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.4: </span><span class="title-name">Enabling Secure Authentication </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-security">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Security</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Enable Security Auth</span>. </p></li><li class="step"><p> For a newly created cluster, click <span class="guimenu">Generate Auth Key
      File</span>. An authentication key is created and written to
      <code class="filename">/etc/corosync/authkey</code>. </p><p> If you want the current machine to join an existing cluster, do not
     generate a new key file. Instead, copy the
      <code class="filename">/etc/corosync/authkey</code> from one of the nodes to the
     current machine (either manually or with Csync2). </p></li><li class="step"><p> Confirm your changes. YaST writes the configuration to
      <code class="filename">/etc/corosync/corosync.conf</code>. </p></li></ol></div></div><div class="figure" id="id-1.3.3.6.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_security.png"><img src="images/yast2_cluster_security.png" width="75%" alt="YaST Cluster—Security" title="YaST Cluster—Security"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.3: </span><span class="title-name">YaST Cluster—Security </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-conntrackd" data-id-title="Synchronizing Connection Status Between Cluster Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">Synchronizing Connection Status Between Cluster Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-conntrackd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To enable <span class="emphasis"><em>stateful</em></span> packet inspection for iptables,
    configure and use the conntrack tools. This requires the following basic
    steps:
   </p><div class="procedure" id="pro-ha-installation-setup-conntrackd" data-id-title="Configuring the conntrackd with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.5: </span><span class="title-name">Configuring the <code class="systemitem">conntrackd</code> with YaST </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     Use the YaST cluster module to configure the user space
     <code class="systemitem">conntrackd</code> (see <a class="xref" href="#fig-ha-installation-setup-conntrackd" title="YaST Cluster—conntrackd">Figure 4.4, “YaST Cluster—<code class="systemitem">conntrackd</code>”</a>).  It needs a
     dedicated network interface that is not used for other communication
     channels. The daemon can be started via a resource agent afterward.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Configure
      conntrackd</span> category.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span> to be used for
      synchronizing the connection status.
     </p></li><li class="step"><p>
      In <span class="guimenu">Group Number</span>, define a numeric ID for the group
      to synchronize the connection status to.
      
     </p></li><li class="step"><p>
      Click <span class="guimenu">Generate /etc/conntrackd/conntrackd.conf</span> to
      create the configuration file for
      <code class="systemitem">conntrackd</code>.
     </p></li><li class="step"><p>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module.
     </p></li><li class="step"><p>
      For further cluster configuration, click <span class="guimenu">Next</span> and
      proceed with <a class="xref" href="#sec-ha-installation-setup-services" title="4.6. Configuring Services">Section 4.6, “Configuring Services”</a>.
     </p></li><li class="step"><p>
      Select a <span class="guimenu">Dedicated Interface</span> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in YaST. It must already be
      configured and it must support multicast.
      
     </p></li></ol></div></div><div class="figure" id="fig-ha-installation-setup-conntrackd"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_conntrackd.png"><img src="images/yast2_cluster_conntrackd.png" width="75%" alt="YaST Cluster—conntrackd" title="YaST Cluster—conntrackd"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.4: </span><span class="title-name">YaST Cluster—<code class="systemitem">conntrackd</code> </span></span><a title="Permalink" class="permalink" href="#fig-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div><p>
    After having configured the conntrack tools, you can use them for Linux Virtual Server,
    see <a class="xref" href="#cha-ha-lb" title="Chapter 13. Load Balancing"><em>Load Balancing</em></a>.
   </p></section><section class="sect1" id="sec-ha-installation-setup-services" data-id-title="Configuring Services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">Configuring Services</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-services">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In the YaST cluster module define whether to start certain services
    on a node at boot time. You can also use the module to start and stop
    the services manually. To bring the cluster nodes online and start the
    cluster resource manager, Pacemaker must be running as a service.
   </p><div class="procedure" id="pro-ha-installation-setup-services" data-id-title="Enabling Pacemaker"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.6: </span><span class="title-name">Enabling Pacemaker </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-services">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      In the YaST cluster module, switch to the
      <span class="guimenu">Service</span> category.
     </p></li><li class="step"><p>
      To start Pacemaker each time this cluster node is booted, select the
      respective option in the <span class="guimenu">Booting</span> group. If you
      select <span class="guimenu">Off</span> in the <span class="guimenu">Booting</span> group,
      you must start Pacemaker manually each time this node is booted. To
      start Pacemaker manually, use the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></li><li class="step"><p>
      To start or stop Pacemaker immediately, click the respective button.
     </p></li><li class="step"><p>
      To open the ports in the firewall that are needed for cluster
      communication on the current machine, activate <span class="guimenu">Open Port in
      Firewall</span>. The configuration is written to
      <code class="filename">/etc/sysconfig/SuSEfirewall2.d/services/cluster</code>.
     </p></li><li class="step"><p>
      Confirm your changes. Note that the configuration only
      applies to the current machine, not to all cluster nodes.
     </p></li></ol></div></div><div class="figure" id="id-1.3.3.6.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_services.png"><img src="images/yast2_cluster_services.png" width="75%" alt="YaST Cluster—Services" title="YaST Cluster—Services"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.5: </span><span class="title-name">YaST Cluster—Services </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-csync2" data-id-title="Transferring the Configuration to All Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.7 </span><span class="title-name">Transferring the Configuration to All Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-csync2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Instead of copying the resulting configuration files to all nodes
    manually, use the <code class="command">csync2</code> tool for replication across
    all nodes in the cluster.
   </p><p>
    This requires the following basic steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Configuring Csync2 with YaST</a>.
     </p></li><li class="step"><p>
      <a class="xref" href="#pro-ha-installation-setup-csync2-start" title="Synchronizing the Configuration Files with Csync2">Synchronizing the Configuration Files with Csync2</a>.
     </p></li></ol></div></div><p>
    Csync2 helps you to keep track of configuration changes and to keep
    files synchronized across the cluster nodes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can define a list of files that are important for operation.
     </p></li><li class="listitem"><p>
      You can show changes to these files (against the other cluster nodes).
     </p></li><li class="listitem"><p>
      You can synchronize the configured files with a single command.
     </p></li><li class="listitem"><p>
      With a simple shell script in <code class="filename">~/.bash_logout</code>, you
      can be reminded about unsynchronized changes before logging out of the
      system.
     </p></li></ul></div><p>
    Find detailed information about Csync2 at
    <a class="link" href="http://oss.linbit.com/csync2/" target="_blank">http://oss.linbit.com/csync2/</a> and
    <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>.
   </p><section class="sect2" id="pro-ha-installation-setup-csync2-yast" data-id-title="Configuring Csync2 with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.1 </span><span class="title-name">Configuring Csync2 with YaST</span></span> <a title="Permalink" class="permalink" href="#pro-ha-installation-setup-csync2-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.3.3.6.9.8.2" data-id-title="Configuring Csync2 with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.7: </span><span class="title-name">Configuring Csync2 with YaST </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.9.8.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
       <span class="guimenu">Csync2</span> category. </p></li><li class="step"><p> To specify the synchronization group, click <span class="guimenu">Add</span>
      in the <span class="guimenu">Sync Host</span> group and enter the local host names
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <code class="command">hostname</code> command. </p><div id="id-1.3.3.6.9.8.2.3.2" data-id-title="Host name resolution" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Host name resolution</div><p> If host name resolution does not work properly in your
       network, you can also specify a combination of host name and IP address
       for each cluster node. To do so, use the string
        <em class="replaceable">HOSTNAME@IP</em> such as
        <code class="literal">alice@192.168.2.100</code>, for example. Csync2
       will then use the IP addresses when connecting. </p></div></li><li class="step" id="step-csync2-generate-key"><p> Click <span class="guimenu">Generate Pre-Shared-Keys</span> to create a key
      file for the synchronization group. The key file is written to
       <code class="filename">/etc/csync2/key_hagroup</code>. After it has been created,
      it must be copied manually to all members of the cluster. </p></li><li class="step"><p> To populate the <span class="guimenu">Sync File</span> list with the files
      that usually need to be synchronized among all nodes, click <span class="guimenu">Add
       Suggested Files</span>. </p></li><li class="step"><p> To <span class="guimenu">Edit</span>, <span class="guimenu">Add</span> or
       <span class="guimenu">Remove</span> files from the list of files to be synchronized
      use the respective buttons. You must enter the absolute path name for each
      file. </p></li><li class="step"><p> Activate Csync2 by clicking <span class="guimenu">Turn Csync2
       ON</span>. This will execute the following command to start
      Csync2 automatically at boot time: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable csync2.socket</pre></div></li><li class="step"><p>Click <span class="guimenu">Finish</span>. YaST writes the Csync2
      configuration to <code class="filename">/etc/csync2/csync2.cfg</code>.</p></li></ol></div></div><div class="figure" id="id-1.3.3.6.9.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_cluster_sync.png"><img src="images/yast2_cluster_sync.png" width="75%" alt="YaST Cluster—Csync2" title="YaST Cluster—Csync2"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.6: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—Csync2 </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.9.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-setup-yast-csync2-sync" data-id-title="Synchronizing Changes with Csync2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.2 </span><span class="title-name">Synchronizing Changes with Csync2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-setup-yast-csync2-sync">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p> Before running Csync2 for the first time, you need to make the
    following preparations: </p><div class="procedure" id="id-1.3.3.6.9.9.3" data-id-title="Preparing for Initial Synchronization with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.8: </span><span class="title-name">Preparing for Initial Synchronization with Csync2 </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.9.9.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Copy the file <code class="filename">/etc/csync2/csync2.cfg</code>
      manually to all nodes after you have configured it as described in <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1, “Configuring Csync2 with YaST”</a>. </p></li><li class="step"><p> Copy the file <code class="filename">/etc/csync2/key_hagroup</code> that you
      have generated on one node in <a class="xref" href="#step-csync2-generate-key" title="Step 3">Step 3</a>
      of <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1</a>
      to <span class="emphasis"><em>all</em></span> nodes in the cluster. It is needed for
      authentication by Csync2. However, do <span class="emphasis"><em>not</em></span>
      regenerate the file on the other nodes—it needs to be the same
      file on all nodes. </p></li><li class="step"><p>Execute the following command on all nodes to start the service now: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start csync2.socket</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-installation-setup-csync2-start" data-id-title="Synchronizing the Configuration Files with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.9: </span><span class="title-name">Synchronizing the Configuration Files with Csync2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-csync2-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>To initially synchronize all files once, execute the following
      command on the machine that you want to copy the configuration
       <span class="emphasis"><em>from</em></span>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p> This will synchronize all the files once by pushing them to the
      other nodes. If all files are synchronized successfully, Csync2 will
      finish with no errors. </p><p> If one or several files that are to be synchronized have been
      modified on other nodes (not only on the current one), Csync2
      reports a conflict. You will get an output similar to the one below: </p><div class="verbatim-wrap"><pre class="screen">While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</pre></div></li><li class="step"><p> If you are sure that the file version on the current node is the
       <span class="quote">“<span class="quote">best</span>”</span> one, you can resolve the conflict by forcing this
      file and resynchronizing: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-f</code> /etc/corosync/corosync.conf
<code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-x</code></pre></div></li></ol></div></div><p> For more information on the Csync2 options, run</p><div class="verbatim-wrap"><pre class="screen">csync2 -help</pre></div><div id="id-1.3.3.6.9.9.7" data-id-title="Pushing synchronization after any changes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Pushing synchronization after any changes</div><p> Csync2 only pushes changes. It does <span class="emphasis"><em>not</em></span>
     continuously synchronize files between the machines. </p><p> Each time you update files that need to be synchronized, you need to
     push the changes to the other machines: Run <code class="command">csync2 </code>
     <code class="option">-xv</code> on the machine where you did the changes. If you run
     the command on any of the other machines with unchanged files, nothing will
     happen. </p></div></section></section><section class="sect1" id="sec-ha-installation-start" data-id-title="Bringing the Cluster Online"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.8 </span><span class="title-name">Bringing the Cluster Online</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After the initial cluster configuration is done, start the Pacemaker
    service on <span class="emphasis"><em>each</em></span> cluster node to bring the stack
    online:
   </p><div class="procedure" id="id-1.3.3.6.10.3" data-id-title="Starting Pacemaker and Checking the Status"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.10: </span><span class="title-name">Starting Pacemaker and Checking the Status </span></span><a title="Permalink" class="permalink" href="#id-1.3.3.6.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_installation.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to an existing node.
     </p></li><li class="step"><p>
      Check if the service is already running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status pacemaker</pre></div><p>
      If not, start Pacemaker now:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></li><li class="step"><p>
      Repeat the steps above for each of the cluster nodes.
     </p></li><li class="step"><p>
      On one of the nodes, check the cluster status with the
      <code class="command">crm status</code> command. If all nodes are
      online, the output should be similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status
Last updated: Thu Jul  3 11:07:10 2014
Last change: Thu Jul  3 10:58:43 2014
Current DC: alice (175704363) - partition with quorum
2 Nodes configured
0 Resources configured

Online: [ alice bob ]</pre></div><p>
      This output indicates that the cluster resource manager is started and
      is ready to manage resources.
     </p></li></ol></div></div><p>
    After the basic configuration is done and the nodes are online, you can
    start to configure cluster resources. Use one of the cluster management
    tools like the crm shell (crmsh) or the HA Web Console. For more
    information, see <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a> or
    <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>.
   </p></section></section></div><div class="part" id="part-config" data-id-title="Configuration and Administration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part II </span><span class="title-name">Configuration and Administration </span></span><a title="Permalink" class="permalink" href="#part-config">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-config-basics"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></span></li><dd class="toc-abstract"><p>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped—the nature of the service is
    irrelevant to the cluster.
   </p><p>
    In this chapter, we will introduce some basic concepts you need to know
    when configuring resources and administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools SUSE Linux Enterprise High Availability
    provides.
   </p></dd><li><span class="chapter"><a href="#cha-conf-hawk2"><span class="title-number">6 </span><span class="title-name">Configuring and Managing Cluster Resources with Hawk2</span></a></span></li><dd class="toc-abstract"><p>
    To configure and manage cluster resources, either use HA Web Console (Hawk2), or
    the crm shell (crmsh) command line utility. If you upgrade from an
    earlier version of SUSE® Linux Enterprise High Availability where Hawk was installed, the package
    will be replaced with the current version, Hawk2.
   </p><p>
    Hawk2's Web-based user-interface allows you to monitor and administer
    your Linux cluster from non-Linux machines. Furthermore, it is the ideal
    solution in case your system only provides a minimal graphical user
    interface.
   </p></dd><li><span class="chapter"><a href="#cha-ha-manual-config"><span class="title-number">7 </span><span class="title-name">Configuring and Managing Cluster Resources (Command Line)</span></a></span></li><dd class="toc-abstract"><p>
    To configure and manage cluster resources, either use the crm shell
    (crmsh) command line utility or HA Web Console (Hawk2), a Web-based
    user interface.
   </p><p>
    This chapter introduces <code class="command">crm</code>, the command line tool
    and covers an overview of this tool, how to use templates, and mainly
    configuring and managing cluster resources: creating basic and advanced
    types of resources (groups and clones), configuring constraints,
    specifying failover nodes and failback nodes, configuring resource
    monitoring, starting, cleaning up or removing resources, and migrating
    resources manually.
   </p></dd><li><span class="chapter"><a href="#cha-ha-agents"><span class="title-number">8 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></span></li><dd class="toc-abstract"><p>
    All tasks that need to be managed by a cluster must be available as a
    resource. There are two major groups here to consider: resource agents
    and STONITH agents. For both categories, you can add your own
    agents, extending the abilities of the cluster to your own needs.
   </p></dd><li><span class="chapter"><a href="#cha-ha-fencing"><span class="title-number">9 </span><span class="title-name">Fencing and STONITH</span></a></span></li><dd class="toc-abstract"><p>
    Fencing is a very important concept in computer clusters for HA (High
    Availability). A cluster sometimes detects that one of the nodes is
    behaving strangely and needs to remove it. This is called
    <span class="emphasis"><em>fencing</em></span> and is commonly done with a STONITH
    resource. Fencing may be defined as a method to bring an HA cluster to a
    known state.
   </p><p>
    Every resource in a cluster has a state attached. For example:
    <span class="quote">“<span class="quote">resource r1 is started on alice</span>”</span>. In an HA cluster, such
    a state implies that <span class="quote">“<span class="quote">resource r1 is stopped on all nodes except
    alice</span>”</span>, because the cluster must make sure that every resource
    may be started on only one node. Every node must report every change
    that happens to a resource. The cluster state is thus a collection of
    resource states and node states.
   </p><p>
    When the state of a node or resource cannot be established with
    certainty, fencing comes in. Even when the cluster is not aware of what
    is happening on a given node, fencing can ensure that the node does not
    run any important resources.
   </p></dd><li><span class="chapter"><a href="#cha-ha-storage-protect"><span class="title-number">10 </span><span class="title-name">Storage Protection and SBD</span></a></span></li><dd class="toc-abstract"><p>
    SBD (STONITH Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </p><p>
    The <span class="package">ha-cluster-bootstrap</span> scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <em class="citetitle">Installation and Setup Quick Start</em>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </p><p>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split brain scenario.
   </p><p>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM2 exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </p></dd><li><span class="chapter"><a href="#cha-ha-acl"><span class="title-number">11 </span><span class="title-name">Access Control Lists</span></a></span></li><dd class="toc-abstract"><p>
    The cluster administration tools like crm shell (crmsh) or
    Hawk2 can be used by <code class="systemitem">root</code> or any user in the group
    <code class="systemitem">haclient</code>. By default, these
    users have full read/write access. To limit access or assign more
    fine-grained access rights, you can use <span class="emphasis"><em>Access control
    lists</em></span> (ACLs).
   </p><p>
    Access control lists consist of an ordered set of access rules. Each
    rule allows read or write access or denies access to a part of the
    cluster configuration. Rules are typically combined to produce a
    specific role, then users may be assigned to a role that matches their
    tasks.
   </p></dd><li><span class="chapter"><a href="#cha-ha-netbonding"><span class="title-number">12 </span><span class="title-name">Network Device Bonding</span></a></span></li><dd class="toc-abstract"><p>
   For many systems, it is desirable to implement network connections that
   comply to more than the standard data security or availability
   requirements of a typical Ethernet device. In these cases, several
   Ethernet devices can be aggregated to a single bonding device.
   </p></dd><li><span class="chapter"><a href="#cha-ha-lb"><span class="title-number">13 </span><span class="title-name">Load Balancing</span></a></span></li><dd class="toc-abstract"><p>
  <span class="emphasis"><em>Load Balancing</em></span> makes a cluster of servers appear as
  one large, fast server to outside clients. This apparent single server is
  called a <span class="emphasis"><em>virtual server</em></span>. It consists of one or more
  load balancers dispatching incoming requests and several real servers
  running the actual services. With a load balancing setup of SUSE Linux Enterprise High Availability, you
  can build highly scalable and highly available network services, such as
  Web, cache, mail, FTP, media and VoIP services.
 </p></dd><li><span class="chapter"><a href="#cha-ha-geo"><span class="title-number">14 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></span></li><dd class="toc-abstract"><p>
    Apart from local clusters and metro area clusters, SUSE® Linux Enterprise High Availability
    12 SP4 also supports geographically dispersed clusters (Geo
    clusters, sometimes also called multi-site clusters). That means you can
    have multiple, geographically dispersed sites with a local cluster each.
    Failover between these clusters is coordinated by a higher level entity,
    the so-called <code class="literal">booth</code>. Support for Geo clusters is
    available as a separate extension to Geo Clustering for SUSE Linux Enterprise High Availability. For details on how to
    use and set up Geo clusters, refer to the <em class="citetitle">Geo Clustering Quick Start</em>
    or the <em class="citetitle">Geo Clustering Guide</em>. Both are available from
    <a class="link" href="http://www.suse.com/documentation/sle-ha-geo-12" target="_blank">http://www.suse.com/documentation/sle-ha-geo-12</a>.
   </p></dd></ul></div><section class="chapter" id="cha-ha-config-basics" data-id-title="Configuration and Administration Basics"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></span> <a title="Permalink" class="permalink" href="#cha-ha-config-basics">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped—the nature of the service is
    irrelevant to the cluster.
   </p><p>
    In this chapter, we will introduce some basic concepts you need to know
    when configuring resources and administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools SUSE Linux Enterprise High Availability
    provides.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-scenarios" data-id-title="Use Case Scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-scenarios">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>In general, clusters fall into one of two categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Two-node clusters</p></li><li class="listitem"><p>Clusters with more than two nodes. This usually means an odd number of nodes.</p></li></ul></div><p>
    Adding also different topologies, different use cases can be derived.
    The following use cases are the most common:
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name"> </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.4.3.3.5.2"><span class="term">Two-node cluster in one location</span></dt><dd><p><span class="formalpara-title">Configuration: </span>FC SAN or similar shared storage, layer 2 network.</p><p><span class="formalpara-title">Usage scenario: </span>Embedded clusters that focus on service high
       availability and not data redundancy for data replication.
       Such a setup is used for radio stations or assembly line controllers,
       for example.
       </p></dd><dt id="vl-2x2node-2locs"><span class="term">Two-node clusters in two locations (most widely used)</span></dt><dd><p><span class="formalpara-title">Configuration: </span>Symmetrical stretched cluster, FC SAN, and layer 2 network
        all across two locations.</p><p><span class="formalpara-title">Usage scenario: </span>Classic stretched clusters, focus on high availability of services
        and local data redundancy. For databases and enterprise
        resource planning. One of the most popular setups during the last few
        years.
       </p></dd><dt id="vl-n-nodes-3locs"><span class="term">Odd number of nodes in three locations</span></dt><dd><p><span class="formalpara-title">Configuration: </span>2×N+1 nodes, FC SAN across two main locations. Auxiliary
        third site with no FC SAN, but acts as a majority maker.
        Layer 2 network at least across two main locations.
       </p><p><span class="formalpara-title">Usage scenario: </span>Classic stretched cluster, focus on high availability of services
        and data redundancy. For example, databases, enterprise resource planning.
       </p></dd></dl></div></section><section class="sect1" id="sec-ha-config-basics-global" data-id-title="Quorum Determination"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Whenever communication fails between one or more nodes and the rest of the
   cluster, a cluster partition occurs. The nodes can only communicate with
   other nodes in the same partition and are unaware of the separated nodes.
   A cluster partition is defined as having quorum (can <span class="quote">“<span class="quote">quorate</span>”</span>)
   if it has the majority of nodes (or votes).
   How this is achieved is done by <span class="emphasis"><em>quorum calculation</em></span>.
   Quorum is a requirement for fencing.
   </p><p>
   Quorum calculation has changed between SUSE Linux Enterprise High Availability 11 and
   SUSE Linux Enterprise High Availability 12. For SUSE Linux Enterprise High Availability 11, quorum was calculated by
   Pacemaker.
   Starting with SUSE Linux Enterprise High Availability 12, Corosync can handle quorum for
   two-node clusters directly without changing the Pacemaker configuration.
  </p><p>How quorum is calculated is influenced by the following factors:</p><div class="variablelist"><dl class="variablelist"><dt id="vl-ha-config-basics-global-number-of-cluster-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>To keep services running, a cluster with more than two nodes
       relies on quorum (majority vote) to resolve cluster partitions.
       Based on the following formula, you can calculate the minimum
       number of operational nodes required for the cluster to function:</p><div class="verbatim-wrap"><pre class="screen">N ≥ C/2 + 1

N = minimum number of operational nodes
C = number of cluster nodes</pre></div><p>For example, a five-node cluster needs a minimum of three operational
       nodes (or two nodes which can fail). </p><p>
       We strongly recommend to use either a two-node cluster or an odd number
       of cluster nodes.
       Two-node clusters make sense for stretched setups across two sites.
       Clusters with an odd number of nodes can be built on either one single
       site or might being spread across three sites.
      </p></dd><dt id="id-1.3.4.3.4.5.2"><span class="term">Corosync Configuration</span></dt><dd><p>Corosync is a messaging and membership layer, see
      <a class="xref" href="#sec-ha-config-basics-corosync-2-node" title="5.2.4. Corosync Configuration for Two-Node Clusters">Section 5.2.4, “Corosync Configuration for Two-Node Clusters”</a> and
       <a class="xref" href="#sec-ha-config-basics-corosync-n-node" title="5.2.5. Corosync Configuration for N-Node Clusters">Section 5.2.5, “Corosync Configuration for N-Node Clusters”</a>.
      </p></dd></dl></div><section class="sect2" id="sec-ha-config-basics-global-options" data-id-title="Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-options">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> Global cluster options control how the cluster behaves when
    confronted with certain situations. They are grouped into sets and can be
    viewed and modified with the cluster management tools like Hawk2 and
    the <code class="command">crm</code> shell. </p><p> The predefined values can usually be kept. However, to make key
    functions of your cluster work correctly, you need to adjust the
    following parameters after basic cluster setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.2.2. Global Option no-quorum-policy">Global Option <code class="literal">no-quorum-policy</code></a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#sec-ha-config-basics-global-stonith" title="5.2.3. Global Option stonith-enabled">Global Option <code class="literal">stonith-enabled</code></a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-global-quorum" data-id-title="Global Option no-quorum-policy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">Global Option <code class="literal">no-quorum-policy</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-quorum">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This global option defines what to do when a cluster partition does not
    have quorum (no majority of nodes is part of the partition).
   </p><p>
    Allowed values are:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.4.7.4.1"><span class="term"><code class="literal">ignore</code>
     </span></dt><dd><p>
       Setting <code class="literal">no-quorum-policy</code> to <code class="literal">ignore</code> makes
       the cluster behave like it has quorum. Resource management is
       continued.
      </p><p>
       On SLES 11 this was the recommended setting for a two-node cluster.
       Starting with SLES 12, this option is obsolete.
       Based on configuration and conditions, Corosync gives cluster nodes
       or a single node <span class="quote">“<span class="quote">quorum</span>”</span>—or not.
      </p><p>
      For two-node clusters the only meaningful behavior is to always
      react in case of quorum loss. The first step should always be
      to try to fence the lost node.
      </p></dd><dt id="id-1.3.4.3.4.7.4.2"><span class="term"><code class="literal">freeze</code>
     </span></dt><dd><p>
       If quorum is lost, the cluster partition freezes. Resource management
       is continued: running resources are not stopped (but possibly
       restarted in response to monitor events), but no further resources
       are started within the affected partition.
      </p><p>
       This setting is recommended for clusters where certain resources
       depend on communication with other nodes (for example, OCFS2 mounts).
       In this case, the default setting
       <code class="literal">no-quorum-policy=stop</code> is not useful, as it would
       lead to the following scenario: Stopping those resources would not be
       possible while the peer nodes are unreachable. Instead, an attempt to
       stop them would eventually time out and cause a <code class="literal">stop
       failure</code>, triggering escalated recovery and fencing.
      </p></dd><dt id="id-1.3.4.3.4.7.4.3"><span class="term"><code class="literal">stop</code> (default value)</span></dt><dd><p>
       If quorum is lost, all resources in the affected cluster partition
       are stopped in an orderly fashion.
      </p></dd><dt id="id-1.3.4.3.4.7.4.4"><span class="term"><code class="literal">suicide</code>
     </span></dt><dd><p>
       If quorum is lost, all nodes in the affected cluster partition are
       fenced. This option works only in combination with SBD, see
       <a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-global-stonith" data-id-title="Global Option stonith-enabled"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.3 </span><span class="title-name">Global Option <code class="literal">stonith-enabled</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This global option defines whether to apply fencing, allowing STONITH
    devices to shoot failed nodes and nodes with resources that cannot be
    stopped. By default, this global option is set to
    <code class="literal">true</code>, because for normal cluster operation it is
    necessary to use STONITH devices. According to the default value,
    the cluster will refuse to start any resources if no STONITH
    resources have been defined.
   </p><p>
    If you need to disable fencing for any reasons, set
    <code class="literal">stonith-enabled</code> to <code class="literal">false</code>, but be
    aware that this has impact on the support status for your product.
    Furthermore, with <code class="literal">stonith-enabled="false"</code>, resources
    like the Distributed Lock Manager (DLM) and all services depending on
    DLM (such as cLVM, GFS2, and OCFS2) will fail to start.
   </p><div id="id-1.3.4.3.4.8.4" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><p>
     A cluster without STONITH is not supported.
    </p></div></section><section class="sect2" id="sec-ha-config-basics-corosync-2-node" data-id-title="Corosync Configuration for Two-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.4 </span><span class="title-name">Corosync Configuration for Two-Node Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-corosync-2-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When using the bootstrap scripts, the Corosync configuration contains
    a <code class="literal">quorum</code> section with the following options:
   </p><div class="example" id="ex-ha-config-basics-corosync-quorum" data-id-title="Excerpt of Corosync Configuration for a Two-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.1: </span><span class="title-name">Excerpt of Corosync Configuration for a Two-Node Cluster </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-basics-corosync-quorum">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   # Enable and configure quorum subsystem (default: off)
   # see also corosync.conf.5 and votequorum.5
   provider: corosync_votequorum
   expected_votes: 2
   two_node: 1
}</pre></div></div></div><p>
    As opposed to SUSE Linux Enterprise 11, the votequorum subsystem in SUSE Linux Enterprise 12 and later
    is powered by Corosync version 2.x. This means that the
    <code class="literal">no-quorum-policy=ignore</code> option must not be used.
   </p><p>
    By default, when <code class="literal">two_node: 1</code> is set, the
    <code class="literal">wait_for_all</code> option is automatically enabled.
    If <code class="literal">wait_for_all</code> is not enabled, the cluster should be
    started on both nodes in parallel. Otherwise the first node will perform
    a startup-fencing on the missing second node.
   </p></section><section class="sect2" id="sec-ha-config-basics-corosync-n-node" data-id-title="Corosync Configuration for N-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.5 </span><span class="title-name">Corosync Configuration for N-Node Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-corosync-n-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> When not using a two-node cluster, we strongly recommend an odd
    number of nodes for your N-node cluster. With regard to quorum
    configuration, you have the following options: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Adding additional nodes with the <code class="command">ha-cluster-join</code>
     command, or</p></li><li class="listitem"><p>Adapting the Corosync configuration manually.</p></li></ul></div><p>
    If you adjust <code class="filename">/etc/corosync/corosync.conf</code> manually,
    use the following settings:
   </p><div class="example" id="id-1.3.4.3.4.10.5" data-id-title="Excerpt of Corosync Configuration for an N-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.2: </span><span class="title-name">Excerpt of Corosync Configuration for an N-Node Cluster </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.4.10.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   provider: corosync_votequorum <span class="callout" id="co-corosync-quorum-n-node-corosync-votequorum">1</span>
   expected_votes: <em class="replaceable">N</em> <span class="callout" id="co-corosync-quorum-n-node-expected-votes">2</span>
   wait_for_all: 1 <span class="callout" id="co-corosync-quorum-n-node-wait-for-all">3</span>
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-corosync-votequorum"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Use the quorum service from Corosync</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-expected-votes"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The number of votes to expect. This parameter can either be
       provided inside the <code class="literal">quorum</code> section, or is
       automatically calculated when the <code class="literal">nodelist</code>
       section is available.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-wait-for-all"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Enables the wait for all (WFA) feature.
       When WFA is enabled, the cluster will be quorate for the first time
       only after all nodes have become visible.
       To avoid some start-up race conditions, setting <code class="option">wait_for_all</code>
       to <code class="literal">1</code> may help.
       For example, in a five-node cluster every node has one vote and thus,
       <code class="option">expected_votes</code> is set to <code class="literal">5</code>.
       When three or more nodes are visible to each other, the cluster
       partition becomes quorate and can start operating.
      </p></td></tr></table></div></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources" data-id-title="Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </p><section class="sect2" id="sec-ha-config-basics-resources-management" data-id-title="Resource Management"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Resource Management</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-management">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before you can use a resource in the cluster, it must be set up. For
    example, to use an Apache server as a cluster resource, set
    up the Apache server first and complete the Apache configuration before
    starting the respective resource in your cluster.
   </p><p>
    If a resource has specific environment requirements, make sure they are
    present and identical on all cluster nodes. This kind of configuration
    is not managed by the High Availability software. You must do this yourself.
   </p><div id="id-1.3.4.3.5.3.4" data-id-title="Do Not Touch Services Managed by the Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Do Not Touch Services Managed by the Cluster</div><p>
     When managing a resource with the SUSE Linux Enterprise High Availability, the same resource must not
     be started or stopped otherwise (outside of the cluster, for example
     manually or on boot or reboot). The High Availability software is responsible
     for all service start or stop actions.
    </p><p>
     If you need to execute testing or maintenance tasks after the services
     are already running under cluster control, make sure to put the
     resources, nodes, or the whole cluster into maintenance mode before you
     touch any of them manually. For details, see
     <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a>.
    </p></div><p>
    After having configured the resources in the cluster, use the cluster
    management tools to start, stop, clean up, remove or migrate any
    resources manually. For details how to do so with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>
     </p></li></ul></div><div id="id-1.3.4.3.5.3.7" data-id-title="Resource IDs and Node Names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Resource IDs and Node Names</div><p>Cluster resources and cluster nodes should be named differently.
     Otherwise Hawk2 will fail.</p></div></section><section class="sect2" id="sec-ha-config-basics-raclasses" data-id-title="Supported Resource Agent Classes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Supported Resource Agent Classes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-raclasses">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For each cluster resource you add, you need to define the standard that
    the resource agent conforms to. Resource agents abstract the services
    they provide and present an accurate status to the cluster, which allows
    the cluster to be non-committal about the resources it manages. The
    cluster relies on the resource agent to react appropriately when given a
    start, stop or monitor command.
   </p><p>
    Typically, resource agents come in the form of shell scripts. SUSE Linux Enterprise High Availability
    supports the following classes of resource agents:
   </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-resources-ocf-ra"><span class="term">Open Cluster Framework (OCF) Resource Agents</span></dt><dd><p>
       OCF RA agents are best suited for use with High Availability, especially when
       you need multi-state resources or special monitoring abilities. The
       agents are generally located in
       <code class="filename">/usr/lib/ocf/resource.d/<em class="replaceable">provider</em>/</code>.
       Their functionality is similar to that of LSB scripts. However, the
       configuration is always done with environmental variables which allow
       them to accept and process parameters easily. The OCF specification
       (as it relates to resource agents) can be found at
       <a class="link" href="https://github.com/ClusterLabs/OCF-spec/blob/master/ra/1.0/resource-agent-api.md" target="_blank">https://github.com/ClusterLabs/OCF-spec/blob/master/ra/1.0/resource-agent-api.md</a>.
       OCF specifications have strict definitions of which exit codes must
       be returned by actions, see <a class="xref" href="#sec-ha-errorcodes" title="8.3. OCF Return Codes and Failure Recovery">Section 8.3, “OCF Return Codes and Failure Recovery”</a>. The
       cluster follows these specifications exactly.
      </p><p>
       All OCF Resource Agents are required to have at least the actions
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">status</code>, <code class="literal">monitor</code>, and
       <code class="literal">meta-data</code>. The <code class="literal">meta-data</code> action
       retrieves information about how to configure the agent. For example,
       if you want to know more about the <code class="literal">IPaddr</code> agent by
       the provider <code class="literal">heartbeat</code>, use the following command:
      </p><div class="verbatim-wrap"><pre class="screen">OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/IPaddr meta-data</pre></div><p>
       The output is information in XML format, including several sections
       (general description, available parameters, available actions for the
       agent).
      </p><p>
       Alternatively, use the crmsh to view information on OCF resource
       agents. For details, see <a class="xref" href="#sec-ha-manual-config-ocf" title="7.1.3. Displaying Information about OCF Resource Agents">Section 7.1.3, “Displaying Information about OCF Resource Agents”</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.2"><span class="term">Linux Standards Base (LSB) Scripts</span></dt><dd><p>
       LSB resource agents are generally provided by the operating
       system/distribution and are found in
       <code class="filename">/etc/init.d</code>. To be used with the cluster, they
       must conform to the LSB init script specification. For example, they
       must have several actions implemented, which are, at minimum,
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">restart</code>, <code class="literal">reload</code>,
       <code class="literal">force-reload</code>, and <code class="literal">status</code>. For
       more information, see
       <a class="link" href="http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html" target="_blank">http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html</a>.
      </p><p>
       The configuration of those services is not standardized. If you
       intend to use an LSB script with High Availability, make sure that you
       understand how the relevant script is configured. Often you can find
       information about this in the documentation of the relevant package
       in
       <code class="filename">/usr/share/doc/packages/<em class="replaceable">PACKAGENAME</em></code>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.3"><span class="term">Systemd</span></dt><dd><p>
       Starting with SUSE Linux Enterprise 12, systemd is a replacement for the popular
       System V init daemon. Pacemaker can manage systemd services if they
       are present. Instead of init scripts, systemd has unit files.
       Generally the services (or unit files) are provided by the operating
       system. In case you want to convert existing init scripts, find more
       information at
       <a class="link" href="http://0pointer.de/blog/projects/systemd-for-admins-3.html" target="_blank">http://0pointer.de/blog/projects/systemd-for-admins-3.html</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.4"><span class="term">Service</span></dt><dd><p>
       There are currently many <span class="quote">“<span class="quote">common</span>”</span> types of system
       services that exist in parallel: <code class="literal">LSB</code> (belonging to
       System V init), <code class="literal">systemd</code>, and (in some
       distributions) <code class="literal">upstart</code>. Therefore, Pacemaker
       supports a special alias which intelligently figures out which one
       applies to a given cluster node. This is particularly useful when the
       cluster contains a mix of systemd, upstart, and LSB services.
       Pacemaker will try to find the named service in the following order:
       as an LSB (SYS-V) init script, a systemd unit file, or an Upstart
       job.
      </p></dd><dt id="id-1.3.4.3.5.4.4.5"><span class="term">Nagios</span></dt><dd><p>
       Monitoring plug-ins (formerly called Nagios plug-ins) allow to
       monitor services on remote hosts. Pacemaker can do remote monitoring
       with the monitoring plug-ins if they are present. For detailed
       information, see
       <a class="xref" href="#sec-ha-config-basics-remote-nagios" title="5.6.1. Monitoring Services on Remote Hosts with Monitoring Plug-ins">Section 5.6.1, “Monitoring Services on Remote Hosts with Monitoring Plug-ins”</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.6"><span class="term">STONITH (Fencing) Resource Agents</span></dt><dd><p>
       This class is used exclusively for fencing related resources. For
       more information, see <a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>.
      </p></dd></dl></div><p>
    The agents supplied with SUSE Linux Enterprise High Availability are written to OCF
    specifications.
   </p></section><section class="sect2" id="sec-ha-config-basics-resources-types" data-id-title="Types of Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Types of Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-types">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following types of resources can be created:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.5.3.1"><span class="term">Primitives</span></dt><dd><p>
       A primitive resource, the most basic type of resource.
      </p><p>
       Learn how to create primitive resources with your preferred cluster
       management tool:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Hawk2: <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>
        </p></li><li class="listitem"><p>
         crmsh: <a class="xref" href="#sec-ha-manual-config-create" title="7.4.2. Creating Cluster Resources">Section 7.4.2, “Creating Cluster Resources”</a>
        </p></li></ul></div></dd><dt id="id-1.3.4.3.5.5.3.2"><span class="term">Groups</span></dt><dd><p>
       Groups contain a set of resources that need to be located together,
       started sequentially and stopped in the reverse order. For more
       information, refer to
       <a class="xref" href="#sec-ha-config-basics-resources-advanced-groups" title="5.3.5.1. Groups">Section 5.3.5.1, “Groups”</a>.
      </p></dd><dt id="id-1.3.4.3.5.5.3.3"><span class="term">Clones</span></dt><dd><p>
       Clones are resources that can be active on multiple hosts. Any
       resource can be cloned, provided the respective resource agent
       supports it. For more information, refer to
       <a class="xref" href="#sec-ha-config-basics-resources-advanced-clones" title="5.3.5.2. Clones">Section 5.3.5.2, “Clones”</a>.
      </p></dd><dt id="id-1.3.4.3.5.5.3.4"><span class="term">Multi-state Resources (formerly known as Master/Slave Resources)</span></dt><dd><p>
       Multi-state resources are a special type of clone resources that can
       have multiple modes. For more information, refer to
       <a class="xref" href="#sec-ha-config-basics-resources-advanced-masters" title="5.3.5.3. Multi-state Resources">Section 5.3.5.3, “Multi-state Resources”</a>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-resources-templates" data-id-title="Resource Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.4 </span><span class="title-name">Resource Templates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-templates">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to create lots of resources with similar configurations,
    defining a resource template is the easiest way. After having been
    defined, it can be referenced in primitives—or in certain types
    of constraints, as described in
    <a class="xref" href="#sec-ha-config-basics-constraints-templates" title="5.5.3. Resource Templates and Constraints">Section 5.5.3, “Resource Templates and Constraints”</a>.
   </p><p>
    If a template is referenced in a primitive, the primitive will inherit
    all operations, instance attributes (parameters), meta attributes, and
    utilization attributes defined in the template. Additionally, you can
    define specific operations or attributes for your primitive. If any of
    these are defined in both the template and the primitive, the values
    defined in the primitive will take precedence over the ones defined in
    the template.
   </p><p>
    Learn how to define resource templates with your preferred cluster
    configuration tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#pro-conf-hawk2-template-add" title="Adding a Resource Template">Procedure 6.6, “Adding a Resource Template”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-rsc-template" title="7.4.3. Creating Resource Templates">Section 7.4.3, “Creating Resource Templates”</a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-resources-advanced" data-id-title="Advanced Resource Types"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.5 </span><span class="title-name">Advanced Resource Types</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Whereas primitives are the simplest kind of resources and therefore easy
    to configure, you will probably also need more advanced resource types
    for cluster configuration, such as groups, clones or multi-state
    resources.
   </p><section class="sect3" id="sec-ha-config-basics-resources-advanced-groups" data-id-title="Groups"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.1 </span><span class="title-name">Groups</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-groups">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Some cluster resources depend on other components or resources. They
     require that each component or resource starts in a specific order and
     runs together on the same server with resources it depends on. To
     simplify this configuration, you can use cluster resource groups.
    </p><div class="complex-example"><div class="example" id="ex-ha-config-resource-group" data-id-title="Resource Group for a Web Server"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.3: </span><span class="title-name">Resource Group for a Web Server </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-resource-group">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
      An example of a resource group would be a Web server that requires an
      IP address and a file system. In this case, each component is a
      separate resource that is combined into a cluster resource group. The
      resource group would run on one or more servers. In case of a software
      or hardware malfunction, the group would fail over to another server
      in the cluster, similar to an individual cluster resource.
     </p></div></div></div><div class="figure" id="id-1.3.4.3.5.7.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/webserver_groupresource_a.png"><img src="images/webserver_groupresource_a.png" width="63%" alt="Group Resource" title="Group Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.1: </span><span class="title-name">Group Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.5.7.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div><p>
     Groups have the following properties:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.7.3.6.1"><span class="term">Starting and Stopping</span></dt><dd><p>
        Resources are started in the order they appear in and stopped in the
        reverse order.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.2"><span class="term">Dependency</span></dt><dd><p>
        If a resource in the group cannot run anywhere, then none of the
        resources located after that resource in the group is allowed to
        run.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.3"><span class="term">Contents</span></dt><dd><p>
        Groups may only contain a collection of primitive cluster resources.
        Groups must contain at least one resource, otherwise the
        configuration is not valid. To refer to the child of a group
        resource, use the child’s ID instead of the group’s ID.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.4"><span class="term">Constraints</span></dt><dd><p>
        Although it is possible to reference the group’s children in
        constraints, it is usually preferable to use the group’s name
        instead.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.5"><span class="term">Stickiness</span></dt><dd><p>
        Stickiness is additive in groups. Every <span class="emphasis"><em>active</em></span>
        member of the group will contribute its stickiness value to the
        group’s total. So if the default
        <code class="literal">resource-stickiness</code> is <code class="literal">100</code> and
        a group has seven members (ﬁve of which are active), the group as
        a whole will prefer its current location with a score of
        <code class="literal">500</code>.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.6"><span class="term">Resource Monitoring</span></dt><dd><p>
        To enable resource monitoring for a group, you must configure
        monitoring separately for each resource in the group that you want
        monitored.
       </p></dd></dl></div><p>
     Learn how to create groups with your preferred cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="#pro-conf-hawk2-group" title="Adding a Resource Group">Procedure 6.9, “Adding a Resource Group”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="#sec-ha-manual-config-group" title="7.4.10. Configuring a Cluster Resource Group">Section 7.4.10, “Configuring a Cluster Resource Group”</a>
      </p></li></ul></div></section><section class="sect3" id="sec-ha-config-basics-resources-advanced-clones" data-id-title="Clones"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.2 </span><span class="title-name">Clones</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-clones">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You may want certain resources to run simultaneously on multiple nodes
     in your cluster. To do this you must configure a resource as a clone.
     Examples of resources that might be configured as clones include
     cluster file systems like OCFS2. You can clone any
     resource provided. This is supported by the resource’s Resource
     Agent. Clone resources may even be configured differently depending on
     which nodes they are hosted.
    </p><p>
     There are three types of resource clones:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.7.4.4.1"><span class="term">Anonymous Clones</span></dt><dd><p>
        These are the simplest type of clones. They behave identically
        anywhere they are running. Because of this, there can only be one
        instance of an anonymous clone active per machine.
       </p></dd><dt id="id-1.3.4.3.5.7.4.4.2"><span class="term">Globally Unique Clones</span></dt><dd><p>
        These resources are distinct entities. An instance of the clone
        running on one node is not equivalent to another instance on another
        node; nor would any two instances on the same node be equivalent.
       </p></dd><dt id="id-1.3.4.3.5.7.4.4.3"><span class="term">Stateful Clones (Multi-state Resources)</span></dt><dd><p>
        Active instances of these resources are divided into two states,
        active and passive. These are also sometimes called primary and
        secondary, or master and slave. Stateful clones can be either
        anonymous or globally unique. See also
        <a class="xref" href="#sec-ha-config-basics-resources-advanced-masters" title="5.3.5.3. Multi-state Resources">Section 5.3.5.3, “Multi-state Resources”</a>.
       </p></dd></dl></div><p>
     Clones must contain exactly one group or one regular resource.
    </p><p>
     When configuring resource monitoring or constraints, clones have
     different requirements than simple resources. For details, see
      <em class="citetitle">Pacemaker Explained</em>, available from <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to section
     <em class="citetitle">Clones - Resources That Get Active on Multiple
     Hosts</em>.
    </p><p>
     Learn how to create clones with your preferred cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="#pro-conf-hawk2-clone" title="Adding a Clone Resource">Procedure 6.10, “Adding a Clone Resource”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="#sec-ha-manual-config-clone" title="7.4.11. Configuring a Clone Resource">Section 7.4.11, “Configuring a Clone Resource”</a>.
      </p></li></ul></div></section><section class="sect3" id="sec-ha-config-basics-resources-advanced-masters" data-id-title="Multi-state Resources"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.3 </span><span class="title-name">Multi-state Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-masters">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Multi-state resources are a specialization of clones. They allow the
     instances to be in one of two operating modes (called
     <code class="literal">master</code> or <code class="literal">slave</code>, but can mean
     whatever you want them to mean). Multi-state resources must contain
     exactly one group or one regular resource.
    </p><p>
     When configuring resource monitoring or constraints, multi-state
     resources have different requirements than simple resources. For
     details, see  <em class="citetitle">Pacemaker Explained</em>, available from <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to
     section <em class="citetitle">Multi-state - Resources That Have Multiple
     Modes</em>.
    </p></section></section><section class="sect2" id="sec-ha-config-basics-meta-attr" data-id-title="Resource Options (Meta Attributes)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.6 </span><span class="title-name">Resource Options (Meta Attributes)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-meta-attr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For each resource you add, you can define options. Options are used by
    the cluster to decide how your resource should behave—they tell
    the CRM how to treat a specific resource. Resource options can be set
    with the <code class="command">crm_resource --meta</code> command or with
    Hawk2 as described in
    <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>.
   </p><p>
    The following resource options are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.8.4.1"><span class="term"><code class="literal">priority</code></span></dt><dd><p>
       If not all resources can be active, the cluster stops lower
       priority resources to keep higher priority resources active.
      </p><p>
       The default value is <code class="literal">0</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.2"><span class="term"><code class="literal">target-role</code></span></dt><dd><p>
       In what state should the cluster attempt to keep this resource?
       Allowed values: <code class="literal">Stopped</code>, <code class="literal">Started</code>,
       <code class="literal">Unpromoted</code>, <code class="literal">Promoted</code>.
      </p><p>
       The default value is <code class="literal">Started</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.3"><span class="term"><code class="literal">is-managed</code></span></dt><dd><p>
       Is the cluster allowed to start and stop the resource? Allowed
       values: <code class="literal">true</code>, <code class="literal">false</code>. If the
       value is set to <code class="literal">false</code>, the status of the
       resource is still monitored and any failures are reported. This is
       different from setting a resource to
       <code class="literal">maintenance="true"</code>.
      </p><p>
       The default value is <code class="literal">true</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.4"><span class="term"><code class="literal">maintenance</code></span></dt><dd><p>
       Can the resources be touched manually? Allowed values:
       <code class="literal">true</code>, <code class="literal">false</code>. If set to
       <code class="literal">true</code>, all resources become unmanaged: the
       cluster stops monitoring them and does not know their
       status. You can stop or restart cluster resources without
       the cluster attempting to restart them.
      </p><p>
       The default value is <code class="literal">false</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.5"><span class="term"><code class="literal">resource-stickiness</code></span></dt><dd><p>
       How much does the resource prefer to stay where it is?
      </p><p>
       The default value is <code class="literal">1</code> for individual clone instances,
       and <code class="literal">0</code> for all other resources.
      </p></dd><dt id="id-1.3.4.3.5.8.4.6"><span class="term"><code class="literal">migration-threshold</code></span></dt><dd><p>
       How many failures should occur for this resource on a node before
       making the node ineligible to host this resource?
      </p><p>
       The default value is <code class="literal">INFINITY</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.7"><span class="term"><code class="literal">multiple-active</code></span></dt><dd><p>
       What should the cluster do if it ever finds the resource active on
       more than one node? Allowed values: <code class="literal">block</code> (mark
       the resource as unmanaged), <code class="literal">stop_only</code>,
       <code class="literal">stop_start</code>.
      </p><p>
       The default value is <code class="literal">stop_start</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.8"><span class="term"><code class="literal">failure-timeout</code></span></dt><dd><p>
       How many seconds to wait before acting as if the failure did not
       occur (and potentially allowing the resource back to the node on
       which it failed)?
      </p><p>
       The default value is <code class="literal">0</code> (disabled).
      </p></dd><dt id="id-1.3.4.3.5.8.4.9"><span class="term"><code class="literal">allow-migrate</code></span></dt><dd><p>
       Whether to allow live migration for resources that support
       <code class="literal">migrate_to</code> and <code class="literal">migrate_from</code>
       actions. If the value is set to <code class="literal">true</code>, the resource can
       be migrated without loss of state. If the value is set to <code class="literal">false</code>,
       the resource will be shut down on the first node and restarted on the second node.
      </p><p>
       The default value is <code class="literal">true</code> for
       <code class="literal">ocf:pacemaker:remote</code> resources, and
       <code class="literal">false</code> for all other resources.
      </p></dd><dt id="id-1.3.4.3.5.8.4.10"><span class="term"><code class="literal">remote-node</code></span></dt><dd><p>
       The name of the remote node this resource defines. This both
       enables the resource as a remote node and defines the unique name
       used to identify the remote node. If no other parameters are set,
       this value is also assumed as the host name to connect to at the
       <code class="varname">remote-port</code> port.
      </p><p>
       This option is disabled by default.
      </p><div id="id-1.3.4.3.5.8.4.10.2.3" data-id-title="Use unique IDs" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Use unique IDs</div><p>
        This value must not overlap with any existing resource or node IDs.
       </p></div></dd><dt id="id-1.3.4.3.5.8.4.11"><span class="term"><code class="literal">remote-port</code></span></dt><dd><p>
       Custom port for the guest connection to pacemaker_remote.
      </p><p>
       The default value is <code class="literal">3121</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.12"><span class="term"><code class="literal">remote-addr</code></span></dt><dd><p>
       The IP address or host name to connect to if the remote node's
       name is not the host name of the guest.
      </p><p>
       The default value is the value set by <code class="literal">remote-node</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.13"><span class="term"><code class="literal">remote-connect-timeout</code></span></dt><dd><p>
       How long before a pending guest connection times out?
      </p><p>
       The default value is <code class="literal">60s</code>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-inst-attr" data-id-title="Instance Attributes (Parameters)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.7 </span><span class="title-name">Instance Attributes (Parameters)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-inst-attr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The scripts of all resource classes can be given parameters which
    determine how they behave and which instance of a service they control.
    If your resource agent supports parameters, you can add them with the
    <code class="command">crm_resource</code> command or with

    Hawk2 as described in
    <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>. In the
    <code class="command">crm</code> command line utility and in Hawk2, instance
    attributes are called <code class="literal">params</code> or
    <code class="literal">Parameter</code>, respectively. The list of instance
    attributes supported by an OCF script can be found by executing the
    following command as <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info <em class="replaceable">[class:[provider:]]resource_agent</em></pre></div><p>
    or (without the optional parts):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info <em class="replaceable">resource_agent</em></pre></div><p>
    The output lists all the supported attributes, their purpose and default
    values.
   </p><p>
    For example, the command
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info IPaddr</pre></div><p>
    returns the following output:
   </p><div class="verbatim-wrap"><pre class="screen">Manages virtual IPv4 addresses (portable version) (ocf:heartbeat:IPaddr)

This script manages IP alias IP addresses
It can add an IP alias, or remove one.

Parameters (* denotes required, [] the default):

ip* (string): IPv4 address
The IPv4 address to be configured in dotted quad notation, for example
"192.168.1.1".

nic (string, [eth0]): Network interface
The base network interface on which the IP address will be brought
online.

If left empty, the script will try and determine this from the
routing table.

Do NOT specify an alias interface in the form eth0:1 or anything here;
rather, specify the base interface only.

cidr_netmask (string): Netmask
The netmask for the interface in CIDR format. (ie, 24), or in
dotted quad notation  255.255.255.0).

If unspecified, the script will also try to determine this from the
routing table.

broadcast (string): Broadcast address
Broadcast address associated with the IP. If left empty, the script will
determine this from the netmask.

iflabel (string): Interface label
You can specify an additional label for your IP address here.

lvs_support (boolean, [false]): Enable support for LVS DR
Enable support for LVS Direct Routing configurations. In case a IP
address is stopped, only move it to the loopback device to allow the
local node to continue to service requests, but no longer advertise it
on the network.

local_stop_script (string):
Script called when the IP is released

local_start_script (string):
Script called when the IP is added

ARP_INTERVAL_MS (integer, [500]): milliseconds between gratuitous ARPs
milliseconds between ARPs

ARP_REPEAT (integer, [10]): repeat count
How many gratuitous ARPs to send out when bringing up a new address

ARP_BACKGROUND (boolean, [yes]): run in background
run in background (no longer any reason to do this)

ARP_NETMASK (string, [ffffffffffff]): netmask for ARP
netmask for ARP - in nonstandard hexadecimal format.

Operations' defaults (advisory minimum):

start         timeout=90
stop          timeout=100
monitor_0     interval=5s timeout=20s</pre></div><div id="id-1.3.4.3.5.9.11" data-id-title="Instance Attributes for Groups, Clones or Multi-state Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Instance Attributes for Groups, Clones or Multi-state Resources</div><p>
     Note that groups, clones and multi-state resources do not have instance
     attributes. However, any instance attributes set will be inherited by
     the group's, clone's or multi-state resource's children.
    </p></div></section><section class="sect2" id="sec-ha-config-basics-operations" data-id-title="Resource Operations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.8 </span><span class="title-name">Resource Operations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-operations">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, the cluster will not ensure that your resources are still
    healthy. To instruct the cluster to do this, you need to add a monitor
    operation to the resource’s definition. Monitor operations can be
    added for all classes or resource agents. For more information, refer to
    <a class="xref" href="#sec-ha-config-basics-monitoring" title="5.4. Resource Monitoring">Section 5.4, “Resource Monitoring”</a>.
   </p><div class="table" id="id-1.3.4.3.5.10.3" data-id-title="Resource Operation Properties"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.1: </span><span class="title-name">Resource Operation Properties </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.5.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Operation
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Description
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">id</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Your name for the action. Must be unique. (The ID is not shown).
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">name</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The action to perform. Common values: <code class="literal">monitor</code>,
         <code class="literal">start</code>, <code class="literal">stop</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">interval</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         How frequently to perform the operation. Unit: seconds
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">timeout</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         How long to wait before declaring the action has failed.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">requires</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         What conditions need to be satisfied before this action occurs.
         Allowed values: <code class="literal">nothing</code>,
         <code class="literal">quorum</code>, <code class="literal">fencing</code>. The default
         depends on whether fencing is enabled and if the resource’s class
         is <code class="literal">stonith</code>. For STONITH resources, the
         default is <code class="literal">nothing</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">on-fail</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The action to take if this action ever fails. Allowed values:
        </p>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <code class="literal">ignore</code>: Pretend the resource did not fail.
          </p></li><li class="listitem"><p>
           <code class="literal">block</code>: Do not perform any further operations
           on the resource.
          </p></li><li class="listitem"><p>
           <code class="literal">stop</code>: Stop the resource and do not start it
           elsewhere.
          </p></li><li class="listitem"><p>
           <code class="literal">restart</code>: Stop the resource and start it again
           (possibly on a different node).
          </p></li><li class="listitem"><p>
           <code class="literal">fence</code>: Bring down the node on which the
           resource failed (STONITH).
          </p></li><li class="listitem"><p>
           <code class="literal">standby</code>: Move <span class="emphasis"><em>all</em></span>
           resources away from the node on which the resource failed.
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">enabled</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         If <code class="literal">false</code>, the operation is treated as if it does
         not exist. Allowed values: <code class="literal">true</code>,
         <code class="literal">false</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">role</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Run the operation only if the resource has this role.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">record-pending</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">

        <p>
         Can be set either globally or for individual resources. Makes the
         CIB reflect the state of <span class="quote">“<span class="quote">in-flight</span>”</span> operations on
         resources.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         <code class="literal">description</code>
        </p>
       </td><td>
        <p>
         Description of the operation.
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-ha-config-basics-timeouts" data-id-title="Timeout Values"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.9 </span><span class="title-name">Timeout Values</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-timeouts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Timeouts values for resources can be influenced by the following
    parameters:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="varname">op_defaults</code> (global timeout for operations),
     </p></li><li class="listitem"><p>
      a specific timeout value defined in a resource template,
     </p></li><li class="listitem"><p>
      a specific timeout value defined for a resource.
     </p></li></ul></div><div id="id-1.3.4.3.5.11.4" data-id-title="Priority of Values" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Priority of Values</div><p>
     If a <span class="emphasis"><em>specific</em></span> value is defined for a resource, it
     takes precedence over the global default. A specific value for a
     resource also takes precedence over a value that is defined in a
     resource template.
    </p></div><p>
    Getting timeout values right is very important. Setting them too low
    will result in a lot of (unnecessary) fencing operations for the
    following reasons:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      If a resource runs into a timeout, it fails and the cluster will try
      to stop it.
     </p></li><li class="listitem"><p>
      If stopping the resource also fails (for example, because the timeout
      for stopping is set too low), the cluster will fence the node. It
      considers the node where this happens to be out of control.
     </p></li></ol></div><p>
    You can adjust the global default for operations and set any specific
    timeout values with both crmsh and Hawk2. The best practice for
    determining and setting timeout values is as follows:
   </p><div class="procedure" id="id-1.3.4.3.5.11.8" data-id-title="Determining Timeout Values"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Determining Timeout Values </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.5.11.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check how long it takes your resources to start and stop (under load).
     </p></li><li class="step"><p>
      If needed, add the <code class="varname">op_defaults</code> parameter and set
      the (default) timeout value accordingly:
     </p><ol type="a" class="substeps"><li class="step"><p>
        For example, set <code class="literal">op_defaults</code> to
        <code class="literal">60</code> seconds:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code> op_defaults timeout=60</pre></div></li><li class="step"><p>
        For resources that need longer periods of time, define individual
        timeout values.
       </p></li></ol></li><li class="step"><p>
      When configuring operations for a resource, add separate
      <code class="literal">start</code> and <code class="literal">stop</code> operations. When
      configuring operations with Hawk2, it will provide useful timeout
      proposals for those operations.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-monitoring" data-id-title="Resource Monitoring"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Resource Monitoring</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-monitoring">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to ensure that a resource is running, you must configure
   resource monitoring for it.
  </p><p>
   If the resource monitor detects a failure, the following takes place:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Log file messages are generated, according to the configuration
     specified in the <code class="literal">logging</code> section of
     <code class="filename">/etc/corosync/corosync.conf</code>.

    </p></li><li class="listitem"><p>
     The failure is reflected in the cluster management tools (Hawk2,
     <code class="command">crm status</code>), and in the CIB status section.
    </p></li><li class="listitem"><p>
     The cluster initiates noticeable recovery actions which may include
     stopping the resource to repair the failed state and restarting the
     resource locally or on another node. The resource also may not be
     restarted, depending on the configuration and state of the cluster.
    </p></li></ul></div><p>
   If you do not configure resource monitoring, resource failures after a
   successful start will not be communicated, and the cluster will always
   show the resource as healthy.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.6.6.1"><span class="term">Monitoring Stopped Resources</span></dt><dd><p>
      Usually, resources are only monitored by the cluster as long as they
      are running. However, to detect concurrency violations, also configure
      monitoring for resources which are stopped. For example:
     </p><div class="verbatim-wrap"><pre class="screen">primitive dummy1 Dummy \
    op monitor interval="300s" role="Stopped" timeout="10s" \
    op monitor interval="30s" timeout="10s"</pre></div><p>
      This configuration triggers a monitoring operation every
      <code class="literal">300</code> seconds for the resource
      <code class="literal">dummy1</code> when it is in
      <code class="literal">role="Stopped"</code>. When running, it will be monitored
      every <code class="literal">30</code> seconds.
     </p></dd><dt id="id-1.3.4.3.6.6.2"><span class="term">Probing</span></dt><dd><p>
      The CRM executes an initial monitoring for each resource on every
      node, the so-called <code class="literal">probe</code>. A probe is also executed
      after the cleanup of a resource. If multiple monitoring operations are
      defined for a resource, the CRM will select the one with the smallest
      interval and will use its timeout value as default timeout for
      probing. If no monitor operation is configured, the cluster-wide
      default applies. The default is <code class="literal">20</code> seconds (if not
      specified otherwise by configuring the <code class="varname">op_defaults</code>
      parameter). If you do not want to rely on the automatic calculation or
      the <code class="systemitem">op_defaults</code> value, define a specific
      monitoring operation for the <span class="emphasis"><em>probing</em></span> of this
      resource. Do so by adding a monitoring operation with the
      <code class="literal">interval</code> set to <code class="literal">0</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> rsc1 ocf:pacemaker:Dummy \
    op monitor interval="0" timeout="60"</pre></div><p>
      The probe of <code class="systemitem">rsc1</code> will time out in
      <code class="literal">60s</code>, independent of the global timeout defined in
      <code class="varname">op_defaults</code>, or any other operation timeouts
      configured. If you did not set <code class="literal">interval="0"</code> for
      specifying the probing of the respective resource, the CRM will
      automatically check for any other monitoring operations defined for
      that resource and will calculate the timeout value for probing as
      described above.
     </p></dd></dl></div><p>
   Learn how to add monitor operations to resources with your preferred
   cluster management tool:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hawk2: <a class="xref" href="#pro-hawk2-operations" title="Adding and Modifying an Operation">Procedure 6.13, “Adding and Modifying an Operation”</a>
    </p></li><li class="listitem"><p>
     crmsh: <a class="xref" href="#sec-ha-manual-config-monitor" title="7.4.9. Configuring Resource Monitoring">Section 7.4.9, “Configuring Resource Monitoring”</a>
    </p></li></ul></div></section><section class="sect1" id="sec-ha-config-basics-constraints" data-id-title="Resource Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.5 </span><span class="title-name">Resource Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Having all the resources configured is only part of the job. Even if the
   cluster knows all needed resources, it might still not be able to handle
   them correctly. Resource constraints let you specify which cluster nodes
   resources can run on, what order resources will load, and what other
   resources a specific resource is dependent on.
  </p><section class="sect2" id="sec-ha-config-basics-constraints-types" data-id-title="Types of Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.1 </span><span class="title-name">Types of Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-types">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three different kinds of constraints available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.3.3.1"><span class="term">Resource Location
    </span></dt><dd><p>
       Locational constraints that define on which nodes a resource may be
       run, may not be run or is preferred to be run.
      </p></dd><dt id="id-1.3.4.3.7.3.3.2"><span class="term">Resource Colocation</span></dt><dd><p>
       Colocational constraints that tell the cluster which resources may or
       may not run together on a node.
      </p></dd><dt id="id-1.3.4.3.7.3.3.3"><span class="term">Resource Order</span></dt><dd><p>
       Ordering constraints to define the sequence of actions.
      </p></dd></dl></div><div id="id-1.3.4.3.7.3.4" data-id-title="Restrictions for Constraints and Certain Types of Resources" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Restrictions for Constraints and Certain Types of Resources</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Do not create colocation constraints for <span class="emphasis"><em>members</em></span> of a resource
       group. Create a colocation constraint pointing to the resource group as a whole instead. All
       other types of constraints are safe to use for members of a resource group.</p></li><li class="listitem"><p>Do not use any constraints on a resource that has a clone resource or a multi-state
       resource applied to it. The constraints must apply to the clone or multi-state resource, not
       to the child resource.</p></li></ul></div></div><section class="sect3" id="sec-ha-config-basics-constraints-rscset" data-id-title="Resource Sets"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1 </span><span class="title-name">Resource Sets</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-rscset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect4" id="sec-ha-config-basics-constraints-rscset-constraints" data-id-title="Using Resource Sets for Defining Constraints"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1.1 </span><span class="title-name">Using Resource Sets for Defining Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-rscset-constraints">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      As an alternative format for defining location, colocation or ordering
      constraints, you can use <code class="literal">resource sets</code>, where
      primitives are grouped together in one set. Previously this was
      possible either by defining a resource group (which could not always
      accurately express the design), or by defining each relationship as an
      individual constraint. The latter caused a constraint explosion as the
      number of resources and combinations grew. The configuration via
      resource sets is not necessarily less verbose, but is easier to
      understand and maintain, as the following examples show.
     </p><div class="complex-example"><div class="example" id="ex-config-basic-resourceset-loc" data-id-title="A Resource Set for Location Constraints"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.4: </span><span class="title-name">A Resource Set for Location Constraints </span></span><a title="Permalink" class="permalink" href="#ex-config-basic-resourceset-loc">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
       For example, you can use the following configuration of a resource
       set (<code class="varname">loc-alice</code>) in the crmsh to place
       two virtual IPs (<code class="varname">vip1</code> and <code class="varname">vip2</code>)
       on the same node, <code class="varname">alice</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip1 IPaddr2 params ip=192.168.1.5
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip2 IPaddr2 params ip=192.168.1.6
<code class="prompt custom">crm(live)configure# </code><code class="command">location</code> loc-alice { vip1 vip2 } inf: alice</pre></div></div></div></div><p>
      To use resource sets to replace a configuration of
      colocation constraints, consider the following two examples:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.5" data-id-title="A Chain of Colocated Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.5: </span><span class="title-name">A Chain of Colocated Resources </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.7.3.5.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_colocation id="coloc-1" rsc="B" with-rsc="A" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-2" rsc="C" with-rsc="B" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-3" rsc="D" with-rsc="C" score="INFINITY"/&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same configuration expressed by a resource set:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
    &lt;rsc_colocation id="coloc-1" score="INFINITY" &gt;
     &lt;resource_set id="colocated-set-example" sequential="true"&gt;
      &lt;resource_ref id="A"/&gt;
      &lt;resource_ref id="B"/&gt;
      &lt;resource_ref id="C"/&gt;
      &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
    &lt;/rsc_colocation&gt;
&lt;/constraints&gt;</pre></div><p>
      If you want to use resource sets to replace a configuration of
      ordering constraints, consider the following two examples:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.9" data-id-title="A Chain of Ordered Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.6: </span><span class="title-name">A Chain of Ordered Resources </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.7.3.5.3.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1" first="A" then="B" /&gt;
     &lt;rsc_order id="order-2" first="B" then="C" /&gt;
     &lt;rsc_order id="order-3" first="C" then="D" /&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same purpose can be achieved by using a resource set with ordered
      resources:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.11" data-id-title="A Chain of Ordered Resources Expressed as Resource Set"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.7: </span><span class="title-name">A Chain of Ordered Resources Expressed as Resource Set </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.3.7.3.5.3.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1"&gt;
     &lt;resource_set id="ordered-set-example" sequential="true"&gt;
     &lt;resource_ref id="A"/&gt;
     &lt;resource_ref id="B"/&gt;
     &lt;resource_ref id="C"/&gt;
     &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
     &lt;/rsc_order&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      Sets can be either ordered (<code class="literal">sequential=true</code>) or
      unordered (<code class="literal">sequential=false</code>). Furthermore, the
      <code class="literal">require-all</code> attribute can be used to switch between
      <code class="literal">AND</code> and <code class="literal">OR</code> logic.
     </p></section><section class="sect4" id="sec-ha-config-basics-constraints-rscset-constraints-dep" data-id-title="Resource Sets for Colocation Constraints Without Dependencies"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1.2 </span><span class="title-name">Resource Sets for Colocation Constraints Without Dependencies</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-rscset-constraints-dep">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Sometimes it is useful to place a group of resources on the same node
      (defining a colocation constraint), but without having hard
      dependencies between the resources. For example, you want two
      resources to be placed on the same node, but you do
      <span class="emphasis"><em>not</em></span> want the cluster to restart the other one if
      one of them fails. This can be achieved on the crm shell by using
      the <code class="command">weak bond</code> command.
     </p><p>
      Learn how to set these <span class="quote">“<span class="quote">weak bonds</span>”</span> with your preferred
      cluster management tool:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        crmsh:
        <a class="xref" href="#sec-ha-manual-config-constraints-weak-bond" title="7.4.5.3. Collocating Sets for Resources Without Dependency">Section 7.4.5.3, “Collocating Sets for Resources Without Dependency”</a>
       </p></li></ul></div></section></section><section class="sect3" id="sec-ha-config-basics-constraints-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.1.2 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-more">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Learn how to add the various kinds of constraints with your preferred
     cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="#sec-conf-hawk2-cons" title="6.6. Configuring Constraints">Section 6.6, “Configuring Constraints”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="#sec-ha-manual-config-constraints" title="7.4.5. Configuring Resource Constraints">Section 7.4.5, “Configuring Resource Constraints”</a>
      </p></li></ul></div><p>
     For more information on configuring constraints and detailed background
     information about the basic concepts of ordering and colocation, refer
     to the following documents. They are available at <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <em class="citetitle">Pacemaker Explained</em>, chapter <em class="citetitle">Resource Constraints</em>
      </p></li><li class="listitem"><p>
       <em class="citetitle">Colocation Explained</em>
      </p></li><li class="listitem"><p>
       <em class="citetitle">Ordering Explained</em>
      </p></li></ul></div></section></section><section class="sect2" id="sec-ha-config-basics-constraints-scores" data-id-title="Scores and Infinity"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.2 </span><span class="title-name">Scores and Infinity</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-scores">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When defining constraints, you also need to deal with scores. Scores of
    all kinds are integral to how the cluster works. Practically everything
    from migrating a resource to deciding which resource to stop in a
    degraded cluster is achieved by manipulating scores in some way. Scores
    are calculated on a per-resource basis and any node with a negative
    score for a resource cannot run that resource. After calculating the
    scores for a resource, the cluster then chooses the node with the
    highest score.
   </p><p>
    <code class="literal">INFINITY</code> is currently deﬁned as
    <code class="literal">1,000,000</code>. Additions or subtractions with it stick to
    the following three basic rules:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Any value + INFINITY = INFINITY
     </p></li><li class="listitem"><p>
      Any value - INFINITY = -INFINITY
     </p></li><li class="listitem"><p>
      INFINITY - INFINITY = -INFINITY
     </p></li></ul></div><p>
    When defining resource constraints, you specify a score for each
    constraint. The score indicates the value you are assigning to this
    resource constraint. Constraints with higher scores are applied before
    those with lower scores. By creating additional location constraints
    with different scores for a given resource, you can specify an order for
    the nodes that a resource will fail over to.
   </p></section><section class="sect2" id="sec-ha-config-basics-constraints-templates" data-id-title="Resource Templates and Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.3 </span><span class="title-name">Resource Templates and Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-templates">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you have defined a resource template (see
    <a class="xref" href="#sec-ha-config-basics-resources-templates" title="5.3.4. Resource Templates">Section 5.3.4, “Resource Templates”</a>), it can be
    referenced in the following types of constraints:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      order constraints,
     </p></li><li class="listitem"><p>
      colocation constraints,
     </p></li><li class="listitem"><p>
      rsc_ticket constraints (for Geo clusters).
     </p></li></ul></div><p>
    However, colocation constraints must not contain more than one reference
    to a template. Resource sets must not contain a reference to a template.
   </p><p>
    Resource templates referenced in constraints stand for all primitives
    which are derived from that template. This means, the constraint applies
    to all primitive resources referencing the resource template.
    Referencing resource templates in constraints is an alternative to
    resource sets and can simplify the cluster configuration considerably.
    For details about resource sets, refer to
    <a class="xref" href="#pro-hawk2-constraints-sets" title="Using a Resource Set for Constraints">Procedure 6.17, “Using a Resource Set for Constraints”</a>.
   </p></section><section class="sect2" id="sec-ha-config-basics-failover" data-id-title="Failover Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.4 </span><span class="title-name">Failover Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource will be automatically restarted if it fails. If that cannot
    be achieved on the current node, or it fails <code class="literal">N</code> times
    on the current node, it will try to fail over to another node. Each time
    the resource fails, its failcount is raised. You can define a number of
    failures for resources (a <code class="literal">migration-threshold</code>), after
    which they will migrate to a new node. If you have more than two nodes
    in your cluster, the node a particular resource fails over to is chosen
    by the High Availability software.
   </p><p>
    However, you can specify the node a resource will fail over to by
    configuring one or several location constraints and a
    <code class="literal">migration-threshold</code> for that resource.
   </p><p>
    Learn how to specify failover nodes with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#sec-conf-hawk2-failover" title="6.6.6. Specifying Resource Failover Nodes">Section 6.6.6, “Specifying Resource Failover Nodes”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-failover" title="7.4.6. Specifying Resource Failover Nodes">Section 7.4.6, “Specifying Resource Failover Nodes”</a>
     </p></li></ul></div><div class="complex-example"><div class="example" id="ex-ha-config-basics-failover" data-id-title="Migration Threshold—Process Flow"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.8: </span><span class="title-name">Migration Threshold—Process Flow </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-basics-failover">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     For example, let us assume you have configured a location constraint
     for resource <code class="literal">rsc1</code> to preferably run on
     <code class="literal">alice</code>. If it fails there,
     <code class="literal">migration-threshold</code> is checked and compared to the
     failcount. If failcount &gt;= migration-threshold then the resource is
     migrated to the node with the next best preference.
    </p><p>
     After the threshold has been reached, the node will no longer be
     allowed to run the failed resource until the resource's failcount is
     reset. This can be done manually by the cluster administrator or by
     setting a <code class="literal">failure-timeout</code> option for the resource.
    </p><p>
     For example, a setting of <code class="literal">migration-threshold=2</code> and
     <code class="literal">failure-timeout=60s</code> would cause the resource to
     migrate to a new node after two failures. It would be allowed to move
     back (depending on the stickiness and constraint scores) after one
     minute.
    </p></div></div></div><p>
    There are two exceptions to the migration threshold concept, occurring
    when a resource either fails to start or fails to stop:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Start failures set the failcount to <code class="literal">INFINITY</code> and
      thus always cause an immediate migration.
     </p></li><li class="listitem"><p>
      Stop failures cause fencing (when <code class="literal">stonith-enabled</code>
      is set to <code class="literal">true</code> which is the default).
     </p><p>
      In case there is no STONITH resource defined (or
      <code class="literal">stonith-enabled</code> is set to
      <code class="literal">false</code>), the resource will not migrate.
     </p></li></ul></div><p>
    For details on using migration thresholds and resetting failcounts with
    your preferred cluster management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#sec-conf-hawk2-failover" title="6.6.6. Specifying Resource Failover Nodes">Section 6.6.6, “Specifying Resource Failover Nodes”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-failover" title="7.4.6. Specifying Resource Failover Nodes">Section 7.4.6, “Specifying Resource Failover Nodes”</a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-failback" data-id-title="Failback Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.5 </span><span class="title-name">Failback Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-failback">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource might fail back to its original node when that node is back
    online and in the cluster. To prevent a resource from
    failing back to the node that it was running on, or
    to specify a different node for the resource to fail back to,
    change its resource stickiness value. You can
    either specify resource stickiness when you are creating a resource or
    afterward.
   </p><p>
    Consider the following implications when specifying resource stickiness
    values:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.7.4.1"><span class="term">Value is <code class="literal">0</code>:</span></dt><dd><p>
       This is the default. The resource will be placed optimally in the
       system. This may mean that it is moved when a <span class="quote">“<span class="quote">better</span>”</span>
       or less loaded node becomes available. This option is almost
       equivalent to automatic failback, except that the resource may be
       moved to a node that is not the one it was previously active on.
      </p></dd><dt id="id-1.3.4.3.7.7.4.2"><span class="term">Value is greater than <code class="literal">0</code>:</span></dt><dd><p>
       The resource will prefer to remain in its current location, but may
       be moved if a more suitable node is available. Higher values indicate
       a stronger preference for a resource to stay where it is.
      </p></dd><dt id="id-1.3.4.3.7.7.4.3"><span class="term">Value is less than <code class="literal">0</code>:</span></dt><dd><p>
       The resource prefers to move away from its current location. Higher
       absolute values indicate a stronger preference for a resource to be
       moved.
      </p></dd><dt id="id-1.3.4.3.7.7.4.4"><span class="term">Value is <code class="literal">INFINITY</code>:</span></dt><dd><p>
       The resource will always remain in its current location unless forced
       off because the node is no longer eligible to run the resource (node
       shutdown, node standby, reaching the
       <code class="literal">migration-threshold</code>, or configuration change).
       This option is almost equivalent to completely disabling automatic
       failback.
      </p></dd><dt id="id-1.3.4.3.7.7.4.5"><span class="term">Value is <code class="literal">-INFINITY</code>:</span></dt><dd><p>
       The resource will always move away from its current location.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-utilization" data-id-title="Placing Resources Based on Their Load Impact"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.6 </span><span class="title-name">Placing Resources Based on Their Load Impact</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Not all resources are equal. Some, such as Xen guests, require that
    the node hosting them meets their capacity requirements. If resources
    are placed such that their combined need exceed the provided capacity,
    the resources diminish in performance (or even fail).
   </p><p>
    To take this into account, SUSE Linux Enterprise High Availability allows you to specify the
    following parameters:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      The capacity a certain node <span class="emphasis"><em>provides</em></span>.
     </p></li><li class="listitem"><p>
      The capacity a certain resource <span class="emphasis"><em>requires</em></span>.
     </p></li><li class="listitem"><p>
      An overall strategy for placement of resources.
     </p></li></ol></div><p>
    Learn how to configure these settings with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#sec-config-hawk2-utilization" title="6.6.8. Configuring Placement of Resources Based on Load Impact">Section 6.6.8, “Configuring Placement of Resources Based on Load Impact”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-utilization" title="7.4.8. Configuring Placement of Resources Based on Load Impact">Section 7.4.8, “Configuring Placement of Resources Based on Load Impact”</a>
     </p></li></ul></div><p>
    A node is considered eligible for a resource if it has sufficient free
    capacity to satisfy the resource's requirements. The nature of the
    capacities is completely irrelevant for the High Availability software; it only makes
    sure that all capacity requirements of a resource are satisfied before
    moving a resource to a node.
   </p><p>
    To manually configure the resource's requirements and the capacity a
    node provides, use utilization attributes. You can name the utilization
    attributes according to your preferences and define as many name/value
    pairs as your configuration needs. However, the attribute's values must
    be integers.
   </p><p>
    If multiple resources with utilization attributes are grouped or have
    colocation constraints, SUSE Linux Enterprise High Availability takes that into account. If
    possible, the resources will be placed on a node that can fulfill
    <span class="emphasis"><em>all</em></span> capacity requirements.
   </p><div id="id-1.3.4.3.7.8.10" data-id-title="Utilization Attributes for Groups" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Utilization Attributes for Groups</div><p>
     It is impossible to set utilization attributes directly for a resource
     group. However, to simplify the configuration for a group, you can add
     a utilization attribute with the total capacity needed to any of the
     resources in the group.
    </p></div><p>
    SUSE Linux Enterprise High Availability also provides means to detect and configure both node
    capacity and resource requirements automatically:
   </p><p>
    The <code class="systemitem">NodeUtilization</code> resource agent checks the
    capacity of a node (regarding CPU and RAM).
    To configure automatic detection, create a clone resource of the
    following class, provider, and type:
    <code class="literal">ocf:pacemaker:NodeUtilization</code>. One instance of the
    clone should be running on each node. After the instance has started, a
    utilization section will be added to the node's configuration in CIB.
   </p><p>
    For automatic detection of a resource's minimal requirements (regarding
    RAM and CPU) the <code class="systemitem">Xen</code> resource agent has been
    improved. Upon start of a <code class="systemitem">Xen</code> resource, it will
    reflect the consumption of RAM and CPU. Utilization attributes will
    automatically be added to the resource configuration.
   </p><div id="id-1.3.4.3.7.8.14" data-id-title="Different Resource Agents for Xen and libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Different Resource Agents for Xen and libvirt</div><p>
     The <code class="systemitem">ocf:heartbeat:Xen</code> resource agent should not be
     used with <code class="literal">libvirt</code>, as <code class="literal">libvirt</code> expects
     to be able to modify the machine description file.
    </p><p>
     For <code class="literal">libvirt</code>, use the
     <code class="systemitem">ocf:heartbeat:VirtualDomain</code> resource agent.
    </p></div><p>
    Apart from detecting the minimal requirements, SUSE Linux Enterprise High Availability also allows
    to monitor the current utilization via the
    <code class="systemitem">VirtualDomain</code> resource agent. It detects CPU
    and RAM use of the virtual machine. To use this feature, configure a
    resource of the following class, provider and type:
    <code class="literal">ocf:heartbeat:VirtualDomain</code>. The following instance
    attributes are available: <code class="varname">autoset_utilization_cpu</code> and
    <code class="varname">autoset_utilization_hv_memory</code>. Both default to
    <code class="literal">true</code>. This updates the utilization values in the CIB
    during each monitoring cycle.
   </p><p>
    Independent of manually or automatically configuring capacity and
    requirements, the placement strategy must be specified with the
    <code class="literal">placement-strategy</code> property (in the global cluster
    options). The following values are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.8.17.1"><span class="term"><code class="literal">default</code> (default value)</span></dt><dd><p>
       Utilization values are not considered. Resources are allocated
       according to location scoring. If scores are equal, resources are
       evenly distributed across nodes.
      </p></dd><dt id="id-1.3.4.3.7.8.17.2"><span class="term"><code class="literal">utilization</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. However,
       load-balancing is still done based on the number of resources
       allocated to a node.
      </p></dd><dt id="id-1.3.4.3.7.8.17.3"><span class="term"><code class="literal">minimal</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to concentrate the resources on as few nodes as possible
       (to achieve power savings on the remaining nodes).
      </p></dd><dt id="id-1.3.4.3.7.8.17.4"><span class="term"><code class="literal">balanced</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to distribute the resources evenly, thus optimizing resource
       performance.
      </p></dd></dl></div><div id="id-1.3.4.3.7.8.18" data-id-title="Configuring Resource Priorities" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Configuring Resource Priorities</div><p>
     The available placement strategies are best-effort—they do not
     yet use complex heuristic solvers to always reach optimum allocation
     results. Ensure that resource priorities are properly set so that
     your most important resources are scheduled first.
    </p></div><div class="complex-example"><div class="example" id="ex-ha-config-basics-utilization" data-id-title="Example Configuration for Load-Balanced Placing"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.9: </span><span class="title-name">Example Configuration for Load-Balanced Placing </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-basics-utilization">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The following example demonstrates a three-node cluster of equal nodes,
     with four virtual machines.
    </p><div class="verbatim-wrap"><pre class="screen">node alice utilization hv_memory="4000"
node bob utilization hv_memory="4000"
node charlie utilization hv_memory="4000"
primitive xenA Xen utilization hv_memory="3500" \
     params xmfile="/etc/xen/shared-vm/vm1"
     meta priority="10"
primitive xenB Xen utilization hv_memory="2000" \
     params xmfile="/etc/xen/shared-vm/vm2"
     meta priority="1"
primitive xenC Xen utilization hv_memory="2000" \
     params xmfile="/etc/xen/shared-vm/vm3"
     meta priority="1"
primitive xenD Xen utilization hv_memory="1000" \
     params xmfile="/etc/xen/shared-vm/vm4"
     meta priority="5"
property placement-strategy="minimal"</pre></div><p>
     With all three nodes up, resource <code class="literal">xenA</code> will be
     placed onto a node first, followed by <code class="literal">xenD</code>.
     <code class="literal">xenB</code> and <code class="literal">xenC</code> would either be
     allocated together or one of them with <code class="literal">xenD</code>.
    </p><p>
     If one node failed, too little total memory would be available to host
     them all. <code class="literal">xenA</code> would be ensured to be allocated, as
     would <code class="literal">xenD</code>. However, only one of the remaining
     resources <code class="literal">xenB</code> or <code class="literal">xenC</code> could
     still be placed. Since their priority is equal, the result would still
     be open. To resolve this ambiguity as well, you would need to set a
     higher priority for either one.
    </p></div></div></div></section><section class="sect2" id="sec-ha-config-basics-tags" data-id-title="Grouping Resources by Using Tags"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.7 </span><span class="title-name">Grouping Resources by Using Tags</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-tags">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Tags are a new feature that has been added to Pacemaker recently. Tags
    are a way to refer to multiple resources at once, without creating any
    colocation or ordering relationship between them. This can be useful for
    grouping conceptually related resources. For example, if you have
    several resources related to a database, create a tag called
    <code class="literal">databases</code> and add all resources related to the
    database to this tag. This allows you to stop or start them all with a
    single command.
   </p><p>
    Tags can also be used in constraints. For example, the following
    location constraint <code class="literal">loc-db-prefer</code> applies to the set
    of resources tagged with <code class="literal">databases</code>:
   </p><div class="verbatim-wrap"><pre class="screen">location loc-db-prefer databases 100: alice</pre></div><p>
    Learn how to create tags with your preferred cluster management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="#pro-conf-hawk2-tag" title="Adding a Tag">Procedure 6.12, “Adding a Tag”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-tag" title="7.5.6. Grouping/Tagging Resources">Section 7.5.6, “Grouping/Tagging Resources”</a>
     </p></li></ul></div></section></section><section class="sect1" id="sec-ha-config-basics-remote" data-id-title="Managing Services on Remote Hosts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.6 </span><span class="title-name">Managing Services on Remote Hosts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The possibilities for monitoring and managing services on remote hosts
   has become increasingly important during the last few years.
   SUSE Linux Enterprise High Availability 11 SP3 offered fine-grained monitoring of services on
   remote hosts via monitoring plug-ins. The recent addition of the
   <code class="literal">pacemaker_remote</code> service now allows SUSE Linux Enterprise High Availability
   12 SP4 to fully manage and monitor resources on remote hosts
   just as if they were a real cluster node—without the need to
   install the cluster stack on the remote machines.
  </p><section class="sect2" id="sec-ha-config-basics-remote-nagios" data-id-title="Monitoring Services on Remote Hosts with Monitoring Plug-ins"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.6.1 </span><span class="title-name">Monitoring Services on Remote Hosts with Monitoring Plug-ins</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote-nagios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Monitoring of virtual machines can be done with the VM agent (which only
    checks if the guest shows up in the hypervisor), or by external scripts
    called from the VirtualDomain or Xen agent. Up to now, more fine-grained
    monitoring was only possible with a full setup of the High Availability stack
    within the virtual machines.
   </p><p>
    By providing support for monitoring plug-ins (formerly named Nagios
    plug-ins), SUSE Linux Enterprise High Availability now also allows you to monitor services on
    remote hosts. You can collect external statuses on the guests without
    modifying the guest image. For example, VM guests might run Web services
    or simple network resources that need to be accessible. With the Nagios
    resource agents, you can now monitor the Web service or the network
    resource on the guest. If these services are not reachable anymore,
    SUSE Linux Enterprise High Availability will trigger a restart or migration of the respective
    guest.
   </p><p>
    If your guests depend on a service (for example, an NFS server to be
    used by the guest), the service can either be an ordinary resource,
    managed by the cluster, or an external service that is monitored with
    Nagios resources instead.
   </p><p>
    To configure the Nagios resources, the following packages must be
    installed on the host:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="systemitem">monitoring-plugins</code>
     </p></li><li class="listitem"><p>
      <code class="systemitem">monitoring-plugins-metadata</code>
     </p></li></ul></div><p>
    YaST or Zypper will resolve any dependencies on further packages,
    if required.
   </p><p>
    A typical use case is to configure the monitoring plug-ins as resources
    belonging to a resource container, which usually is a VM. The container
    will be restarted if any of its resources has failed. Refer to
    <a class="xref" href="#ex-ha-nagios-config" title="Configuring Resources for Monitoring Plug-ins">Example 5.10, “Configuring Resources for Monitoring Plug-ins”</a> for a configuration example.
    Alternatively, Nagios resource agents can also be configured as ordinary
    resources if you want to use them for monitoring hosts or services via
    the network.
   </p><div class="complex-example"><div class="example" id="ex-ha-nagios-config" data-id-title="Configuring Resources for Monitoring Plug-ins"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.10: </span><span class="title-name">Configuring Resources for Monitoring Plug-ins </span></span><a title="Permalink" class="permalink" href="#ex-ha-nagios-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">primitive vm1 VirtualDomain \
    params hypervisor="qemu:///system" config="/etc/libvirt/qemu/vm1.xml" \
    op start interval="0" timeout="90" \
    op stop interval="0" timeout="90" \
    op monitor interval="10" timeout="30"
primitive vm1-sshd nagios:check_tcp \
    params hostname="vm1" port="22" \ <span class="callout" id="co-nagios-hostname">1</span>
    op start interval="0" timeout="120" \ <span class="callout" id="co-nagios-startinterval">2</span>
    op monitor interval="10"
group g-vm1-and-services vm1 vm1-sshd \
    meta container="vm1" <span class="callout" id="co-nagios-container">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-hostname"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The supported parameters are the same as the long options of a
       monitoring plug-in. Monitoring plug-ins connect to services with the
       parameter <code class="literal">hostname</code>. Therefore the attribute's
       value must be a resolvable host name or an IP address.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-startinterval"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       As it takes some time to get the guest operating system up and its
       services running, the start timeout of the monitoring resource must
       be long enough.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-container"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A cluster resource container of type
       <code class="literal">ocf:heartbeat:Xen</code>,
       <code class="literal">ocf:heartbeat:VirtualDomain</code> or
       <code class="literal">ocf:heartbeat:lxc</code>. It can either be a VM or a
       Linux Container.
      </p></td></tr></table></div><p>
     The example above contains only one resource for the
     <code class="literal">check_tcp</code>plug-in, but multiple resources for
     different plug-in types can be configured (for example,
     <code class="literal">check_http</code> or <code class="literal">check_udp</code>).
    </p><p>
     If the host names of the services are the same, the
     <code class="literal">hostname</code> parameter can also be specified for the
     group, instead of adding it to the individual primitives. For example:
    </p><div class="verbatim-wrap"><pre class="screen">group g-vm1-and-services vm1 vm1-sshd vm1-httpd \
     meta container="vm1" \
     params hostname="vm1"</pre></div><p>
     If any of the services monitored by the monitoring plug-ins fail within
     the VM, the cluster will detect that and restart the container resource
     (the VM). Which action to take in this case can be configured by
     specifying the <code class="literal">on-fail</code> attribute for the service's
     monitoring operation. It defaults to
     <code class="literal">restart-container</code>.
    </p><p>
     Failure counts of services will be taken into account when considering
     the VM's migration-threshold.
    </p></div></div></div></section><section class="sect2" id="sec-ha-config-basics-remote-pace-remote" data-id-title="Managing Services on Remote Nodes with pacemaker_remote"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.6.2 </span><span class="title-name">Managing Services on Remote Nodes with <code class="literal">pacemaker_remote</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote-pace-remote">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With the <code class="literal">pacemaker_remote</code> service, High Availability clusters
    can be extended to virtual nodes or remote bare-metal machines. They do
    not need to run the cluster stack to become members of the cluster.
   </p><p>
    SUSE Linux Enterprise High Availability can now launch virtual environments (KVM and LXC), plus
    the resources that live within those virtual environments without
    requiring the virtual environments to run Pacemaker or Corosync.
   </p><p>
    For the use case of managing both virtual machines as cluster resources
    plus the resources that live within the VMs, you can now use the
    following setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The <span class="quote">“<span class="quote">normal</span>”</span> (bare-metal) cluster nodes run SUSE Linux Enterprise High Availability.
     </p></li><li class="listitem"><p>
      The virtual machines run the <code class="literal">pacemaker_remote</code>
      service (almost no configuration required on the VM's side).
     </p></li><li class="listitem"><p>
      The cluster stack on the <span class="quote">“<span class="quote">normal</span>”</span> cluster nodes launches
      the VMs and connects to the <code class="literal">pacemaker_remote</code>
      service running on the VMs to integrate them as remote nodes into the
      cluster.
     </p></li></ul></div><p>
    As the remote nodes do not have the cluster stack installed, this has
    the following implications:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Remote nodes do not take part in quorum.
     </p></li><li class="listitem"><p>
      Remote nodes cannot become the DC.
     </p></li><li class="listitem"><p>
      Remote nodes are not bound by the scalability limits (Corosync
      has a member limit of 32 nodes).
     </p></li></ul></div><p>
    Find more information about the <code class="literal">remote_pacemaker</code>
    service, including multiple use cases with detailed setup instructions
    in <em class="citetitle">Pacemaker Remote—Extending High Availability into
    Virtual Nodes</em>, available at
    <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>.
   </p></section></section><section class="sect1" id="sec-ha-config-basics-monitor-health" data-id-title="Monitoring System Health"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.7 </span><span class="title-name">Monitoring System Health</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-monitor-health">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To prevent a node from running out of disk space and thus being unable to
   manage any resources that have been assigned to it, SUSE Linux Enterprise High Availability
   provides a resource agent,
   <code class="systemitem">ocf:pacemaker:SysInfo</code>. Use it to monitor a
   node's health with regard to disk partitions.
   The SysInfo RA creates a node attribute named
   <code class="literal">#health_disk</code> which will be set to
   <code class="literal">red</code> if any of the monitored disks' free space is below
   a specified limit.
  </p><p>
   To define how the CRM should react in case a node's health reaches a
   critical state, use the global cluster option
   <code class="systemitem">node-health-strategy</code>.
  </p><div class="procedure" id="pro-ha-health-monitor" data-id-title="Configuring System Health Monitoring"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.2: </span><span class="title-name">Configuring System Health Monitoring </span></span><a title="Permalink" class="permalink" href="#pro-ha-health-monitor">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To automatically move resources away from a node in case the node runs
    out of disk space, proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Configure an <code class="systemitem">ocf:pacemaker:SysInfo</code> resource:
    </p><div class="verbatim-wrap"><pre class="screen">primitive sysinfo ocf:pacemaker:SysInfo \
     params disks="/tmp /var"<span class="callout" id="co-disks">1</span> min_disk_free="100M"<span class="callout" id="co-min-disk-free">2</span> disk_unit="M"<span class="callout" id="co-disk-unit">3</span> \
     op monitor interval="15s"</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disks"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Which disk partitions to monitor. For example,
       <code class="filename">/tmp</code>, <code class="filename">/usr</code>,
       <code class="filename">/var</code>, and <code class="filename">/dev</code>. To specify
       multiple partitions as attribute values, separate them with a blank.
      </p><div id="id-1.3.4.3.9.4.3.3.1.2" data-id-title="/ File System Always Monitored" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="filename">/</code> File System Always Monitored</div><p>
        You do not need to specify the root partition
        (<code class="filename">/</code>) in <code class="literal">disks</code>. It is always
        monitored by default.
       </p></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-min-disk-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minimum free disk space required for those partitions.
       Optionally, you can specify the unit to use for measurement (in the
       example above, <code class="literal">M</code> for megabytes is used). If not
       specified, <code class="systemitem">min_disk_free</code> defaults to the
       unit defined in the <code class="systemitem">disk_unit</code> parameter.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-unit"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The unit in which to report the disk space.
      </p></td></tr></table></div></li><li class="step"><p>
     To complete the resource configuration, create a clone of
     <code class="systemitem">ocf:pacemaker:SysInfo</code> and start it on each
     cluster node.
    </p></li><li class="step"><p>
     Set the <code class="systemitem">node-health-strategy</code> to
     <code class="literal">migrate-on-red</code>:
    </p><div class="verbatim-wrap"><pre class="screen">property node-health-strategy="migrate-on-red"</pre></div><p>
     In case of a <code class="systemitem">#health_disk</code> attribute set to
     <code class="literal">red</code>, the policy engine adds <code class="literal">-INF</code>
     to the resources' score for that node. This will cause any resources to
     move away from this node. The STONITH resource will be the last
     one to be stopped but even if the STONITH resource is not running
     anymore, the node can still be fenced. Fencing has direct access to the
     CIB and will continue to work.
    </p></li></ol></div></div><p>
   After a node's health status has turned to <code class="literal">red</code>, solve
   the issue that led to the problem. Then clear the <code class="literal">red</code>
   status to make the node eligible again for running resources. Log in to
   the cluster node and use one of the following methods:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Execute the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node status-attr <em class="replaceable">NODE</em> delete #health_disk</pre></div></li><li class="listitem"><p>
     Restart Pacemaker on that node.
    </p></li><li class="listitem"><p>
     Reboot the node.
    </p></li></ul></div><p>
   The node will be returned to service and can run resources again.
  </p></section><section class="sect1" id="sec-ha-config-basics-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.10.2.1"><span class="term"><a class="link" href="http://crmsh.github.io/" target="_blank">http://crmsh.github.io/</a>
    </span></dt><dd><p>
      Home page of the crm shell (crmsh), the advanced command line
      interface for High Availability cluster management.
     </p></dd><dt id="id-1.3.4.3.10.2.2"><span class="term"><a class="link" href="http://crmsh.github.io/documentation" target="_blank">http://crmsh.github.io/documentation</a>
    </span></dt><dd><p>
      Holds several documents about the crm shell, including a
      <em class="citetitle">Getting Started</em> tutorial for basic cluster
      setup with crmsh and the comprehensive
      <em class="citetitle">Manual</em> for the crm shell. The latter is
      available at <a class="link" href="http://crmsh.github.io/man-2.0/" target="_blank">http://crmsh.github.io/man-2.0/</a>.
      Find the tutorial at
      <a class="link" href="http://crmsh.github.io/start-guide/" target="_blank">http://crmsh.github.io/start-guide/</a>.
     </p></dd><dt id="id-1.3.4.3.10.2.3"><span class="term"><a class="link" href="http://clusterlabs.org/" target="_blank">http://clusterlabs.org/</a>
    </span></dt><dd><p>
      Home page of Pacemaker, the cluster resource manager shipped with
      SUSE Linux Enterprise High Availability.
     </p></dd><dt id="id-1.3.4.3.10.2.4"><span class="term"><a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>
    </span></dt><dd><p>
      Holds several comprehensive manuals and some shorter documents
      explaining general concepts. For example:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <em class="citetitle">Pacemaker Explained</em>: Contains comprehensive and very detailed information
        for reference.
       </p></li><li class="listitem"><p>
        <em class="citetitle">Configuring Fencing with crmsh</em>: How to
        configure and use STONITH devices.
       </p></li><li class="listitem"><p>
        <em class="citetitle">Colocation Explained</em>
       </p></li><li class="listitem"><p>
        <em class="citetitle">Ordering Explained</em>
       </p></li></ul></div></dd><dt id="id-1.3.4.3.10.2.5"><span class="term"><a class="link" href="https://clusterlabs.org" target="_blank">https://clusterlabs.org</a>
    </span></dt><dd><p>
      Home page of the High Availability Linux Project.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-conf-hawk2" data-id-title="Configuring and Managing Cluster Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Configuring and Managing Cluster Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#cha-conf-hawk2">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    To configure and manage cluster resources, either use HA Web Console (Hawk2), or
    the crm shell (crmsh) command line utility. If you upgrade from an
    earlier version of SUSE® Linux Enterprise High Availability where Hawk was installed, the package
    will be replaced with the current version, Hawk2.
   </p><p>
    Hawk2's Web-based user-interface allows you to monitor and administer
    your Linux cluster from non-Linux machines. Furthermore, it is the ideal
    solution in case your system only provides a minimal graphical user
    interface.
   </p></div></div></div></div><section class="sect1" id="sec-conf-hawk2-req" data-id-title="Hawk2 Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Hawk2 Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-req">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before users can log in to Hawk2, the following requirements need to be
   fulfilled:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.3.3.1"><span class="term"><span class="package">hawk2</span> Package</span></dt><dd><p>
      The <span class="package">hawk2</span> package must be installed on all
      cluster nodes you want to connect to with Hawk2.
     </p></dd><dt id="id-1.3.4.4.3.3.2"><span class="term">Web browser</span></dt><dd><p>
      On the machine from which to access a cluster node using
      Hawk2, you need a (graphical) Web browser (with JavaScript and
      cookies enabled) to establish the connection.
     </p></dd><dt id="id-1.3.4.4.3.3.3"><span class="term">Hawk2 service</span></dt><dd><p>
      To use Hawk2, the respective Web service must be started on
      the node that you want to connect to via the Web interface. See <a class="xref" href="#pro-ha-hawk2-service" title="Starting Hawk2 Services">Procedure 6.1, “Starting Hawk2 Services”</a>.
     </p><p>
      If you have set up your cluster with the scripts from the
       <code class="systemitem">ha-cluster-bootstrap</code> package,
      the Hawk2 service is already enabled.
     </p></dd><dt id="id-1.3.4.4.3.3.4"><span class="term">Username, group and password on each cluster node</span></dt><dd><p>
      Hawk2 users must be members of the <code class="systemitem">haclient</code> group. The installation creates a
      Linux user named <code class="systemitem">hacluster</code>, who
      is added to the <code class="systemitem">haclient</code> group.
     </p><p>
      When using the <code class="command">ha-cluster-init</code> script for setup,
      a default password is set for the <code class="systemitem">hacluster</code> user. Before starting Hawk2, change it to a
      secure password. If you did not use the <code class="command">ha-cluster-init</code>
      script, either set a password for the <code class="systemitem">hacluster</code> first or create a new user which is a member of
      the <code class="systemitem">haclient</code> group. Do this on
      every node you will connect to with Hawk2.
     </p></dd><dt id="id-1.3.4.4.3.3.5"><span class="term">Wildcard certificate handling</span></dt><dd><p>
      A wildcard certificate is a public key certificate which is valid for
      multiple sub-domains. For example, a wildcard certificate for
      <code class="systemitem">*.example.com</code> secures the domains
      <code class="systemitem">www.example.com</code>,
      <code class="systemitem">login.example.com</code>,
      and possibly more.
     </p><p>
      Hawk2 supports wildcard certificates as well as conventional certificates.
      A self-signed default private key and certificate is generated by
      <code class="filename">/srv/www/hawk/bin/generate-ssl-cert</code>.
     </p><p>
      To use your own certificate (conventional or wildcard), replace the
      generated certificate at <code class="filename">/etc/ssl/certs/hawk.pem</code>
      with your own.
     </p></dd></dl></div><div class="procedure" id="pro-ha-hawk2-service" data-id-title="Starting Hawk2 Services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.1: </span><span class="title-name">Starting Hawk2 Services </span></span><a title="Permalink" class="permalink" href="#pro-ha-hawk2-service">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the node you want to connect to, open a shell and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check the status of the service by entering
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status hawk</pre></div></li><li class="step"><p>
     If the service is not running, start it with
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start hawk</pre></div><p>
     If you want Hawk2 to start automatically at boot time, execute the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable hawk</pre></div></li></ol></div></div></section><section class="sect1" id="sec-conf-hawk2-login" data-id-title="Logging In"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Logging In</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-login">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Hawk2 Web interface uses the HTTPS protocol and port
   <code class="literal">7630</code>.
  </p><p>
   Instead of logging in to an individual cluster node with Hawk2, you can
   configure a floating, virtual IP address (<code class="literal">IPaddr</code> or
   <code class="literal">IPaddr2</code>) as a cluster resource. It does not need any
   special configuration. It allows clients to connect to the Hawk service no
   matter which physical node the service is running on.
  </p><p>
   When setting up the cluster with the
   <code class="systemitem">ha-cluster-bootstrap</code> scripts,
   you will be asked whether to configure a virtual IP for cluster
   administration.
  </p><div class="procedure" id="pro-ha-hawk2-login" data-id-title="Logging In to the Hawk2 Web Interface"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.2: </span><span class="title-name">Logging In to the Hawk2 Web Interface </span></span><a title="Permalink" class="permalink" href="#pro-ha-hawk2-login">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On any machine, start a Web browser and enter the following URL:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div><p>
     Replace <em class="replaceable">HAWKSERVER</em> with the IP address or host
     name of any cluster node running the Hawk Web service. If a virtual IP
     address has been configured for cluster administration with Hawk2,
     replace <em class="replaceable">HAWKSERVER</em> with the virtual IP address.
    </p><div id="id-1.3.4.4.4.5.2.4" data-id-title="Certificate Warning" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Certificate Warning</div><p>
      If a certificate warning appears when you try to access the URL for the
      first time, a self-signed certificate is in use. Self-signed certificates
      are not considered trustworthy by default.
     </p><p>
      To verify the certificate, ask your cluster operator for the certificate
      details.
     </p><p>
      To proceed anyway, you can add an exception in the browser to bypass the
      warning.
     </p><p>
      For information on how to replace the self-signed certificate with a
      certificate signed by an official Certificate Authority, refer to
      <a class="xref" href="#vle-trouble-hawk2-cert">Replacing the Self-Signed Certificate</a>.
     </p></div></li><li class="step"><p>
     On the Hawk2 login screen, enter the <span class="guimenu">Username</span> and
     <span class="guimenu">Password</span> of the
     <code class="systemitem">hacluster</code> user (or of any other
     user that is a member of the
     <code class="systemitem">haclient</code> group).
    </p></li><li class="step"><p>
     Click <span class="guimenu">Log In</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-conf-hawk2-overview" data-id-title="Hawk2 Overview: Main Elements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Hawk2 Overview: Main Elements</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After logging in to Hawk2, you will see a navigation bar on the left-hand
   side and a top-level row with several links on the right-hand side.
  </p><div id="id-1.3.4.4.5.3" data-id-title="Available Functions in Hawk2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Available Functions in Hawk2</div><p>
    By default, users logged in as <code class="systemitem">root</code> or
    <code class="systemitem">hacluster</code> have full
    read-write access to all cluster configuration tasks. However,
    <a class="xref" href="#cha-ha-acl" title="Chapter 11. Access Control Lists"><em>Access Control Lists</em></a> (ACLs) can be used to
    define fine-grained access permissions.
   </p><p>
    If ACLs are enabled in the CRM, the available functions in Hawk2 depend
    on the user role and their assigned access permissions. The
    <span class="guimenu">History Explorer</span> in Hawk2 can only be executed by the
    user <code class="systemitem">hacluster</code>.
   </p></div><section class="sect2" id="sec-conf-hawk2-overview-leftnav" data-id-title="Left Navigation Bar"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.3.1 </span><span class="title-name">Left Navigation Bar</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview-leftnav">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.5.4.2.1"><span class="term"><span class="guimenu">Manage</span>
     </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Status</span>: Displays the current cluster status at a
         glance (similar to <code class="command">crm status</code> on the crmsh). For
         details, see <a class="xref" href="#sec-conf-hawk2-manage-monitor-status" title="6.8.1. Monitoring a Single Cluster">Section 6.8.1, “Monitoring a Single Cluster”</a>. If
         your cluster includes <code class="literal">guest nodes</code> (nodes that run
         the <code class="literal">pacemaker_remote</code> daemon), they are displayed, too.
         
         The screen refreshes in near real-time: any status changes for nodes
         or resources are visible almost immediately.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Dashboard</span>: Allows you to monitor multiple clusters
         (also located on different sites, in case you have a Geo cluster
         setup). For details, see
         <a class="xref" href="#sec-conf-hawk2-manage-monitor-dash" title="6.8.2. Monitoring Multiple Clusters">Section 6.8.2, “Monitoring Multiple Clusters”</a>. If your cluster
         includes <code class="literal">guest nodes</code> (nodes that run
         the <code class="literal">pacemaker_remote</code> daemon), they are displayed, too.
         The screen
         refreshes in near real-time: any status changes for nodes or resources
         are visible almost immediately.
        </p></li><li class="listitem"><p>
         <span class="guimenu">History</span>: Opens the <span class="guimenu">History
         Explorer</span> from which you can generate cluster reports. For
         details, see <a class="xref" href="#sec-conf-hawk2-history" title="6.10. Viewing the Cluster History">Section 6.10, “Viewing the Cluster History”</a>.
        </p></li></ul></div></dd><dt id="id-1.3.4.4.5.4.2.2"><span class="term"><span class="guimenu">Configuration</span>
     </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Add Resource</span>: Opens the resource configuration
         screen. For details, see <a class="xref" href="#sec-conf-hawk2-rsc" title="6.5. Configuring Cluster Resources">Section 6.5, “Configuring Cluster Resources”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Add Constraint</span>: Opens the constraint configuration
         screen. For details, see <a class="xref" href="#sec-conf-hawk2-cons" title="6.6. Configuring Constraints">Section 6.6, “Configuring Constraints”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Wizards</span>: Allows you to select from several wizards
         that guide you through the creation of resources for a certain
         workload, for example, a DRBD block device. For details, see
         <a class="xref" href="#sec-conf-hawk2-rsc-wizard" title="6.5.2. Adding Resources with the Wizard">Section 6.5.2, “Adding Resources with the Wizard”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Edit Configuration</span>: Allows you to edit resources,
         constraints, node names and attributes, tags, alerts, and fencing
         topologies.
         
         
        </p></li><li class="listitem"><p>
         <span class="guimenu">Cluster Configuration</span>: Allows you to modify global
         cluster options and resource and operation defaults. For details, see
         <a class="xref" href="#sec-conf-hawk2-cluster-config" title="6.4. Configuring Global Cluster Options">Section 6.4, “Configuring Global Cluster Options”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Command Log</span>: Lists the crmsh commands recently
         executed by Hawk2.
        </p></li></ul></div></dd><dt id="id-1.3.4.4.5.4.2.3"><span class="term"><span class="guimenu">Access Control</span>
     </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Roles</span>: Opens a screen where you can create roles
         for access control lists (sets of rules describing access rights to
         the CIB). For details, see <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 11.2, “Adding a Monitor Role with Hawk2”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Targets</span>: Opens a screen where you can create
         targets (system users) for access control lists and assign roles to
         them. For details, see <a class="xref" href="#pro-ha-acl-hawk2-target" title="Assigning a Role to a Target with Hawk2">Procedure 11.3, “Assigning a Role to a Target with Hawk2”</a>.
        </p></li></ul></div></dd></dl></div></section><section class="sect2" id="sec-conf-hawk2-overview-toprow" data-id-title="Top-Level Row"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.3.2 </span><span class="title-name">Top-Level Row</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview-toprow">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Hawk2's top-level row shows the following entries:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="guimenu">Batch</span>: Click to switch to batch mode. This allows you
      to simulate and stage changes and to apply them as a single transaction.
      For details, see <a class="xref" href="#sec-conf-hawk2-batch" title="6.9. Using the Batch Mode">Section 6.9, “Using the Batch Mode”</a>.
     </p></li><li class="listitem"><p>
      <span class="guimenu"><em class="replaceable">USERNAME</em></span>: Allows you to set
      preferences for Hawk2 (for example, the language for the Web interface,
      or whether to display a warning if STONITH is disabled).
     </p></li><li class="listitem"><p>
      <span class="guimenu">Help</span>: Access the SUSE Linux Enterprise High Availability documentation, read the
      release notes or report a bug.
     </p></li><li class="listitem"><p>
      <span class="guimenu">Logout</span>: Click to log out.
     </p></li></ul></div></section></section><section class="sect1" id="sec-conf-hawk2-cluster-config" data-id-title="Configuring Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Configuring Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cluster-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Global cluster options control how the cluster behaves when confronted with
   certain situations. They are grouped into sets and can be viewed and
   modified with cluster management tools like Hawk2 and crmsh. The
   predefined values can usually be kept. However, to ensure the key functions
   of your cluster work correctly, you need to adjust the following parameters
   after basic cluster setup:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.2.2. Global Option no-quorum-policy">Global Option <code class="literal">no-quorum-policy</code></a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-stonith" title="5.2.3. Global Option stonith-enabled">Global Option <code class="literal">stonith-enabled</code></a>
    </p></li></ul></div><div class="procedure" id="pro-conf-hawk2-global" data-id-title="Modifying Global Cluster Options"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.3: </span><span class="title-name">Modifying Global Cluster Options </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Cluster
     Configuration</span>.
    </p><p>
     The <span class="guimenu">Cluster Configuration</span> screen opens. It displays the
     global cluster options and their current values.
    </p><p>
     To display a short description of the parameter on the right-hand side of
     the screen, hover your mouse over a parameter.
     
    </p><div class="figure" id="id-1.3.4.4.6.4.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-cluster-config.png"><img src="images/hawk2-cluster-config.png" width="100%" alt="Hawk2—Cluster Configuration" title="Hawk2—Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.1: </span><span class="title-name">Hawk2—Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.6.4.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     Check the values for <span class="guimenu">no-quorum-policy</span> and
     <span class="guimenu">stonith-enabled</span> and adjust them, if necessary:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Set <span class="guimenu">no-quorum-policy</span> to the appropriate value. See
       <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.2.2. Global Option no-quorum-policy">Section 5.2.2, “Global Option <code class="literal">no-quorum-policy</code>”</a> for more details.
      </p></li><li class="step"><p>
       If you need to disable fencing for any reason, set
       <span class="guimenu">stonith-enabled</span> to <code class="literal">no</code>. By default,
       it is set to <code class="literal">true</code>, because using STONITH devices is
       necessary for normal cluster operation. According to the
       default value, the cluster will refuse to start any resources if no
       STONITH resources have been configured.
      </p><div id="id-1.3.4.4.6.4.4.2.2.2" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div></li><li class="step"><p>
       To remove a parameter from the cluster configuration, click the
       <span class="guimenu">Minus</span> icon next to the parameter. If a parameter is
       deleted, the cluster will behave as if that parameter had the default
       value.
      </p></li><li class="step"><p>
       To add a new parameter to the cluster configuration, choose one from the
       drop-down box.
      </p></li></ol></li><li class="step"><p>
     If you need to change <span class="guimenu">Resource Defaults</span> or
     <span class="guimenu">Operation Defaults</span>, proceed as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       To adjust a value, either select a different value from the drop-down
       box or edit the value directly.
      </p></li><li class="step"><p>
       To add a new resource default or operation default, choose one from the
       empty drop-down box and enter a
       value. If there are default values, Hawk2 proposes them automatically.
      </p></li><li class="step"><p>
       To remove a parameter, click the <span class="guimenu">Minus</span> icon next to
       it. If no values are specified for <span class="guimenu">Resource Defaults</span>
       and <span class="guimenu">Operation Defaults</span>, the cluster uses the default
       values that are documented in
       <a class="xref" href="#sec-ha-config-basics-meta-attr" title="5.3.6. Resource Options (Meta Attributes)">Section 5.3.6, “Resource Options (Meta Attributes)”</a> and
       <a class="xref" href="#sec-ha-config-basics-operations" title="5.3.8. Resource Operations">Section 5.3.8, “Resource Operations”</a>.
      </p></li></ol></li><li class="step"><p>
     Confirm your changes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-conf-hawk2-rsc" data-id-title="Configuring Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Configuring Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A cluster administrator needs to create cluster resources for every resource
  or application that runs on the servers in your cluster. Cluster resources
  can include Web sites, mail servers, databases, file systems, virtual
  machines, and any other server-based applications or services you want to
  make available to users at all times.
 </p><p>
  For an overview of the resource types you can create, refer to
  <a class="xref" href="#sec-ha-config-basics-resources-types" title="5.3.3. Types of Resources">Section 5.3.3, “Types of Resources”</a>. After you have
  specified the resource basics (ID, class, provider, and type), Hawk2 shows
  the following categories:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.7.5.1"><span class="term">Parameters (Instance Attributes)</span></dt><dd><p>
     Determines which instance of a service the resource controls. For more
     information, refer to <a class="xref" href="#sec-ha-config-basics-inst-attr" title="5.3.7. Instance Attributes (Parameters)">Section 5.3.7, “Instance Attributes (Parameters)”</a>.
    </p><p>
     When creating a resource, Hawk2 automatically shows any required
     parameters. Edit them to get a valid resource configuration.
    </p></dd><dt id="id-1.3.4.4.7.5.2"><span class="term">Operations</span></dt><dd><p>
     Needed for resource monitoring. For more information, refer to
     <a class="xref" href="#sec-ha-config-basics-operations" title="5.3.8. Resource Operations">Section 5.3.8, “Resource Operations”</a>.
    </p><p>
     When creating a resource, Hawk2 displays the most important resource
     operations (<code class="literal">monitor</code>, <code class="literal">start</code>, and
     <code class="literal">stop</code>).
    </p></dd><dt id="id-1.3.4.4.7.5.3"><span class="term">Meta Attributes</span></dt><dd><p>
     Tells the CRM how to treat a specific resource. For more information,
     refer to <a class="xref" href="#sec-ha-config-basics-meta-attr" title="5.3.6. Resource Options (Meta Attributes)">Section 5.3.6, “Resource Options (Meta Attributes)”</a>.
    </p><p>
     When creating a resource, Hawk2 automatically lists the important meta
     attributes for that resource (for example, the
     <code class="literal">target-role</code> attribute that defines the initial state of
     a resource. By default, it is set to <code class="literal">Stopped</code>, so the
     resource will not start immediately).
    </p></dd><dt id="id-1.3.4.4.7.5.4"><span class="term">Utilization</span></dt><dd><p>
     Tells the CRM what capacity a certain resource requires from a node. For
     more information, refer to
     <a class="xref" href="#sec-config-hawk2-utilization" title="6.6.8. Configuring Placement of Resources Based on Load Impact">Section 6.6.8, “Configuring Placement of Resources Based on Load Impact”</a>.
    </p></dd></dl></div><p>
  You can adjust the entries and values in those categories either during
  resource creation or later.
 </p><section class="sect2" id="sec-conf-hawk2-rsc-show" data-id-title="Showing the Current Cluster Configuration (CIB)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.1 </span><span class="title-name">Showing the Current Cluster Configuration (CIB)</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-show">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Sometimes a cluster administrator needs to know the cluster configuration.
   Hawk2 can show the current configuration in crm shell syntax, as XML and
   as a graph. To view the cluster configuration in crm shell syntax, from the
   left navigation bar select <span class="guimenu">Edit Configuration</span> and click
   <span class="guimenu">Show</span>. To show the configuration in raw XML instead, click
   <span class="guimenu">XML</span>. Click <span class="guimenu">Graph</span> for a graphical
   representation of the nodes and resources configured in the CIB. It also
   shows the relationships between resources.
  </p></section><section class="sect2" id="sec-conf-hawk2-rsc-wizard" data-id-title="Adding Resources with the Wizard"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.2 </span><span class="title-name">Adding Resources with the Wizard</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-wizard">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Hawk2 wizard is a convenient way of setting up simple resources like a
   virtual IP address or an SBD STONITH resource, for example. It is
   also useful for complex configurations that include multiple resources,
   like the resource configuration for a DRBD block device or an Apache Web
   server. The wizard guides you through the configuration steps and provides
   information about the parameters you need to enter.
  </p><div class="procedure" id="pro-conf-hawk2-wizard" data-id-title="Using the Resource Wizard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.4: </span><span class="title-name">Using the Resource Wizard </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-wizard">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Wizards</span>.
    </p></li><li class="step"><p>
     Expand the individual categories by clicking the arrow down icon next to
     them and select the desired wizard.
    </p></li><li class="step"><p>
     Follow the instructions on the screen. After the last configuration step,
     <span class="guimenu">Verify</span> the values you have entered.
    </p><p>
     Hawk2 shows which actions it is going to perform and what the
     configuration looks like. Depending on the configuration, you might be
     prompted for the <code class="systemitem">root</code> password before you can
     <span class="guimenu">Apply</span> the configuration.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.7.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-wizard-apache.png"><img src="images/hawk2-wizard-apache.png" width="100%" alt="Hawk2—Wizard for Apache Web Server" title="Hawk2—Wizard for Apache Web Server"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.2: </span><span class="title-name">Hawk2—Wizard for Apache Web Server </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-primitive" data-id-title="Adding Simple Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.3 </span><span class="title-name">Adding Simple Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-primitive">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To create the most basic type of resource, proceed as follows:
  </p><div class="procedure" id="pro-conf-hawk2-primitive-add" data-id-title="Adding a Primitive Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.5: </span><span class="title-name">Adding a Primitive Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-primitive-add">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Primitive</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Resource ID</span>.
    </p></li><li class="step"><p>
     In case a resource template exists on which you want to base the resource
     configuration, select the respective <span class="guimenu">Template</span>. For
     details about configuring templates, see
     <a class="xref" href="#pro-conf-hawk2-template-add" title="Adding a Resource Template">Procedure 6.6, “Adding a Resource Template”</a>.
    </p></li><li class="step" id="step-ha-config-hawk2-primitive-start"><p>
     Select the resource agent <span class="guimenu">Class</span> you want to use:
     <code class="literal">lsb</code>, <code class="literal">ocf</code>,
     <code class="literal">service</code>, <code class="literal">stonith</code>, or
     <code class="literal">systemd</code>. For more information, see
     <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>.
    </p></li><li class="step"><p>
     If you selected <code class="literal">ocf</code> as class, specify the
     <span class="guimenu">Provider</span> of your OCF resource agent. The OCF
     specification allows multiple vendors to supply the same resource agent.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Type</span> list, select the resource agent you want
     to use (for example, <span class="guimenu">IPaddr</span> or
     <span class="guimenu">Filesystem</span>). A short description for this resource
     agent is displayed.
    </p><p>
     With that, you have specified the resource basics.
    </p><div id="id-1.3.4.4.7.9.3.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
      The selection you get in the <span class="guimenu">Type</span> list depends on the
      <span class="guimenu">Class</span> (and for OCF resources also on the
      <span class="guimenu">Provider</span>) you have chosen.
     </p></div><div class="figure" id="id-1.3.4.4.7.9.3.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-basic.png"><img src="images/hawk2-primitive-basic.png" width="100%" alt="Hawk2—Primitive Resource" title="Hawk2—Primitive Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.3: </span><span class="title-name">Hawk2—Primitive Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.9.3.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     To keep the <span class="guimenu">Parameters</span>, <span class="guimenu">Operations</span>,
     and <span class="guimenu">Meta Attributes</span> as suggested by Hawk2, click
     <span class="guimenu">Create</span> to finish the configuration. A message at the
     top of the screen shows if the action has been successful.
    </p><p>
     To adjust the parameters, operations, or meta attributes, refer to
     <a class="xref" href="#sec-conf-hawk2-rsc-modify" title="6.5.5. Modifying Resources">Section 6.5.5, “Modifying Resources”</a>. To configure
     <span class="guimenu">Utilization</span> attributes for the resource, see
     <a class="xref" href="#pro-hawk2-utilization-rsc" title="Configuring the Capacity a Resource Requires">Procedure 6.21, “Configuring the Capacity a Resource Requires”</a>.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-template" data-id-title="Adding Resource Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.4 </span><span class="title-name">Adding Resource Templates</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To create lots of resources with similar configurations,
   defining a resource template is the easiest way. After being defined, it can
   be referenced in primitives or in certain types of constraints. For detailed
   information about function and use of resource templates, refer to
   <a class="xref" href="#sec-ha-config-basics-constraints-templates" title="5.5.3. Resource Templates and Constraints">Section 5.5.3, “Resource Templates and Constraints”</a>.
  </p><div class="procedure" id="pro-conf-hawk2-template-add" data-id-title="Adding a Resource Template"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.6: </span><span class="title-name">Adding a Resource Template </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-template-add">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Resource templates are configured like primitive resources.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Template</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Resource ID</span>.
    </p></li><li class="step"><p>
     Follow the instructions in <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>,
     starting from
     <a class="xref" href="#step-ha-config-hawk2-primitive-start" title="Step 5">Step 5</a>.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-modify" data-id-title="Modifying Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.5 </span><span class="title-name">Modifying Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-modify">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have created a resource, you can edit its configuration at any time
   by adjusting parameters, operations, or meta attributes as needed.
  </p><div class="procedure" id="pro-conf-hawk2-rsc-modify" data-id-title="Modifying Parameters, Operations, or Meta Attributes for a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.7: </span><span class="title-name">Modifying Parameters, Operations, or Meta Attributes for a Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-rsc-modify">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     On the Hawk2 <span class="guimenu">Status</span> screen, go to the
     <span class="guimenu">Resources</span> list.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column, click the arrow down icon
     next to the resource or group you want to modify and select
     <span class="guimenu">Edit</span>.
    </p><p>
     The resource configuration screen opens.
    </p><div class="figure" id="id-1.3.4.4.7.11.3.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-edit.png"><img src="images/hawk2-primitive-edit.png" width="100%" alt="Hawk2—Editing A Primitive Resource" title="Hawk2—Editing A Primitive Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.4: </span><span class="title-name">Hawk2—Editing A Primitive Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.11.3.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step" id="step-hawk2-rsc-modify-params"><p>
     To add a new parameter, operation, or meta attribute, select an entry from
     the empty drop-down box.
    </p></li><li class="step"><p>
     To edit any values in the <span class="guimenu">Operations</span> category, click
     the <span class="guimenu">Edit</span> icon of the respective entry, enter a
     different value for the operation, and click <span class="guimenu">Apply</span>.
    </p></li><li class="step"><p>
     When you are finished, click the <span class="guimenu">Apply</span> button in the
     resource configuration screen to confirm your changes to the parameters,
     operations, or meta attributes.
    </p><p>
     A message at the top of the screen shows if the action has been
     successful.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-stonith" data-id-title="Adding STONITH Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.6 </span><span class="title-name">Adding STONITH Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.4.4.7.12.2" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div><p>
   By default, the global cluster option <code class="literal">stonith-enabled</code> is
   set to <code class="literal">true</code>. If no STONITH resources have been defined,
   the cluster will refuse to start any resources. Configure one or more
   STONITH resources to complete the STONITH setup. To add a STONITH
   resource for SBD, for libvirt (KVM/Xen) or for vCenter/ESX Server, the
   easiest way is to use the Hawk2 wizard (see
   <a class="xref" href="#sec-conf-hawk2-rsc-wizard" title="6.5.2. Adding Resources with the Wizard">Section 6.5.2, “Adding Resources with the Wizard”</a>). While STONITH resources
   are configured similarly to other resources, their behavior is different in
   some respects. For details refer to <a class="xref" href="#sec-ha-fencing-config" title="9.3. STONITH Resources and Configuration">Section 9.3, “STONITH Resources and Configuration”</a>.
  </p><div class="procedure" id="pro-conf-hawk2-stonith" data-id-title="Adding a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.8: </span><span class="title-name">Adding a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Primitive</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Resource ID</span>.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Class</span> list, select the resource agent class
     <span class="guimenu">stonith</span>.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Type</span> list, select the STONITH plug-in to
     control your STONITH device. A short description for this plug-in is
     displayed.
    </p></li><li class="step"><p>
     Hawk2 automatically shows the required <span class="guimenu">Parameters</span> for
     the resource. Enter values for each parameter.
    </p></li><li class="step"><p>
     Hawk2 displays the most important resource <span class="guimenu">Operations</span>
     and proposes default values. If you do not modify any settings here,
     Hawk2 adds the proposed operations and their default values when you
     confirm.
    </p></li><li class="step"><p>
     If there is no reason to change them, keep the default <span class="guimenu">Meta
     Attributes</span> settings.
    </p><div class="figure" id="id-1.3.4.4.7.12.4.9.2"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-stonith.png"><img src="images/hawk2-primitive-stonith.png" width="100%" alt="Hawk2—STONITH Resource" title="Hawk2—STONITH Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.5: </span><span class="title-name">Hawk2—STONITH Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.12.4.9.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     Confirm your changes to create the STONITH resource.
    </p><p>
     A message at the top of the screen shows if the action has been
     successful.
    </p></li></ol></div></div><p>
   To complete your fencing configuration, add constraints. For more details,
   refer to <a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>.
  </p></section><section class="sect2" id="sec-conf-hawk2-rsc-group" data-id-title="Adding Cluster Resource Groups"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.7 </span><span class="title-name">Adding Cluster Resource Groups</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-group">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Some cluster resources depend on other components or resources. They require
   that each component or resource starts in a specific order and runs on the
   same server. To simplify this configuration SUSE Linux Enterprise High Availability supports the
   concept of groups.
  </p><p>
   Resource groups contain a set of resources that need to be located together,
   be started sequentially and stopped in the reverse order. For an example of
   a resource group and more information about groups and their properties,
   refer to
   <a class="xref" href="#sec-ha-config-basics-resources-advanced-groups" title="5.3.5.1. Groups">Section 5.3.5.1, “Groups”</a>.
  </p><div id="id-1.3.4.4.7.13.4" data-id-title="Empty Groups" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Empty Groups</div><p>
    Groups must contain at least one resource, otherwise the configuration is
    not valid. While creating a group, Hawk2 allows you to create more
    primitives and add them to the group. For details, see
    <a class="xref" href="#sec-conf-hawk2-manage-edit" title="6.7.1. Editing Resources and Groups">Section 6.7.1, “Editing Resources and Groups”</a>.
   </p></div><div class="procedure" id="pro-conf-hawk2-group" data-id-title="Adding a Resource Group"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.9: </span><span class="title-name">Adding a Resource Group </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-group">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Group</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Group ID</span>.
    </p></li><li class="step"><p>
     To define the group members, select one or multiple entries in the list of
     <span class="guimenu">Children</span>. Re-sort group members by dragging and
     dropping them into the order you want by using the <span class="quote">“<span class="quote">handle</span>”</span>
     icon on the right.
    </p></li><li class="step"><p>
     If needed, modify or add <span class="guimenu">Meta Attributes</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.7.13.6"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-rsc-group.png"><img src="images/hawk2-rsc-group.png" width="100%" alt="Hawk2—Resource Group" title="Hawk2—Resource Group"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.6: </span><span class="title-name">Hawk2—Resource Group </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.13.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-clone" data-id-title="Adding Clone Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.8 </span><span class="title-name">Adding Clone Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-clone">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want certain resources to run simultaneously on multiple nodes in
   your cluster, configure these resources as clones. An example of a resource
   that can be configured as a clone is
   <code class="literal">ocf:pacemaker:controld</code> for cluster file systems like
   OCFS2. Any regular resources or resource groups can be cloned. Instances of
   cloned resources may behave identically. However, they may also be
   configured differently, depending on which node they are hosted on.
  </p><p>
   For an overview of the available types of resource clones, refer to
   <a class="xref" href="#sec-ha-config-basics-resources-advanced-clones" title="5.3.5.2. Clones">Section 5.3.5.2, “Clones”</a>.
  </p><div id="id-1.3.4.4.7.14.4" data-id-title="Child Resources for Clones" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Child Resources for Clones</div><p>
    Clones can either contain a primitive or a group as child resources. In
    Hawk2, child resources cannot be created or modified while creating a
    clone. Before adding a clone, create child resources and configure them as
    desired. For details, refer to
    <a class="xref" href="#sec-conf-hawk2-rsc-primitive" title="6.5.3. Adding Simple Resources">Section 6.5.3, “Adding Simple Resources”</a> or
    <a class="xref" href="#sec-conf-hawk2-rsc-group" title="6.5.7. Adding Cluster Resource Groups">Section 6.5.7, “Adding Cluster Resource Groups”</a>.
   </p></div><div class="procedure" id="pro-conf-hawk2-clone" data-id-title="Adding a Clone Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.10: </span><span class="title-name">Adding a Clone Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-clone">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Clone</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Clone ID</span>.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Child Resource</span> list, select the primitive or
     group to use as a sub-resource for the clone.
    </p></li><li class="step"><p>
     If needed, modify or add <span class="guimenu">Meta Attributes</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.7.14.6"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-clone.png"><img src="images/hawk2-clone.png" width="100%" alt="Hawk2—Clone Resource" title="Hawk2—Clone Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.7: </span><span class="title-name">Hawk2—Clone Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.14.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-ms" data-id-title="Adding Multi-state Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.9 </span><span class="title-name">Adding Multi-state Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-ms">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Multi-state resources are a specialization of clones. They allow the
   instances to be in one of two operating modes (called
   <code class="literal">active/passive</code>, <code class="literal">primary/secondary</code>, or
   <code class="literal">master/slave</code>). Multi-state resources must contain exactly
   one group or one regular resource.
  </p><p>
   When configuring resource monitoring or constraints, multi-state resources
   have different requirements than simple resources. For details, see
    <em class="citetitle">Pacemaker Explained</em>, available from
   <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to section
   <em class="citetitle">Multi-state - Resources That Have Multiple Modes</em>.
  </p><div id="id-1.3.4.4.7.15.4" data-id-title="Child Resources for Multi-state Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Child Resources for Multi-state Resources</div><p>
    Multi-state resources can either contain a primitive or a group as child
    resources. In Hawk2, child resources cannot be created or modified while
    creating a multi-state resource. Before adding a multi-state resource,
    create child resources and configure them as desired. For details, refer to
    <a class="xref" href="#sec-conf-hawk2-rsc-primitive" title="6.5.3. Adding Simple Resources">Section 6.5.3, “Adding Simple Resources”</a> or
    <a class="xref" href="#sec-conf-hawk2-rsc-group" title="6.5.7. Adding Cluster Resource Groups">Section 6.5.7, “Adding Cluster Resource Groups”</a>.
   </p></div><div class="procedure" id="pro-conf-hawk2-ms" data-id-title="Adding a Multi-state Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.11: </span><span class="title-name">Adding a Multi-state Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-ms">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Multi-state</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Multi-state ID</span>.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Child Resource</span> list, select the primitive or
     group to use as a sub-resource for the multi-state resource.
    </p></li><li class="step"><p>
     If needed, modify or add <span class="guimenu">Meta Attributes</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.7.15.6"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-multi-state.png"><img src="images/hawk2-multi-state.png" width="100%" alt="Hawk2—Multi-state Resource" title="Hawk2—Multi-state Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.8: </span><span class="title-name">Hawk2—Multi-state Resource </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.15.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-tag" data-id-title="Grouping Resources by Using Tags"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.10 </span><span class="title-name">Grouping Resources by Using Tags</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-tag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Tags are a way to refer to multiple resources at once, without creating any
   colocation or ordering relationship between them. You can use tags for
   grouping conceptually related resources. For example, if you have several
   resources related to a database, you can add all related resources to a tag
   named <code class="literal">database</code>.
  </p><p>
   All resources belonging to a tag can be started or stopped with a single
   command.
  </p><div class="procedure" id="pro-conf-hawk2-tag" data-id-title="Adding a Tag"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.12: </span><span class="title-name">Adding a Tag </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-tag">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Resource</span> › <span class="guimenu">Tag</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Tag ID</span>.
    </p></li><li class="step"><p>
     From the <span class="guimenu">Objects</span> list, select the resources you want to
     refer to with the tag.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.7.16.5"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-tag.png"><img src="images/hawk2-tag.png" width="100%" alt="Hawk2—Tag" title="Hawk2—Tag"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.9: </span><span class="title-name">Hawk2—Tag </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.16.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-monitor" data-id-title="Configuring Resource Monitoring"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.11 </span><span class="title-name">Configuring Resource Monitoring</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-monitor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE Linux Enterprise High Availability not only detects node failures, but also when an individual
   resource on a node has failed. If you want to ensure that a resource is
   running, configure resource monitoring for it. Usually, resources are only
   monitored by the cluster while they are running. However, to detect
   concurrency violations, also configure monitoring for resources which are
   stopped. For resource monitoring, specify a timeout and/or start delay
   value, and an interval. The interval tells the CRM how often it should check
   the resource status. You can also set particular parameters such as
   <code class="literal">timeout</code> for <code class="literal">start</code> or
   <code class="literal">stop</code> operations.
  </p><div class="procedure" id="pro-hawk2-operations" data-id-title="Adding and Modifying an Operation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.13: </span><span class="title-name">Adding and Modifying an Operation </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-operations">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     Add a resource as described in
     <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a> or select an existing
     primitive to edit.
    </p><p>
     Hawk2 automatically shows the most important
     <span class="guimenu">Operations</span> (<code class="literal">start</code>,
     <code class="literal">stop</code>, <code class="literal">monitor</code>) and proposes default
     values.
    </p><p>
     To see the attributes belonging to each proposed value, hover the mouse
     pointer over the respective value.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-monitor-op.png"><img src="images/hawk2-monitor-op.png" width="60%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     To change the suggested <code class="literal">timeout</code> values for the
     <code class="literal">start</code> or <code class="literal">stop</code> operation:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click the pen icon next to the operation.
      </p></li><li class="step"><p>
       In the dialog that opens, enter a different value for the
       <code class="literal">timeout</code> parameter, for example <code class="literal">10</code>,
       and confirm your change.
      </p></li></ol></li><li class="step"><p>
     To change the suggested <span class="guimenu">interval</span> value for the
     <code class="literal">monitor</code> operation:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click the pen icon next to the operation.
      </p></li><li class="step"><p>
       In the dialog that opens, enter a different value for the monitoring
       <code class="literal">interval</code>.
      </p></li><li class="step"><p>
       To configure resource monitoring in the case that the resource is
       stopped:
      </p><ol type="i" class="substeps"><li class="step"><p>
         Select the <code class="literal">role</code> entry from the empty drop-down box
         below.
        </p></li><li class="step"><p>
         From the <code class="literal">role</code> drop-down box, select
         <code class="literal">Stopped</code>.
        </p></li><li class="step"><p>
         Click <span class="guimenu">Apply</span> to confirm your changes and to close
         the dialog for the operation.
        </p></li></ol></li></ol></li><li class="step"><p>
     Confirm your changes in the resource configuration screen. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><p>
   For the processes that take place if the resource monitor detects a failure,
   refer to <a class="xref" href="#sec-ha-config-basics-monitoring" title="5.4. Resource Monitoring">Section 5.4, “Resource Monitoring”</a>.
  </p><p>
   To view resource failures, switch to the <span class="guimenu">Status</span> screen in
   Hawk2 and select the resource you are interested in. In the
   <span class="guimenu">Operations</span> column click the arrow down icon and select
   <span class="guimenu">Recent Events</span>. The dialog that opens lists recent actions
   performed for the resource. Failures are displayed in red. To view the
   resource details, click the magnifier icon in the
   <span class="guimenu">Operations</span> column.
  </p><div class="figure" id="id-1.3.4.4.7.17.6"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-rsc-details.png"><img src="images/hawk2-rsc-details.png" width="60%" alt="Hawk2—Resource Details" title="Hawk2—Resource Details"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.10: </span><span class="title-name">Hawk2—Resource Details </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.7.17.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_rsc_i.xml" title="Edit source document"> </a></div></div></div></section></section><section class="sect1" id="sec-conf-hawk2-cons" data-id-title="Configuring Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.6 </span><span class="title-name">Configuring Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  After you have configured all resources, specify how the cluster should
  handle them correctly. Resource constraints let you specify on which cluster
  nodes resources can run, in which order to load resources, and what
  other resources a specific resource depends on.
 </p><p>
  For an overview of available types of constraints, refer to
  <a class="xref" href="#sec-ha-config-basics-constraints-types" title="5.5.1. Types of Constraints">Section 5.5.1, “Types of Constraints”</a>. When defining
  constraints, you also need to specify scores. For more information on scores
  and their implications in the cluster, see
  <a class="xref" href="#sec-ha-config-basics-constraints-scores" title="5.5.2. Scores and Infinity">Section 5.5.2, “Scores and Infinity”</a>.
 </p><section class="sect2" id="sec-conf-hawk2-cons-loc" data-id-title="Adding Location Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.1 </span><span class="title-name">Adding Location Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-loc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A location constraint determines on which node a resource may be run, is
   preferably run, or may not be run. An example of a location constraint is to
   place all resources related to a certain database on the same node.
  </p><div class="procedure" id="pro-hawk2-constraints-location" data-id-title="Adding a Location Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.14: </span><span class="title-name">Adding a Location Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-location">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Constraint</span> › <span class="guimenu">Location</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step" id="step-hawk2-loc-rsc"><p>
     From the list of <span class="guimenu">Resources</span> select the resource or
     resources for which to define the constraint.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. The score indicates the value you are
     assigning to this resource constraint. Positive values indicate the
     resource can run on the <span class="guimenu">Node</span> you specify in the next
     step. Negative values mean it should not run on that node. Constraints
     with higher scores are applied before those with lower scores.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To force the resources to run on the node, click the arrow
       icon and select <code class="literal">Always</code>. This sets the score to
       <code class="literal">INFINITY</code>.
      </p></li><li class="listitem"><p>
       If you never want the resources to run on the node, click the arrow icon
       and select <code class="literal">Never</code>. This sets the score to
       <code class="literal">-INFINITY</code>, meaning that the resources must not run on
       the node.
      </p></li><li class="listitem"><p>
       To set the score to <code class="literal">0</code>, click the arrow icon and
       select <code class="literal">Advisory</code>. This disables the constraint. This
       is useful when you want to set resource discovery but do not want to
       constrain the resources.
      </p></li></ul></div></li><li class="step"><p>
     Select a <span class="guimenu">Node</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.8.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-loc-constraint.png"><img src="images/hawk2-loc-constraint.png" width="100%" alt="Hawk2—Location Constraint" title="Hawk2—Location Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.11: </span><span class="title-name">Hawk2—Location Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.8.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-cons-col" data-id-title="Adding Colocation Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.2 </span><span class="title-name">Adding Colocation Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-col">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A colocational constraint tells the cluster which resources may or may not
   run together on a node. As a colocation constraint defines a dependency
   between resources, you need at least two resources to create a colocation
   constraint.
  </p><div class="procedure" id="pro-hawk2-constraints-colocation" data-id-title="Adding a Colocation Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.15: </span><span class="title-name">Adding a Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-colocation">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Constraint</span> › <span class="guimenu">Colocation</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. The score determines the location
     relationship between the resources. Positive values indicate that the
     resources should run on the same node. Negative values indicate that the
     resources should not run on the same node. The score will be combined with
     other factors to decide where to put the resource.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If you want to force the resources to run on the same node, click the
       arrow icon and select <code class="literal">Always</code>. This sets the score to
       <code class="literal">INFINITY</code>.
      </p></li><li class="listitem"><p>
       If you never want the resources to run on the same node, click the arrow
       icon and select <code class="literal">Never</code>. This sets the score to
       <code class="literal">-INFINITY</code>, meaning that the resources must not run on
       the same node.
      </p></li></ul></div></li><li class="step"><p>
     To define the resources for the constraint:
    </p><ol type="a" class="substeps"><li class="step" id="step-hawk2-col-rsc"><p>
       From the drop-down box in the <span class="guimenu">Resources</span> category,
       select a resource (or a template).
      </p><p>
       The resource is added and a new empty drop-down box appears beneath.
      </p></li><li class="step"><p>
       Repeat this step to add more resources.
      </p><p>
       As the topmost resource depends on the next resource and so on, the
       cluster will first decide where to put the last resource, then place the
       depending ones based on that decision. If the constraint cannot be
       satisﬁed, the cluster may not allow the dependent resource to run.
      </p></li><li class="step"><p>
       To swap the order of resources within the colocation constraint, click
       the arrow up icon next to a resource to swap it with the entry above.
      </p></li></ol></li><li class="step"><p>
     If needed, specify further parameters for each resource (such as
     <code class="literal">Started</code>, <code class="literal">Stopped</code>,
     <code class="literal">Master</code>, <code class="literal">Slave</code>,
     <code class="literal">Promote</code>, <code class="literal">Demote</code>): Click the empty
     drop-down box next to the resource and select the desired entry.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.8.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-col-constraint.png"><img src="images/hawk2-col-constraint.png" width="100%" alt="Hawk2—Colocation Constraint" title="Hawk2—Colocation Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.12: </span><span class="title-name">Hawk2—Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.8.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-cons-order" data-id-title="Adding Order Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.3 </span><span class="title-name">Adding Order Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-order">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Order constraints define the order in which resources are started and
   stopped. As an order constraint defines a dependency between resources, you
   need at least two resources to create an order constraint.
  </p><div class="procedure" id="pro-hawk2-constraints-order" data-id-title="Adding an Order Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.16: </span><span class="title-name">Adding an Order Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-order">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Add
     Constraint</span> › <span class="guimenu">Order</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. If the score is greater than zero, the
     order constraint is mandatory, otherwise it is optional.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To make the order constraint mandatory, click the arrow icon
       and select <code class="literal">Mandatory</code>.
      </p></li><li class="listitem"><p>
       If you want the order constraint to be a suggestion only, click the
       arrow icon and select <code class="literal">Optional</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">Serialize</code>: To ensure that no two stop/start actions
       occur concurrently for the resources, click the arrow icon and select
       <code class="literal">Serialize</code>. This makes sure that one resource must
       complete starting before the other can be started. A typical use case
       are resources that put a high load on the host during start-up.
      </p></li></ul></div></li><li class="step"><p>
     For order constraints, you can usually keep the option
     <span class="guimenu">Symmetrical</span> enabled. This specifies that resources are
     stopped in reverse order.
    </p></li><li class="step"><p>
     To define the resources for the constraint:
    </p><ol type="a" class="substeps"><li class="step" id="step-hawk2-order-rsc"><p>
       From the drop-down box in the <span class="guimenu">Resources</span> category,
       select a resource (or a template).
      </p><p>
       The resource is added and a new empty drop-down box appears beneath.
      </p></li><li class="step"><p>
       Repeat this step to add more resources.
      </p><p>
       The topmost resource will start first, then the second, etc. Usually the
       resources will be stopped in reverse order.
      </p></li><li class="step"><p>
       To swap the order of resources within the order constraint, click the
       arrow up icon next to a resource to swap it with the entry above.
      </p></li></ol></li><li class="step"><p>
     If needed, specify further parameters for each resource (like
     <code class="literal">Started</code>, <code class="literal">Stopped</code>,
     <code class="literal">Master</code>, <code class="literal">Slave</code>,
     <code class="literal">Promote</code>, <code class="literal">Demote</code>): Click the empty
     drop-down box next to the resource and select the desired entry.
    </p></li><li class="step"><p>
     Confirm your changes to finish the configuration. A message at the top of
     the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.3.4.4.8.7.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-order-constraint.png"><img src="images/hawk2-order-constraint.png" width="100%" alt="Hawk2—Order Constraint" title="Hawk2—Order Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.13: </span><span class="title-name">Hawk2—Order Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.8.7.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-cons-set" data-id-title="Using Resource Sets for Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.4 </span><span class="title-name">Using Resource Sets for Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-set">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As an alternative format for defining constraints, you can use
   <a class="xref" href="#sec-ha-config-basics-constraints-rscset" title="5.5.1.1. Resource Sets">Resource Sets</a>.
   They have the same ordering semantics as
   <a class="xref" href="#sec-ha-config-basics-resources-advanced-groups" title="5.3.5.1. Groups">Groups</a>.
  </p><div class="procedure" id="pro-hawk2-constraints-sets" data-id-title="Using a Resource Set for Constraints"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.17: </span><span class="title-name">Using a Resource Set for Constraints </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-sets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     To use a resource set within a location constraint:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Proceed as outlined in <a class="xref" href="#pro-hawk2-constraints-location" title="Adding a Location Constraint">Procedure 6.14, “Adding a Location Constraint”</a>,
       apart from
       <a class="xref" href="#step-hawk2-loc-rsc" title="Step 4">Step 4</a>: Instead
       of selecting a single resource, select multiple resources by pressing
       <span class="keycap">Ctrl</span> or <span class="keycap">Shift</span> and mouse
       click. This creates a resource set within the location constraint.
      </p></li><li class="step"><p>
       To remove a resource from the location constraint, press
       <span class="keycap">Ctrl</span> and click the resource again to deselect
       it.
      </p></li></ol></li><li class="step"><p>
     To use a resource set within a colocation or order constraint:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Proceed as described in
       <a class="xref" href="#pro-hawk2-constraints-colocation" title="Adding a Colocation Constraint">Procedure 6.15, “Adding a Colocation Constraint”</a> or
       <a class="xref" href="#pro-hawk2-constraints-order" title="Adding an Order Constraint">Procedure 6.16, “Adding an Order Constraint”</a>, apart from the
       step where you define the resources for the constraint
       (<a class="xref" href="#step-hawk2-col-rsc" title="Step 5.a">Step 5.a</a> or
       <a class="xref" href="#step-hawk2-order-rsc" title="Step 6.a">Step 6.a</a>):
      </p></li><li class="step"><p>
       Add multiple resources.
      </p></li><li class="step"><p>
       To create a resource set, click the chain icon next to a resource to
       link it to the resource above. A resource set is visualized by a frame
       around the resources belonging to a set.
      </p></li><li class="step"><p>
       You can combine multiple resources in a resource set or create multiple
       resource sets.
      </p><div class="figure" id="id-1.3.4.4.8.8.3.3.2.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-constraint-set.png"><img src="images/hawk2-constraint-set.png" width="100%" alt="Hawk2—Two Resource Sets in a Colocation Constraint" title="Hawk2—Two Resource Sets in a Colocation Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.14: </span><span class="title-name">Hawk2—Two Resource Sets in a Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.8.8.3.3.2.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
       To unlink a resource from the resource above, click the scissors icon
       next to the resource.
      </p></li></ol></li><li class="step"><p>
     Confirm your changes to finish the constraint configuration.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-cons-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.5 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-more">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For more information on configuring constraints and detailed background
   information about the basic concepts of ordering and colocation, refer to
   the documentation available at
   <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <em class="citetitle">Pacemaker Explained</em>, chapter <em class="citetitle">Resource Constraints</em>
    </p></li><li class="listitem"><p>
     <em class="citetitle">Colocation Explained</em>
    </p></li><li class="listitem"><p>
     <em class="citetitle">Ordering Explained</em>
    </p></li></ul></div></section><section class="sect2" id="sec-conf-hawk2-failover" data-id-title="Specifying Resource Failover Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.6 </span><span class="title-name">Specifying Resource Failover Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A resource will be automatically restarted if it fails. If that cannot be
   achieved on the current node, or it fails <em class="replaceable">N</em> times
   on the current node, it will try to fail over to another node. You can
   define several failures for resources (a
   <code class="literal">migration-threshold</code>), after which they will migrate to a
   new node. If you have more than two nodes in your cluster, the node to which
   a particular resource fails over is chosen by the High Availability software.
  </p><p>
   You can specify a specific node to which a resource will fail over by
   proceeding as follows:
  </p><div class="procedure" id="pro-hawk2-failover" data-id-title="Specifying a Failover Node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.18: </span><span class="title-name">Specifying a Failover Node </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-failover">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     Configure a location constraint for the resource as described in
     <a class="xref" href="#pro-hawk2-constraints-location" title="Adding a Location Constraint">Procedure 6.14, “Adding a Location Constraint”</a>.
    </p></li><li class="step"><p>
     Add the <code class="literal">migration-threshold</code> meta attribute to the
     resource as described in
     <a class="xref" href="#pro-conf-hawk2-rsc-modify" title="Modifying Parameters, Operations, or Meta Attributes for a Resource">Procedure 6.7: Modifying Parameters, Operations, or Meta Attributes for a Resource</a>,
     <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 4">Step 4</a>
     and enter a value for the migration-threshold. The value should be
     positive and less than INFINITY.
    </p></li><li class="step"><p>
     If you want to automatically expire the failcount for a resource, add the
     <code class="literal">failure-timeout</code> meta attribute to the resource as
     described in
     <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5: Adding a Primitive Resource</a>,
     <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 4">Step 4</a>
     and enter a <span class="guimenu">Value</span> for the
     <code class="literal">failure-timeout</code>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-failover-node.png"><img src="images/hawk2-failover-node.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     If you want to specify additional failover nodes with preferences for a
     resource, create additional location constraints.
    </p></li></ol></div></div><p>
   The process flow regarding migration thresholds and failcounts is
   demonstrated in <a class="xref" href="#ex-ha-config-basics-failover" title="Migration Threshold—Process Flow">Example 5.8, “Migration Threshold—Process Flow”</a>.
  </p><p>
   Instead of letting the failcount for a resource expire automatically, you
   can also clean up failcounts for a resource manually at any time. Refer to
   <a class="xref" href="#sec-conf-hawk2-manage-cleanup" title="6.7.3. Cleaning Up Resources">Section 6.7.3, “Cleaning Up Resources”</a> for details.
  </p></section><section class="sect2" id="sec-config-hawk2-failback" data-id-title="Specifying Resource Failback Nodes (Resource Stickiness)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.7 </span><span class="title-name">Specifying Resource Failback Nodes (Resource Stickiness)</span></span> <a title="Permalink" class="permalink" href="#sec-config-hawk2-failback">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A resource may fail back to its original node when that node is back online
   and in the cluster. To prevent this or to specify a different node for the
   resource to fail back to, change the stickiness value of the resource. You
   can either specify the resource stickiness when creating it or afterward.
  </p><p>
   For the implications of different resource stickiness values, refer to
   <a class="xref" href="#sec-ha-config-basics-failback" title="5.5.5. Failback Nodes">Section 5.5.5, “Failback Nodes”</a>.
  </p><div class="procedure" id="pro-hawk2-stickiness" data-id-title="Specifying Resource Stickiness"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.19: </span><span class="title-name">Specifying Resource Stickiness </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-stickiness">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     Add the <code class="literal">resource-stickiness</code> meta attribute to the
     resource as described in
     <a class="xref" href="#pro-conf-hawk2-rsc-modify" title="Modifying Parameters, Operations, or Meta Attributes for a Resource">Procedure 6.7: Modifying Parameters, Operations, or Meta Attributes for a Resource</a>,
     <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 4">Step 4</a>.
    </p></li><li class="step"><p>
     Specify a value between <code class="literal">-INFINITY</code> and
     <code class="literal">INFINITY</code> for <code class="literal">resource-stickiness</code>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-rsc-stickiness.png"><img src="images/hawk2-rsc-stickiness.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ol></div></div></section><section class="sect2" id="sec-config-hawk2-utilization" data-id-title="Configuring Placement of Resources Based on Load Impact"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.8 </span><span class="title-name">Configuring Placement of Resources Based on Load Impact</span></span> <a title="Permalink" class="permalink" href="#sec-config-hawk2-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Not all resources are equal. Some, such as Xen guests, require that the
   node hosting them meets their capacity requirements. If resources are placed
   so that their combined needs exceed the provided capacity, the performance
   of the resources diminishes or they fail.
  </p><p>
   To take this into account, SUSE Linux Enterprise High Availability allows you to specify the following
   parameters:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     The capacity a certain node <span class="emphasis"><em>provides</em></span>.
    </p></li><li class="listitem"><p>
     The capacity a certain resource <span class="emphasis"><em>requires</em></span>.
    </p></li><li class="listitem"><p>
     An overall strategy for placement of resources.
    </p></li></ol></div><p>
   For more details and a configuration example, refer to
   <a class="xref" href="#sec-ha-config-basics-utilization" title="5.5.6. Placing Resources Based on Their Load Impact">Section 5.5.6, “Placing Resources Based on Their Load Impact”</a>.
  </p><p>
   Utilization attributes are used to configure both the resource's
   requirements and the capacity a node provides. You first need to configure a
   node's capacity before you can configure the capacity a resource requires.
  </p><div class="procedure" id="pro-hawk2-utilization-node" data-id-title="Configuring the Capacity a Node Provides"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.20: </span><span class="title-name">Configuring the Capacity a Node Provides </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-utilization-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     On the <span class="guimenu">Nodes</span> tab, select the node whose capacity you
     want to configure.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column, click the arrow down icon and
     select <span class="guimenu">Edit</span>.
    </p><p>
     The <span class="guimenu">Edit Node</span> screen opens.
    </p></li><li class="step"><p>
     Below <span class="guimenu">Utilization</span>, enter a name for a utilization
     attribute into the empty drop-down box.
    </p><p>
     The name can be arbitrary (for example, <code class="literal">RAM_in_GB</code>).
    </p></li><li class="step"><p>
     Click the <span class="guimenu">Add</span> icon to add the attribute.
    </p></li><li class="step"><p>
     In the empty text box next to the attribute, enter an attribute value. The
     value must be an integer.
    </p></li><li class="step"><p>
     Add as many utilization attributes as you need and add values for all of
     them.
    </p></li><li class="step"><p>
     Confirm your changes. A message at the top of the screen shows if the
     action has been successful.
    </p></li></ol></div></div><div class="procedure" id="pro-hawk2-utilization-rsc" data-id-title="Configuring the Capacity a Resource Requires"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.21: </span><span class="title-name">Configuring the Capacity a Resource Requires </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-utilization-rsc">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Configure the capacity a certain resource requires from a node either when
    creating a primitive resource or when editing an existing primitive resource.
   </p><p>
    Before you can add utilization attributes to a resource, you need to have
    set utilization attributes for your cluster nodes as described in
    <a class="xref" href="#pro-hawk2-utilization-node" title="Configuring the Capacity a Node Provides">Procedure 6.20</a>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     To add a utilization attribute to an existing resource: Go to <span class="guimenu">Manage</span> › <span class="guimenu">Status</span> and open
     the resource configuration dialog as described in
     <a class="xref" href="#sec-conf-hawk2-manage-edit" title="6.7.1. Editing Resources and Groups">Section 6.7.1, “Editing Resources and Groups”</a>.
    </p><p>
     If you create a new resource: Go to <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> and proceed as described in
     <a class="xref" href="#sec-conf-hawk2-rsc-primitive" title="6.5.3. Adding Simple Resources">Section 6.5.3, “Adding Simple Resources”</a>.
    </p></li><li class="step"><p>
     In the resource configuration dialog, go to the
     <span class="guimenu">Utilization</span> category.
    </p></li><li class="step"><p>
     From the empty drop-down box, select one of the utilization attributes
     that you have configured for the nodes in
     <a class="xref" href="#pro-hawk2-utilization-node" title="Configuring the Capacity a Node Provides">Procedure 6.20</a>.
    </p></li><li class="step"><p>
     In the empty text box next to the attribute, enter an attribute value. The
     value must be an integer.
    </p></li><li class="step"><p>
     Add as many utilization attributes as you need and add values for all of
     them.
    </p></li><li class="step"><p>
     Confirm your changes. A message at the top of the screen shows if the
     action has been successful.
    </p></li></ol></div></div><p>
   After you have configured the capacities your nodes provide and the
   capacities your resources require, set the placement strategy in
   the global cluster options. Otherwise the capacity configurations have no
   effect. Several strategies are available to schedule the load: for example,
   you can concentrate it on as few nodes as possible, or balance it evenly
   over all available nodes. For more information, refer to
   <a class="xref" href="#sec-ha-config-basics-utilization" title="5.5.6. Placing Resources Based on Their Load Impact">Section 5.5.6, “Placing Resources Based on Their Load Impact”</a>.
  </p><div class="procedure" id="pro-ha-config-hawk2-placement" data-id-title="Setting the Placement Strategy"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.22: </span><span class="title-name">Setting the Placement Strategy </span></span><a title="Permalink" class="permalink" href="#pro-ha-config-hawk2-placement">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_cons_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster
     Configuration</span> to open the respective screen. It shows global
     cluster options and resource and operation defaults.
    </p></li><li class="step"><p>
     From the empty drop-down box in the upper part of the screen, select
     <code class="literal">placement-strategy</code>.
    </p><p>
     By default, its value is set to <span class="guimenu">Default</span>, which means
     that utilization attributes and values are not considered.
    </p></li><li class="step"><p>
     Depending on your requirements, set <span class="guimenu">Placement Strategy</span>
     to the appropriate value.
    </p></li><li class="step"><p>
     Confirm your changes.
    </p></li></ol></div></div></section></section><section class="sect1" id="sec-conf-hawk2-manage" data-id-title="Managing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.7 </span><span class="title-name">Managing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  In addition to configuring your cluster resources, Hawk2 allows you to
  manage existing resources from the <span class="guimenu">Status</span> screen. For a
  general overview of the screen refer to
  <a class="xref" href="#sec-conf-hawk2-manage-monitor-status" title="6.8.1. Monitoring a Single Cluster">Section 6.8.1, “Monitoring a Single Cluster”</a>.
 </p><section class="sect2" id="sec-conf-hawk2-manage-edit" data-id-title="Editing Resources and Groups"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.1 </span><span class="title-name">Editing Resources and Groups</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-edit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case you need to edit existing resources, go to the
   <span class="guimenu">Status</span> screen. In the <span class="guimenu">Operations</span>
   column, click the arrow down icon next to the resource or group you want to
   modify and select <span class="guimenu">Edit</span>.
  </p><p>
   The editing screen appears. If you edit a primitive resource, the following
   operations are available:
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Operations for Primitives </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.9.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Copying the resource.
    </p></li><li class="listitem"><p>
     Renaming the resource (changing its ID).
    </p></li><li class="listitem"><p>
     Deleting the resource.
    </p></li></ul></div><p>
   If you edit a group, the following operations are available:
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Operations for Groups </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.9.4.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Creating a new primitive which will be added to this group.
    </p></li><li class="listitem"><p>
     Renaming the group (changing its ID).
    </p></li><li class="listitem"><p>
     Re-sort group members by dragging and dropping them into the order you
     want using the <span class="quote">“<span class="quote">handle</span>”</span> icon on the right.
    </p></li></ul></div></section><section class="sect2" id="sec-conf-hawk2-manage-start" data-id-title="Starting Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.2 </span><span class="title-name">Starting Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you start a cluster resource, make sure it is set up correctly. For
   example, if you use an Apache server as a cluster resource, set up the
   Apache server first. Complete the Apache configuration before starting the
   respective resource in your cluster.
  </p><div id="id-1.3.4.4.9.5.3" data-id-title="Do Not Touch Services Managed by the Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Do Not Touch Services Managed by the Cluster</div><p>
    When managing a resource with SUSE Linux Enterprise High Availability, the resource must not be started
    or stopped otherwise (outside of the cluster, for example manually or on
    boot or reboot). The High Availability software is responsible for all service start
    or stop actions.
   </p><p>
    However, if you want to check if the service is configured properly, start
    it manually, but make sure that it is stopped again before the High Availability software takes
    over.
   </p><p>
    For interventions in resources that are currently managed by the cluster,
    set the resource to <code class="literal">maintenance mode</code> first. For details,
    see <a class="xref" href="#pro-ha-maint-mode-rsc-hawk2" title="Putting a Resource into Maintenance Mode with Hawk2">Procedure 23.5, “Putting a Resource into Maintenance Mode with Hawk2”</a>.
   </p></div><p>
   When creating a resource with Hawk2, you can set its initial state with
   the <code class="literal">target-role</code> meta attribute. If you set its value to
   <code class="literal">stopped</code>, the resource does not start automatically after
   being created.
  </p><div class="procedure" id="pro-hawk2-rsc-start" data-id-title="Starting A New Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.23: </span><span class="title-name">Starting A New Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     Select the resource to start. In its <span class="guimenu">Operations</span> column
     click the <span class="guimenu">Start</span> icon. To continue, confirm the message
     that appears.
    </p><p>
     When the resource has started, Hawk2 changes the resource's
     <span class="guimenu">Status</span> to green and shows on which node it is running.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-manage-cleanup" data-id-title="Cleaning Up Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.3 </span><span class="title-name">Cleaning Up Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-cleanup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A resource will be automatically restarted if it fails, but each failure
   increases the resource's failcount.
  </p><p>
   If a <code class="literal">migration-threshold</code> has been set for the resource,
   the node will no longer run the resource when the number of failures reaches
   the migration threshold.
  </p><p>
   A resource's failcount can either be reset automatically (by setting a
   <code class="literal">failure-timeout</code> option for the resource) or it can be
   reset manually as described below.
  </p><div class="procedure" id="pro-hawk2-clean" data-id-title="Cleaning Up A Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.24: </span><span class="title-name">Cleaning Up A Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-clean">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     Go to the resource to clean up. In the <span class="guimenu">Operations</span>
     column click the arrow down button and select <span class="guimenu">Cleanup</span>.
     To continue, confirm the message that appears.
    </p><p>
     This executes the command <code class="command">crm resource cleanup</code> and
     cleans up the resource on all nodes.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-manage-remove" data-id-title="Removing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.4 </span><span class="title-name">Removing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-remove">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you need to remove a resource from the cluster, follow the procedure
   below to avoid configuration errors:
  </p><div class="procedure" id="pro-hawk2-rsc-rm" data-id-title="Removing a Cluster Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.25: </span><span class="title-name">Removing a Cluster Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-rm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     Clean up the resource on all nodes as described in
     <a class="xref" href="#pro-hawk2-clean" title="Cleaning Up A Resource">Procedure 6.24, “Cleaning Up A Resource”</a>.
    </p></li><li class="step"><p>
     Stop the resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       In the left navigation bar, select <span class="guimenu">Status</span>. The list
       of <span class="guimenu">Resources</span> also shows the
       <span class="guimenu">Status</span>.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column click the
       <span class="guimenu">Stop</span> button next to the resource.
      </p></li><li class="step"><p>
       To continue, confirm the message that appears.
      </p><p>
       The <span class="guimenu">Status</span> column will reflect the change when the
       resource is stopped.
      </p></li></ol></li><li class="step"><p>
     Delete the resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       In the left navigation bar, select <span class="guimenu">Edit
       Configuration</span>.
      </p></li><li class="step"><p>
       In the list of <span class="guimenu">Resources</span>, go to the respective
       resource. From the <span class="guimenu">Operations</span> column click the
       <span class="guimenu">Delete</span> icon next to the resource.
      </p></li><li class="step"><p>
       To continue, confirm the message that appears.
      </p></li></ol></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-manage-migrate" data-id-title="Migrating Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.5 </span><span class="title-name">Migrating Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As mentioned in <a class="xref" href="#sec-conf-hawk2-failover" title="6.6.6. Specifying Resource Failover Nodes">Section 6.6.6, “Specifying Resource Failover Nodes”</a>, the cluster will
   fail over (migrate) resources automatically in case of software or hardware
   failures—according to certain parameters you can define (for example,
   migration threshold or resource stickiness). You can also manually migrate a
   resource to another node in the cluster. Or you decide to move the resource
   away from the current node and let the cluster decide where to put it.
  </p><div class="procedure" id="pro-hawk2-rsc-migrate" data-id-title="Manually Migrating a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.26: </span><span class="title-name">Manually Migrating a Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-migrate">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     In the list of <span class="guimenu">Resources</span>, select the respective
     resource.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column click the arrow down button
     and select <span class="guimenu">Migrate</span>.
    </p></li><li class="step"><p>
     In the window that opens you have the following choices:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="guimenu">Away from current node</span>: This creates a location
       constraint with a <code class="literal">-INFINITY</code> score for the current
       node.
      </p></li><li class="listitem"><p>
       Alternatively, you can move the resource to another node. This creates a
       location constraint with an <code class="literal">INFINITY</code> score for the
       destination node.
      </p></li></ul></div></li><li class="step"><p>
     Confirm your choice.
    </p></li></ol></div></div><p>
   To allow a resource to move back again, proceed as follows:
  </p><div class="procedure" id="pro-hawk2-rsc-migrate-back" data-id-title="Unmigrating a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.27: </span><span class="title-name">Unmigrating a Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-migrate-back">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_manage_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     In the list of <span class="guimenu">Resources</span>, go to the respective
     resource.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column click the arrow down button
     and select <span class="guimenu">Unmigrate</span>. To continue, confirm the message
     that appears.
    </p><p>
     Hawk2 uses the <code class="command">crm_resource </code> <code class="option">-U</code>
     command. The resource can move back to its original location or it may
     stay where it is (depending on resource stickiness).
    </p></li></ol></div></div><p>
   For more information, see  <em class="citetitle">Pacemaker Explained</em>, available from
   <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to section
   <em class="citetitle">Resource Migration</em>.
  </p></section></section><section class="sect1" id="sec-conf-hawk2-monitor" data-id-title="Monitoring Clusters"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.8 </span><span class="title-name">Monitoring Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 has different screens for monitoring single clusters and multiple
  clusters: the <span class="guimenu">Status</span> and the <span class="guimenu">Dashboard</span>
  screen.
 </p><section class="sect2" id="sec-conf-hawk2-manage-monitor-status" data-id-title="Monitoring a Single Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.8.1 </span><span class="title-name">Monitoring a Single Cluster</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-monitor-status">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To monitor a single cluster, use the <span class="guimenu">Status</span> screen. After
   you have logged in to Hawk2, the <span class="guimenu">Status</span> screen is
   displayed by default. An icon in the upper right corner shows the cluster
   status at a glance. For further details, have a look at the following
   categories:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.10.4.3.1"><span class="term">Errors</span></dt><dd><p>
      If errors have occurred, they are shown at the top of the page.
     </p></dd><dt id="id-1.3.4.4.10.4.3.2"><span class="term">Resources</span></dt><dd><p>
      Shows the configured resources including their <span class="guimenu">Status</span>,
      <span class="guimenu">Name</span> (ID), <span class="guimenu">Location</span> (node on which
      they are running), and resource agent <span class="guimenu">Type</span>. From the
      <span class="guimenu">Operations</span> column, you can start or stop a resource,
      trigger several actions, or view details. Actions that can be triggered
      include setting the resource to maintenance mode (or removing maintenance
      mode), migrating it to a different node, cleaning up the resource,
      showing any recent events, or editing the resource.
     </p></dd><dt id="id-1.3.4.4.10.4.3.3"><span class="term">Nodes</span></dt><dd><p>
      Shows the nodes belonging to the cluster site you are logged in to,
      including the nodes' <span class="guimenu">Status</span> and
      <span class="guimenu">Name</span>. In the <span class="guimenu">Maintenance</span> and
      <span class="guimenu">Standby</span> columns, you can set or remove the
      <code class="literal">maintenance</code> or <code class="literal">standby</code> flag for a
      node. The <span class="guimenu">Operations</span> column allows you to view recent
      events for the node or further
      details: for example, if a <code class="literal">utilization</code>,
      <code class="literal">standby</code> or <code class="literal">maintenance</code> attribute is
      set for the respective node.
     </p></dd><dt id="id-1.3.4.4.10.4.3.4"><span class="term">Tickets</span></dt><dd><p>
      Only shown if tickets have been configured (for use with Geo
      clustering).
     </p></dd></dl></div><div class="figure" id="id-1.3.4.4.10.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-cluster-status-main.png"><img src="images/hawk2-cluster-status-main.png" width="100%" alt="Hawk2—Cluster Status" title="Hawk2—Cluster Status"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.15: </span><span class="title-name">Hawk2—Cluster Status </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.10.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-manage-monitor-dash" data-id-title="Monitoring Multiple Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.8.2 </span><span class="title-name">Monitoring Multiple Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-monitor-dash">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To monitor multiple clusters, use the Hawk2 <span class="guimenu">Dashboard</span>.
   The cluster information displayed in the <span class="guimenu">Dashboard</span> screen
   is stored on the server side. It is synchronized between the cluster nodes (if
   passwordless SSH access between the cluster nodes has been configured). For
   details, see <a class="xref" href="#sec-crmreport-nonroot-ssh" title="D.2. Configuring a Passwordless SSH Account">Section D.2, “Configuring a Passwordless SSH Account”</a>. However, the
   machine running Hawk2 does not even need to be part of any
   cluster for that purpose—it can be a separate, unrelated system.
  </p><p>
   In addition to the general
   <a class="xref" href="#sec-conf-hawk2-req" title="6.1. Hawk2 Requirements">Hawk2 Requirements</a>, the following
   prerequisites need to be fulfilled to monitor multiple clusters with
   Hawk2:
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.10.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     All clusters to be monitored from Hawk2's <span class="guimenu">Dashboard</span>
     must be running SUSE Linux Enterprise High Availability 12 SP4.
    </p></li><li class="listitem"><p>
     If you did not replace the self-signed certificate for Hawk2 on every
     cluster node with your own certificate (or a certificate signed by an
     official Certificate Authority) yet, do the following: Log in to Hawk2 on
     <span class="emphasis"><em>every</em></span> node in <span class="emphasis"><em>every</em></span> cluster
     at least once. Verify the certificate (or add an exception in the
     browser to bypass the warning). Otherwise Hawk2 cannot connect to the
     cluster.
    </p></li></ul></div><div class="procedure" id="pro-conf-hawk2-dashboard" data-id-title="Monitoring Multiple Clusters with the Dashboard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.28: </span><span class="title-name">Monitoring Multiple Clusters with the Dashboard </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-dashboard">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Dashboard</span>.
    </p><p>
    Hawk2 shows an overview of the resources and nodes on the current
    cluster site. In addition, it shows any <span class="guimenu">Tickets</span> that
    have been configured for use with a Geo cluster. If you need information
    about the icons used in this view, click <span class="guimenu">Legend</span>. To
    search for a resource ID, enter the name (ID) into the <span class="guimenu">Search</span>
    text box. To only show specific nodes, click the filter icon and select a
    filtering option.
   </p><div class="figure" id="id-1.3.4.4.10.5.5.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-dashboard-site1.png"><img src="images/hawk2-dashboard-site1.png" width="95%" alt="Hawk2 Dashboard with One Cluster Site (amsterdam)" title="Hawk2 Dashboard with One Cluster Site (amsterdam)"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.16: </span><span class="title-name">Hawk2 Dashboard with One Cluster Site (<code class="literal">amsterdam</code>) </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.10.5.5.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     To add dashboards for multiple clusters:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Add Cluster</span>.
      </p></li><li class="step"><p>
       Enter the <span class="guimenu">Cluster name</span> with which to identify the
       cluster in the <span class="guimenu">Dashboard</span>. For example,
       <code class="literal">berlin</code>.
      </p></li><li class="step"><p>
       Enter the fully qualified host name of one of the nodes in the second
       cluster. For example, <code class="literal">charlie</code>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-dashboard-add-cluster.png"><img src="images/hawk2-dashboard-add-cluster.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Click <span class="guimenu">Add</span>. Hawk2 will display a second tab for the
       newly added cluster site with an overview of its nodes and resources.
      </p><div id="id-1.3.4.4.10.5.5.4.2.4.2" data-id-title="Connection Error" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Connection Error</div><p>If instead you are prompted to log in to this node by entering a
        password, you probably did not connect to this node yet and have not
        replaced the self-signed certificate. In that case, even after entering
        the password, the connection will fail with the following message:
       </p><div class="verbatim-wrap"><pre class="screen">Error connecting to server. Retrying every 5 seconds...</pre></div><p>
        To proceed, see <a class="xref" href="#vle-trouble-hawk2-cert">Replacing the Self-Signed Certificate</a>.
       </p></div></li></ol></li><li class="step"><p>
     To view more details for a cluster site or to manage it, switch to the
     site's tab and click the chain icon.
    </p><p>
     Hawk2 opens the <span class="guimenu">Status</span> view for this site in a new
     browser window or tab. From there, you can administer this part of the
     Geo cluster.
    </p></li><li class="step"><p>
     To remove a cluster from the dashboard, click the <code class="literal">x</code>
     icon on the right-hand side of the cluster's details.
    </p></li></ol></div></div></section></section><section class="sect1" id="sec-conf-hawk2-batch" data-id-title="Using the Batch Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.9 </span><span class="title-name">Using the Batch Mode</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-batch">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides a <span class="guimenu">Batch Mode</span>, including a
  <span class="emphasis"><em>cluster simulator</em></span>. It can be used for the following:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Staging changes to the cluster and applying them as a single transaction,
    instead of having each change take effect immediately.
   </p></li><li class="listitem"><p>
    Simulating changes and cluster events, for example, to explore potential
    failure scenarios.
   </p></li></ul></div><p>
  For example, batch mode can be used when creating groups of resources that
  depend on each other. Using batch mode, you can avoid applying intermediate
  or incomplete configurations to the cluster.
 </p><p>
  While batch mode is enabled, you can add or edit resources and constraints or
  change the cluster configuration. It is also possible to simulate events in
  the cluster, including nodes going online or offline, resource operations and
  tickets being granted or revoked. See
  <a class="xref" href="#pro-hawk2-batch-inject" title="Injecting Node, Resource or Ticket Events">Procedure 6.30, “Injecting Node, Resource or Ticket Events”</a> for details.
 </p><p>
  The <span class="emphasis"><em>cluster simulator</em></span> runs automatically after every
  change and shows the expected outcome in the user interface. For example,
  this also means: If you stop a resource while in batch mode, the user
  interface shows the resource as stopped—while actually, the resource is
  still running.
 </p><div id="id-1.3.4.4.11.8" data-id-title="Wizards and Changes to the Live System" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Wizards and Changes to the Live System</div><p>
   Some wizards include actions beyond mere cluster configuration. When using
   those wizards in batch mode, any changes that go beyond cluster
   configuration would be applied to the live system immediately.
  </p><p>
   Therefore wizards that require <code class="systemitem">root</code> permission cannot be executed in
   batch mode.
  </p></div><div class="procedure" id="pro-conf-hawk2-manage-monitor-batch" data-id-title="Working with the Batch Mode"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.29: </span><span class="title-name">Working with the Batch Mode </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-manage-monitor-batch">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    To activate the batch mode, select <span class="guimenu">Batch</span> from the
    top-level row.
   </p><p>
    An additional bar appears below the top-level row. It indicates that batch
    mode is active and contains links to actions that you can execute in batch
    mode.
   </p><div class="figure" id="fig-hawk2-batch"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-batchmode-active.png"><img src="images/hawk2-batchmode-active.png" width="100%" alt="Hawk2 Batch Mode Activated" title="Hawk2 Batch Mode Activated"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.17: </span><span class="title-name">Hawk2 Batch Mode Activated </span></span><a title="Permalink" class="permalink" href="#fig-hawk2-batch">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
    While batch mode is active, perform any changes to your cluster, like
    adding or editing resources and constraints or editing the cluster
    configuration.
   </p><p>
    The changes will be simulated and shown in all screens.
   </p></li><li class="step"><p>
    To view details of the changes you have made, select
    <span class="guimenu">Show</span> from the batch mode bar. The <span class="guimenu">Batch
    Mode</span> window opens.
   </p><p>
    For any configuration changes it shows the difference between the live
    state and the simulated changes in crmsh syntax: Lines starting with a
    <code class="literal">-</code> character represent the current state whereas lines
    starting with <code class="literal">+</code> show the proposed state.
   </p></li><li class="step"><p>
    To inject events or view even more details, see
    <a class="xref" href="#pro-hawk2-batch-inject" title="Injecting Node, Resource or Ticket Events">Procedure 6.30</a>.
    Otherwise <span class="guimenu">Close</span> the window.
   </p></li><li class="step"><p>
    Choose to either <span class="guimenu">Discard</span> or <span class="guimenu">Apply</span> the
    simulated changes and confirm your choice. This also deactivates batch mode
    and takes you back to normal mode.
   </p></li></ol></div></div><p>
  When running in batch mode, Hawk2 also allows you to inject <span class="guimenu">Node
  Events</span> and <span class="guimenu">Resource Events</span>.
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.11.11.1"><span class="term"><span class="guimenu">Node Events</span>
   </span></dt><dd><p>
     Let you change the state of a node. Available states are
     <span class="guimenu">online</span>, <span class="guimenu">offline</span>, and
     <span class="guimenu">unclean</span>.
    </p></dd><dt id="id-1.3.4.4.11.11.2"><span class="term"><span class="guimenu">Resource Events</span>
   </span></dt><dd><p>
     Let you change some properties of a resource. For example, you can set an
     operation (like <code class="literal">start</code>, <code class="literal">stop</code>,
     <code class="literal">monitor</code>), the node it applies to, and the expected
     result to be simulated.
    </p></dd><dt id="id-1.3.4.4.11.11.3"><span class="term"><span class="guimenu">Ticket Events</span>
   </span></dt><dd><p>
     Let you test the impact of granting and revoking tickets (used for Geo
     clusters).
    </p></dd></dl></div><div class="procedure" id="pro-hawk2-batch-inject" data-id-title="Injecting Node, Resource or Ticket Events"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.30: </span><span class="title-name">Injecting Node, Resource or Ticket Events </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-batch-inject">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    If batch mode is not active yet, click <span class="guimenu">Batch</span> at the
    top-level row to switch to batch mode.
   </p></li><li class="step"><p>
    In the batch mode bar, click <span class="guimenu">Show</span> to open the
    <span class="guimenu">Batch Mode</span> window.
   </p></li><li class="step"><p>
    To simulate a status change of a node:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Node
      Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Node</span> you want to manipulate and select its
      target <span class="guimenu">State</span>.
     </p></li><li class="step"><p>
      Confirm your changes. Your event is added to the queue of events listed
      in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li></ol></li><li class="step"><p>
    To simulate a resource operation:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Resource
      Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Resource</span> you want to manipulate and select
      the <span class="guimenu">Operation</span> to simulate.
     </p></li><li class="step"><p>
      If necessary, define an <span class="guimenu">Interval</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Node</span> on which to run the operation and the
      targeted <span class="guimenu">Result</span>. Your event is added to the queue of
      events listed in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li></ol></li><li class="step"><p>
    To simulate a ticket action:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Ticket
       Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Ticket</span> you want to manipulate and select
      the <span class="guimenu">Action</span> to simulate.
     </p></li><li class="step"><p>
      Confirm your changes. Your event is added to the queue of events listed
      in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li></ol></li><li class="step"><p>
    The <span class="guimenu">Batch Mode</span> dialog
    (<a class="xref" href="#fig-hawk2-batch-show" title="Hawk2 Batch Mode—Injected Invents and Configuration Changes">Figure 6.18</a>)
    shows a new line per injected event. Any event listed here is simulated
    immediately and is reflected on the <span class="guimenu">Status</span> screen.
   </p><p>
    If you have made any configuration changes, too, the difference between the
    live state and the simulated changes is shown below the injected events.
   </p><div class="figure" id="fig-hawk2-batch-show"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-batchmode-show.png"><img src="images/hawk2-batchmode-show.png" width="100%" alt="Hawk2 Batch Mode—Injected Invents and Configuration Changes" title="Hawk2 Batch Mode—Injected Invents and Configuration Changes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.18: </span><span class="title-name">Hawk2 Batch Mode—Injected Invents and Configuration Changes </span></span><a title="Permalink" class="permalink" href="#fig-hawk2-batch-show">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
    To remove an injected event, click the <span class="guimenu">Remove</span> icon next
    to it. Hawk2 updates the <span class="guimenu">Status</span> screen accordingly.
   </p></li><li class="step"><p>
    To view more details about the simulation run, click
    <span class="guimenu">Simulator</span> and choose one of the following:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.11.12.10.2.1"><span class="term"><span class="guimenu">Summary</span>
     </span></dt><dd><p>
       Shows a detailed summary.
      </p></dd><dt id="id-1.3.4.4.11.12.10.2.2"><span class="term"><span class="guimenu">CIB (in)</span>/<span class="guimenu">CIB (out)</span>
     </span></dt><dd><p>
       <span class="guimenu">CIB (in)</span> shows the initial CIB state. <span class="guimenu">CIB
       (out)</span> shows what the CIB would look like after the transition.
      </p></dd><dt id="id-1.3.4.4.11.12.10.2.3"><span class="term"><span class="guimenu">Transition Graph</span>
     </span></dt><dd><p>
       Shows a graphical representation of the transition.
      </p></dd><dt id="id-1.3.4.4.11.12.10.2.4"><span class="term"><span class="guimenu">Transition</span>
     </span></dt><dd><p>
       Shows an XML representation of the transition.
      </p></dd></dl></div></li><li class="step"><p>
    If you have reviewed the simulated changes, close the <span class="guimenu">Batch
    Mode</span> window.
   </p></li><li class="step"><p>
    To leave the batch mode, either <span class="guimenu">Apply</span> or
    <span class="guimenu">Discard</span> the simulated changes.
   </p></li></ol></div></div></section><section class="sect1" id="sec-conf-hawk2-history" data-id-title="Viewing the Cluster History"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.10 </span><span class="title-name">Viewing the Cluster History</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides the following possibilities to view past events on the
  cluster (on different levels and in varying detail):
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-recent" title="6.10.1. Viewing Recent Events of Nodes or Resources">Section 6.10.1, “Viewing Recent Events of Nodes or Resources”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-explorer" title="6.10.2. Using the History Explorer for Cluster Reports">Section 6.10.2, “Using the History Explorer for Cluster Reports”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-transitions" title="6.10.3. Viewing Transition Details in the History Explorer">Section 6.10.3, “Viewing Transition Details in the History Explorer”</a>
   </p></li></ul></div><section class="sect2" id="sec-conf-hawk2-history-recent" data-id-title="Viewing Recent Events of Nodes or Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.10.1 </span><span class="title-name">Viewing Recent Events of Nodes or Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-recent">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Status</span>. It lists
     <span class="guimenu">Resources</span> and <span class="guimenu">Nodes</span>.
    </p></li><li class="step"><p>
     To view recent events of a resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Resources</span> and select the respective resource.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column for the resource, click the
       arrow down button and select <span class="guimenu">Recent events</span>.
      </p><p>
       Hawk2 opens a new window and displays a table view of the latest
       events.
      </p></li></ol></li><li class="step"><p>
     To view recent events of a node:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Nodes</span> and select the respective node.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column for the node, select
       <span class="guimenu">Recent events</span>.
      </p><p>
       Hawk2 opens a new window and displays a table view of the latest
       events.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-node-events.png"><img src="images/hawk2-node-events.png" width="90%" alt="Image" title="Image"/></a></div></div></li></ol></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-history-explorer" data-id-title="Using the History Explorer for Cluster Reports"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.10.2 </span><span class="title-name">Using the History Explorer for Cluster Reports</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-explorer">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   From the left navigation bar, select <span class="guimenu">History</span> to access the
   <span class="guimenu">History Explorer</span>.
   The <span class="guimenu">History Explorer</span> allows you to create detailed
   cluster reports and view transition information. It provides the following
   options:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.12.6.3.1"><span class="term"><span class="guimenu">Generate</span>
    </span></dt><dd><p>
      Create a cluster report for a certain time. Hawk2 calls the
      <code class="command">crm report</code> command to generate the report.
     </p></dd><dt id="id-1.3.4.4.12.6.3.2"><span class="term"><span class="guimenu">Upload</span>
    </span></dt><dd><p>
      Allows you to upload <code class="literal">crm report</code> archives that have
      either been created with the crm shell directly or even on a different
      cluster.
     </p></dd></dl></div><p>
   After reports have been generated or uploaded, they are shown below
   <span class="guimenu">Reports</span>. From the list of reports, you can show a
   report's details, download or delete the report.
  </p><div class="figure" id="id-1.3.4.4.12.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-history-explorer-main.png"><img src="images/hawk2-history-explorer-main.png" width="90%" alt="Hawk2—History Explorer Main View" title="Hawk2—History Explorer Main View"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.19: </span><span class="title-name">Hawk2—History Explorer Main View </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.4.12.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div><div class="procedure" id="pro-hawk2-history-report" data-id-title="Generating or Uploading a Cluster Report"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.31: </span><span class="title-name">Generating or Uploading a Cluster Report </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-history-report">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">History</span>.
    </p><p>
     The <span class="guimenu">History Explorer</span> screen opens in the
     <span class="guimenu">Generate</span> view. By default, the suggested time frame for
     a report is the last hour.
    </p></li><li class="step"><p>
     To create a cluster report:
    </p><ol type="a" class="substeps"><li class="step"><p>
       To immediately start a report, click <span class="guimenu">Generate</span>.
      </p></li><li class="step"><p>
       To modify the time frame for the report, click anywhere on the suggested
       time frame and select another option from the drop-down box. You can
       also enter a <span class="guimenu">Custom</span> start date, end date and hour,
       respectively. To start the report, click <span class="guimenu">Generate</span>.
      </p><p>
       After the report has finished, it is shown below
       <span class="guimenu">Reports</span>.
      </p></li></ol></li><li class="step"><p>
     To upload a cluster report, the <code class="command">crm report</code> archive must
     be located on a file system that you can access with Hawk2. Proceed as
     follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Switch to the <span class="guimenu">Upload</span> tab.
      </p></li><li class="step"><p>
       <span class="guimenu">Browse</span> for the cluster report archive and click
       <span class="guimenu">Upload</span>.
      </p><p>
       After the report is uploaded, it is shown below
       <span class="guimenu">Reports</span>.
      </p></li></ol></li><li class="step"><p>
     To download or delete a report, click the respective icon next to the
     report in the <span class="guimenu">Operations</span> column.
    </p></li><li class="step"><p>
     To view
     <a class="xref" href="#il-hawk2-history-report-details" title="Report Details in History Explorer">Report Details in History Explorer</a>,
     click the report's name or select <span class="guimenu">Show</span> from the
     <span class="guimenu">Operations</span> column.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-history-report-details.png"><img src="images/hawk2-history-report-details.png" width="90%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Return to the list of reports by clicking the <span class="guimenu">Reports</span>
     button.
    </p></li></ol></div></div><div class="itemizedlist" id="il-hawk2-history-report-details" data-id-title="Report Details in History Explorer"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Report Details in History Explorer </span></span><a title="Permalink" class="permalink" href="#il-hawk2-history-report-details">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Name of the report.
    </p></li><li class="listitem"><p>
     Start time of the report.
    </p></li><li class="listitem"><p>
     End time of the report.
    </p></li><li class="listitem"><p>
     Number of transitions plus time line of all transitions in the cluster
     that are covered by the report. To learn how to view more details for a
     transition, see
     <a class="xref" href="#sec-conf-hawk2-history-transitions" title="6.10.3. Viewing Transition Details in the History Explorer">Section 6.10.3</a>.
    </p></li><li class="listitem"><p>
     Node events.
    </p></li><li class="listitem"><p>
     Resource events.
    </p></li></ul></div></section><section class="sect2" id="sec-conf-hawk2-history-transitions" data-id-title="Viewing Transition Details in the History Explorer"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.10.3 </span><span class="title-name">Viewing Transition Details in the History Explorer</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-transitions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For each transition, the cluster saves a copy of the state which it provides
   as input to the policy engine (PE). The path to this archive is logged. All
   <code class="filename">pe-input*</code> files are generated on the Designated
   Coordinator (DC). As the DC can change in a cluster, there may be
   <code class="filename">pe-input*</code> files from several nodes. Any
   <code class="filename">pe-input*</code> files show what the PE
   <span class="emphasis"><em>planned</em></span> to do.
  </p><p>
   In Hawk2, you can display the name of each <code class="filename">pe-input*</code>
   file plus the time and node on which it was created. In addition, the
   <span class="guimenu">History Explorer</span> can visualize the following details,
   based on the respective <code class="filename">pe-input*</code> file:
  </p><div class="variablelist" id="vl-hawk2-history-transition-details" data-id-title="Transition Details in the History Explorer"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Transition Details in the History Explorer </span></span><a title="Permalink" class="permalink" href="#vl-hawk2-history-transition-details">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.4.4.12.7.4.2"><span class="term"><span class="guimenu">Details</span>
    </span></dt><dd><p>
      Shows snippets of logging data that belongs to the transition. Displays
      the output of the following command (including the resource agents' log
      messages):
     </p><div class="verbatim-wrap"><pre class="screen">crm history transition <em class="replaceable">peinput</em></pre></div></dd><dt id="id-1.3.4.4.12.7.4.3"><span class="term"><span class="guimenu">Configuration</span>
    </span></dt><dd><p>
      Shows the cluster configuration at the time that the
      <code class="filename">pe-input*</code> file was created.
     </p></dd><dt id="id-1.3.4.4.12.7.4.4"><span class="term"><span class="guimenu">Diff</span>
    </span></dt><dd><p>
      Shows the differences of configuration and status between the selected
      <code class="filename">pe-input*</code> file and the following one.
     </p></dd><dt id="id-1.3.4.4.12.7.4.5"><span class="term"><span class="guimenu">Log</span>
    </span></dt><dd><p>
      Shows snippets of logging data that belongs to the transition. Displays
      the output of the following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm history transition log <em class="replaceable">peinput</em></pre></div><p>
      This includes details from the <code class="systemitem">pengine</code>,
      <code class="systemitem">crmd</code>, and <code class="systemitem">lrmd</code>.
     </p></dd><dt id="id-1.3.4.4.12.7.4.6"><span class="term"><span class="guimenu">Graph</span>
    </span></dt><dd><p>
      Shows a graphical representation of the transition. If you click
      <span class="guimenu">Graph</span>, the PE is re-invoked (using the
      <code class="filename">pe-input*</code> files), and generates a graphical
      visualization of the transition.
     </p></dd></dl></div><div class="procedure" id="pro-hawk2-history-transitions" data-id-title="Viewing Transition Details"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.32: </span><span class="title-name">Viewing Transition Details </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-history-transitions">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">History</span>.
    </p><p>
     If reports have already been generated or uploaded, they are shown in the
     list of <span class="guimenu">Reports</span>. Otherwise generate or upload a report
     as described in
     <a class="xref" href="#pro-hawk2-history-report" title="Generating or Uploading a Cluster Report">Procedure 6.31</a>.
    </p></li><li class="step"><p>
     Click the report's name or select <span class="guimenu">Show</span> from the
     <span class="guimenu">Operations</span> column to open the
     <a class="xref" href="#il-hawk2-history-report-details" title="Report Details in History Explorer">Report Details in History Explorer</a>.
    </p></li><li class="step"><p>
     To access the transition details, you need to select a transition point in
     the transition time line that is shown below. Use the
     <span class="guimenu">Previous</span> and <span class="guimenu">Next</span> icons and the
     <span class="guimenu">Zoom In</span> and <span class="guimenu">Zoom Out</span> icons to find
     the transition that you are interested in.
    </p></li><li class="step"><p>
     To display the name of a <code class="filename">pe-input*</code> file plus the time
     and node on which it was created, hover the mouse pointer over a
     transition point in the time line.
    </p></li><li class="step"><p>
     To view the <a class="xref" href="#vl-hawk2-history-transition-details" title="Transition Details in the History Explorer">Transition Details in the History Explorer</a>, click
     the transition point for which you want to know more.
    </p></li><li class="step"><p>
     To show <span class="guimenu">Details</span>, <span class="guimenu">Configuration</span>,
     <span class="guimenu">Diff</span>, <span class="guimenu">Logs</span> or
     <span class="guimenu">Graph</span>, click the respective buttons to show the content
     described in <a class="xref" href="#vl-hawk2-history-transition-details" title="Transition Details in the History Explorer">Transition Details in the History Explorer</a>.
    </p></li><li class="step"><p>
     To return to the list of reports, click the <span class="guimenu">Reports</span>
     button.
    </p></li></ol></div></div><p>
   
  </p></section></section><section class="sect1" id="sec-conf-hawk2-health" data-id-title="Verifying Cluster Health"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.11 </span><span class="title-name">Verifying Cluster Health</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-health">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_hawk2_health_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides a wizard which checks and detects issues with your cluster.
  After the analysis is complete, Hawk2 creates a cluster report with further
  details. To verify cluster health and generate the report, Hawk2 requires
  passwordless SSH access between the nodes. Otherwise it can only collect data
  from the current node. If you have set up your cluster with the bootstrap scripts,
  provided by the
  <code class="systemitem">ha-cluster-bootstrap</code>
  package, passwordless SSH access is already configured. In case you need to
  configure it manually, see <a class="xref" href="#sec-crmreport-nonroot-ssh" title="D.2. Configuring a Passwordless SSH Account">Section D.2, “Configuring a Passwordless SSH Account”</a>.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    From the left navigation bar, select <span class="guimenu">Wizards</span>.
   </p></li><li class="step"><p>
    Expand the <span class="guimenu">Basic</span> category.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Verify health and configuration</span> wizard.
   </p></li><li class="step"><p>
    Confirm with <span class="guimenu">Verify</span>.
   </p></li><li class="step"><p>
    Enter the root password for your cluster and click
    <span class="guimenu">Apply</span>. Hawk2 will generate the report.
   </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-manual-config" data-id-title="Configuring and Managing Cluster Resources (Command Line)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Configuring and Managing Cluster Resources (Command Line)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-manual-config">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    To configure and manage cluster resources, either use the crm shell
    (crmsh) command line utility or HA Web Console (Hawk2), a Web-based
    user interface.
   </p><p>
    This chapter introduces <code class="command">crm</code>, the command line tool
    and covers an overview of this tool, how to use templates, and mainly
    configuring and managing cluster resources: creating basic and advanced
    types of resources (groups and clones), configuring constraints,
    specifying failover nodes and failback nodes, configuring resource
    monitoring, starting, cleaning up or removing resources, and migrating
    resources manually.
   </p></div></div></div></div><div id="id-1.3.4.5.3" data-id-title="User Privileges" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: User Privileges</div><p>
   Sufficient privileges are necessary to manage a cluster. The
   <code class="command">crm</code> command and its subcommands need to be run either
   as <code class="systemitem">root</code> user or as the CRM owner user (typically the user
   <code class="systemitem">hacluster</code>).
  </p><p>
   However, the <code class="option">user</code> option allows you to run
   <code class="command">crm</code> and its subcommands as a regular (unprivileged)
   user and to change its ID using <code class="command">sudo</code> whenever
   necessary. For example, with the following command <code class="command">crm</code>
   will use <code class="systemitem">hacluster</code> as the
   privileged user ID:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> options user hacluster</pre></div><p>
   Note that you need to set up <code class="filename">/etc/sudoers</code> so that
   <code class="command">sudo</code> does not ask for a password.
  </p></div><section class="sect1" id="sec-ha-manual-config-crm" data-id-title="crmsh—Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">crmsh—Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">crm</code> command has several subcommands which manage
   resources, CIBs, nodes, resource agents, and others. It offers a thorough
   help system with embedded examples. All examples follow a naming
   convention described in
   <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B</a>.
  </p><div id="id-1.3.4.5.4.3" data-id-title="Interactive crm Prompt" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Interactive crm Prompt</div><p>By using crm without arguments (or with only one sublevel as
    argument), the crm shell enters the interactive mode. This mode is
    indicated by the following prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live/HOSTNAME)</code></pre></div><p>
    For readability reasons, we omit the host name in the interactive crm
    prompts in our documentation. We only include the host name if you need
    to run the interactive shell on a specific node, like alice for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live/alice)</code></pre></div></div><section class="sect2" id="sec-ha-manual-config-crm-help" data-id-title="Getting Help"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.1 </span><span class="title-name">Getting Help</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-help">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Help can be accessed in several ways:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      To output the usage of <code class="command">crm</code> and its command line
      options:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> --help</pre></div></li><li class="listitem"><p>
      To give a list of all available commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help</pre></div></li><li class="listitem"><p>
      To access other help sections, not just the command reference:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help topics</pre></div></li><li class="listitem"><p>
      To view the extensive help text of the <code class="command">configure</code>
      subcommand:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure help</pre></div></li><li class="listitem"><p>
      To print the syntax, its usage, and examples of the <code class="command">group</code>
      subcommand of <code class="command">configure</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure help group</pre></div><p>
      This is the same:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help configure group</pre></div></li></ul></div><p>
    Almost all output of the <code class="command">help</code> subcommand (do not mix
    it up with the <code class="option">--help</code> option) opens a text viewer. This
    text viewer allows you to scroll up or down and read the help text more
    comfortably. To leave the text viewer, press the <span class="keycap">Q</span> key.
   </p><div id="tip-crm-tabcompletion" data-id-title="Use Tab Completion in Bash and Interactive Shell" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Use Tab Completion in Bash and Interactive Shell</div><p>
     The crmsh supports full tab completion in Bash directly, not only
     for the interactive shell. For example, typing <code class="literal">crm help
     config</code><span class="keycap">→|</span> will complete the word
     like in the interactive shell.
    </p></div></section><section class="sect2" id="sec-ha-manual-config-crm-run" data-id-title="Executing crmshs Subcommands"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.2 </span><span class="title-name">Executing crmsh's Subcommands</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-run">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">crm</code> command itself can be used in the following
    ways:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Directly: </span>
       Concatenate all subcommands to <code class="command">crm</code>, press
       <span class="keycap">Enter</span> and you see the output immediately. For
       example, enter <code class="command">crm</code> <code class="option">help ra</code> to get
       information about the <code class="command">ra</code> subcommand (resource
       agents).
      </p><p>It is possible to abbreviate subcommands as long as they are
        unique. For example, you can shorten <code class="command">status</code> as
      <code class="command">st</code> and crmsh will know what you have meant.
      </p><p>Another feature is to shorten parameters. Usually, you add
        parameters through the <code class="command">params</code> keyword.
        You can leave out the params section if it is the first and only section.
        For example, this line:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> primitive ipaddr IPaddr2 params ip=192.168.0.55</pre></div><p>is equivalent to this line:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> primitive ipaddr IPaddr2 ip=192.168.0.55</pre></div></li><li class="listitem"><p><span class="formalpara-title">As crm Shell Script: </span>
       Crm shell scripts contain subcommands of <code class="command">crm</code>.
       For more information, see <a class="xref" href="#sec-ha-manual-config-crmshellscripts" title="7.1.4. Using crmsh's Shell Scripts">Section 7.1.4, “Using crmsh's Shell Scripts”</a>.
      </p></li><li class="listitem"><p><span class="formalpara-title">As crmsh Cluster Scripts: </span>These are a collection of metadata, references to RPM packages,
          configuration files, and crmsh subcommands bundled under a single,
          yet descriptive name. They are managed through the
          <code class="command">crm script</code> command.
        </p><p>Do not confuse them with crmsh shell scripts: although both share
        some common objectives, the crm shell scripts only contain subcommands
        whereas cluster scripts incorporate much more than a simple
        enumeration of commands. For more information, see <a class="xref" href="#sec-ha-manual-config-clusterscripts" title="7.1.5. Using crmsh's Cluster Scripts">Section 7.1.5, “Using crmsh's Cluster Scripts”</a>.
      </p></li><li class="listitem"><p><span class="formalpara-title">Interactive as Internal Shell: </span>
       Type <code class="command">crm</code> to enter the internal shell. The prompt
       changes to <code class="literal">crm(live)</code>. With
       <code class="command">help</code> you can get an overview of the available
       subcommands. As the internal shell has different levels of
       subcommands, you can <span class="quote">“<span class="quote">enter</span>”</span> one by typing this
       subcommand and press <span class="keycap">Enter</span>.
      </p><p>
      For example, if you type <code class="command">resource</code> you enter the
      resource management level. Your prompt changes to
      <code class="literal">crm(live)resource#</code>. If you want to leave the
      internal shell, use the commands <code class="command">quit</code>,
      <code class="command">bye</code>, or <code class="command">exit</code>. If you need to go
      one level back, use <code class="command">back</code>, <code class="command">up</code>,
      <code class="command">end</code>, or <code class="command">cd</code>.
     </p><p>
      You can enter the level directly by typing <code class="command">crm</code> and
      the respective subcommand(s) without any options and press
      <span class="keycap">Enter</span>.
     </p><p>
      The internal shell supports also tab completion for subcommands and
      resources. Type the beginning of a command, press
      <span class="keycap">→|</span> and <code class="command">crm</code> completes the
      respective object.
     </p></li></ul></div><p>
    In addition to previously explained methods, crmsh also supports
    synchronous command execution. Use the <code class="option">-w</code> option to
    activate it. If you have started <code class="command">crm</code> without
    <code class="option">-w</code>, you can enable it later with the user preference's
    <code class="command">wait</code> set to <code class="literal">yes</code> (<code class="command">options
    wait yes</code>). If this option is enabled, <code class="command">crm</code>
    waits until the transition is finished. Whenever a transaction is
    started, dots are printed to indicate progress. Synchronous command
    execution is only applicable for commands like <code class="command">resource
    start</code>.
   </p><div id="id-1.3.4.5.4.5.5" data-id-title="Differentiate Between Management and Configuration Subcommands" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Differentiate Between Management and Configuration Subcommands</div><p>
     The <code class="command">crm</code> tool has management capability (the
     subcommands <code class="command">resource</code> and <code class="command">node</code>)
     and can be used for configuration (<code class="command">cib</code>,
     <code class="command">configure</code>).
    </p></div><p>
    The following subsections give you an overview of some important aspects
    of the <code class="command">crm</code> tool.
   </p></section><section class="sect2" id="sec-ha-manual-config-ocf" data-id-title="Displaying Information about OCF Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.3 </span><span class="title-name">Displaying Information about OCF Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-ocf">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    As you need to deal with resource agents in your cluster configuration
    all the time, the <code class="command">crm</code> tool contains the
    <code class="command">ra</code> command. Use it to show information about resource
    agents and to manage them (for additional information, see also
    <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra
<code class="prompt custom">crm(live)ra# </code></pre></div><p>
    The command <code class="command">classes</code> lists all classes and providers:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">classes</code>
 lsb
 ocf / heartbeat linbit lvm2 ocfs2 pacemaker
 service
 stonith
 systemd</pre></div><p>
    To get an overview of all available resource agents for a class (and
    provider) use the <code class="command">list</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">list</code> ocf
AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          HealthCPU           HealthSMART         ICP
IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID
ManageVE            Pure-FTPd           Raid1               Route
SAPDatabase         SAPInstance         SendArp             ServeRAID
...</pre></div><p>
    An overview of a resource agent can be viewed with
    <code class="command">info</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">info</code> ocf:linbit:drbd
This resource agent manages a DRBD* resource
as a master/slave resource. DRBD is a shared-nothing replicated storage
device. (ocf:linbit:drbd)

Master/Slave OCF Resource Agent for DRBD

Parameters (* denotes required, [] the default):

drbd_resource* (string): drbd resource name
    The name of the drbd resource from the drbd.conf file.

drbdconf (string, [/etc/drbd.conf]): Path to drbd.conf
    Full path to the drbd.conf file.

Operations' defaults (advisory minimum):

    start         timeout=240
    promote       timeout=90
    demote        timeout=90
    notify        timeout=90
    stop          timeout=100
    monitor_Slave_0 interval=20 timeout=20 start-delay=1m
    monitor_Master_0 interval=10 timeout=20 start-delay=1m</pre></div><p>
    Leave the viewer by pressing <span class="keycap">Q</span>.
   </p><div id="id-1.3.4.5.4.6.11" data-id-title="Use crm Directly" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Use <code class="command">crm</code> Directly</div><p>
     In the former example we used the internal shell of the
     <code class="command">crm</code> command. However, you do not necessarily need to
     use it. You get the same results if you add the respective subcommands
     to <code class="command">crm</code>. For example, you can list all the OCF
     resource agents by entering <code class="command">crm</code> <code class="option">ra list
     ocf</code> in your shell.
    </p></div></section><section class="sect2" id="sec-ha-manual-config-crmshellscripts" data-id-title="Using crmshs Shell Scripts"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.4 </span><span class="title-name">Using crmsh's Shell Scripts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crmshellscripts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The crmsh shell scripts provide a convenient way to enumerate crmsh
    subcommands into a file. This makes it easy to comment specific lines or
    to replay them later. Keep in mind that a crmsh shell script can contain
    <span class="emphasis"><em>only crmsh subcommands</em></span>. Any other commands are not
    allowed.
   </p><p>
    Before you can use a crmsh shell script, create a file with specific
    commands. For example, the following file prints the status of the cluster
    and gives a list of all nodes:
   </p><div class="example" id="ex-ha-manual-config-crmshellscripts" data-id-title="A Simple crmsh Shell Script"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.1: </span><span class="title-name">A Simple crmsh Shell Script </span></span><a title="Permalink" class="permalink" href="#ex-ha-manual-config-crmshellscripts">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># A small example file with some crm subcommands
<code class="command">status</code>
<code class="command">node</code> list</pre></div></div></div><p>
    Any line starting with the hash symbol (<code class="literal">#</code>) is a
    comment and is ignored. If a line is too long, insert a backslash
    (<code class="literal">\</code>) at the end and continue in the next line. It is
    recommended to indent lines that belong to a certain subcommand to improve
    readability.
   </p><p>To use this script, use one of the following methods:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> -f example.cli
<code class="prompt root"># </code><code class="command">crm</code> &lt; example.cli</pre></div></section><section class="sect2" id="sec-ha-manual-config-clusterscripts" data-id-title="Using crmshs Cluster Scripts"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.5 </span><span class="title-name">Using crmsh's Cluster Scripts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>Collecting information from all cluster nodes and deploying any
      changes is a key cluster administration task. Instead of performing
      the same procedures manually on different nodes (which is error-prone),
      you can use the crmsh cluster scripts.
   </p><p>
    Do not confuse them with the <span class="emphasis"><em>crmsh shell scripts</em></span>,
    which are explained in <a class="xref" href="#sec-ha-manual-config-crmshellscripts" title="7.1.4. Using crmsh's Shell Scripts">Section 7.1.4, “Using crmsh's Shell Scripts”</a>.
    </p><p>In contrast to crmsh shell scripts, cluster scripts performs
    additional tasks like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Installing software that is required for a specific task.</p></li><li class="listitem"><p>Creating or modifying any configuration files.</p></li><li class="listitem"><p>Collecting information and reporting potential problems with the
          cluster.</p></li><li class="listitem"><p>Deploying the changes to all nodes.</p></li></ul></div><p>crmsh cluster scripts do not replace other tools for managing
      clusters—they provide an integrated way to perform the above
      tasks across the cluster. Find detailed information at <a class="link" href="http://crmsh.github.io/scripts/" target="_blank">http://crmsh.github.io/scripts/</a>.
    </p><section class="sect3" id="sec-ha-manual-config-clusterscripts-usage" data-id-title="Usage"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.1.5.1 </span><span class="title-name">Usage</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts-usage">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>To get a list of all available cluster scripts, run:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script list</pre></div><p>To view the components of a script, use the
        <code class="command">show</code> command and the name of the cluster script,
        for example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script show mailto
mailto (Basic)
MailTo

 This is a resource agent for MailTo. It sends email to a sysadmin
whenever  a takeover occurs.

1. Notifies recipients by email in the event of resource takeover

  id (required)  (unique)
      Identifier for the cluster resource
  email (required)
      Email address
  subject
      Subject</pre></div><p>The output of <code class="command">show</code> contains a title, a
          short description, and a procedure. Each procedure is divided
          into a series of steps, performed in the given order. </p><p>Each step contains a list of required and optional parameters,
        along with a short description and its default value.</p><p>Each cluster script understands a set of common parameters.
        These parameters can be passed to any script:</p><div class="table" id="id-1.3.4.5.4.8.8.9" data-id-title="Common Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 7.1: </span><span class="title-name">Common Parameters </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.8.8.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Parameter</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Argument</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">action</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">INDEX</em></td><td style="border-bottom: 1px solid ; ">If set, only execute a single action (index, as
                returned by verify)</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">dry_run</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">BOOL</em></td><td style="border-bottom: 1px solid ; ">If set, simulate execution only (default: no) </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">nodes</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">LIST</em></td><td style="border-bottom: 1px solid ; ">List of nodes to execute the script for</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">port</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">NUMBER</em></td><td style="border-bottom: 1px solid ; ">Port to connect to</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">statefile</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">FILE</em></td><td style="border-bottom: 1px solid ; ">When single-stepping, the state is saved in the given
                file </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">sudo</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">BOOL</em></td><td style="border-bottom: 1px solid ; ">If set, crm will prompt for a sudo password and use sudo
                where appropriate (default: no) </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">timeout</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">NUMBER</em></td><td style="border-bottom: 1px solid ; ">Execution timeout in seconds (default: 600) </td></tr><tr><td style="border-right: 1px solid ; "><em class="parameter">user</em></td><td style="border-right: 1px solid ; "><em class="replaceable">USER</em></td><td>Run script as the given user </td></tr></tbody></table></div></div></section><section class="sect3" id="sec-ha-manual-config-clusterscripts-verify-run" data-id-title="Verifying and Running a Cluster Script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.1.5.2 </span><span class="title-name">Verifying and Running a Cluster Script</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts-verify-run">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>Before running a cluster script, review the actions that it will
          perform and verify its parameters to avoid problems. A cluster script
          can potentially perform a series of actions and may fail for
          various reasons. Thus, verifying your parameters before
          running it helps to avoid problems.</p><p>For example, the <code class="systemitem">mailto</code> resource agent
        requires a unique identifier and an e-mail address. To verify these
        parameters, run:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script verify mailto id=sysadmin email=tux@example.org
1. Ensure mail package is installed

        mailx

2. Configure cluster resources

        primitive sysadmin MailTo
                email="tux@example.org"
                op start timeout="10"
                op stop timeout="10"
                op monitor interval="10" timeout="10"

        clone c-sysadmin sysadmin</pre></div><p>The <code class="command">verify</code> prints the steps and replaces
        any placeholders with your given parameters. If <code class="command">verify</code>
        finds any problems, it will report it.
        If everything is ok, replace the <code class="command">verify</code>
        command with <code class="command">run</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script run mailto id=sysadmin email=tux@example.org
INFO: MailTo
INFO: Nodes: alice, bob
OK: Ensure mail package is installed
OK: Configure cluster resources</pre></div><p>Check whether your resource is integrated into your cluster
          with <code class="command">crm status</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> status
[...]
 Clone Set: c-sysadmin [sysadmin]
     Started: [ alice bob ]</pre></div></section></section><section class="sect2" id="sec-ha-manual-config-template" data-id-title="Using Configuration Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.6 </span><span class="title-name">Using Configuration Templates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.4.5.4.9.3" data-id-title="Deprecation Notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Deprecation Notice</div><p>The use of configuration templates is deprecated and will
        be removed in the future. Configuration templates will be replaced
        by cluster scripts, see <a class="xref" href="#sec-ha-manual-config-clusterscripts" title="7.1.5. Using crmsh's Cluster Scripts">Section 7.1.5, “Using crmsh's Cluster Scripts”</a>.
      </p></div><p>
    Configuration templates are ready-made cluster configurations for
    crmsh. Do not confuse them with the <span class="emphasis"><em>resource
    templates</em></span> (as described in
    <a class="xref" href="#sec-ha-manual-config-rsc-template" title="7.4.3. Creating Resource Templates">Section 7.4.3, “Creating Resource Templates”</a>). Those are
    templates for the <span class="emphasis"><em>cluster</em></span> and not for the crm
    shell.
   </p><p>
    Configuration templates require minimum effort to be tailored to the
    particular user's needs. Whenever a template creates a configuration,
    warning messages give hints which can be edited later for further
    customization.
   </p><p>
    The following procedure shows how to create a simple yet functional
    Apache configuration:
   </p><div class="procedure" id="pro-ha-manual-config-template"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Create a new configuration from a configuration template:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Switch to the <code class="command">template</code> subcommand:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">template</code></pre></div></li><li class="step"><p>
        List the available configuration templates:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">list</code> templates
gfs2-base   filesystem  virtual-ip  apache   clvm     ocfs2    gfs2</pre></div></li><li class="step"><p>
        Decide which configuration template you need. As we need an Apache
        configuration, we select the <code class="literal">apache</code> template and
        name it <code class="literal">g-intranet</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">new</code> g-intranet apache
INFO: pulling in template apache
INFO: pulling in template virtual-ip</pre></div></li></ol></li><li class="step"><p>
      Define your parameters:
     </p><ol type="a" class="substeps"><li class="step"><p>
        List the configuration you have created:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">list</code>
g-intranet</pre></div></li><li class="step" id="st-config-cli-show"><p>
        Display the minimum required changes that need to be filled out by
        you:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">show</code>
ERROR: 23: required parameter ip not set
ERROR: 61: required parameter id not set
ERROR: 65: required parameter configfile not set</pre></div></li><li class="step" id="st-config-cli-edit"><p>
        Invoke your preferred text editor and fill out all lines that have
        been displayed as errors in <a class="xref" href="#st-config-cli-show" title="Step 3.b">Step 3.b</a>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">edit</code></pre></div></li></ol></li><li class="step"><p>
      Show the configuration and check whether it is valid (bold text
      depends on the configuration you have entered in
      <a class="xref" href="#st-config-cli-edit" title="Step 3.c">Step 3.c</a>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">show</code>
primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip=<span class="strong"><strong>"192.168.1.101"</strong></span>
primitive apache apache \
    params configfile=<span class="strong"><strong>"/etc/apache2/httpd.conf"</strong></span>
    monitor apache 120s:60s
group <span class="strong"><strong>g-intranet</strong></span> \
    apache virtual-ip</pre></div></li><li class="step"><p>
      Apply the configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">apply</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">cd ..</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
      Submit your changes to the CIB:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    It is possible to simplify the commands even more, if you know the
    details. The above procedure can be summarized with the following
    command on the shell:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure template \
   new g-intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</pre></div><p>
    If you are inside your internal <code class="command">crm</code> shell, use the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">new</code> intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</pre></div><p>
    However, the previous command only creates its configuration from the
    configuration template. It does not apply nor commit it to the CIB.
   </p></section><section class="sect2" id="sec-ha-manual-config-shadowconfig" data-id-title="Testing with Shadow Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.7 </span><span class="title-name">Testing with Shadow Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-shadowconfig">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A shadow configuration is used to test different configuration
    scenarios. If you have created several shadow configurations, you can
    test them one by one to see the effects of your changes.
   </p><p>
    The usual process looks like this:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Create a new shadow configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">cib</code> new myNewConfig
INFO: myNewConfig shadow CIB created</pre></div><p>
      If you omit the name of the shadow CIB, a temporary name
      <code class="literal">@tmp@</code> is created.
     </p></li><li class="step"><p>
      If you want to copy the current live configuration into your shadow
      configuration, use the following command, otherwise skip this step:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)# <code class="command">cib</code> reset myNewConfig</pre></div><p>
      The previous command makes it easier to modify any existing resources
      later.
     </p></li><li class="step"><p>
      Make your changes as usual. After you have created the shadow
      configuration, all changes go there. To save all your changes, use the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)# <code class="command">commit</code></pre></div></li><li class="step"><p>
      If you need the live cluster configuration again, switch back with the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)configure# <code class="command">cib</code> use live
<code class="prompt custom">crm(live)# </code></pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-debugging" data-id-title="Debugging Your Configuration Changes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.8 </span><span class="title-name">Debugging Your Configuration Changes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-debugging">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before loading your configuration changes back into the cluster, it is
    recommended to review your changes with <code class="command">ptest</code>. The
    <code class="command">ptest</code> command can show a diagram of actions that will
    be induced by committing the changes. You need the
    <code class="systemitem">graphviz</code> package to display the diagrams. The
    following example is a transcript, adding a monitor operation:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code> fence-bob
primitive fence-bob stonith:apcsmart \
        params hostlist="bob"
<code class="prompt custom">crm(live)configure# </code><code class="command">monitor</code> fence-bob 120m:60s
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code> changed
primitive fence-bob stonith:apcsmart \
        params hostlist="bob" \
        op monitor interval="120m" timeout="60s"
<code class="prompt custom">crm(live)configure# </code><span class="strong"><strong>ptest</strong></span>
<code class="prompt custom">crm(live)configure# </code>commit</pre></div></section><section class="sect2" id="sec-ha-manual-config-diagram" data-id-title="Cluster Diagram"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.1.9 </span><span class="title-name">Cluster Diagram</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-diagram">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To output a cluster diagram, use the command
    <code class="command">crm</code> <code class="command">configure graph</code>. It displays
    the current configuration on its current window, therefore requiring
    X11.
   </p><p>
    If you prefer Scalable Vector Graphics (SVG), use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure graph dot config.svg svg</pre></div></section></section><section class="sect1" id="sec-ha-manual-config-crm-corosync" data-id-title="Managing Corosync Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Managing Corosync Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-corosync">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Corosync is the underlying messaging layer for most HA clusters. The
   <code class="command">corosync</code> subcommand provides commands for editing and
   managing the Corosync configuration.
  </p><p>
   For example, to list the status of the cluster, use
   <code class="command">status</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> corosync status
Printing ring status.
Local node ID 175704363
RING ID 0
        id      = 10.121.9.43
        status  = ring 0 active with no faults
Quorum information
------------------
Date:             Thu May  8 16:41:56 2014
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          175704363
Ring ID:          4032
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           2
Flags:            Quorate

Membership information
----------------------
    Nodeid      Votes Name
 175704363          1 alice.example.com (local)
 175704619          1 bob.example.com</pre></div><p>
   The <code class="command">diff</code> command is very helpful: It compares the
   Corosync configuration on all nodes (if not stated otherwise) and
   prints the difference between:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> corosync diff
--- bob
+++ alice
@@ -46,2 +46,2 @@
-       expected_votes: 2
-       two_node: 1
+       expected_votes: 1
+       two_node: 0</pre></div><p>
   For more details, see
   <a class="link" href="http://crmsh.nongnu.org/crm.8.html#cmdhelp_corosync" target="_blank">http://crmsh.nongnu.org/crm.8.html#cmdhelp_corosync</a>.
  </p></section><section class="sect1" id="sec-ha-config-crm-global" data-id-title="Configuring Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Configuring Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-global">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Global cluster options control how the cluster behaves when confronted
   with certain situations. The predefined values can usually be kept.
   However, to make key functions of your cluster work correctly, you need
   to adjust the following parameters after basic cluster setup:
  </p><div class="procedure" id="id-1.3.4.5.6.5" data-id-title="Modifying Global Cluster Options With crm"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.1: </span><span class="title-name">Modifying Global Cluster Options With <code class="command">crm</code> </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code> tool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
     Use the following commands to set the options for two-node clusters
     only:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> no-quorum-policy=stop
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-enabled=true</pre></div><div id="id-1.3.4.5.6.5.3.3" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><p>
      A cluster without STONITH is not supported.
     </p></div></li><li class="step"><p>
     Show your changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">show</code>
property $id="cib-bootstrap-options" \
   dc-version="1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51" \
   cluster-infrastructure="corosync" \
   expected-quorum-votes="2" \
   no-quorum-policy="stop" \
   stonith-enabled="true"</pre></div></li><li class="step"><p>
     Commit your changes and exit:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">exit</code></pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-config-crm-resources" data-id-title="Configuring Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">Configuring Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-resources">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </p><p>
   For an overview of resource types you can create, refer to
   <a class="xref" href="#sec-ha-config-basics-resources-types" title="5.3.3. Types of Resources">Section 5.3.3, “Types of Resources”</a>.
  </p><section class="sect2" id="sec-ha-manual-config-load" data-id-title="Loading Cluster Resources from a File"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.1 </span><span class="title-name">Loading Cluster Resources from a File</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-load">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Parts or all of the configuration can be loaded from a local file or a
    network URL. Three different methods can be defined:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.7.4.3.1"><span class="term"><code class="option">replace</code></span></dt><dd><p>
       This option replaces the current configuration with the new source
       configuration.
      </p></dd><dt id="id-1.3.4.5.7.4.3.2"><span class="term"><code class="option">update</code></span></dt><dd><p>
       This option tries to import the source configuration. It adds new items
       or updates existing items to the current configuration.
       
      </p></dd><dt id="id-1.3.4.5.7.4.3.3"><span class="term"><code class="option">push</code></span></dt><dd><p>
       This option imports the content from the source into the current
       configuration (same as <code class="option">update</code>). However, it removes
       objects that are not available in the new configuration.
      </p></dd></dl></div><p>
    To load the new configuration from the file <code class="filename">mycluster-config.txt</code>
    use the following syntax:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure load push mycluster-config.txt</pre></div></section><section class="sect2" id="sec-ha-manual-config-create" data-id-title="Creating Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.2 </span><span class="title-name">Creating Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-create">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three types of RAs (Resource Agents) available with the
    cluster (for background information, see
    <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>). To add a new resource
    to the cluster, proceed as follows:
   </p><div class="procedure" id="pro-ha-manual-config-create"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code> tool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Configure a primitive IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> myIP IPaddr \
     params ip=127.0.0.99 op monitor interval=60s</pre></div><p>
      The previous command configures a <span class="quote">“<span class="quote">primitive</span>”</span> with the
      name <code class="literal">myIP</code>. You need to choose a class (here
      <code class="literal">ocf</code>), provider (<code class="literal">heartbeat</code>), and
      type (<code class="literal">IPaddr</code>). Furthermore, this primitive expects
      other parameters like the IP address. Change the address to your
      setup.
     </p></li><li class="step"><p>
      Display and review the changes you have made:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
      Commit your changes to take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-rsc-template" data-id-title="Creating Resource Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.3 </span><span class="title-name">Creating Resource Templates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-rsc-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to create several resources with similar configurations, a
    resource template simplifies the task. See also
    <a class="xref" href="#sec-ha-config-basics-constraints-templates" title="5.5.3. Resource Templates and Constraints">Section 5.5.3, “Resource Templates and Constraints”</a> for some
    basic background information. Do not confuse them with the
    <span class="quote">“<span class="quote">normal</span>”</span> templates from
    <a class="xref" href="#sec-ha-manual-config-template" title="7.1.6. Using Configuration Templates">Section 7.1.6, “Using Configuration Templates”</a>. Use the
    <code class="command">rsc_template</code> command to get familiar with the syntax:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure rsc_template
usage: rsc_template &lt;name&gt; [&lt;class&gt;:[&lt;provider&gt;:]]&lt;type&gt;
        [params &lt;param&gt;=&lt;value&gt; [&lt;param&gt;=&lt;value&gt;...]]
        [meta &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
        [utilization &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
        [operations id_spec
            [op op_type [&lt;attribute&gt;=&lt;value&gt;...] ...]]</pre></div><p>
    For example, the following command creates a new resource template with
    the name <code class="literal">BigVM</code> derived from the
    <code class="literal">ocf:heartbeat:Xen</code> resource and some default values
    and operations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">rsc_template</code> BigVM ocf:heartbeat:Xen \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</pre></div><p>
    Once you defined the new resource template, you can use it in primitives
    or reference it in order, colocation, or rsc_ticket constraints. To
    reference the resource template, use the <code class="literal">@</code> sign:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> MyVM1 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1"</pre></div><p>
    The new primitive MyVM1 is going to inherit everything from the BigVM
    resource templates. For example, the equivalent of the above two would
    be:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> MyVM1 Xen \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1" \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</pre></div><p>
    If you want to overwrite some options or operations, add them to your
    (primitive) definition. For example, the following new primitive MyVM2
    doubles the timeout for monitor operations but leaves others untouched:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> MyVM2 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM2" name="MyVM2" \
   op monitor timeout=120s interval=30s</pre></div><p>
    A resource template may be referenced in constraints to stand for all
    primitives which are derived from that template. This helps to produce a
    more concise and clear cluster configuration. Resource template
    references are allowed in all constraints except location constraints.
    Colocation constraints may not contain more than one template reference.
   </p></section><section class="sect2" id="sec-ha-manual-create-stonith" data-id-title="Creating a STONITH Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.4 </span><span class="title-name">Creating a STONITH Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-create-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    From the <code class="command">crm</code> perspective, a STONITH device is
    just another resource. To create a STONITH resource, proceed as
    follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Get a list of all STONITH types with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">ra</code> list stonith
apcmaster                  apcmastersnmp              apcsmart
baytech                    bladehpi                   cyclades
drac3                      external/drac5             external/dracmc-telnet
external/hetzner           external/hmchttp           external/ibmrsa
external/ibmrsa-telnet     external/ipmi              external/ippower9258
external/kdumpcheck        external/libvirt           external/nut
external/rackpdu           external/riloe             external/sbd
external/vcenter           external/vmware            external/xen0
external/xen0-ha           fence_legacy               ibmhmc
ipmilan                    meatware                   nw_rpc100s
rcd_serial                 rps10                      suicide
wti_mpc                    wti_nps</pre></div></li><li class="step" id="st-ha-manual-create-stonith-type"><p>
      Choose a STONITH type from the above list and view the list of
      possible options. Use the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">ra</code> info stonith:external/ipmi
IPMI STONITH external device (stonith:external/ipmi)

ipmitool based power management. Apparently, the power off
method of ipmitool is intercepted by ACPI which then makes
a regular shutdown. If case of a split brain on a two-node
it may happen that no node survives. For two-node clusters
use only the reset method.

Parameters (* denotes required, [] the default):

hostname (string): Hostname
    The name of the host to be managed by this STONITH device.
...</pre></div></li><li class="step"><p>
      Create the STONITH resource with the <code class="literal">stonith</code>
      class, the type you have chosen in
      <a class="xref" href="#st-ha-manual-create-stonith-type" title="Step 3">Step 3</a>,
      and the respective parameters if needed, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> my-stonith stonith:external/ipmi \
    params hostname="alice" \
    ipaddr="192.168.1.221" \
    userid="admin" passwd="secret" \
    op monitor interval=60m timeout=120s</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-constraints" data-id-title="Configuring Resource Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.5 </span><span class="title-name">Configuring Resource Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Having all the resources configured is only one part of the job. Even if
    the cluster knows all needed resources, it might still not be able to
    handle them correctly. For example, try not to mount the file system on
    the slave node of DRBD (in fact, this would fail with DRBD). Define
    constraints to make these kind of information available to the cluster.
   </p><p>
    For more information about constraints, see
    <a class="xref" href="#sec-ha-config-basics-constraints" title="5.5. Resource Constraints">Section 5.5, “Resource Constraints”</a>.
   </p><section class="sect3" id="sec-ha-manual-config-constraints-locational" data-id-title="Locational Constraints"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.5.1 </span><span class="title-name">Locational Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-locational">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The <code class="command">location</code> command defines on which nodes a
     resource may be run, may not be run or is preferred to be run.
    </p><p>
     This type of constraint may be added multiple times for each resource.
     All <code class="literal">location</code> constraints are evaluated for a given
     resource. A simple example that expresses a preference to run the
     resource <code class="literal">fs1</code> on the node with the name
     <code class="systemitem">alice</code> to 100 would be the
     following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location</code> loc-fs1 fs1 100: alice</pre></div><p>
     Another example is a location with ping:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ping ping \
    params name=ping dampen=5s multiplier=100 host_list="r1 r2"
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ping ping meta interleave=true
<code class="prompt custom">crm(live)configure# </code><code class="command">location</code> loc-node_pref internal_www \
    rule 50: #uname eq alice \
    rule ping: defined ping</pre></div><p>
     The parameter <em class="parameter">host_list</em> is a space-separated list
     of hosts to ping and count.
     Another use case for location constraints are grouping primitives as a
     <span class="emphasis"><em>resource set</em></span>. This can be useful if several
     resources depend on, for example, a ping attribute for network
     connectivity. In former times, the <code class="literal">-inf/ping</code> rules
     needed to be duplicated several times in the configuration, making it
     unnecessarily complex.
    </p><p>
     The following example creates a resource set
     <code class="varname">loc-alice</code>, referencing the virtual IP addresses
     <code class="varname">vip1</code> and <code class="varname">vip2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip1 IPaddr2 params ip=192.168.1.5
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip2 IPaddr2 params ip=192.168.1.6
<code class="prompt custom">crm(live)configure# </code><code class="command">location</code> loc-alice { vip1 vip2 } inf: alice</pre></div><p>
     In some cases it is much more efficient and convenient to use resource
     patterns for your <code class="command">location</code> command. A resource
     pattern is a regular expression between two slashes. For example, the
     above virtual IP addresses can be all matched with the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location</code>  loc-alice /vip.*/ inf: alice</pre></div></section><section class="sect3" id="sec-ha-manual-config-constraints-collocational" data-id-title="Colocational Constraints"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.5.2 </span><span class="title-name">Colocational Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-collocational">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The <code class="command">colocation</code> command is used to define what
     resources should run on the same or on different hosts.
    </p><p>
     It is only possible to set a score of either +inf or -inf, defining
     resources that must always or must never run on the same node. It is
     also possible to use non-infinite scores. In that case the colocation
     is called <span class="emphasis"><em>advisory</em></span> and the cluster may decide not
     to follow them in favor of not stopping other resources if there is a
     conflict.
    </p><p>
     For example, to run the resources with the IDs
     <code class="literal">filesystem_resource</code> and <code class="literal">nfs_group</code>
     always on the same host, use the following constraint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> nfs_on_filesystem inf: nfs_group filesystem_resource</pre></div><p>
     For a master slave configuration, it is necessary to know if the
     current node is a master in addition to running the resource locally.
    </p></section><section class="sect3" id="sec-ha-manual-config-constraints-weak-bond" data-id-title="Collocating Sets for Resources Without Dependency"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.5.3 </span><span class="title-name">Collocating Sets for Resources Without Dependency</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-weak-bond">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Sometimes it is useful to be able to place a group of resources on the
     same node (defining a colocation constraint), but without having hard
     dependencies between the resources.
    </p><p>
     Use the command <code class="command">weak-bond</code> if you want to place
     resources on the same node, but without any action if one of them
     fails.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure assist weak-bond RES1 RES2</pre></div><p>
     The implementation of <code class="command">weak-bond</code> creates a dummy
     resource and a colocation constraint with the given resources
     automatically.
    </p></section><section class="sect3" id="sec-ha-manual-config-constraints-ordering" data-id-title="Ordering Constraints"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.5.4 </span><span class="title-name">Ordering Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-ordering">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The <code class="command">order</code> command defines a sequence of action.
    </p><p>
     Sometimes it is necessary to provide an order of resource actions or
     operations. For example, you cannot mount a file system before the
     device is available to a system. Ordering constraints can be used to
     start or stop a service right before or after a different resource
     meets a special condition, such as being started, stopped, or promoted
     to master.
    </p><p>
     Use the following command in the <code class="command">crm</code> shell to
     configure an ordering constraint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> nfs_after_filesystem mandatory: filesystem_resource nfs_group</pre></div></section><section class="sect3" id="sec-ha-manual-config-constraints-example" data-id-title="Constraints for the Example Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.5.5 </span><span class="title-name">Constraints for the Example Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-example">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The example used for this section would not work without additional
     constraints. It is essential that all resources run on the same machine
     as the master of the DRBD resource. The DRBD resource must be master
     before any other resource starts. Trying to mount the DRBD device when
     it is not the master simply fails. The following constraints must be
     fulfilled:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The file system must always be on the same node as the master of the
       DRBD resource.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> filesystem_on_master inf: \
    filesystem_resource drbd_resource:Master</pre></div></li><li class="listitem"><p>
       The NFS server and the IP address must be on the same node as the
       file system.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> nfs_with_fs inf: \
   nfs_group filesystem_resource</pre></div></li><li class="listitem"><p>
       The NFS server and the IP address start after the file system is
       mounted:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> nfs_second mandatory: \
   filesystem_resource:start nfs_group</pre></div></li><li class="listitem"><p>
       The file system must be mounted on a node after the DRBD resource is
       promoted to master on this node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> drbd_first inf: \
    drbd_resource:promote filesystem_resource:start</pre></div></li></ul></div></section></section><section class="sect2" id="sec-ha-manual-config-failover" data-id-title="Specifying Resource Failover Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.6 </span><span class="title-name">Specifying Resource Failover Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To determine a resource failover, use the meta attribute
    migration-threshold. In case failcount exceeds migration-threshold on
    all nodes, the resource will remain stopped. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location</code> rsc1-alice rsc1 100: alice</pre></div><p>
    Normally, rsc1 prefers to run on alice. If it fails there,
    migration-threshold is checked and compared to the failcount. If
    failcount &gt;= migration-threshold then it is migrated to the node with
    the next best preference.
   </p><p>
    Start failures set the failcount to inf depend on the
    <code class="option">start-failure-is-fatal</code> option. Stop failures cause
    fencing. If there is no STONITH defined, the resource will not migrate.
   </p><p>
    For an overview, refer to
    <a class="xref" href="#sec-ha-config-basics-failover" title="5.5.4. Failover Nodes">Section 5.5.4, “Failover Nodes”</a>.
   </p></section><section class="sect2" id="sec-ha-manual-config-failback" data-id-title="Specifying Resource Failback Nodes (Resource Stickiness)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.7 </span><span class="title-name">Specifying Resource Failback Nodes (Resource Stickiness)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-failback">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource might fail back to its original node when that node is back
    online and in the cluster. To prevent a resource from
    failing back to the node that it was running on, or
    to specify a different node for the resource to fail back to,
    change its resource stickiness value. You can
    either specify resource stickiness when you are creating a resource or
    afterward.
   </p><p>
    For an overview, refer to
    <a class="xref" href="#sec-ha-config-basics-failback" title="5.5.5. Failback Nodes">Section 5.5.5, “Failback Nodes”</a>.
   </p></section><section class="sect2" id="sec-ha-manual-config-utilization" data-id-title="Configuring Placement of Resources Based on Load Impact"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.8 </span><span class="title-name">Configuring Placement of Resources Based on Load Impact</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Some resources may have specific capacity requirements such as minimum
    amount of memory. Otherwise, they may fail to start completely or run
    with degraded performance.
   </p><p>
    To take this into account, SUSE Linux Enterprise High Availability allows you to specify the
    following parameters:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      The capacity a certain node <span class="emphasis"><em>provides</em></span>.
     </p></li><li class="listitem"><p>
      The capacity a certain resource <span class="emphasis"><em>requires</em></span>.
     </p></li><li class="listitem"><p>
      An overall strategy for placement of resources.
     </p></li></ol></div><p>
    For detailed background information about the parameters and a
    configuration example, refer to
    <a class="xref" href="#sec-ha-config-basics-utilization" title="5.5.6. Placing Resources Based on Their Load Impact">Section 5.5.6, “Placing Resources Based on Their Load Impact”</a>.
   </p><p>
    To configure the resource's requirements and the capacity a node
    provides, use utilization attributes.

    
    You can name the utilization attributes according to your preferences
    and define as many name/value pairs as your configuration needs. In
    certain cases, some agents update the utilization themselves, for
    example the <code class="systemitem">VirtualDomain</code>.
   </p><p>
    In the following example, we assume that you already have a basic
    configuration of cluster nodes and resources. You now additionally want
    to configure the capacities a certain node provides and the capacity a
    certain resource requires.

   </p><div class="procedure" id="id-1.3.4.5.7.11.9" data-id-title="Adding Or Modifying Utilization Attributes With crm"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.2: </span><span class="title-name">Adding Or Modifying Utilization Attributes With <code class="command">crm</code> </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.5.7.11.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      To specify the capacity a node <span class="emphasis"><em>provides</em></span>, use the
      following command and replace the placeholder
      <em class="replaceable">NODE_1</em> with the name of your node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">node</code> <em class="replaceable">NODE_1</em> utilization hv_memory=16384 cpu=8</pre></div><p>
      With these values, <em class="replaceable">NODE_1</em> would be assumed
      to provide 16GB of memory and 8 CPU cores to resources.
     </p></li><li class="step"><p>
      To specify the capacity a resource <span class="emphasis"><em>requires</em></span>, use:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> xen1 Xen ... \
     utilization hv_memory=4096 cpu=4</pre></div><p>
      This would make the resource consume 4096 of those memory units from
      <em class="replaceable">NODE_1</em>, and 4 of the CPU units.
     </p></li><li class="step"><p>
      Configure the placement strategy with the <code class="command">property</code>
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> ...</pre></div><p>
      The following values are available:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.7.11.9.5.4.1"><span class="term"><code class="literal">default</code> (default value)</span></dt><dd><p>
       Utilization values are not considered. Resources are allocated
       according to location scoring. If scores are equal, resources are
       evenly distributed across nodes.
      </p></dd><dt id="id-1.3.4.5.7.11.9.5.4.2"><span class="term"><code class="literal">utilization</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. However,
       load-balancing is still done based on the number of resources
       allocated to a node.
      </p></dd><dt id="id-1.3.4.5.7.11.9.5.4.3"><span class="term"><code class="literal">minimal</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to concentrate the resources on as few nodes as possible
       (to achieve power savings on the remaining nodes).
      </p></dd><dt id="id-1.3.4.5.7.11.9.5.4.4"><span class="term"><code class="literal">balanced</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to distribute the resources evenly, thus optimizing resource
       performance.
      </p></dd></dl></div><div id="id-1.3.4.5.7.11.9.5.5" data-id-title="Configuring Resource Priorities" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Configuring Resource Priorities</div><p>
     The available placement strategies are best-effort—they do not
     yet use complex heuristic solvers to always reach optimum allocation
     results. Ensure that resource priorities are properly set so that
     your most important resources are scheduled first.
    </p></div></li><li class="step"><p>
      Commit your changes before leaving crmsh:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    The following example demonstrates a three node cluster of equal nodes,
    with 4 virtual machines:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">node</code> alice utilization hv_memory="4000"
<code class="prompt custom">crm(live)configure# </code><code class="command">node</code> bob utilization hv_memory="4000"
<code class="prompt custom">crm(live)configure# </code><code class="command">node</code> charlie utilization hv_memory="4000"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> xenA Xen \
    utilization hv_memory="3500" meta priority="10" \
    params xmfile="/etc/xen/shared-vm/vm1"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> xenB Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm2"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> xenC Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm3"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> xenD Xen \
    utilization hv_memory="1000" meta priority="5" \
    params xmfile="/etc/xen/shared-vm/vm4"
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> placement-strategy="minimal"</pre></div><p>
    With all three nodes up, xenA will be placed onto a node first, followed
    by xenD. xenB and xenC would either be allocated together or one of them
    with xenD.
   </p><p>
    If one node failed, too little total memory would be available to host
    them all. xenA would be ensured to be allocated, as would xenD. However,
    only one of xenB or xenC could still be placed, and since their priority
    is equal, the result is not defined yet. To resolve this ambiguity as
    well, you would need to set a higher priority for either one.
   </p></section><section class="sect2" id="sec-ha-manual-config-monitor" data-id-title="Configuring Resource Monitoring"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.9 </span><span class="title-name">Configuring Resource Monitoring</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-monitor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To monitor a resource, there are two possibilities: either define a
    monitor operation with the <code class="command">op</code> keyword or use the
    <code class="command">monitor</code> command. The following example configures an
    Apache resource and monitors it every 60 seconds with the
    <code class="literal">op</code> keyword:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> apache apache \
  params ... \
  <span class="emphasis"><em>op monitor interval=60s timeout=30s</em></span></pre></div><p>
    The same can be done with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> apache apache \
   params ...
<code class="prompt custom">crm(live)configure# </code><code class="command">monitor</code> apache 60s:30s</pre></div><p>
    For an overview, refer to
    <a class="xref" href="#sec-ha-config-basics-monitoring" title="5.4. Resource Monitoring">Section 5.4, “Resource Monitoring”</a>.
   </p></section><section class="sect2" id="sec-ha-manual-config-group" data-id-title="Configuring a Cluster Resource Group"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.10 </span><span class="title-name">Configuring a Cluster Resource Group</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-group">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    One of the most common elements of a cluster is a set of resources that
    needs to be located together. Start sequentially and stop in the reverse
    order. To simplify this configuration we support the concept of groups.
    The following example creates two primitives (an IP address and an
    e-mail resource):
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run the <code class="command">crm</code> command as system administrator. The
      prompt changes to <code class="literal">crm(live)</code>.
     </p></li><li class="step"><p>
      Configure the primitives:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> Public-IP ocf:heartbeat:IPaddr \
   params ip=1.2.3.4 id= Public-IP
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> Email systemd:postfix \
   params id=Email</pre></div></li><li class="step"><p>
      Group the primitives with their relevant identifiers in the correct
      order:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-mailsvc Public-IP Email</pre></div></li></ol></div></div><p>
    To change the order of a group member, use the
    <code class="command">modgroup</code> command from the
    <code class="command">configure</code> subcommand. Use the following commands to
    move the primitive <code class="literal">Email</code> before
    <code class="literal">Public-IP</code>. (This is just to demonstrate the feature):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-mailsvc add Email before Public-IP</pre></div><p>
    To remove a resource from a group (for example,
    <code class="literal">Email</code>), use this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-mailsvc remove Email</pre></div><p>
    For an overview, refer to
    <a class="xref" href="#sec-ha-config-basics-resources-advanced-groups" title="5.3.5.1. Groups">Section 5.3.5.1, “Groups”</a>.
   </p></section><section class="sect2" id="sec-ha-manual-config-clone" data-id-title="Configuring a Clone Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.11 </span><span class="title-name">Configuring a Clone Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clone">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>

    Clones were initially conceived as a convenient way to start N instances
    of an IP resource and have them distributed throughout the cluster for
    load balancing. They have turned out to be useful for several other
    purposes, including integrating with DLM, the fencing subsystem and
    OCFS2. You can clone any resource, provided the resource agent supports
    it.
   </p><p>
    Learn more about cloned resources in
    <a class="xref" href="#sec-ha-config-basics-resources-advanced-clones" title="5.3.5.2. Clones">Section 5.3.5.2, “Clones”</a>.
   </p><section class="sect3" id="sec-ha-manual-config-clone-anonymous" data-id-title="Creating Anonymous Clone Resources"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.11.1 </span><span class="title-name">Creating Anonymous Clone Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clone-anonymous">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To create an anonymous clone resource, first create a primitive
     resource and then refer to it with the <code class="command">clone</code>
     command. Do the following:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
       interactive shell:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
       Configure the primitive, for example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> Apache apache</pre></div></li><li class="step"><p>
       Clone the primitive:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-apache Apache</pre></div></li></ol></div></div></section><section class="sect3" id="sec-ha-manual-config-clone-stateful" data-id-title="Creating Stateful/Multi-State Clone Resources"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">7.4.11.2 </span><span class="title-name">Creating Stateful/Multi-State Clone Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clone-stateful">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Multi-state resources are a specialization of clones. This type allows
     the instances to be in one of two operating modes, be it active/passive,
     primary/secondary, or master/slave.
    </p><p>
     To create a stateful clone resource, first create a primitive resource
     and then the multi-state resource. The multi-state resource must
     support at least promote and demote operations.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
       interactive shell:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
       Configure the primitive. Change the intervals if needed:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> my-rsc ocf:myCorp:myAppl \
    op monitor interval=60 \
    op monitor interval=61 role=Master</pre></div></li><li class="step"><p>
       Create the multi-state resource:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms</code> ms-rsc my-rsc</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="sec-ha-config-crm" data-id-title="Managing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Managing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Apart from the possibility to configure your cluster resources, the
   <code class="command">crm</code> tool also allows you to manage existing resources.
   The following subsections gives you an overview.
  </p><section class="sect2" id="sec-ha-manual-config-show" data-id-title="Showing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">Showing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-show">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When administering a cluster the command <code class="command">crm configure show</code>
    lists the current CIB objects like cluster configuration, global options,
    primitives, and others:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show
node 178326192: alice
node 178326448: bob
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20
primitive stonith-sbd stonith:external/sbd \
        params pcmk_delay_max=30
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version=1.1.15-17.1-e174ec8 \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=true \
        placement-strategy=balanced \
        standby-mode=true
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true</pre></div><p>
    In case you have lots of resources, the output of <code class="command">show</code>
    is too verbose. To restrict the output, use the name of the resource.
    For example, to list the properties of the primitive
    <code class="systemitem">admin_addr</code> only, append the resource name to
    <code class="command">show</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show admin_addr
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20</pre></div><p>
    However, in some cases, you want to limit the output of specific resources
    even more. This can be achieved with <span class="emphasis"><em>filters</em></span>. Filters
    limit the output to specific components. For example, to list the
    nodes only, use <code class="literal">type:node</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show type:node
node 178326192: alice
node 178326448: bob</pre></div><p>In case you are also interested in primitives, use the
   <code class="literal">or</code> operator:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show type:node or type:primitive
node 178326192: alice
node 178326448: bob
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20
primitive stonith-sbd stonith:external/sbd \
        params pcmk_delay_max=30</pre></div><p>
    Furthermore, to search for an object that starts with a certain string,
    use this notation:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show type:primitive and and 'admin*'
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20</pre></div><p>
    To list all available types, enter <code class="command">crm configure show type:</code>
    and press the <span class="keycap">→|</span> key. The Bash completion will give
    you a list of all types.
   </p></section><section class="sect2" id="sec-ha-manual-config-start" data-id-title="Starting a New Cluster Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">Starting a New Cluster Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To start a new cluster resource you need the respective identifier.
    Proceed as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code></pre></div></li><li class="step"><p>
      Switch to the resource level:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource</code></pre></div></li><li class="step"><p>
      Start the resource with <code class="command">start</code> and press the
      <span class="keycap">→|</span> key to show all known resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code><code class="command">start</code> <em class="replaceable">ID</em></pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-cleanup" data-id-title="Cleaning Up Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.3 </span><span class="title-name">Cleaning Up Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-cleanup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource will be automatically restarted if it fails, but each failure
    raises the resource's failcount. If a
    <code class="literal">migration-threshold</code> has been set for that resource,
    the node will no longer be allowed to run the resource when the
    number of failures has reached the migration threshold.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a shell and log in as user <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Get a list of all your resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource list
  ...
Resource Group: dlm-clvm:1
         dlm:1  (ocf:pacemaker:controld) Started
         clvm:1 (ocf:heartbeat:clvm) Started</pre></div></li><li class="step"><p>
      To clean up the resource <code class="literal">dlm</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource cleanup dlm</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-remove" data-id-title="Removing a Cluster Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.4 </span><span class="title-name">Removing a Cluster Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-remove">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Proceed as follows to remove a cluster resource:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Run the following command to get a list of your resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource</code> status</pre></div><p>
      For example, the output can look like this (whereas myIP is the
      relevant identifier of your resource):
     </p><div class="verbatim-wrap"><pre class="screen">myIP    (ocf:IPaddr:heartbeat) ...</pre></div></li><li class="step"><p>
      Delete the resource with the relevant identifier (which implies a
      <code class="command">commit</code> too):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code> delete <em class="replaceable">YOUR_ID</em></pre></div></li><li class="step"><p>
      Commit the changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code> commit</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-migrate" data-id-title="Migrating a Cluster Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.5 </span><span class="title-name">Migrating a Cluster Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Although resources are configured to automatically fail over (or
    migrate) to other nodes of the cluster if a hardware or
    software failure occurs, you can also manually move a resource to another node
    using either Hawk2 or the command line.
   </p><p>
    Use the <code class="command">migrate</code> command for this task. For example,
    to migrate the resource <code class="literal">ipaddress1</code> to a cluster node
    named <code class="systemitem">bob</code>, use these
    commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource
<code class="prompt custom">crm(live)resource# </code><code class="command">migrate</code> ipaddress1 bob</pre></div></section><section class="sect2" id="sec-ha-manual-config-tag" data-id-title="Grouping/Tagging Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.6 </span><span class="title-name">Grouping/Tagging Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-tag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Tags are a way to refer to multiple resources at once, without creating
    any colocation or ordering relationship between them. This can be useful
    for grouping conceptually related resources. For example, if you have
    several resources related to a database, create a tag called
    <code class="literal">databases</code> and add all resources related to the
    database to this tag:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure tag databases: db1 db2 db3</pre></div><p>
    This allows you to start them all with a single command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource start databases</pre></div><p>
    Similarly, you can stop them all too:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource stop databases</pre></div></section><section class="sect2" id="sec-ha-manual-config-cli-health" data-id-title="Getting Health Status"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.7 </span><span class="title-name">Getting Health Status</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-cli-health">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <span class="quote">“<span class="quote">health</span>”</span> status of a cluster or node can be displayed
    with so called <span class="emphasis"><em>scripts</em></span>. A script can perform
    different tasks—they are not targeted to health. However, for
    this subsection, we focus on how to get the health status.
   </p><p>
    To get all the details about the <code class="command">health</code> command, use
    <code class="command">describe</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script describe health</pre></div><p>
    It shows a description and a list of all parameters and their default
    values. To execute a script, use <code class="command">run</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script run health</pre></div><p>
    If you prefer to run only one step from the suite, the
    <code class="command">describe</code> command lists all available steps in the
    <em class="citetitle">Steps</em> category.
   </p><p>
    For example, the following command executes the first step of the
    <code class="command">health</code> command. The output is stored in the
    <code class="filename">health.json</code> file for further investigation:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script run health
    statefile='health.json'</pre></div><p>It is also possible to run the above commands with
    <code class="command">crm cluster health</code>.</p><p>
    For additional information regarding scripts, see
    <a class="link" href="http://crmsh.github.io/scripts/" target="_blank">http://crmsh.github.io/scripts/</a>.
   </p></section></section><section class="sect1" id="sec-ha-config-crm-setpwd" data-id-title="Setting Passwords Independent of cib.xml"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Setting Passwords Independent of <code class="filename">cib.xml</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-setpwd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case your cluster configuration contains sensitive information, such
   as passwords, it should be stored in local files. That way, these
   parameters will never be logged or leaked in support reports.
  </p><p>
   Before using <code class="command">secret</code>, better run the
   <code class="command">show</code> command first to get an overview of all your
   resources:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show
primitive mydb mysql \
   params replication_user=admin ...</pre></div><p>
   If you want to set a password for the above <code class="literal">mydb</code>
   resource, use the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource secret mydb set passwd linux
INFO: syncing /var/lib/heartbeat/lrm/secrets/mydb/passwd to [your node list]</pre></div><p>
   You can get the saved password back with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource secret mydb show passwd
linux</pre></div><p>
   Note that the parameters need to be synchronized between nodes; the
   <code class="command">crm resource secret</code> command will take care of that. We
   highly recommend to only use this command to manage secret parameters.
  </p></section><section class="sect1" id="sec-ha-config-crm-history" data-id-title="Retrieving History Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.7 </span><span class="title-name">Retrieving History Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-history">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Investigating the cluster history is a complex task. To simplify this
   task, crmsh contains the <code class="command">history</code> command with its
   subcommands. It is assumed SSH is configured correctly.
  </p><p>
   Each cluster moves states, migrates resources, or starts important
   processes. All these actions can be retrieved by subcommands of
   <code class="command">history</code>. 
    
  </p><p>
   By default, all <code class="command">history</code> commands look at the events of
   the last hour. To change this time frame, use the
   <code class="command">limit</code> subcommand. The syntax is:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history
<code class="prompt custom">crm(live)history# </code><code class="command">limit</code> <em class="replaceable">FROM_TIME</em> [<em class="replaceable">TO_TIME</em>]</pre></div><p>
   Some valid examples include:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.10.7.1"><span class="term"><code class="command">limit</code><code class="literal">4:00pm</code>
    , </span><span class="term"><code class="command">limit</code><code class="literal">16:00</code>
    </span></dt><dd><p>
      Both commands mean the same, today at 4pm.
     </p></dd><dt id="id-1.3.4.5.10.7.2"><span class="term"><code class="command">limit</code><code class="literal">2012/01/12 6pm</code>
    </span></dt><dd><p>
      January 12th 2012 at 6pm
     </p></dd><dt id="id-1.3.4.5.10.7.3"><span class="term"><code class="command">limit</code><code class="literal">"Sun 5 20:46"</code>
    </span></dt><dd><p>
      In the current year of the current month at Sunday the 5th at 8:46pm
     </p></dd></dl></div><p>
   Find more examples and how to create time frames at
   <a class="link" href="http://labix.org/python-dateutil" target="_blank">http://labix.org/python-dateutil</a>.
  </p><p>
   The <code class="command">info</code> subcommand shows all the parameters which are
   covered by the <code class="command">crm report</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">info</code>
Source: live
Period: 2012-01-12 14:10:56 - end
Nodes: alice
Groups:
Resources:</pre></div><p>
   To limit <code class="command">crm report</code> to certain parameters view the
   available options with the subcommand <code class="command">help</code>.
  </p><p>
   To narrow down the level of detail, use the subcommand
   <code class="command">detail</code> with a level:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">detail</code> 1</pre></div><p>
   The higher the number, the more detailed your report will be. Default is
   <code class="literal">0</code> (zero).
  </p><p>
   After you have set above parameters, use <code class="command">log</code> to show
   the log messages.
  </p><p>
   To display the last transition, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">transition</code> -1
INFO: fetching new logs, please wait ...</pre></div><p>
   This command fetches the logs and runs <code class="command">dotty</code> (from the
   <code class="systemitem">graphviz</code> package) to show the
   transition graph. The shell opens the log file which you can browse with
   the <span class="keycap">↓</span> and <span class="keycap">↑</span> cursor keys.
  </p><p>
   If you do not want to open the transition graph, use the
   <code class="option">nograph</code> option:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">transition</code> -1 nograph</pre></div></section><section class="sect1" id="sec-ha-config-crm-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The crm man page.
    </p></li><li class="listitem"><p>
     Visit the upstream project documentation at
     <a class="link" href="http://crmsh.github.io/documentation" target="_blank">http://crmsh.github.io/documentation</a>.
    </p></li><li class="listitem"><p>
     See <span class="intraxref">Article “Highly Available NFS Storage with DRBD and Pacemaker”</span> for an exhaustive example.
    </p></li></ul></div></section></section><section class="chapter" id="cha-ha-agents" data-id-title="Adding or Modifying Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Adding or Modifying Resource Agents</span></span> <a title="Permalink" class="permalink" href="#cha-ha-agents">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    All tasks that need to be managed by a cluster must be available as a
    resource. There are two major groups here to consider: resource agents
    and STONITH agents. For both categories, you can add your own
    agents, extending the abilities of the cluster to your own needs.
   </p></div></div></div></div><section class="sect1" id="sec-ha-stonithagents" data-id-title="STONITH Agents"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">STONITH Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-stonithagents">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A cluster sometimes detects that one of the nodes is behaving strangely
   and needs to remove it. This is called <span class="emphasis"><em>fencing</em></span> and
   is commonly done with a STONITH resource.
  </p><div id="id-1.3.4.6.3.3" data-id-title="External SSH/STONITH Are Not Supported" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: External SSH/STONITH Are Not Supported</div><p>
    It is impossible to know how SSH might react to other system problems.
    For this reason, external SSH/STONITH agents (like
    <code class="literal">stonith:external/ssh</code>) are not supported for
    production environments. If you still want to use such agents for
    testing, install the
    <code class="systemitem">libglue-devel</code> package.
   </p></div><p>
   To get a list of all currently available STONITH devices (from the
   software side), use the command <code class="command">crm ra list stonith</code>.
   If you do not find your favorite agent, install the
   <code class="systemitem">-devel</code> package.
   For more information on STONITH devices and resource agents,
   see <a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>.
  </p><p>
   As of yet there is no documentation about writing STONITH agents. If
   you want to write new STONITH agents, consult the examples available
   in the source of the
   <code class="systemitem">cluster-glue</code> package.
  </p></section><section class="sect1" id="sec-ha-writingresourceagents" data-id-title="Writing OCF Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Writing OCF Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-writingresourceagents">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All OCF resource agents (RAs) are available in
   <code class="filename">/usr/lib/ocf/resource.d/</code>, see
   <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a> for more information.
   Each resource agent must supported the following operations to control
   it:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.4.3.1"><span class="term"><code class="command">start</code>
    </span></dt><dd><p>
      start or enable the resource
     </p></dd><dt id="id-1.3.4.6.4.3.2"><span class="term"><code class="command">stop</code>
    </span></dt><dd><p>
      stop or disable the resource
     </p></dd><dt id="id-1.3.4.6.4.3.3"><span class="term"><code class="command">status</code>
    </span></dt><dd><p>
      returns the status of the resource
     </p></dd><dt id="id-1.3.4.6.4.3.4"><span class="term"><code class="command">monitor</code>
    </span></dt><dd><p>
      similar to <code class="command">status</code>, but checks also for unexpected
      states
     </p></dd><dt id="id-1.3.4.6.4.3.5"><span class="term"><code class="command">validate</code>
    </span></dt><dd><p>
      validate the resource's configuration
     </p></dd><dt id="id-1.3.4.6.4.3.6"><span class="term"><code class="command">meta-data</code>
    </span></dt><dd><p>
      returns information about the resource agent in XML
     </p></dd></dl></div><p>
   The general procedure of how to create an OCF RA is like the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Load the file
     <code class="filename">/usr/lib/ocf/resource.d/pacemaker/Dummy</code> as a
     template.
    </p></li><li class="step"><p>
     Create a new subdirectory for each new resource agents to avoid naming
     contradictions. For example, if you have a resource group
     <code class="systemitem">kitchen</code> with the resource
     <code class="systemitem">coffee_machine</code>, add this resource to the directory
     <code class="filename">/usr/lib/ocf/resource.d/kitchen/</code>. To access this
     RA, execute the command <code class="command">crm</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure primitive coffee_1 ocf:coffee_machine:kitchen ...</pre></div></li><li class="step"><p>
     Implement the different shell functions and save your file under a
     different name.
    </p></li></ol></div></div><p>
   More details about writing OCF resource agents can be found at
   <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a> in the guide
   <em class="citetitle">Pacemaker Administration</em>. Find
   special information about several concepts at
   <a class="xref" href="#cha-ha-concepts" title="Chapter 1. Product Overview">Chapter 1, <em>Product Overview</em></a>.
  </p></section><section class="sect1" id="sec-ha-errorcodes" data-id-title="OCF Return Codes and Failure Recovery"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.3 </span><span class="title-name">OCF Return Codes and Failure Recovery</span></span> <a title="Permalink" class="permalink" href="#sec-ha-errorcodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  According to the OCF specification, there are strict definitions of the
  exit codes an action must return. The cluster always checks the return
  code against the expected result. If the result does not match the
  expected value, then the operation is considered to have failed and a
  recovery action is initiated. There are three types of failure recovery:
 </p><div class="table" id="id-1.3.4.6.5.4" data-id-title="Failure Recovery Types"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 8.1: </span><span class="title-name">Failure Recovery Types </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Recovery Type
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Description
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Action Taken by the Cluster
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       A transient error occurred.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Restart the resource or move it to a new location.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       A non-transient error occurred. The error may be specific to the
       current node.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Move the resource elsewhere and prevent it from being retried on the
       current node.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       fatal
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       A non-transient error occurred that will be common to all cluster
       nodes. This means a bad configuration was specified.
      </p>
     </td><td>
      <p>
       Stop the resource and prevent it from being started on any cluster
       node.
      </p>
     </td></tr></tbody></table></div></div><p>
  Assuming an action is considered to have failed, the following table
  outlines the different OCF return codes. It also shows the type of
  recovery the cluster will initiate when the respective error code is
  received.
 </p><div class="table" id="id-1.3.4.6.5.7" data-id-title="OCF Return Codes"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 8.2: </span><span class="title-name">OCF Return Codes </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF Return Code
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF Alias
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Description
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Recovery Type
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       0
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_SUCCESS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Success. The command completed successfully. This is the expected
       result for all start, stop, promote and demote commands.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       1
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­GENERIC
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Generic <span class="quote">“<span class="quote">there was a problem</span>”</span> error code.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       2
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_ARGS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource’s configuration is not valid on this machine (for
       example, it refers to a location/tool not found on the node).
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       3
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_­ERR_­UN­IMPLEMENTED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The requested action is not implemented.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       4
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_PERM
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource agent does not have sufficient privileges to complete
       the task.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       5
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­INSTALLED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The tools required by the resource are not installed on this machine.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       6
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­CONFIGURED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource’s configuration is invalid (for example, required
       parameters are missing).
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       fatal
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       7
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_NOT_­RUNNING
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is not running. The cluster will not attempt to stop a
       resource that returns this for any action.
      </p>
      <p>
       This OCF return code may or may not require resource
       recovery—it depends on what is the expected resource status.
       If unexpected, then <code class="literal">soft</code> recovery.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       N/A
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       8
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_RUNNING_­MASTER
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is running in Master mode.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       9
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_FAILED_­MASTER
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is in Master mode but has failed. The resource will be
       demoted, stopped and then started (and possibly promoted) again.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       other
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       N/A
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       Custom error code.
      </p>
     </td><td>
      <p>
       soft
      </p>
     </td></tr></tbody></table></div></div></section></section><section class="chapter" id="cha-ha-fencing" data-id-title="Fencing and STONITH"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Fencing and STONITH</span></span> <a title="Permalink" class="permalink" href="#cha-ha-fencing">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Fencing is a very important concept in computer clusters for HA (High
    Availability). A cluster sometimes detects that one of the nodes is
    behaving strangely and needs to remove it. This is called
    <span class="emphasis"><em>fencing</em></span> and is commonly done with a STONITH
    resource. Fencing may be defined as a method to bring an HA cluster to a
    known state.
   </p><p>
    Every resource in a cluster has a state attached. For example:
    <span class="quote">“<span class="quote">resource r1 is started on alice</span>”</span>. In an HA cluster, such
    a state implies that <span class="quote">“<span class="quote">resource r1 is stopped on all nodes except
    alice</span>”</span>, because the cluster must make sure that every resource
    may be started on only one node. Every node must report every change
    that happens to a resource. The cluster state is thus a collection of
    resource states and node states.
   </p><p>
    When the state of a node or resource cannot be established with
    certainty, fencing comes in. Even when the cluster is not aware of what
    is happening on a given node, fencing can ensure that the node does not
    run any important resources.
   </p></div></div></div></div><section class="sect1" id="sec-ha-fencing-classes" data-id-title="Classes of Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Classes of Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-classes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are two classes of fencing: resource level and node level fencing.
   The latter is the primary subject of this chapter.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.3.3.1"><span class="term">Resource Level Fencing</span></dt><dd><p>Resource level fencing ensures exclusive access to a given resource.
      Common examples of this are changing the zoning of the node from a SAN fiber
      channel switch (thus locking the node out of access to its disks) or methods
      like SCSI reserve. For examples, refer to <a class="xref" href="#sec-ha-storage-protect-rsc-fencing" title="10.10. Additional Mechanisms for Storage Protection">Section 10.10, “Additional Mechanisms for Storage Protection”</a>.
     </p></dd><dt id="id-1.3.4.7.3.3.2"><span class="term">Node Level Fencing</span></dt><dd><p>
      Node level fencing prevents a failed node from accessing shared resources
      entirely. This is usually done in a simple and abrupt way: reset or power
      off the node.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-fencing-nodes" data-id-title="Node Level Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Node Level Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In a Pacemaker cluster, the implementation of node level fencing is STONITH
   (Shoot The Other Node in the Head). SUSE Linux Enterprise High Availability
   includes the <code class="command">stonith</code> command line tool, an extensible
   interface for remotely powering down a node in the cluster. For an
   overview of the available options, run <code class="command">stonith --help</code>
   or refer to the man page of <code class="command">stonith</code> for more
   information.
  </p><section class="sect2" id="sec-ha-fencing-nodes-devices" data-id-title="STONITH Devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.1 </span><span class="title-name">STONITH Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes-devices">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To use node level fencing, you first need to have a fencing device. To
    get a list of STONITH devices which are supported by SUSE Linux Enterprise High Availability, run
    one of the following commands on any of the nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>stonith -L</pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm ra list stonith</pre></div><p>
    STONITH devices may be classified into the following categories:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.4.3.7.1"><span class="term">Power Distribution Units (PDU)</span></dt><dd><p>
       Power Distribution Units are an essential element in managing power
       capacity and functionality for critical network, server and data
       center equipment. They can provide remote load monitoring of
       connected equipment and individual outlet power control for remote
       power recycling.
      </p></dd><dt id="id-1.3.4.7.4.3.7.2"><span class="term">Uninterruptible Power Supplies (UPS)</span></dt><dd><p>
       A stable power supply provides emergency power to connected equipment
       by supplying power from a separate source if a utility
       power failure occurs.
      </p></dd><dt id="id-1.3.4.7.4.3.7.3"><span class="term">Blade Power Control Devices</span></dt><dd><p>
       If you are running a cluster on a set of blades, then the power
       control device in the blade enclosure is the only candidate for
       fencing. Of course, this device must be capable of managing single
       blade computers.
      </p></dd><dt id="id-1.3.4.7.4.3.7.4"><span class="term">Lights-out Devices </span></dt><dd><p>
       Lights-out devices (IBM RSA, HP iLO, Dell DRAC) are becoming
       increasingly popular and may even become standard in off-the-shelf
       computers. However, if they share a power supply with their host (a
       cluster node), they might not work when needed. If a node stays without
       power, the device supposed to control it would be useless. Therefore, it
       is highly recommended to use battery backed lights-out devices.
       Another aspect is that these devices are accessed by network. This might
       imply a single point of failure, or security concerns.
      </p></dd><dt id="id-1.3.4.7.4.3.7.5"><span class="term">Testing Devices</span></dt><dd><p>
       Testing devices are used exclusively for testing purposes. They are
       usually more gentle on the hardware. Before the cluster goes into
       production, they must be replaced with real fencing devices.
      </p></dd></dl></div><p>
    The choice of the STONITH device depends mainly on your budget and the
    kind of hardware you use.
   </p></section><section class="sect2" id="sec-ha-fencing-nodes-implementation" data-id-title="STONITH Implementation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.2.2 </span><span class="title-name">STONITH Implementation</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes-implementation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The STONITH implementation of SUSE® Linux Enterprise High Availability consists of two
    components:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.4.4.3.1"><span class="term">stonithd</span></dt><dd><p>
       stonithd is a daemon which can be accessed by local processes or over
       the network. It accepts the commands which correspond to fencing
       operations: reset, power-off, and power-on. It can also check the
       status of the fencing device.
      </p><p>
       The stonithd daemon runs on every node in the CRM HA cluster. The
       stonithd instance running on the DC node receives a fencing request
       from the CRM. It is up to this and other stonithd programs to carry
       out the desired fencing operation.
      </p></dd><dt id="id-1.3.4.7.4.4.3.2"><span class="term">STONITH Plug-ins</span></dt><dd><p>
       For every supported fencing device there is a STONITH plug-in which
       is capable of controlling said device. A STONITH plug-in is the
       interface to the fencing device. The STONITH plug-ins contained in
       the <code class="systemitem">cluster-glue</code> package reside in
       <code class="filename">/usr/lib64/stonith/plugins</code> on each node.
       (If you installed the
       <code class="systemitem"> fence-agents</code> package, too,
       the plug-ins contained there are installed in
       <code class="filename">/usr/sbin/fence_*</code>.) All STONITH plug-ins look
       the same to stonithd, but are quite different on the other side
       reflecting the nature of the fencing device.
      </p><p>
       Some plug-ins support more than one device. A typical example is
       <code class="literal">ipmilan</code> (or <code class="literal">external/ipmi</code>)
       which implements the IPMI protocol and can control any device which
       supports this protocol.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-fencing-config" data-id-title="STONITH Resources and Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.3 </span><span class="title-name">STONITH Resources and Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To set up fencing, you need to configure one or more STONITH
   resources—the stonithd daemon requires no configuration. All
   configuration is stored in the CIB. A STONITH resource is a resource of
   class <code class="literal">stonith</code> (see
   <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>). STONITH resources
   are a representation of STONITH plug-ins in the CIB. Apart from the
   fencing operations, the STONITH resources can be started, stopped and
   monitored, like any other resource. Starting or stopping STONITH
   resources means loading and unloading the STONITH device driver on a
   node. Starting and stopping are thus only administrative operations and
   do not translate to any operation on the fencing device itself. However,
   monitoring does translate to logging it to the device (to verify that the
   device will work in case it is needed). When a STONITH resource fails
   over to another node it enables the current node to talk to the STONITH
   device by loading the respective driver.
  </p><p>
   STONITH resources can be configured like any other resource. For
   details how to do so with your preferred cluster management tool:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hawk2: <a class="xref" href="#sec-conf-hawk2-rsc-stonith" title="6.5.6. Adding STONITH Resources">Section 6.5.6, “Adding STONITH Resources”</a>
    </p></li><li class="listitem"><p>
     crmsh: <a class="xref" href="#sec-ha-manual-create-stonith" title="7.4.4. Creating a STONITH Resource">Section 7.4.4, “Creating a STONITH Resource”</a>
    </p></li></ul></div><p>
   The list of parameters (attributes) depends on the respective STONITH
   type. To view a list of parameters for a specific device, use the
   <code class="command">stonith</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">stonith-device-type</em> -n</pre></div><p>
   For example, to view the parameters for the <code class="literal">ibmhmc</code>
   device type, enter the following:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t ibmhmc -n</pre></div><p>
   To get a short help text for the device, use the <code class="option">-h</code>
   option:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">stonith-device-type</em> -h</pre></div><section class="sect2" id="sec-ha-fencing-config-examples" data-id-title="Example STONITH Resource Configurations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">9.3.1 </span><span class="title-name">Example STONITH Resource Configurations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-config-examples">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In the following, find some example configurations written in the syntax
    of the <code class="command">crm</code> command line tool. To apply them, put the
    sample in a text file (for example, <code class="filename">sample.txt</code>) and
    run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> &lt; sample.txt</pre></div><p>
    For more information about configuring resources with the
    <code class="command">crm</code> command line tool, refer to
    <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>.
   </p><div class="complex-example"><div class="example" id="id-1.3.4.7.5.11.5" data-id-title="Configuration of an IBM RSA Lights-out Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.1: </span><span class="title-name">Configuration of an IBM RSA Lights-out Device </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.7.5.11.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     An IBM RSA lights-out device might be configured like this:
    </p><div class="verbatim-wrap"><pre class="screen">configure
primitive st-ibmrsa-1 stonith:external/ibmrsa-telnet \
params nodename=alice ip_address=192.168.0.101 \
username=USERNAME password=PASSW0RD
primitive st-ibmrsa-2 stonith:external/ibmrsa-telnet \
params nodename=bob ip_address=192.168.0.102 \
username=USERNAME password=PASSW0RD
location l-st-alice st-ibmrsa-1 -inf: alice
location l-st-bob st-ibmrsa-2 -inf: bob
commit</pre></div><p>
     In this example, location constraints are used for the following
     reason: There is always a certain probability that the STONITH
     operation is going to fail. Therefore, a STONITH operation on the
     node which is the executioner as well is not reliable. If the node is
     reset, it cannot send the notification about the fencing operation
     outcome. The only way to do that is to assume that the operation is
     going to succeed and send the notification beforehand. But if the
     operation fails, problems could arise. Therefore, by convention,
     stonithd refuses to terminate its host.
    </p></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.7.5.11.6" data-id-title="Configuration of a UPS Fencing Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.2: </span><span class="title-name">Configuration of a UPS Fencing Device </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.7.5.11.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The configuration of a UPS type fencing device is similar to the
     examples above. The details are not covered here. All UPS devices
     employ the same mechanics for fencing. How the device is accessed
     varies. Old UPS devices only had a serial port, usually connected at
     1200baud using a special serial cable. Many new ones still have a
     serial port, but often they also use a USB or Ethernet interface. The
     kind of connection you can use depends on what the plug-in supports.
    </p><p>
     For example, compare the <code class="literal">apcmaster</code> with the
     <code class="literal">apcsmart</code> device by using the <code class="command">stonith
     -t</code> <em class="replaceable">stonith-device-type</em> -n command:
    </p><div class="verbatim-wrap"><pre class="screen">stonith -t apcmaster -h</pre></div><p>
     returns the following information:
    </p><div class="verbatim-wrap"><pre class="screen">STONITH Device: apcmaster - APC MasterSwitch (via telnet)
NOTE: The APC MasterSwitch accepts only one (telnet)
connection/session a time. When one session is active,
subsequent attempts to connect to the MasterSwitch will fail.
For more information see http://www.apc.com/
List of valid parameter names for apcmaster STONITH device:
ipaddr
login
 password</pre></div><p>
     With
    </p><div class="verbatim-wrap"><pre class="screen">stonith -t apcsmart -h</pre></div><p>
     you get the following output:
    </p><div class="verbatim-wrap"><pre class="screen">STONITH Device: apcsmart - APC Smart UPS
(via serial port - NOT USB!).
Works with higher-end APC UPSes, like
Back-UPS Pro, Smart-UPS, Matrix-UPS, etc.
(Smart-UPS may have to be &gt;= Smart-UPS 700?).
See http://www.networkupstools.org/protocols/apcsmart.html
for protocol compatibility details.
For more information see http://www.apc.com/
List of valid parameter names for apcsmart STONITH device:
ttydev
hostlist</pre></div><p>
     The first plug-in supports APC UPS with a network port and telnet
     protocol. The second plug-in uses the APC SMART protocol over the
     serial line, which is supported by many APC UPS product
     lines.
    </p></div></div></div><div class="complex-example"><div class="example" id="ex-ha-fencing-kdump" data-id-title="Configuration of a Kdump Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.3: </span><span class="title-name">Configuration of a Kdump Device </span></span><a title="Permalink" class="permalink" href="#ex-ha-fencing-kdump">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>Kdump belongs to the <a class="xref" href="#sec-ha-fencing-special" title="9.5. Special Fencing Devices">Special Fencing Devices</a> and is in fact the opposite of a fencing device.
     The plug-in checks if a Kernel dump is in progress on a node. If so, it
     returns true, and acts <span class="emphasis"><em>as if</em></span> the node has been fenced.
    </p><p>
     The Kdump plug-in must be used in concert with another, real STONITH
     device, for example, <code class="literal">external/ipmi</code>. For the fencing
     mechanism to work properly, you must specify that Kdump is checked before
     a real STONITH device is triggered. Use <code class="command">crm configure
     fencing_topology</code> to specify the order of the fencing devices as
     shown in the following procedure.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the <code class="literal">stonith:fence_kdump</code> resource agent (provided
     by the package <code class="systemitem">fence-agents</code>)
     to monitor all nodes with the Kdump function enabled. Find a
     configuration example for the resource below:
    </p><div class="verbatim-wrap"><pre class="screen">configure
  primitive st-kdump stonith:fence_kdump \
    params nodename="alice "\ <span class="callout" id="co-ha-fenc-kdump-nodename">1</span>
    pcmk_host_check="static-list" \
    pcmk_reboot_action="off" \
    pcmk_monitor_action="metadata" \
    pcmk_reboot_retries="1" \
    timeout="60"
commit</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-fenc-kdump-nodename"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Name of the node to be monitored. If you need to monitor more than one
       node, configure more STONITH resources. To prevent a specific node
       from using a fencing device, add location constraints.
      </p></td></tr></table></div><p>
     The fencing action will be started after the timeout of the resource.
    </p></li><li class="step"><p>
     In <code class="filename">/etc/sysconfig/kdump</code> on each node, configure
     <code class="literal">KDUMP_POSTSCRIPT</code> to send a notification to all nodes
     when the Kdump process is finished. For example:
    </p><div class="verbatim-wrap"><pre class="screen">/usr/lib/fence_kdump_send -i <em class="replaceable">INTERVAL</em> -p <em class="replaceable">PORT</em> -c 1 alice bob charlie [...]</pre></div><p>
     The node that does a Kdump will restart automatically after Kdump has
     finished.
    </p></li><li class="step"><p>
     Write a new <code class="filename">initrd</code> to include the library <code class="literal">fence_kdump_send</code>
     with network enabled. Use the <code class="option">-f</code> option to overwrite
     the existing file, so the new file will be used for the next boot process:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>dracut -f -a kdump</pre></div></li><li class="step"><p>
     Open a port in the firewall for the <code class="literal">fence_kdump</code> resource.
     The default port is <code class="literal">7410</code>.
    </p></li><li class="step"><p>
     To have Kdump checked before triggering a real fencing
     mechanism (like <code class="literal">external/ipmi</code>),
     use a configuration similar to the following:</p><div class="verbatim-wrap"><pre class="screen">fencing_topology \
  alice: kdump-node1 ipmi-node1 \
  bob: kdump-node2 ipmi-node2</pre></div><p>For more details on <code class="option">fencing_topology</code>:
    </p><div class="verbatim-wrap"><pre class="screen">crm configure help fencing_topology</pre></div></li></ol></div></div></div></div></div></section></section><section class="sect1" id="sec-ha-fencing-monitor" data-id-title="Monitoring Fencing Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.4 </span><span class="title-name">Monitoring Fencing Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Like any other resource, the STONITH class agents also support the
   monitoring operation for checking status.
  </p><div id="id-1.3.4.7.6.3" data-id-title="Monitoring STONITH Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Monitoring STONITH Resources</div><p>
    Monitor STONITH resources regularly, yet sparingly. For most devices a
    monitoring interval of at least 1800 seconds (30 minutes) should
    suffice.
   </p></div><p>
   Fencing devices are an indispensable part of an HA cluster, but the less
   you need to use them, the better. Power management equipment is often
   affected by too much broadcast traffic. Some devices cannot handle more
   than ten or so connections per minute. Some get confused if two clients
   try to connect at the same time. Most cannot handle more than one session
   at a time.
  </p><p>
   Checking the status of fencing devices once every few hours should
   usually be enough. The probability that a fencing operation needs to be
   performed and the power switch fails is low.
  </p><p>
   For detailed information on how to configure monitor operations, refer to

   <a class="xref" href="#sec-ha-manual-config-monitor" title="7.4.9. Configuring Resource Monitoring">Section 7.4.9, “Configuring Resource Monitoring”</a> for the command line
   approach.
  </p></section><section class="sect1" id="sec-ha-fencing-special" data-id-title="Special Fencing Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.5 </span><span class="title-name">Special Fencing Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-special">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In addition to plug-ins which handle real STONITH devices, there are
   special purpose STONITH plug-ins.
  </p><div id="id-1.3.4.7.7.3" data-id-title="For Testing Only" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: For Testing Only</div><p>
    Some STONITH plug-ins mentioned below are for demonstration and
    testing purposes only. Do not use any of the following devices in
    real-life scenarios because this may lead to data corruption and
    unpredictable results:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">external/ssh </code>
     </p></li><li class="listitem"><p>
      <code class="literal">ssh </code>
     </p></li></ul></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-fence-kdump"><span class="term"><code class="literal">fence_kdump</code>
    </span></dt><dd><p>
      This plug-in checks if a Kernel dump is in progress on a node. If so,
      it returns <code class="literal">true</code>, and acts as if the node has been
      fenced. The node cannot run any resources during the dump anyway. This
      avoids fencing a node that is already down but doing a dump, which
      takes some time. The plug-in must be used in concert with another,
      real STONITH device.
     </p><p>
      For configuration details, see <a class="xref" href="#ex-ha-fencing-kdump" title="Configuration of a Kdump Device">Example 9.3, “Configuration of a Kdump Device”</a>.
     </p></dd><dt id="id-1.3.4.7.7.4.2"><span class="term"><code class="literal">external/sbd</code>
    </span></dt><dd><p>
      This is a self-fencing device. It reacts to a so-called <span class="quote">“<span class="quote">poison
      pill</span>”</span> which can be inserted into a shared disk. On
      shared-storage connection loss, it stops the node from operating.
      Learn how to use this STONITH agent to implement storage-based
      fencing in
      <a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10</a>,
      <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the Cluster to Use SBD">Procedure 10.7, “Configuring the Cluster to Use SBD”</a>. See also
      <a class="link" href="https://github.com/ClusterLabs/sbd" target="_blank">https://github.com/ClusterLabs/sbd</a>
      for more details.
     </p><div id="id-1.3.4.7.7.4.2.2.2" data-id-title="external/sbd and DRBD" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="literal">external/sbd</code> and DRBD</div><p>
       The <code class="literal">external/sbd</code> fencing mechanism requires that
       the SBD partition is readable directly from each node. Thus, a DRBD*
       device must not be used for an SBD partition.
      </p><p>
       However, you can use the fencing mechanism for a DRBD cluster,
       provided the SBD partition is located on a shared disk that is not
       mirrored or replicated.
      </p></div></dd><dt id="id-1.3.4.7.7.4.3"><span class="term"><code class="literal">external/ssh</code>
    </span></dt><dd><p>
      Another software-based <span class="quote">“<span class="quote">fencing</span>”</span> mechanism. The nodes
      must be able to log in to each other as <code class="systemitem">root</code> without passwords.
      It takes a single parameter, <code class="literal">hostlist</code>, specifying
      the nodes that it will target. As it is not able to reset a truly
      failed node, it must not be used for real-life clusters—for
      testing and demonstration purposes only. Using it for shared storage
      would result in data corruption.
     </p></dd><dt id="id-1.3.4.7.7.4.4"><span class="term"><code class="literal">meatware</code>
    </span></dt><dd><p>
      <code class="literal">meatware</code> requires help from the user to operate.
      Whenever invoked, <code class="literal">meatware</code> logs a CRIT severity
      message which shows up on the node's console. The operator then
      confirms that the node is down and issues a
      <code class="command">meatclient(8)</code> command. This tells
      <code class="literal">meatware</code> to inform the cluster that the node should
      be considered dead. See
      <code class="filename">/usr/share/doc/packages/cluster-glue/README.meatware</code>
      for more information.
     </p></dd><dt id="id-1.3.4.7.7.4.5"><span class="term"><code class="literal">suicide</code>
    </span></dt><dd><p>
      This is a software-only device, which can reboot a node it is running
      on, using the <code class="command">reboot</code> command. This requires action
      by the node's operating system and can fail under certain
      circumstances. Therefore avoid using this device whenever possible.
      However, it is safe to use on one-node clusters.
     </p></dd><dt id="id-1.3.4.7.7.4.6"><span class="term">Diskless SBD</span></dt><dd><p>This configuration is useful if you want a fencing mechanism without
     shared storage. In this diskless mode, SBD fences nodes by using the
     hardware watchdog without relying on any shared device.
     However, diskless SBD cannot handle a split brain scenario for
     a two-node cluster. Therefore, three or more nodes are required for
     using diskless SBD.</p></dd></dl></div><p>
   <code class="literal">suicide</code>

   is the only exception to the <span class="quote">“<span class="quote">I do not shoot my host</span>”</span> rule.
  </p></section><section class="sect1" id="sec-ha-fencing-recommend" data-id-title="Basic Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.6 </span><span class="title-name">Basic Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-recommend">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Check the following list of recommendations to avoid common mistakes:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Do not configure several power switches in parallel.
    </p></li><li class="listitem"><p>
     To test your STONITH devices and their configuration, pull the plug
     once from each node and verify that fencing the node does takes place.
    </p></li><li class="listitem"><p>
     Test your resources under load and verify the timeout values are
     appropriate. Setting timeout values too low can trigger (unnecessary)
     fencing operations. For details, refer to
     <a class="xref" href="#sec-ha-config-basics-timeouts" title="5.3.9. Timeout Values">Section 5.3.9, “Timeout Values”</a>.
    </p></li><li class="listitem"><p>
     Use appropriate fencing devices for your setup. For details, also refer
     to <a class="xref" href="#sec-ha-fencing-special" title="9.5. Special Fencing Devices">Section 9.5, “Special Fencing Devices”</a>.
    </p></li><li class="listitem"><p>
     Configure one or more STONITH resources. By default, the global
     cluster option <code class="literal">stonith-enabled</code> is set to
     <code class="literal">true</code>. If no STONITH resources have been defined,
     the cluster will refuse to start any resources.
    </p></li><li class="listitem"><p>
     Do not set the global cluster option
     <code class="systemitem">stonith-enabled</code> to <code class="literal">false</code>
     for the following reasons:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Clusters without STONITH enabled are not supported.
      </p></li><li class="listitem"><p>
       DLM/OCFS2 will block forever waiting for a fencing operation that
       will never happen.
      </p></li></ul></div></li><li class="listitem"><p>
     Do not set the global cluster option
     <code class="systemitem">startup-fencing</code> to <code class="literal">false</code>.
     By default, it is set to <code class="literal">true</code> for the following
     reason: If a node is in an unknown state during cluster start-up, the
     node will be fenced once to clarify its status.
    </p></li></ul></div></section><section class="sect1" id="sec-ha-fencing-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.7 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.9.2.1"><span class="term"><code class="filename">/usr/share/doc/packages/cluster-glue</code>
    </span></dt><dd><p>
      In your installed system, this directory contains README files for
      many STONITH plug-ins and devices.
     </p></dd><dt id="id-1.3.4.7.9.2.2"><span class="term"><a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>
    </span></dt><dd><p>
       <em class="citetitle">Pacemaker Explained</em>: Explains the concepts used to configure Pacemaker.
      Contains comprehensive and detailed information for reference.
     </p></dd><dt id="id-1.3.4.7.9.2.3"><span class="term"><a class="link" href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html" target="_blank">http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html</a>
    </span></dt><dd><p>
      Article explaining the concepts of split brain, quorum and fencing in
      HA clusters.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-ha-storage-protect" data-id-title="Storage Protection and SBD"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Storage Protection and SBD</span></span> <a title="Permalink" class="permalink" href="#cha-ha-storage-protect">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    SBD (STONITH Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </p><p>
    The <span class="package">ha-cluster-bootstrap</span> scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <em class="citetitle">Installation and Setup Quick Start</em>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </p><p>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split brain scenario.
   </p><p>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM2 exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </p></div></div></div></div><section class="sect1" id="sec-ha-storage-protect-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>SBD expands to <span class="emphasis"><em>Storage-Based Death</em></span> or
        <span class="emphasis"><em>STONITH Block Device</em></span>.
      </p><p>
        The highest priority of the High Availability cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </p><p>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. If this
        so-called split brain scenario were allowed to unfold, data corruption
        might occur.
      </p><p>
        Node fencing via STONITH is the primary mechanism to prevent this.
        Using SBD as a node fencing mechanism is one way of shutting down nodes
        without using an external power off device in case of a split brain scenario.
      </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">SBD Components and Mechanisms </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.8.3.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.4.8.3.6.2"><span class="term">SBD Partition</span></dt><dd><p> In an environment where all nodes have access to shared storage, a
      small partition of the device is formatted for use with SBD. The size of
      the partition depends on the block size of the used disk (for example,
      1 MB for standard SCSI disks with 512 byte block size or
      4 MB for DASD disks with 4 kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</p></dd><dt id="id-1.3.4.8.3.6.3"><span class="term">SBD Daemon</span></dt><dd><p> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </p></dd><dt id="id-1.3.4.8.3.6.4"><span class="term">Messages</span></dt><dd><p>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </p><p>
      Also, the daemon constantly monitors connectivity to the storage device, and
      terminates itself in case the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure: The workload will terminate
      anyway if the storage connectivity has been lost.
     </p></dd><dt id="id-1.3.4.8.3.6.5"><span class="term">Watchdog</span></dt><dd><p>
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <span class="emphasis"><em>hardware watchdog</em></span>
      that needs to be <span class="quote">“<span class="quote">tickled</span>”</span> or <span class="quote">“<span class="quote">fed</span>”</span> by a
      software component. The software component (in this case, the SBD daemon)
      <span class="quote">“<span class="quote">feeds</span>”</span> the watchdog by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      will enforce a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an I/O error.
     </p></dd></dl></div><p>
   If Pacemaker integration is activated, SBD will not self-fence if device
   majority is lost. For example, your cluster contains three nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions: one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would immediately commit suicide, but nodes B and C would
   continue to run.
   </p></section><section class="sect1" id="sec-ha-storage-protect-steps" data-id-title="Overview of Manually Setting Up SBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Overview of Manually Setting Up SBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-steps">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following steps are necessary to manually set up storage-based fencing.
  They must be executed as <code class="systemitem">root</code>. Before you start, check <a class="xref" href="#sec-ha-storage-protect-req" title="10.3. Requirements">Section 10.3, “Requirements”</a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     <a class="xref" href="#sec-ha-storage-protect-watchdog" title="10.6. Setting Up the Watchdog">Setting Up the Watchdog</a>
    </p></li><li class="step"><p>Depending on your scenario, either use SBD with one to three devices or in diskless mode.
     For an outline, see <a class="xref" href="#sec-ha-storage-protect-fencing-number" title="10.4. Number of SBD Devices">Section 10.4, “Number of SBD Devices”</a>. The detailed setup
     is described in:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <a class="xref" href="#sec-ha-storage-protect-fencing-setup" title="10.7. Setting Up SBD with Devices">Setting Up SBD with Devices</a>
      </p></li><li class="listitem"><p>
       <a class="xref" href="#sec-ha-storage-protect-diskless-sbd" title="10.8. Setting Up Diskless SBD">Setting Up Diskless SBD</a>
      </p></li></ul></div></li><li class="step"><p>
     <a class="xref" href="#sec-ha-storage-protect-test" title="10.9. Testing SBD and Fencing">Testing SBD and Fencing</a>
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-storage-protect-req" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-req">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You can use up to three SBD devices for storage-based fencing.
     When using one to three devices, the shared storage must be accessible from all nodes.</p></li><li class="listitem"><p>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <code class="filename">/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</code>.
     </p></li><li class="listitem"><p>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. In virtualized
     environments, the hypervisor might provide shared block devices. In any
     case, content on that shared block device needs to be consistent for
     all cluster nodes. Make sure that caching does not break that
     consistency.
    </p></li><li class="listitem"><p> The shared storage segment <span class="emphasis"><em>must not</em></span>
     use host-based RAID, LVM2, or DRBD*. DRBD can be split, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </p></li><li class="listitem"><p> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </p></li><li class="listitem"><p>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </p></li><li class="listitem"><p>For clusters with more than two nodes, you can also use SBD in
    <span class="emphasis"><em>diskless</em></span> mode.
   </p></li></ul></div></section><section class="sect1" id="sec-ha-storage-protect-fencing-number" data-id-title="Number of SBD Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.4 </span><span class="title-name">Number of SBD Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-fencing-number">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p> SBD supports the use of up to three devices: </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.6.3.1"><span class="term">One Device</span></dt><dd><p>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </p></dd><dt id="id-1.3.4.8.6.3.2"><span class="term">Two Devices</span></dt><dd><p>
      This configuration is primarily useful for environments that use
      host-based mirroring but where no third storage device is available.
      SBD will not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      will not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </p></dd><dt id="id-1.3.4.8.6.3.3"><span class="term">Three Devices</span></dt><dd><p>
      The most reliable configuration. It is resilient against outages of
      one device—be it because of failures or maintenance. SBD
      will terminate itself only if more than one device is lost and if required,
      depending on the status of the cluster partition or node. If at least
      two devices are still accessible, fencing messages can be successfully
      transmitted.
     </p><p>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </p></dd><dt id="id-1.3.4.8.6.3.4"><span class="term">Diskless</span></dt><dd><p>This configuration is useful if you want a fencing mechanism without
     shared storage. In this diskless mode, SBD fences nodes by using the
     hardware watchdog without relying on any shared device.
     However, diskless SBD cannot handle a split brain scenario for
     a two-node cluster. Therefore, three or more nodes are required for
     using diskless SBD.</p></dd></dl></div></section><section class="sect1" id="sec-ha-storage-protect-watchdog-timings" data-id-title="Calculation of Timeouts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.5 </span><span class="title-name">Calculation of Timeouts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-watchdog-timings">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      When using SBD as a fencing mechanism, it is vital to consider the timeouts
      of all components, because they depend on each other.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.7.3.1"><span class="term">Watchdog Timeout</span></dt><dd><p>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node might self-fence.
       </p><div id="id-1.3.4.8.7.3.1.2.2" data-id-title="Multipath or iSCSI Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Multipath or iSCSI Setup</div><p>
          If your SBD device(s) reside on a multipath setup or iSCSI, the timeout
          should be  set to the time required to detect a path failure and switch
          to the next path.
          </p><p>
           This also means that in <code class="filename">/etc/multipath.conf</code> the
           value of  <code class="literal">max_polling_interval</code> must be less than
           <code class="literal">watchdog</code> timeout.
         </p></div></dd><dt id="id-1.3.4.8.7.3.2"><span class="term"><code class="literal">msgwait</code> Timeout</span></dt><dd><p>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </p><p>
        However, if the <code class="literal">msgwait</code> timeout is relatively long,
        a fenced cluster node might rejoin before the fencing action returns.
        This can be mitigated by setting the <code class="varname">SBD_DELAY_START</code>
        parameter in the SBD configuration, as described in
        <a class="xref" href="#pro-ha-storage-protect-sbd-config" title="Editing the SBD Configuration File">Procedure 10.4</a>
        in
        <a class="xref" href="#st-ha-storage-protect-sbd-delay-start" title="Step 4">Step 4</a>.
       </p></dd><dt id="id-1.3.4.8.7.3.3"><span class="term"><code class="literal">stonith-timeout</code> in the CIB</span></dt><dd><p>
        This timeout is set in the CIB as a global cluster property. It defines
        how long to wait for the STONITH action (reboot, on, off) to complete.
       </p></dd><dt id="id-1.3.4.8.7.3.4"><span class="term"><code class="literal">stonith-watchdog-timeout</code> in the CIB</span></dt><dd><p>
        This timeout is set in the CIB as a global cluster property. If not set
        explicitly, it defaults to <code class="literal">0</code>, which is appropriate for
        using SBD with one to three devices. For use of SBD in diskless mode, see <a class="xref" href="#pro-ha-storage-protect-confdiskless" title="Configuring Diskless SBD">Procedure 10.8, “Configuring Diskless SBD”</a> for more details.</p></dd></dl></div><p>
   If you change the watchdog timeout, you need to adjust the other two timeouts
   as well. The following <span class="quote">“<span class="quote">formula</span>”</span> expresses the relationship
   between these three values:
  </p><div class="example" id="ex-ha-storage-protect-sbd-timings" data-id-title="Formula for Timeout Calculation"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 10.1: </span><span class="title-name">Formula for Timeout Calculation </span></span><a title="Permalink" class="permalink" href="#ex-ha-storage-protect-sbd-timings">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">Timeout (msgwait) &gt;= (Timeout (watchdog) * 2)
stonith-timeout = Timeout (msgwait) + 20%</pre></div></div></div><p>
    For example, if you set the watchdog timeout to <code class="literal">120</code>,
    set the <code class="literal">msgwait</code> timeout to <code class="literal">240</code> and the
    <code class="literal">stonith-timeout</code> to <code class="literal">288</code>.
   </p><p>
     If you use the <span class="package">ha-cluster-bootstrap</span> scripts to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </p></section><section class="sect1" id="sec-ha-storage-protect-watchdog" data-id-title="Setting Up the Watchdog"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.6 </span><span class="title-name">Setting Up the Watchdog</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-watchdog">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p> SUSE Linux Enterprise High Availability ships with several kernel modules that provide
   hardware-specific watchdog drivers. For a list of the most commonly used
   ones, see <a class="xref" href="#tab-ha-storage-protect-watchdog-drivers" title="Commonly Used Watchdog Drivers">Commonly Used Watchdog Drivers</a>.
 </p><p>
  For clusters in production environments we recommend to use a hardware-specific
  watchdog driver. However, if no watchdog matches your hardware,
  <code class="systemitem">softdog</code> can be used as kernel
  watchdog module.
 </p><p>
   SUSE Linux Enterprise High Availability uses the SBD daemon as the software component that <span class="quote">“<span class="quote">feeds</span>”</span>
   the watchdog.</p><section class="sect2" id="sec-ha-storage-protect-hw-watchdog" data-id-title="Using a Hardware Watchdog"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.6.1 </span><span class="title-name">Using a Hardware Watchdog</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-hw-watchdog">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails very often. As a result, lots of modules
    are already loaded before the right one gets a chance.</p><p>
   <a class="xref" href="#tab-ha-storage-protect-watchdog-drivers" title="Commonly Used Watchdog Drivers">Table 10.1</a>
   lists the most commonly used watchdog drivers. If your hardware is not listed there,
   the directories
   <code class="filename">/lib/modules/<em class="replaceable">KERNEL_VERSION</em>/kernel/drivers/watchdog</code>
   or
   <code class="filename">/lib/modules/<em class="replaceable">KERNEL_VERSION</em>/kernel/drivers/ipmi</code>
   give you a list of choices, too. Alternatively, ask your hardware or
   system vendor for details on system specific watchdog configuration.
   </p><div class="table" id="tab-ha-storage-protect-watchdog-drivers" data-id-title="Commonly Used Watchdog Drivers"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 10.1: </span><span class="title-name">Commonly Used Watchdog Drivers </span></span><a title="Permalink" class="permalink" href="#tab-ha-storage-protect-watchdog-drivers">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Hardware</th><th style="border-bottom: 1px solid ; ">Driver</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HP</td><td style="border-bottom: 1px solid ; "><code class="systemitem">hpwdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Dell, Lenovo (Intel TCO)</td><td style="border-bottom: 1px solid ; "><code class="systemitem">iTCO_wdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Fujitsu</td><td style="border-bottom: 1px solid ; "><code class="systemitem">ipmi_watchdog</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VM on z/VM on IBM mainframe</td><td style="border-bottom: 1px solid ; "><code class="systemitem">vmwatchdog</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Xen VM (DomU)</td><td style="border-bottom: 1px solid ; "><code class="systemitem">xen_wdt</code></td></tr><tr><td style="border-right: 1px solid ; ">Generic</td><td><code class="systemitem">softdog</code></td></tr></tbody></table></div></div><div id="id-1.3.4.8.8.5.5" data-id-title="Accessing the Watchdog Timer" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Accessing the Watchdog Timer</div><p>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If the watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </p></div><div class="procedure" id="pro-ha-storage-protect-watchdog" data-id-title="Loading the Correct Kernel Module"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.1: </span><span class="title-name">Loading the Correct Kernel Module </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-watchdog">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>To make sure the correct watchdog module is loaded, proceed as follows:</p><ol class="procedure" type="1"><li class="step"><p>List the drivers that have been installed with your kernel version:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rpm</code> -ql kernel-<em class="replaceable">VERSION</em> | <code class="command">grep</code> watchdog</pre></div></li><li class="step" id="st-ha-storage-listwatchdog-modules"><p>List any watchdog modules that are currently loaded in the kernel:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">egrep</code> "(wd|dog)"</pre></div></li><li class="step"><p>If you get a result, unload the wrong module:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rmmod</code> <em class="replaceable">WRONG_MODULE</em></pre></div></li><li class="step"><p> Enable the watchdog module that matches your hardware: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo</code> <em class="replaceable">WATCHDOG_MODULE</em> &gt; /etc/modules-load.d/watchdog.conf
<code class="prompt root"># </code><code class="command">systemctl</code> restart systemd-modules-load</pre></div></li><li class="step"><p>Test whether the watchdog module is loaded correctly:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">egrep</code> "(wd|dog)"</pre></div></li><li class="step"><p>Verify if the watchdog device is available and works:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ls -l</code> /dev/watchdog*
<code class="prompt root"># </code><code class="command">sbd</code> query-watchdog</pre></div><p> If your watchdog device is not available, stop here and check the
      module name and options. Maybe use another driver. </p></li><li class="step"><p>
      Verify if the watchdog device works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -w <em class="replaceable">WATCHDOG_DEVICE</em> test-watchdog</pre></div></li><li class="step"><p>
      Reboot your machine to make sure there are no conflicting kernel modules. For example,
      if you find the message <code class="literal">cannot register ...</code> in your log, this would indicate
      such conflicting modules. To ignore such modules, refer to <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist" target="_blank">https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist</a>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-storage-protect-sw-watchdog" data-id-title="Using the Software Watchdog (softdog)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.6.2 </span><span class="title-name">Using the Software Watchdog (softdog)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-sw-watchdog">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <code class="systemitem">softdog</code> can be used as kernel watchdog module. </p><div id="id-1.3.4.8.8.6.3" data-id-title="Softdog Limitations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Softdog Limitations</div><p>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </p></div><div class="procedure" id="pro-ha-storage-protect-sw-watchdog" data-id-title="Loading the Softdog Kernel Module"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.2: </span><span class="title-name">Loading the Softdog Kernel Module </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sw-watchdog">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Enable the softdog driver:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo</code> softdog &gt; /etc/modules-load.d/watchdog.conf</pre></div></li><li class="step"><p>Add the <code class="systemitem">softdog</code>
          module in <code class="filename">/etc/modules-load.d/watchdog.conf</code>
          and restart a service:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo</code> softdog &gt; /etc/modules-load.d/watchdog.conf
<code class="prompt root"># </code><code class="command">systemctl</code> restart systemd-modules-load</pre></div></li><li class="step"><p>Test whether the softdog watchdog module is loaded correctly:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">grep</code> softdog</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-storage-protect-fencing-setup" data-id-title="Setting Up SBD with Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.7 </span><span class="title-name">Setting Up SBD with Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-fencing-setup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps are necessary for setup:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Initializing the SBD Devices</a>
        </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-config" title="Editing the SBD Configuration File">Editing the SBD Configuration File</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-services" title="Enabling and Starting the SBD Service">Enabling and Starting the SBD Service</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-test" title="Testing the SBD Devices">Testing the SBD Devices</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the Cluster to Use SBD">Configuring the Cluster to Use SBD</a>
    </p></li></ol></div></div><p>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <a class="xref" href="#sec-ha-storage-protect-req" title="10.3. Requirements">Section 10.3</a>.
  </p><p>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="10.5. Calculation of Timeouts">Section 10.5, “Calculation of Timeouts”</a>.
  </p><p>
   The node will terminate itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </p><div class="procedure" id="pro-ha-storage-protect-sbd-create" data-id-title="Initializing the SBD Devices"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.3: </span><span class="title-name">Initializing the SBD Devices </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-create">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <code class="command">sbd create</code> command
    will write a metadata header to the specified device or devices. It will also
    initialize the messaging slots for up to 255 nodes. If executed without any
    further options, the command will use the default timeout settings.</p><div id="id-1.3.4.8.9.7.3" data-id-title="Overwriting Existing Data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Overwriting Existing Data</div><p> Make sure the device or devices you want to use for SBD do not hold any
       important data. When you execute the <code class="command">sbd create</code>
       command, roughly the first megabyte of the specified block devices
       will be overwritten without further requests or backup.
      </p></div><ol class="procedure" type="1"><li class="step"><p>Decide which block device or block devices to use for SBD.</p></li><li class="step"><p>Initialize the SBD device with the following command: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD</em> create</pre></div><p>(Replace <code class="filename">/dev/<em class="replaceable">SBD</em></code>
       with your actual path name, for example:
       <code class="filename">/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</code>.)</p><p> To use more than one device for SBD, specify the <code class="option">-d</code> option multiple times, for
      example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD1</em> -d /dev/<em class="replaceable">SBD2</em> -d /dev/<em class="replaceable">SBD3</em> create</pre></div></li><li class="step"><p>If your SBD device resides on a multipath group, use the <code class="option">-1</code>
      and <code class="option">-4</code> options to adjust the timeouts to use for SBD. For
      details, see <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="10.5. Calculation of Timeouts">Section 10.5, “Calculation of Timeouts”</a>.
      All timeouts are given in seconds:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD</em> -4 180<span class="callout" id="co-ha-sbd-msgwait">1</span> -1 60<span class="callout" id="co-ha-sbd-watchdog">2</span> create</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-msgwait"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p> The <code class="option">-4</code> option is used to specify the
         <code class="literal">msgwait</code> timeout. In the example above, it is set to
         <code class="literal">180</code> seconds. </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-watchdog"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p> The <code class="option">-1</code> option is used to specify the
         <code class="literal">watchdog</code> timeout. In the example above, it is set
        to <code class="literal">60</code> seconds. The minimum allowed value for the
        emulated watchdog is <code class="literal">15</code> seconds. </p></td></tr></table></div></li><li class="step"><p>Check what has been written to the device: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD</em> dump
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 60
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 180
==Header on disk /dev/<em class="replaceable">SBD</em> is dumped</pre></div><p> As you can see, the timeouts are also stored in the header, to ensure
    that all participating nodes agree on them. </p></li></ol></div></div><p>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </p><div class="procedure" id="pro-ha-storage-protect-sbd-config" data-id-title="Editing the SBD Configuration File"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.4: </span><span class="title-name">Editing the SBD Configuration File </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Open the file <code class="filename">/etc/sysconfig/sbd</code> and use
      the following entries:</p><div class="verbatim-wrap"><pre class="screen">SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</pre></div><p>
       The <code class="varname">SBD_DEVICE</code> entry is not needed as no shared
       disk is used. When this parameter is missing, the <code class="systemitem">sbd</code>
       service does not start any watcher process for SBD devices.
      </p></li><li class="step"><p>Search for the following parameter: <em class="parameter">SBD_DEVICE</em>.
     </p><p>It specifies the devices to monitor and to use for exchanging SBD messages.
     </p></li><li class="step"><p> Edit this line by replacing <em class="replaceable">SBD</em> with your SBD device:</p><div class="verbatim-wrap"><pre class="screen">SBD_DEVICE="/dev/<em class="replaceable">SBD</em>"</pre></div><p> If you need to specify multiple devices in the first line, separate them with semicolons
     (the order of the devices does not matter):</p><div class="verbatim-wrap"><pre class="screen">SBD_DEVICE="/dev/<em class="replaceable">SBD1</em>;/dev/<em class="replaceable">SBD2</em>;/dev/<em class="replaceable">SBD3</em>"</pre></div><p> If the SBD device is not accessible, the daemon will fail to start and inhibit
     cluster start-up. </p></li><li class="step" id="st-ha-storage-protect-sbd-delay-start"><p>Search for the following parameter: <em class="parameter">SBD_DELAY_START</em>.</p><p>
      Enables or disables a delay. Set <em class="parameter">SBD_DELAY_START</em>
      to <code class="literal">yes</code> if <code class="literal">msgwait</code> is relatively
      long, but your cluster nodes boot very fast.
      Setting this parameter to <code class="literal">yes</code> delays the start of
      SBD on boot.  This is sometimes necessary with virtual machines.
    </p></li></ol></div></div><p>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <code class="systemitem">sbd</code> service is started as a dependency whenever
  the <code class="systemitem">pacemaker</code> service is started.</p><div class="procedure" id="pro-ha-storage-protect-sbd-services" data-id-title="Enabling and Starting the SBD Service"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.5: </span><span class="title-name">Enabling and Starting the SBD Service </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-services">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>On each node, enable the SBD service:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable sbd</pre></div><p>It will be started together with the Corosync service whenever the Pacemaker
     service is started.</p></li><li class="step"><p>Restart the cluster stack on each node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> stop pacemaker
<code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div><p> This automatically triggers the start of the SBD daemon. </p></li></ol></div></div><p>
   As a next step, test the SBD devices as described in <a class="xref" href="#pro-ha-storage-protect-sbd-test" title="Testing the SBD Devices">Procedure 10.6</a>.
  </p><div class="procedure" id="pro-ha-storage-protect-sbd-test" data-id-title="Testing the SBD Devices"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.6: </span><span class="title-name">Testing the SBD Devices </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-test">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> The following command will dump the node slots and their current
      messages from the SBD device: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD</em> list</pre></div><p> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <code class="literal">clear</code> for both nodes:</p><div class="verbatim-wrap"><pre class="screen">0       alice        clear
1       bob          clear</pre></div></li><li class="step"><p> Try sending a test message to one of the nodes: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -d /dev/<em class="replaceable">SBD</em> message alice test</pre></div></li><li class="step"><p> The node will acknowledge the receipt of the message in the system
      log files: </p><div class="verbatim-wrap"><pre class="screen">May 03 16:08:31 alice sbd[66139]: /dev/<em class="replaceable">SBD</em>: notice: servant: Received command test from bob on disk /dev/<em class="replaceable">SBD</em></pre></div><p> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </p></li></ol></div></div><p>
   As a final step, you need to adjust the cluster configuration as described in
   <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the Cluster to Use SBD">Procedure 10.7</a>.
  </p><div class="procedure" id="pro-ha-storage-protect-fencing" data-id-title="Configuring the Cluster to Use SBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.7: </span><span class="title-name">Configuring the Cluster to Use SBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-fencing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>Enter the following:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-enabled="true" <span class="callout" id="co-ha-sbd-st-enabled">1</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-watchdog-timeout=0 <span class="callout" id="co-ha-sbd-watchdog-timeout">2</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-timeout="220s" <span class="callout" id="co-ha-sbd-st-timeout">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-st-enabled"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This is the default configuration, because clusters without STONITH are not supported.
       But in case STONITH has been deactivated for testing purposes,
       make sure this parameter is set to <code class="literal">true</code> again.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-watchdog-timeout"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>If not explicitly set, this value defaults to <code class="literal">0</code>,
        which is appropriate for use of SBD with one to three devices.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-st-timeout"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       To calculate the <em class="parameter">stonith-timeout</em>, refer to
       <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="10.5. Calculation of Timeouts">Section 10.5, “Calculation of Timeouts”</a>.
       A <code class="systemitem">stonith-timeout</code> value of <code class="literal">220</code>
       would be appropriate if the <code class="literal">msgwait</code> timeout value for
       SBD was set to <code class="literal">30</code> seconds.</p></td></tr></table></div></li><li class="step" id="st-ha-storage-protect-fencing-static-random"><p>
    Configure the SBD STONITH resource. You do not need to clone this resource.
   </p><p>
    For a two-node cluster, in case of split brain, there will be fencing issued from
    each node to the other as expected. To prevent both nodes from being reset at practically
    the same time, it is recommended to apply the following fencing
    delays to help one of the nodes, or even the preferred node, win the fencing match.
    For clusters with more than two nodes, you do not need to apply these delays.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.9.15.5.3.1"><span class="term">Priority fencing delay</span></dt><dd><p>
        The <code class="literal">priority-fencing-delay</code> cluster property is disabled by
        default. By configuring a delay value, if the other node is lost and it has
        the higher total resource priority, the fencing targeting it will be delayed
        for the specified amount of time. This means that in case of split-brain,
        the more important node wins the fencing match .
      </p><p>
        Resources that matter can be configured with priority meta attribute. On
        calculation, the priority values of the resources or instances that are running
        on each node are summed up to be accounted. A promoted resource instance takes the
        configured base priority plus one, so that it receives a higher value than any
        unpromoted instance.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property priority-fencing-delay=30</pre></div><p>
        Even if <code class="literal">priority-fencing-delay</code> is used, we still
        recommend also using <code class="literal">pcmk_delay_base</code> or
        <code class="literal">pcmk_delay_max</code> as described below to address any
        situations where the nodes happen to have equal priority.
        The value of <code class="literal">priority-fencing-delay</code> should be significantly
        greater than the maximum of <code class="literal">pcmk_delay_base</code> / <code class="literal">pcmk_delay_max</code>,
        and preferably twice the maximum.
       </p></dd><dt id="id-1.3.4.8.9.15.5.3.2"><span class="term">Predictable static delays</span></dt><dd><p>This parameter adds a static delay before executing STONITH actions.
      To prevent the nodes from getting reset at the same time under split-brain of
      a two-node cluster, configure separate fencing resources with different delay values.
      The preferred node can be marked with the parameter to be targeted with a longer
      fencing delay, so that it wins any fencing match.
      To make this succeed, it is essential to create two primitive STONITH
      devices for each node. In the following configuration, alice will win
      and survive in case of a split brain scenario:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> st-sbd-alice stonith:external/sbd params \
pcmk_host_list=alice pcmk_delay_base=20
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> st-sbd-bob stonith:external/sbd params \
pcmk_host_list=bob pcmk_delay_base=0</pre></div></dd><dt id="id-1.3.4.8.9.15.5.3.3"><span class="term">Dynamic Random Delays</span></dt><dd><p>This parameter adds a random delay for STONITH actions on the fencing device.
       Rather than a static delay targeting a specific node, the parameter
       <em class="parameter">pcmk_delay_max</em> adds a random delay for any fencing
       with the fencing resource to prevent double reset. Unlike
       <em class="parameter">pcmk_delay_base</em>, this parameter can be specified for
       an unified fencing resource targeting multiple nodes.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> stonith_sbd stonith:external/sbd \
params pcmk_delay_max=30</pre></div></dd></dl></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     Submit your changes with <code class="command">commit</code> and leave the crm live
     configuration with <code class="command">exit</code>.
    </p></li></ol></div></div><p> After the resource has started, your cluster is successfully
    configured for use of SBD. It will use this method in case a
    node needs to be fenced.</p></section><section class="sect1" id="sec-ha-storage-protect-diskless-sbd" data-id-title="Setting Up Diskless SBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.8 </span><span class="title-name">Setting Up Diskless SBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-diskless-sbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>SBD can be operated in a diskless mode. In this mode, a watchdog device
    will be used to reset the node in the following cases: if it loses quorum,
    if any monitored daemon is lost and not recovered, or if Pacemaker decides
    that the node requires fencing. Diskless SBD is based on
    <span class="quote">“<span class="quote">self-fencing</span>”</span> of a node, depending on the status of the cluster,
    the quorum and some reasonable assumptions. No STONITH SBD resource
    primitive is needed in the CIB.
   </p><div id="id-1.3.4.8.10.3" data-id-title="Number of Cluster Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Number of Cluster Nodes</div><p>
         Do <span class="emphasis"><em>not</em></span> use diskless SBD as a fencing mechanism
         for two-node clusters. Use it only in clusters with three or more
         nodes. SBD in diskless mode cannot handle split brain
         scenarios for two-node clusters.
      </p></div><div class="procedure" id="pro-ha-storage-protect-confdiskless" data-id-title="Configuring Diskless SBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.8: </span><span class="title-name">Configuring Diskless SBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-confdiskless">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Open the file <code class="filename">/etc/sysconfig/sbd</code> and use
      the following entries:</p><div class="verbatim-wrap"><pre class="screen">SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</pre></div><p>
       The <code class="varname">SBD_DEVICE</code> entry is not needed as no shared
       disk is used. When this parameter is missing, the <code class="systemitem">sbd</code>
       service does not start any watcher process for SBD devices.
      </p></li><li class="step"><p>On each node, enable the SBD service:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable sbd</pre></div><p>It will be started together with the Corosync service whenever the Pacemaker
      service is started.</p></li><li class="step"><p>Restart the cluster stack on each node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> stop pacemaker
<code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div><p> This automatically triggers the start of the SBD daemon. </p></li><li class="step"><p>
       Check if the parameter <em class="parameter">have-watchdog=true</em> has
       been automatically set:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show | grep have-watchdog
         have-watchdog=true</pre></div></li><li class="step"><p>Run <code class="command">crm configure</code> and set the following cluster
      properties on the crm shell:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-enabled="true" <span class="callout" id="co-ha-sbd-stonith-enabled">1</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-watchdog-timeout=10 <span class="callout" id="co-ha-sbd-diskless-watchdog-timeout">2</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-stonith-enabled"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This is the default configuration, because clusters without STONITH are not supported.
       But in case STONITH has been deactivated for testing purposes,
       make sure this parameter is set to <code class="literal">true</code> again.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-diskless-watchdog-timeout"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>For diskless SBD, this parameter must not equal zero.
       It defines after how long it is assumed that the fencing target has already
       self-fenced. Therefore its value needs to be &gt;= the value of
       <code class="varname">SBD_WATCHDOG_TIMEOUT</code> in <code class="filename">/etc/sysconfig/sbd</code>.
       Starting with SUSE Linux Enterprise High Availability 15, if you set <em class="parameter">stonith-watchdog-timeout</em>
       to a negative value, Pacemaker will automatically calculate this timeout
       and set it to twice the value of <em class="parameter">SBD_WATCHDOG_TIMEOUT</em>.
      </p></td></tr></table></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     Submit your changes with <code class="command">commit</code> and leave the crm live
     configuration with <code class="command">exit</code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-storage-protect-test" data-id-title="Testing SBD and Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.9 </span><span class="title-name">Testing SBD and Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-test">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>To test whether SBD works as expected for node fencing purposes, use one or all
    of the following methods:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.11.3.1"><span class="term">Manually Triggering Fencing of a Node</span></dt><dd><p>To trigger a fencing action for node <em class="replaceable">NODENAME</em>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node fence <em class="replaceable">NODENAME</em></pre></div><p>Check if the node is fenced and if the other nodes consider the node as fenced
      after the <em class="parameter">stonith-watchdog-timeout</em>.</p></dd><dt id="id-1.3.4.8.11.3.2"><span class="term">Simulating an SBD Failure</span></dt><dd><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Identify the process ID of the SBD inquisitor:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status sbd
● sbd.service - Shared-storage based fencing daemon

   Loaded: loaded (/usr/lib/systemd/system/sbd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-04-17 15:24:51 CEST; 6 days ago
     Docs: man:sbd(8)
  Process: 1844 ExecStart=/usr/sbin/sbd $SBD_OPTS -p /var/run/sbd.pid watch (code=exited, status=0/SUCCESS)
 Main PID: 1859 (sbd)
    Tasks: 4 (limit: 4915)
   CGroup: /system.slice/sbd.service
           ├─<span class="strong"><strong>1859 sbd: inquisitor</strong></span>
[...]</pre></div></li><li class="step"><p>Simulate an SBD failure by terminating the SBD inquisitor process.
       In our example, the process ID of the SBD inquisitor is
         <code class="literal">1859</code>):</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">kill</code> -9 1859</pre></div><p>
        The node proactively self-fences. The other nodes notice the loss of
        the node and consider it has self-fenced after the
        <em class="parameter">stonith-watchdog-timeout</em>.
       </p></li></ol></div></div></dd><dt id="id-1.3.4.8.11.3.3"><span class="term">Triggering Fencing through a Monitor Operation Failure</span></dt><dd><p>With a normal configuration, a failure of a resource <span class="emphasis"><em>stop operation</em></span>
      will trigger fencing. To trigger fencing manually, you can produce a failure
      of a resource stop operation. Alternatively, you can temporarily change
      the configuration of a resource <span class="emphasis"><em>monitor operation</em></span>
      and produce a monitor failure as described below:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Configure an <code class="literal">on-fail=fence</code> property for a resource monitor
        operation:</p><div class="verbatim-wrap"><pre class="screen">op monitor interval=10 on-fail=fence</pre></div></li><li class="step"><p>Let the monitoring operation fail (for example, by terminating the respective
        daemon, if the resource relates to a service).</p><p>This failure triggers a fencing action.</p></li></ol></div></div></dd></dl></div></section><section class="sect1" id="sec-ha-storage-protect-rsc-fencing" data-id-title="Additional Mechanisms for Storage Protection"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.10 </span><span class="title-name">Additional Mechanisms for Storage Protection</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-rsc-fencing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>Apart from node fencing via STONITH there are other methods to achieve
    storage protection at a resource level. For example, SCSI-3 and SCSI-4 use
    persistent reservations, whereas <code class="literal">sfex</code> provides a locking
    mechanism. Both methods are explained in the following subsections.
  </p><section class="sect2" id="sec-ha-storage-protect-sgpersist" data-id-title="Configuring an sg_persist Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.10.1 </span><span class="title-name">Configuring an sg_persist Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-sgpersist">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The SCSI specifications 3 and 4 define <span class="emphasis"><em>persistent reservations</em></span>.
    These are SCSI protocol features and can be used for I/O fencing and failover.
    This feature is implemented in the <code class="command">sg_persist</code> Linux
    command.
   </p><div id="id-1.3.4.8.12.4.4" data-id-title="SCSI Disk Compatibility" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: SCSI Disk Compatibility</div><p> Any backing disks for <code class="literal">sg_persist</code> must be SCSI
     disk compatible. <code class="literal">sg_persist</code> only works for devices like
     SCSI disks or iSCSI LUNs.
     
     Do <span class="emphasis"><em>not</em></span> use it for IDE, SATA, or any block devices
     which do not support the SCSI protocol. </p></div><p>Before you proceed, check if your disk supports
    persistent reservations. Use the following command (replace
     <em class="replaceable">DISK</em> with your device name):</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sg_persist</code> -n --in --read-reservation -d /dev/<em class="replaceable">DISK</em></pre></div><p>The result shows whether your disk supports persistent reservations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Supported disk:</p><div class="verbatim-wrap"><pre class="screen">PR generation=0x0, there is NO reservation held</pre></div></li><li class="listitem"><p>Unsupported disk:</p><div class="verbatim-wrap"><pre class="screen">PR in (Read reservation): command not supported
Illegal request, Invalid opcode</pre></div></li></ul></div><p>If you get an error message (like the one above), replace the old
    disk with an SCSI compatible disk. Otherwise proceed as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To create the primitive resource <code class="literal">sg_persist</code>,
      run the following commands as <code class="systemitem">root</code>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sg sg_persist \
    params devs="/dev/sdc" reservation_type=3 \
    op monitor interval=60 timeout=60</pre></div></li><li class="step"><p> Add the <code class="literal">sg_persist</code> primitive to a master-slave
      group: 
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms</code> ms-sg sg \
    meta master-max=1 notify=true</pre></div></li><li class="step"><p> Do some tests. When the resource is in master/slave status, you can
      mount and write on <code class="filename">/dev/sdc1</code> on the cluster node where
     the master instance is running, while you cannot write on the cluster node
      where the slave instance is running.</p></li><li class="step"><p> Add a file system primitive for Ext4: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ext4 Filesystem \
    params device="/dev/sdc1" directory="/mnt/ext4" fstype=ext4</pre></div></li><li class="step"><p> Add the following order relationship plus a collocation between the
      <code class="literal">sg_persist</code> master and the file system resource: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-ms-sg-before-ext4 inf: ms-sg:promote ext4:start
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-ext4-with-sg-persist inf: ext4 ms-sg:Master</pre></div></li><li class="step"><p> Check all your changes with the <code class="command">show changed</code> command.
     </p></li><li class="step"><p> Commit your changes. </p></li></ol></div></div><p>For more information, refer to the <code class="command">sg_persist</code> man
    page.</p></section><section class="sect2" id="sec-ha-storage-protect-exstoract" data-id-title="Ensuring Exclusive Storage Activation with sfex"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.10.2 </span><span class="title-name">Ensuring Exclusive Storage Activation with <code class="literal">sfex</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     
    This section introduces <code class="literal">sfex</code>, an additional low-level
    mechanism to lock access to shared storage exclusively to one node. Note
    that sfex does not replace STONITH. As sfex requires shared
    storage, it is recommended that the SBD node fencing mechanism described
    above is used on another partition of the storage.
   </p><p>
    By design, sfex cannot be used with workloads that require concurrency
    (such as OCFS2). It serves as a layer of protection for classic failover
    style workloads. This is similar to an SCSI-2 reservation in effect, but
    more general.
   </p><section class="sect3" id="sec-ha-storage-protect-exstoract-description" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.10.2.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract-description">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In a shared storage environment, a small partition of the storage is set
     aside for storing one or more locks.
    </p><p>
     Before acquiring protected resources, the node must first acquire the
     protecting lock. The ordering is enforced by Pacemaker. The sfex
     component ensures that even if Pacemaker were subject to a split brain
     situation, the lock will never be granted more than once.
    </p><p>
     These locks must also be refreshed periodically, so that a node's death
     does not permanently block the lock and other nodes can proceed.
    </p></section><section class="sect3" id="sec-ha-storage-protect-exstoract-requirements" data-id-title="Setup"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.10.2.2 </span><span class="title-name">Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In the following, learn how to create a shared partition for use with
     sfex and how to configure a resource for the sfex lock in the CIB. A
     single sfex partition can hold any number of locks, and needs 1 KB
     of storage space allocated per lock.
     By default, <code class="command">sfex_init</code> creates one lock on the partition.
    </p><div id="id-1.3.4.8.12.5.5.3" data-id-title="Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Requirements</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        The shared partition for sfex should be on the same logical unit as
        the data you want to protect.
       </p></li><li class="listitem"><p>
        The shared sfex partition must not use host-based RAID, nor DRBD.
       </p></li><li class="listitem"><p>
        Using an LVM2 logical volume is possible.
       </p></li></ul></div></div><div class="procedure" id="id-1.3.4.8.12.5.5.4" data-id-title="Creating an sfex Partition"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.9: </span><span class="title-name">Creating an sfex Partition </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.8.12.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a shared partition for use with sfex. Note the name of this
       partition and use it as a substitute for
       <code class="filename">/dev/sfex</code> below.
      </p></li><li class="step"><p>
       Create the sfex metadata with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sfex_init</code> -n 1 /dev/sfex</pre></div></li><li class="step"><p>
       Verify that the metadata has been created correctly:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sfex_stat</code> -i 1 /dev/sfex ; echo $?</pre></div><p>
       This should return <code class="literal">2</code>, since the lock is not
       currently held.
      </p></li></ol></div></div><div class="procedure" id="id-1.3.4.8.12.5.5.5" data-id-title="Configuring a Resource for the sfex Lock"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.10: </span><span class="title-name">Configuring a Resource for the sfex Lock </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.8.12.5.5.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       The sfex lock is represented via a resource in the CIB, configured as
       follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on-fail="fence"</pre></div></li><li class="step"><p>
       To protect resources via an sfex lock, create mandatory ordering and
       placement constraints between the resources to protect the sfex resource. If
       the resource to be protected has the ID
       <code class="literal">filesystem1</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> order-sfex-1 inf: sfex_1 filesystem1
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-sfex-1 inf: filesystem1 sfex_1</pre></div></li><li class="step"><p>
       If using group syntax, add the sfex resource as the first resource to
       the group:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> LAMP sfex_1 filesystem1 apache ipaddr</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="sec-ha-storage-protect-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.11 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For more details, see <code class="command">man sbd</code>.
   </p></section></section><section class="chapter" id="cha-ha-acl" data-id-title="Access Control Lists"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Access Control Lists</span></span> <a title="Permalink" class="permalink" href="#cha-ha-acl">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The cluster administration tools like crm shell (crmsh) or
    Hawk2 can be used by <code class="systemitem">root</code> or any user in the group
    <code class="systemitem">haclient</code>. By default, these
    users have full read/write access. To limit access or assign more
    fine-grained access rights, you can use <span class="emphasis"><em>Access control
    lists</em></span> (ACLs).
   </p><p>
    Access control lists consist of an ordered set of access rules. Each
    rule allows read or write access or denies access to a part of the
    cluster configuration. Rules are typically combined to produce a
    specific role, then users may be assigned to a role that matches their
    tasks.
   </p></div></div></div></div><div id="id-1.3.4.9.4" data-id-title="CIB Syntax Validation Version and ACL Differences" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: CIB Syntax Validation Version and ACL Differences</div><p>
   This ACL documentation only applies if your CIB is validated with the CIB
   syntax version <code class="literal">pacemaker-2.0</code> or higher. For details on
   how to check this and upgrade the CIB version, see
   <a class="xref" href="#note-ha-cib-upgrade" title="Note: Upgrading the CIB Syntax Version">Note: Upgrading the CIB Syntax Version</a>.
  </p><p>
   If you have upgraded from SUSE Linux Enterprise High Availability 11 SPx and kept your former
   CIB version, refer to the <em class="citetitle">Access Control List</em>
   chapter in the Administration Guide for SUSE Linux Enterprise High Availability 11 SP3 or earlier. It is
   available from <a class="link" href="http://www.suse.com/documentation/" target="_blank">http://www.suse.com/documentation/</a>.
  </p></div><section class="sect1" id="sec-ha-acl-require" data-id-title="Requirements and Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Requirements and Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-require">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you start using ACLs on your cluster, make sure the following
   conditions are fulfilled:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ensure you have the same users on all nodes in your cluster, either by
     using NIS, Active Directory, or by manually adding the same users to
     all nodes.
    </p></li><li class="listitem"><p>
     All users for whom you want to modify access rights with ACLs must
     belong to the <code class="systemitem">haclient</code>
     group.
    </p></li><li class="listitem"><p>
     All users need to run crmsh by its absolute path
     <code class="filename">/usr/sbin/crm</code>.
    </p></li><li class="listitem"><p>
     If non-privileged users want to run crmsh, their
     <code class="envar">PATH</code> variable needs to be extended with
     <code class="filename">/usr/sbin</code>.
    </p></li></ul></div><div id="id-1.3.4.9.5.4" data-id-title="Default Access Rights" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Default Access Rights</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      ACLs are an optional feature. By default, use of ACLs is disabled.
     </p></li><li class="listitem"><p>
      If ACLs are not enabled, <code class="systemitem">root</code> and all users belonging to the
      <code class="systemitem">haclient</code> group have full
      read/write access to the cluster configuration.
     </p></li><li class="listitem"><p>
      Even if ACLs are enabled and configured, both <code class="systemitem">root</code> and the
      default CRM owner <code class="systemitem">hacluster</code>
      <span class="emphasis"><em>always</em></span> have full access to the cluster
      configuration.
     </p></li></ul></div></div><p>
   To use ACLs you need some knowledge about XPath. XPath is a language for
   selecting nodes in an XML document. Refer to
   <a class="link" href="http://en.wikipedia.org/wiki/XPath" target="_blank">http://en.wikipedia.org/wiki/XPath</a> or look into the
   specification at <a class="link" href="http://www.w3.org/TR/xpath/" target="_blank">http://www.w3.org/TR/xpath/</a>.
  </p></section><section class="sect1" id="sec-ha-acl-enable" data-id-title="Enabling Use of ACLs in Your Cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Enabling Use of ACLs in Your Cluster</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-enable">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can start configuring ACLs, you need to
   <span class="emphasis"><em>enable</em></span> use of ACLs. To do so, use the following
   command in the crmsh:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property enable-acl=true</pre></div><p>
   Alternatively, use Hawk2 as described in
   <a class="xref" href="#pro-ha-acl-enable-hawk2" title="Enabling Use of ACLs with Hawk2">Procedure 11.1, “Enabling Use of ACLs with Hawk2”</a>.
  </p><div class="procedure" id="pro-ha-acl-enable-hawk2" data-id-title="Enabling Use of ACLs with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.1: </span><span class="title-name">Enabling Use of ACLs with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-enable-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Configuration</span>
     to display the global cluster options and their current values.
    </p></li><li class="step"><p>
     Below <span class="guimenu">Cluster Configuration</span> click the empty drop-down box
     and select <span class="guimenu">enable-acl</span> to add the parameter. It is added
     with its default value <code class="literal">No</code>.
    </p></li><li class="step"><p>
     Set its value to <code class="literal">Yes</code> and apply your changes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-acl-basics" data-id-title="The Basics of ACLs"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.3 </span><span class="title-name">The Basics of ACLs</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-basics">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Access control lists consist of an ordered set of access rules. Each rule
   allows read or write access or denies access to a part of the cluster
   configuration. Rules are typically combined to produce a specific role,
   then users may be assigned to a role that matches their tasks. An ACL
   role is a set of rules which describe access rights to CIB. A rule
   consists of the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     an access right like <code class="literal">read</code>, <code class="literal">write</code>,
     or <code class="literal">deny</code>
    </p></li><li class="listitem"><p>
     a specification where to apply the rule. This specification can be a
     type, an ID reference, or an XPath expression.
    </p></li></ul></div><p>
   Usually, it is convenient to bundle ACLs into roles and assign a specific
   role to system users (ACL targets). There are two methods to create ACL
   rules:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-ha-acl-config-xpath" title="11.3.1. Setting ACL Rules via XPath Expressions">Section 11.3.1, “Setting ACL Rules via XPath Expressions”</a>. You need to know the
     structure of the underlying XML to create ACL rules.
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-ha-acl-config-tag" title="11.3.2. Setting ACL Rules via Abbreviations">Section 11.3.2, “Setting ACL Rules via Abbreviations”</a>. Create a shorthand syntax and
     ACL rules to apply to the matched objects.
    </p></li></ul></div><section class="sect2" id="sec-ha-acl-config-xpath" data-id-title="Setting ACL Rules via XPath Expressions"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.1 </span><span class="title-name">Setting ACL Rules via XPath Expressions</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-xpath">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To manage ACL rules via XPath, you need to know the structure of the
    underlying XML. Retrieve the structure with the following command that
    shows your cluster configuration in XML (see
    <a class="xref" href="#ex-ha-acl-excerpt" title="Excerpt of a Cluster Configuration in XML">Example 11.1</a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show xml</pre></div><div class="example" id="ex-ha-acl-excerpt" data-id-title="Excerpt of a Cluster Configuration in XML"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 11.1: </span><span class="title-name">Excerpt of a Cluster Configuration in XML </span></span><a title="Permalink" class="permalink" href="#ex-ha-acl-excerpt">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;num_updates="59"
      dc-uuid="175704363"
      crm_feature_set="3.0.9"
      validate-with="pacemaker-2.0"
      epoch="96"
      admin_epoch="0"
      cib-last-written="Fri Aug  8 13:47:28 2014"
      have-quorum="1"&gt;
  &lt;configuration&gt;
    &lt;crm_config&gt;
       &lt;cluster_property_set id="cib-bootstrap-options"&gt;
        &lt;nvpair name="stonith-enabled" value="true" id="cib-bootstrap-options-stonith-enabled"/&gt;
       [...]
      &lt;/cluster_property_set&gt;
    &lt;/crm_config&gt;
    &lt;nodes&gt;
      &lt;node id="175704363" uname="alice"/&gt;
      &lt;node id="175704619" uname="bob"/&gt;
    &lt;/nodes&gt;
    &lt;resources&gt; [...]  &lt;/resources&gt;
    &lt;constraints/&gt;
    &lt;rsc_defaults&gt; [...] &lt;/rsc_defaults&gt;
    &lt;op_defaults&gt; [...] &lt;/op_defaults&gt;
  &lt;configuration&gt;
&lt;/cib&gt;</pre></div></div></div><p>
    With the XPath language you can locate nodes in this XML document. For
    example, to select the root node (<code class="literal">cib</code>) use the XPath
    expression <code class="literal">/cib</code>. To locate the global cluster
    configurations, use the XPath expression
    <code class="literal">/cib/configuration/crm_config</code>.
   </p><p>
    As an example, <a class="xref" href="#tab-ha-acl-operator" title="Operator Role—Access Types and XPath Expressions">Table 11.1, “Operator Role—Access Types and XPath Expressions”</a> shows the
    parameters (access type and XPath expression) to create an
    <span class="quote">“<span class="quote">operator</span>”</span> role. Users with this role can only execute the
    tasks mentioned in the second column—they cannot reconfigure
    any resources (for example, change parameters or operations), nor change
    the configuration of colocation or ordering constraints.
   </p><div class="table" id="tab-ha-acl-operator" data-id-title="Operator Role—Access Types and XPath Expressions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 11.1: </span><span class="title-name">Operator Role—Access Types and XPath Expressions </span></span><a title="Permalink" class="permalink" href="#tab-ha-acl-operator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Type
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         XPath/Explanation
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//crm_config//nvpair[@name='maintenance-mode']</pre></div>
        <p>
         Turn cluster maintenance mode on or off.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//op_defaults//nvpair[@name='record-pending']</pre></div>
        <p>
         Choose whether pending operations are recorded.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//nodes/node//nvpair[@name='standby']</pre></div>
        <p>
         Set node in online or standby mode.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//resources//nvpair[@name='target-role']</pre></div>
        <p>
         Start, stop, promote or demote any resource.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//resources//nvpair[@name='maintenance']</pre></div>
        <p>
         Select if a resource should be put to maintenance mode or not.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Write
        </p>
       </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">//constraints/rsc_location</pre></div>
        <p>
         Migrate/move resources from one node to another.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         Read
        </p>
       </td><td>
<div class="verbatim-wrap"><pre class="screen">/cib</pre></div>
        <p>
         View the status of the cluster.
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-ha-acl-config-tag" data-id-title="Setting ACL Rules via Abbreviations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.2 </span><span class="title-name">Setting ACL Rules via Abbreviations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-tag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For users who do not want to deal with the XML structure there is an
    easier method.

   </p><p>
    For example, consider the following XPath:
   </p><div class="verbatim-wrap"><pre class="screen">//*[@id="rsc1"]</pre></div><p>
    which locates all the XML nodes with the ID <code class="literal">rsc1</code>.
   </p><p>
    The abbreviated syntax is written like this:
   </p><div class="verbatim-wrap"><pre class="screen">ref:"rsc1"</pre></div><p>
    This also works for constraints. Here is the verbose XPath:
   </p><div class="verbatim-wrap"><pre class="screen">//constraints/rsc_location</pre></div><p>
    The abbreviated syntax is written like this:
   </p><div class="verbatim-wrap"><pre class="screen">type:"rsc_location"</pre></div><p>
    The abbreviated syntax can be used in crmsh and Hawk2. The CIB
    daemon knows how to apply the ACL rules to the matching objects.
   </p></section></section><section class="sect1" id="sec-ha-acl-config-hawk2" data-id-title="Configuring ACLs with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.4 </span><span class="title-name">Configuring ACLs with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following procedures show how to configure read-only access to the
   cluster configuration by defining a <code class="literal">monitor</code> role and
   assigning it to a user. Alternatively, you can use crmsh to do so,
   as described in <a class="xref" href="#pro-ha-acl-crm" title="Adding a Monitor Role and Assigning a User with crmsh">Procedure 11.4, “Adding a Monitor Role and Assigning a User with crmsh”</a>.
  </p><div class="procedure" id="pro-ha-acl-hawk2-role" data-id-title="Adding a Monitor Role with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.2: </span><span class="title-name">Adding a Monitor Role with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-hawk2-role">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Roles</span>.
    </p></li><li class="step"><p>
      Click <span class="guimenu">Create</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Role ID</span>, for example,
     <code class="literal">monitor</code>.
    </p></li><li class="step"><p>
      As access <span class="guimenu">Right</span>, select <code class="literal">Read</code>.
    </p></li><li class="step"><p>
     As <span class="guimenu">Xpath</span>, enter the XPath expression
     <code class="literal">/cib</code>.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-acl-role.png"><img src="images/hawk2-acl-role.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Click <span class="guimenu">Create</span>.
    </p><p>
     This creates a new role with the name <code class="literal">monitor</code>, sets
     the <code class="literal">read</code> rights and applies this to all elements in
     the CIB by using the XPath expression<code class="literal">/cib</code>.
    </p></li><li class="step"><p>
     If necessary, add more rules by clicking the plus icon and specifying
     the respective parameters.
    </p></li><li class="step"><p>
     Sort the individual rules by using the arrow up or down buttons.
    </p></li></ol></div></div><div class="procedure" id="pro-ha-acl-hawk2-target" data-id-title="Assigning a Role to a Target with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.3: </span><span class="title-name">Assigning a Role to a Target with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-hawk2-target">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To assign the role we created in <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 11.2</a> to a system user (target), proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Targets</span>.
    </p></li><li class="step"><p>
     To create a system user (ACL Target), click <span class="guimenu">Create</span> and
     enter a unique <span class="guimenu">Target ID</span>, for example, <code class="literal">tux</code>.
     Make sure this user belongs to the <code class="systemitem">haclient</code> group.
    </p></li><li class="step"><p>
     To assign a role to the target, select one or multiple <span class="guimenu">Roles</span>.
    </p><p>
     In our example, select the <code class="literal">monitor</code> role you created
     in <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 11.2</a>.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-acl-user-assign.png"><img src="images/hawk2-acl-user-assign.png" width="80%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Confirm your choice.
      </p></li></ol></div></div><p>
   To configure access rights for resources or constraints, you can also use
   the abbreviated syntax as explained in
   <a class="xref" href="#sec-ha-acl-config-tag" title="11.3.2. Setting ACL Rules via Abbreviations">Section 11.3.2, “Setting ACL Rules via Abbreviations”</a>.
  </p></section><section class="sect1" id="sec-ha-acl-config-crm" data-id-title="Configuring ACLs with crmsh"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.5 </span><span class="title-name">Configuring ACLs with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-crm">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following procedure shows how to configure a read-only access to the
   cluster configuration by defining a <code class="literal">monitor</code> role and
   assigning it to a user.
  </p><div class="procedure" id="pro-ha-acl-crm" data-id-title="Adding a Monitor Role and Assigning a User with crmsh"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.4: </span><span class="title-name">Adding a Monitor Role and Assigning a User with crmsh </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-crm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Start the interactive mode of crmsh:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code></pre></div></li><li class="step"><p>
     Define your ACL role(s):
    </p><ol type="a" class="substeps"><li class="step"><p>
       Use the <code class="command">role</code> command to define a new role:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">role</code> monitor read xpath:"/cib"</pre></div><p>
       The previous command creates a new role with the name
       <code class="literal">monitor</code>, sets the <code class="literal">read</code> rights
       and applies it to all elements in the CIB by using the XPath
       expression <code class="literal">/cib</code>. If necessary, you can add more
       access rights and XPath arguments.
      </p></li><li class="step"><p>
       Add additional roles as needed.
      </p></li></ol></li><li class="step"><p>
     Assign your roles to one or multiple ACL targets, which are the
     corresponding system users. Make sure they belong to the
     <code class="systemitem">haclient</code> group.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">acl_target</code> tux monitor</pre></div></li><li class="step"><p>
     Check your changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
     Commit your changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
   To configure access rights for resources or constraints, you can also use
   the abbreviated syntax as explained in
   <a class="xref" href="#sec-ha-acl-config-tag" title="11.3.2. Setting ACL Rules via Abbreviations">Section 11.3.2, “Setting ACL Rules via Abbreviations”</a>.
  </p></section></section><section class="chapter" id="cha-ha-netbonding" data-id-title="Network Device Bonding"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">12 </span><span class="title-name">Network Device Bonding</span></span> <a title="Permalink" class="permalink" href="#cha-ha-netbonding">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
   For many systems, it is desirable to implement network connections that
   comply to more than the standard data security or availability
   requirements of a typical Ethernet device. In these cases, several
   Ethernet devices can be aggregated to a single bonding device.
   </p></div></div></div></div><p>
  The configuration of the bonding device is done by means of bonding module
  options. The behavior is determined through the mode of the bonding
  device. By default, this is <code class="systemitem">mode=active-backup</code>,
  which means that a different slave device will become active if the active
  slave fails.
 </p><p>
  When using Corosync, the bonding device is not managed by the cluster
  software. Therefore, the bonding device must be configured on each cluster
  node that might possibly need to access the bonding device.
 </p><section class="sect1" id="sec-ha-netbond-yast" data-id-title="Configuring Bonding Devices with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.1 </span><span class="title-name">Configuring Bonding Devices with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbond-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To configure a bonding device, you need to have multiple Ethernet devices
   that can be aggregated to a single bonding device. Proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start YaST as <code class="systemitem">root</code> and select <span class="guimenu">System</span> › <span class="guimenu">Network Settings</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Network Settings</span>, switch to the
     <span class="guimenu">Overview</span> tab, which shows the available devices.
    </p></li><li class="step"><p>
     Check if the Ethernet devices to be aggregate to a bonding device have
     an IP address assigned. If yes, change it:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Select the device to change and click <span class="guimenu">Edit</span>.
      </p></li><li class="step" id="step-bond-slave"><p>
       In the <span class="guimenu">Address</span> tab of the <span class="guimenu">Network Card
       Setup</span> dialog that opens, select the option <span class="guimenu">No Link
       and IP Setup (Bonding Slaves)</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_netw_bond.png"><img src="images/yast2_netw_bond.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Click <span class="guimenu">Next</span> to return to the
       <span class="guimenu">Overview</span> tab in the <span class="guimenu">Network
       Settings</span> dialog.
      </p></li></ol></li><li class="step"><p>
     To add a new bonding device:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Add</span> and set the <span class="guimenu">Device
       Type</span> to <span class="guimenu">Bond</span>. Proceed with
       <span class="guimenu">Next</span>.
      </p></li><li class="step"><p>
       Select how to assign the IP address to the bonding device. Three
       methods are at your disposal:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         No Link and IP Setup (Bonding Slaves)
        </p></li><li class="listitem"><p>
         Dynamic Address (with DHCP or Zeroconf)
        </p></li><li class="listitem"><p>
         Statically assigned IP Address
        </p></li></ul></div><p>
       Use the method that is appropriate for your environment. If
       Corosync manages virtual IP addresses, select
       <span class="guimenu">Statically assigned IP Address</span> and assign an IP
       address to the interface.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">Bond Slaves</span> tab.
      </p></li><li class="step"><p>
       It shows any Ethernet devices that have been configured as bonding
       slaves in
       <a class="xref" href="#step-bond-slave" title="Step 3.b">Step 3.b</a>.
       To select the Ethernet devices that you want to include into the
       bond, below <span class="guimenu">Bond Slaves and Order</span> activate the check box
       in front of the respective devices.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_netw_bond_slaves.png"><img src="images/yast2_netw_bond_slaves.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Edit the <span class="guimenu">Bond Driver Options</span>. The following modes
       are available:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.10.5.3.4.2.5.2.1"><span class="term"><code class="literal">balance-rr</code>
        </span></dt><dd><p>
          Provides load balancing and fault tolerance, at the cost of
          out-of-order packet transmission. This may cause delays, for
          example, for TCP reassembly.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.2"><span class="term"><code class="literal">active-backup</code>
        </span></dt><dd><p>
          Provides fault tolerance.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.3"><span class="term"><code class="literal">balance-xor</code>
        </span></dt><dd><p>
          Provides load balancing and fault tolerance.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.4"><span class="term"><code class="literal">broadcast</code>
        </span></dt><dd><p>
          Provides fault tolerance.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.5"><span class="term"><code class="literal">802.3ad</code>
        </span></dt><dd><p>
          Provides dynamic link aggregation if supported by the connected
          switch.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.6"><span class="term"><code class="literal">balance-tlb</code>
        </span></dt><dd><p>
          Provides load balancing for outgoing traffic.
         </p></dd><dt id="id-1.3.4.10.5.3.4.2.5.2.7"><span class="term"><code class="literal">balance-alb</code>
        </span></dt><dd><p>
          Provides load balancing for incoming and outgoing traffic, if the
          network devices used allow the modifying of the network device's
          hardware address while in use.
         </p></dd></dl></div></li><li class="step"><p>
       Make sure to add the parameter <code class="literal">miimon=100</code> to
       <span class="guimenu">Bond Driver Options</span>. Without this parameter, the
       link is not checked regularly, so the bonding driver might continue
       to lose packets on a faulty link.
      </p></li></ol></li><li class="step"><p>
     Click <span class="guimenu">Next</span> and leave YaST with
     <span class="guimenu">OK</span> to finish the configuration of the bonding
     device. YaST writes the configuration to
     <code class="filename">/etc/sysconfig/network/ifcfg-bond<em class="replaceable">DEVICENUMBER</em></code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-netbond-hotpug-yast" data-id-title="Hotplugging of Bonding Slaves"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.2 </span><span class="title-name">Hotplugging of Bonding Slaves</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbond-hotpug-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Sometimes it is necessary to replace a bonding slave interface with
   another one, for example, if the respective network device constantly
   fails. The solution is to set up hotplugging bonding slaves. It is also
   necessary to change the <code class="systemitem">udev</code> rules to match the
   device by bus ID instead of by MAC address. This enables you to replace
   defective hardware (a network card in the same slot but with a different
   MAC address), if the hardware allows for that.
  </p><div class="procedure" id="id-1.3.4.10.6.3" data-id-title="Configuring Hotplugging of Bonding Slaves with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 12.1: </span><span class="title-name">Configuring Hotplugging of Bonding Slaves with YaST </span></span><a title="Permalink" class="permalink" href="#id-1.3.4.10.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    If you prefer manual configuration instead, refer to the SUSE Linux Enterprise Server SUSE Linux Enterprise High Availability
    Administration Guide, chapter <em class="citetitle">Basic Networking</em>, section
    <em class="citetitle">Hotplugging of Bonding Slaves</em>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start YaST as <code class="systemitem">root</code> and select <span class="guimenu">System</span> › <span class="guimenu">Network Settings</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Network Settings</span>, switch to the
     <span class="guimenu">Overview</span> tab, which shows the already configured
     devices. If bonding slaves are already configured, the
     <span class="guimenu">Note</span> column shows it.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast2_netw_bond_slaves_oview.png"><img src="images/yast2_netw_bond_slaves_oview.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     For each of the Ethernet devices that have been aggregated to a bonding
     device, execute the following steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Select the device to change and click <span class="guimenu">Edit</span>. The
       <span class="guimenu">Network Card Setup</span> dialog opens.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">General</span> tab and make sure that
       <span class="guimenu">Activate device</span> is set to <code class="literal">On
       Hotplug</code>.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">Hardware</span> tab.
      </p></li><li class="step"><p>
       For the <span class="guimenu">Udev rules</span>, click
       <span class="guimenu">Change</span> and select the <span class="guimenu">BusID</span>
       option.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> and <span class="guimenu">Next</span> to return to
       the <span class="guimenu">Overview</span> tab in the <span class="guimenu">Network
       Settings</span> dialog. If you click the Ethernet device entry
       now, the bottom pane shows the device's details, including the bus
       ID.
      </p></li></ol></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to confirm your changes and leave the
     network settings.
    </p></li></ol></div></div><p>
   At boot time, the network setup does not wait for the hotplug slaves, but
   for the bond to become ready, which needs at least one available slave.
   When one of the slave interfaces is removed from the system (unbind from
   NIC driver, <code class="command">rmmod</code> of the NIC driver or true PCI
   hotplug removal), the Kernel removes it from the bond automatically. When
   a new card is added to the system (replacement of the hardware in the
   slot), <code class="systemitem">udev</code> renames it by applying the bus-based
   persistent name rule and calls <code class="command">ifup</code> for it. The
   <code class="command">ifup</code> call automatically joins it into the bond.
  </p></section><section class="sect1" id="sec-ha-netbonding-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.3 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbonding-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All modes and many options are explained in detail in the
   <span class="guimenu">Linux Ethernet Bonding Driver HOWTO</span>. The file can be
   found at
   <code class="filename">/usr/src/linux/Documentation/networking/bonding.txt</code>
   after you have installed the package
   <code class="systemitem">kernel-source</code>.
  </p><p>
   For High Availability setups, the following options described therein are
   especially important: <code class="option">miimon</code> and
   <code class="option">use_carrier</code>.
  </p></section></section><section class="chapter" id="cha-ha-lb" data-id-title="Load Balancing"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">13 </span><span class="title-name">Load Balancing</span></span> <a title="Permalink" class="permalink" href="#cha-ha-lb">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
  <span class="emphasis"><em>Load Balancing</em></span> makes a cluster of servers appear as
  one large, fast server to outside clients. This apparent single server is
  called a <span class="emphasis"><em>virtual server</em></span>. It consists of one or more
  load balancers dispatching incoming requests and several real servers
  running the actual services. With a load balancing setup of SUSE Linux Enterprise High Availability, you
  can build highly scalable and highly available network services, such as
  Web, cache, mail, FTP, media and VoIP services.
 </p></div></div></div></div><section class="sect1" id="sec-ha-lb-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE Linux Enterprise High Availability supports two technologies for load balancing: Linux Virtual Server (LVS) and
   HAProxy. The key difference is Linux Virtual Server operates at OSI layer 4
   (Transport), configuring the network layer of kernel, while HAProxy
   operates at layer 7 (Application), running in user space. Thus Linux Virtual Server
   needs fewer resources and can handle higher loads, while HAProxy can
   inspect the traffic, do SSL termination and make dispatching decisions
   based on the content of the traffic.
  </p><p>
   On the other hand, Linux Virtual Server includes two different software:
   IPVS (IP Virtual Server) and KTCPVS (Kernel TCP Virtual Server).
   IPVS provides layer 4 load balancing whereas KTCPVS provides layer 7
   load balancing.
  </p><p>
   This section gives you a conceptual overview of load balancing
   in combination with high availability, then briefly introduces you
   to Linux Virtual Server and HAProxy. Finally, it points you to further reading.
  </p><p>
   The real servers and the load balancers may be interconnected by either
   high-speed LAN or by geographically dispersed WAN. The load balancers
   dispatch requests to the different servers. They make parallel services
   of the cluster appear as one virtual service on a single IP address (the
   virtual IP address or VIP). Request dispatching can use IP load balancing
   technologies or application-level load balancing technologies.
   Scalability of the system is achieved by transparently adding or removing
   nodes in the cluster.
  </p><p>
   High availability is provided by detecting node or service failures and
   reconfiguring the whole virtual server system appropriately, as usual.
  </p><p>
   There are several load balancing strategies. Here are some Layer 4
   strategies, suitable for Linux Virtual Server:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Round Robin. </span>
      The simplest strategy is to direct each connection to a different
      address, taking turns. For example, a DNS server can have several
      entries for a given host name. With DNS roundrobin, the DNS server
      will return all of them in a rotating order. Thus different clients
      will see different addresses.
     </p></li><li class="listitem"><p><span class="formalpara-title">Selecting the <span class="quote">“<span class="quote">best</span>”</span> server. </span>
      Although this has several drawbacks, balancing could be implemented
      with an <span class="quote">“<span class="quote">the first server who responds</span>”</span> or <span class="quote">“<span class="quote">the
      least loaded server</span>”</span> approach.
     </p></li><li class="listitem"><p><span class="formalpara-title">Balance number of connections per server. </span>
      A load balancer between users and servers can divide the number of
      users across multiple servers.
     </p></li><li class="listitem"><p><span class="formalpara-title">Geo Location. </span>
      It is possible to direct clients to a server nearby.
     </p></li></ul></div><p>
   Here are some Layer 7 strategies, suitable for HAProxy:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">URI. </span>
      Inspect the HTTP content and dispatch to a server most suitable for
      this specific URI.
     </p></li><li class="listitem"><p><span class="formalpara-title">URL parameter, RDP cookie. </span>
      Inspect the HTTP content for a session parameter, possibly in post
      parameters, or the RDP (remote desktop protocol) session cookie, and
      dispatch to the server serving this session.
     </p></li></ul></div><p>
   Although there is some overlap, HAProxy can be used in scenarios
   where LVS/<code class="command">ipvsadm</code> is not adequate and vice versa:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">SSL termination. </span>
      The front-end load balancers can handle the SSL layer. Thus the cloud
      nodes do not need to have access to the SSL keys, or could take
      advantage of SSL accelerators in the load balancers.
     </p></li><li class="listitem"><p><span class="formalpara-title">Application level. </span>
      HAProxy operates at the application level, allowing the load
      balancing decisions to be influenced by the content stream. This
      allows for persistence based on cookies and other such filters.
     </p></li></ul></div><p>
   On the other hand, LVS/<code class="command">ipvsadm</code> cannot be fully
   replaced by HAProxy:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     LVS supports <span class="quote">“<span class="quote">direct routing</span>”</span>, where the load balancer is
     only in the inbound stream, whereas the outbound traffic is routed to
     the clients directly. This allows for potentially much higher
     throughput in asymmetric environments.
    </p></li><li class="listitem"><p>
     LVS supports stateful connection table replication (via
     <code class="systemitem">conntrackd</code>). This allows for
     load balancer failover that is transparent to the client and server.
    </p></li></ul></div></section><section class="sect1" id="sec-ha-lb-lvs" data-id-title="Configuring Load Balancing with Linux Virtual Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.2 </span><span class="title-name">Configuring Load Balancing with Linux Virtual Server</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-lvs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following sections give an overview of the main LVS components and
   concepts. Then we explain how to set up Linux Virtual Server on SUSE Linux Enterprise High Availability.
  </p><section class="sect2" id="sec-ha-lvs-overview-director" data-id-title="Director"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.1 </span><span class="title-name">Director</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-director">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The main component of LVS is the ip_vs (or IPVS) Kernel code. It
    implements transport-layer load balancing inside the Linux Kernel
    (layer-4 switching). The node that runs a Linux Kernel including the
    IPVS code is called <span class="emphasis"><em>director</em></span>. The IPVS code running
    on the director is the essential feature of LVS.
   </p><p>
    When clients connect to the director, the incoming requests are
    load-balanced across all cluster nodes: The director forwards packets to
    the real servers, using a modified set of routing rules that make the
    LVS work. For example, connections do not originate or terminate on the
    director, it does not send acknowledgments. The director acts as a
    specialized router that forwards packets from end users to real servers
    (the hosts that run the applications that process the requests).
   </p><p> By default, the Kernel does not need the IPVS module installed. The
   IPVS Kernel module is included in the <code class="systemitem">kernel-default</code> package. </p></section><section class="sect2" id="sec-ha-lvs-overview-userspace" data-id-title="User Space Controller and Daemons"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.2 </span><span class="title-name">User Space Controller and Daemons</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-userspace">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="systemitem">ldirectord</code> daemon is a
    user space daemon for managing Linux Virtual Server and monitoring the real servers
    in an LVS cluster of load balanced virtual servers. A configuration
    file, <code class="filename">/etc/ha.d/ldirectord.cf</code>, specifies the
    virtual services and their associated real servers and tells
    <code class="systemitem">ldirectord</code> how to configure the
    server as an LVS redirector. When the daemon is initialized, it creates
    the virtual services for the cluster.
   </p><p>
    By periodically requesting a known URL and checking the responses, the
    <code class="systemitem">ldirectord</code> daemon monitors the
    health of the real servers. If a real server fails, it will be removed
    from the list of available servers at the load balancer. When the
    service monitor detects that the dead server has recovered and is
    working again, it will add the server back to the list of available
    servers. In case that all real servers should be down, a fall-back
    server can be specified to which to redirect a Web service. Typically
    the fall-back server is localhost, presenting an emergency page about
    the Web service being temporarily unavailable.
   </p><p>
    The <code class="systemitem">ldirectord</code> uses the
    <code class="systemitem">ipvsadm</code> tool (package
    <code class="systemitem">ipvsadm</code>) to manipulate the
    virtual server table in the Linux Kernel.
   </p></section><section class="sect2" id="sec-ha-lvs-overview-forwarding" data-id-title="Packet Forwarding"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.3 </span><span class="title-name">Packet Forwarding</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-forwarding">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three different methods of how the director can send packets
    from the client to the real servers:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.4.6.3.1"><span class="term">Network Address Translation (NAT)</span></dt><dd><p>
       Incoming requests arrive at the virtual IP. They are forwarded to the
       real servers by changing the destination IP address and port to that
       of the chosen real server. The real server sends the response to the
       load balancer which in turn changes the destination IP address and
       forwards the response back to the client. Thus, the end user receives
       the replies from the expected source. As all traffic goes through the
       load balancer, it usually becomes a bottleneck for the cluster.
      </p></dd><dt id="id-1.3.4.11.4.6.3.2"><span class="term">IP Tunneling (IP-IP Encapsulation)</span></dt><dd><p>
       IP tunneling enables packets addressed to an IP address to be
       redirected to another address, possibly on a different network. The
       LVS sends requests to real servers through an IP tunnel (redirecting
       to a different IP address) and the real servers reply directly to the
       client using their own routing tables. Cluster members can be in
       different subnets.
      </p></dd><dt id="id-1.3.4.11.4.6.3.3"><span class="term">Direct Routing</span></dt><dd><p>
       Packets from end users are forwarded directly to the real server. The
       IP packet is not modified, so the real servers must be configured to
       accept traffic for the virtual server's IP address. The response from
       the real server is sent directly to the client. The real servers and
       load balancers need to be in the same physical network segment.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-lvs-overview-schedulers" data-id-title="Scheduling Algorithms"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.4 </span><span class="title-name">Scheduling Algorithms</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-schedulers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Deciding which real server to use for a new connection requested by a
    client is implemented using different algorithms. They are available as
    modules and can be adapted to specific needs. For an overview of
    available modules, refer to the <code class="command">ipvsadm(8)</code> man page.
    Upon receiving a connect request from a client, the director assigns a
    real server to the client based on a <span class="emphasis"><em>schedule</em></span>. The
    scheduler is the part of the IPVS Kernel code which decides which real
    server will get the next new connection.
   </p><p>More detailed description about Linux Virtual Server scheduling algorithms can be
      found at <a class="link" href="http://kb.linuxvirtualserver.org/wiki/IPVS" target="_blank">http://kb.linuxvirtualserver.org/wiki/IPVS</a>.
      Furthermore, search for <code class="option">--scheduler</code> in the
      <code class="command">ipvsadm</code> man page.
    </p><p>Related load balancing strategies for HAProxy can be found at
      <a class="link" href="http://www.haproxy.org/download/1.6/doc/configuration.txt" target="_blank">http://www.haproxy.org/download/1.6/doc/configuration.txt</a>.
    </p></section><section class="sect2" id="sec-ha-lvs-ldirectord" data-id-title="Setting Up IP Load Balancing with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.5 </span><span class="title-name">Setting Up IP Load Balancing with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can configure Kernel-based IP load balancing with the YaST IP
    Load Balancing module. It is a front-end for
    <code class="systemitem">ldirectord</code>.
   </p><p>
    To access the IP Load Balancing dialog, start YaST as <code class="systemitem">root</code>
    and select <span class="guimenu">High Availability</span> › <span class="guimenu">IP Load
    Balancing</span>. Alternatively, start the YaST
    cluster module as <code class="systemitem">root</code> on a command line with
    <code class="command">yast2 iplb</code>.
   </p><p>
    The YaST module writes its configuration to
    <code class="filename">/etc/ha.d/ldirectord.cf</code>. The tabs available in the
    YaST module correspond to the structure of the
    <code class="filename">/etc/ha.d/ldirectord.cf</code> configuration file,
    defining global options and defining the options for the virtual
    services.
   </p><p>
    For an example configuration and the resulting processes between load
    balancers and real servers, refer to
    <a class="xref" href="#ex-ha-lvs-ldirectord" title="Simple ldirectord Configuration">Example 13.1, “Simple ldirectord Configuration”</a>.
   </p><div id="id-1.3.4.11.4.8.6" data-id-title="Global Parameters and Virtual Server Parameters" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Global Parameters and Virtual Server Parameters</div><p>
     If a certain parameter is specified in both the virtual server section
     and in the global section, the value defined in the virtual server
     section overrides the value defined in the global section.
    </p></div><div class="procedure" id="sec-ha-lvs-ldirectord-global" data-id-title="Configuring Global Parameters"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.1: </span><span class="title-name">Configuring Global Parameters </span></span><a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     The following procedure describes how to configure the most important
     global parameters. For more details about the individual parameters
     (and the parameters not covered here), click <span class="guimenu">Help</span> or
     refer to the <code class="systemitem">ldirectord</code> man
     page.
    </p><ol class="procedure" type="1"><li class="step"><p>
      With <span class="guimenu">Check Interval</span>, define the interval in which
      <code class="systemitem">ldirectord</code> will connect to
      each of the real servers to check if they are still online.
     </p></li><li class="step"><p>
      With <span class="guimenu">Check Timeout</span>, set the time in which the real
      server should have responded after the last check.
     </p></li><li class="step"><p>
      With <span class="guimenu">Failure Count </span> you can define how many times
      <code class="systemitem">ldirectord</code> will attempt to
      request the real servers until the check is considered failed.
     </p></li><li class="step"><p>
      With <span class="guimenu">Negotiate Timeout</span> define a timeout in seconds
      for negotiate checks.
     </p></li><li class="step"><p>
      In <span class="guimenu">Fallback</span>, enter the host name or IP address of
      the Web server onto which to redirect a Web service in case all real
      servers are down.
     </p></li><li class="step"><p>
      If you want the system to send alerts in case the connection status to
      any real server changes, enter a valid e-mail address in
      <span class="guimenu">Email Alert</span>.
     </p></li><li class="step"><p>
      With <span class="guimenu">Email Alert Frequency</span>, define after how many
      seconds the e-mail alert should be repeated if any of the real servers
      remains inaccessible.
     </p></li><li class="step"><p>
      In <span class="guimenu">Email Alert Status</span> specify the server states for
      which e-mail alerts should be sent. If you want to define more than
      one state, use a comma-separated list.
     </p></li><li class="step"><p>
      With <span class="guimenu">Auto Reload</span> define, if
      <code class="systemitem">ldirectord</code> should continuously
      monitor the configuration file for modification. If set to
      <code class="literal">yes</code>, the configuration is automatically reloaded
      upon changes.
     </p></li><li class="step"><p>
      With the <span class="guimenu">Quiescent</span> switch, define whether to remove
      failed real servers from the Kernel's LVS table or not. If set to
      <span class="guimenu">Yes</span>, failed servers are not removed. Instead their
      weight is set to <code class="literal">0</code> which means that no new
      connections will be accepted. Already established connections will
      persist until they time out.
     </p></li><li class="step"><p>
      To use an alternative path for logging, specify a path for
      the log files in <span class="guimenu">Log File</span>. By default,
      <code class="systemitem">ldirectord</code> writes its log
      files to <code class="filename">/var/log/ldirectord.log</code>.
     </p></li></ol></div></div><div class="figure" id="fig-ha-lvs-yast-global"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_iplb_global.png"><img src="images/yast2_iplb_global.png" width="65%" alt="YaST IP Load Balancing—Global Parameters" title="YaST IP Load Balancing—Global Parameters"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 13.1: </span><span class="title-name">YaST IP Load Balancing—Global Parameters </span></span><a title="Permalink" class="permalink" href="#fig-ha-lvs-yast-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div><div class="procedure" id="sec-ha-lvs-ldirectord-virtual" data-id-title="Configuring Virtual Services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.2: </span><span class="title-name">Configuring Virtual Services </span></span><a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord-virtual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     You can configure one or more virtual services by defining a couple of
     parameters for each. The following procedure describes how to configure
     the most important parameters for a virtual service. For more details
     about the individual parameters (and the parameters not covered here),
     click <span class="guimenu">Help</span> or refer to the
     <code class="systemitem">ldirectord</code> man page.
    </p><ol class="procedure" type="1"><li class="step"><p>
      In the YaST IP Load Balancing module, switch to the
      <span class="guimenu">Virtual Server Configuration</span> tab.
     </p></li><li class="step"><p>
      <span class="guimenu">Add</span> a new virtual server or <span class="guimenu">Edit</span>
      an existing virtual server. A new dialog shows the available options.
     </p></li><li class="step"><p>
      In <span class="guimenu">Virtual Server</span> enter the shared virtual IP
      address (IPv4 or IPv6) and port under which the load balancers and the
      real servers are accessible as LVS. Instead of IP address and port
      number you can also specify a host name and a service. Alternatively,
      you can also use a firewall mark. A firewall mark is a way of
      aggregating an arbitrary collection of <code class="literal">VIP:port</code>
      services into one virtual service.
     </p></li><li class="step"><p>
      To specify the <span class="guimenu">Real Servers</span>, you need to enter the
      IP addresses (IPv4, IPv6, or host names) of the servers, the ports (or
      service names) and the forwarding method. The forwarding method must
      either be <code class="literal">gate</code>, <code class="literal">ipip</code> or
      <code class="literal">masq</code>, see
      <a class="xref" href="#sec-ha-lvs-overview-forwarding" title="13.2.3. Packet Forwarding">Section 13.2.3, “Packet Forwarding”</a>.
     </p><p>
      Click the <span class="guimenu">Add</span> button and enter the required
      arguments for each real server.
     </p></li><li class="step"><p>
      As <span class="guimenu">Check Type</span>, select the type of check that should
      be performed to test if the real servers are still alive. For example,
      to send a request and check if the response contains an expected
      string, select <code class="literal">Negotiate</code>.
     </p></li><li class="step" id="step-ha-lvs-ldirectord-service"><p>
      If you have set the <span class="guimenu">Check Type</span> to
      <code class="literal">Negotiate</code>, you also need to define the type of
      service to monitor. Select it from the <span class="guimenu">Service</span>
      drop-down box.
     </p></li><li class="step"><p>
      In <span class="guimenu">Request</span>, enter the URI to the object that is
      requested on each real server during the check intervals.
     </p></li><li class="step"><p>
      If you want to check if the response from the real servers contains a
      certain string (<span class="quote">“<span class="quote">I am alive</span>”</span> message), define a regular
      expression that needs to be matched. Enter the regular expression into
      <span class="guimenu">Receive</span>. If the response from a real server
      contains this expression, the real server is considered to be alive.
     </p></li><li class="step"><p>
      Depending on the type of <span class="guimenu">Service</span> you have selected
      in <a class="xref" href="#step-ha-lvs-ldirectord-service" title="Step 6">Step 6</a>, you also need to
      specify further parameters for authentication. Switch to the
      <span class="guimenu">Auth type</span> tab and enter the details like
      <span class="guimenu">Login</span>, <span class="guimenu">Password</span>,
      <span class="guimenu">Database</span>, or <span class="guimenu">Secret</span>. For more
      information, refer to the YaST help text or to the
      <code class="systemitem">ldirectord</code> man page.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Others</span> tab.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Scheduler</span> to be used for load balancing.
      For information on the available schedulers, refer to the
      <code class="command">ipvsadm(8)</code> man page.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Protocol</span> to be used.

      If the virtual service is specified as an IP address and port, it must
      be either <code class="literal">tcp</code> or <code class="literal">udp</code>. If the
      virtual service is specified as a firewall mark, the protocol must be
      <code class="literal">fwm</code>.
     </p></li><li class="step"><p>
      Define further parameters, if needed. Confirm your configuration with
      <span class="guimenu">OK</span>. YaST writes the configuration to
      <code class="filename">/etc/ha.d/ldirectord.cf</code>.
     </p></li></ol></div></div><div class="figure" id="fig-ha-lvs-yast-virtual"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_iplb_virtual.png"><img src="images/yast2_iplb_virtual.png" width="65%" alt="YaST IP Load Balancing—Virtual Services" title="YaST IP Load Balancing—Virtual Services"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 13.2: </span><span class="title-name">YaST IP Load Balancing—Virtual Services </span></span><a title="Permalink" class="permalink" href="#fig-ha-lvs-yast-virtual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div><div class="complex-example"><div class="example" id="ex-ha-lvs-ldirectord" data-id-title="Simple ldirectord Configuration"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 13.1: </span><span class="title-name">Simple ldirectord Configuration </span></span><a title="Permalink" class="permalink" href="#ex-ha-lvs-ldirectord">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The values shown in <a class="xref" href="#fig-ha-lvs-yast-global" title="YaST IP Load Balancing—Global Parameters">Figure 13.1, “YaST IP Load Balancing—Global Parameters”</a> and
     <a class="xref" href="#fig-ha-lvs-yast-virtual" title="YaST IP Load Balancing—Virtual Services">Figure 13.2, “YaST IP Load Balancing—Virtual Services”</a>, would lead to the following
     configuration, defined in <code class="filename">/etc/ha.d/ldirectord.cf</code>:
    </p><div class="verbatim-wrap"><pre class="screen">autoreload = yes <span class="callout" id="co-ha-ldirectord-autoreload">1</span>
    checkinterval = 5 <span class="callout" id="co-ha-ldirectord-checkintervall">2</span>
    checktimeout = 3 <span class="callout" id="co-ha-ldirectord-checktimeout">3</span>
    quiescent = yes <span class="callout" id="co-ha-ldirectord-quiescent">4</span>
    virtual = 192.168.0.200:80 <span class="callout" id="co-ha-ldirectord-virtual">5</span>
    checktype = negotiate <span class="callout" id="co-ha-ldirectord-checktype">6</span>
    fallback = 127.0.0.1:80 <span class="callout" id="co-ha-ldirectord-fallback">7</span>
    protocol = tcp <span class="callout" id="co-ha-ldirectord-protocol">8</span>
    real = 192.168.0.110:80 gate <span class="callout" id="co-ha-ldirectord-real">9</span>
    real = 192.168.0.120:80 gate <a class="xref" href="#co-ha-ldirectord-real"><span class="callout">9</span></a>
    receive = "still alive" <span class="callout" id="co-ha-ldirectord-receive">10</span>
    request = "test.html" <span class="callout" id="co-ha-ldirectord-request">11</span>
    scheduler = wlc <span class="callout" id="co-ha-ldirectord-scheduler">12</span>
    service = http <span class="callout" id="co-ha-ldirectord-service">13</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-autoreload"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines that <code class="systemitem">ldirectord</code>
       should continuously check the configuration file for modification.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checkintervall"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Interval in which <code class="systemitem">ldirectord</code>
       will connect to each of the real servers to check if they are still
       online.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checktimeout"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Time in which the real server should have responded after the last
       check.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-quiescent"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines not to remove failed real servers from the Kernel's LVS
       table, but to set their weight to <code class="literal">0</code> instead.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-virtual"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Virtual IP address (VIP) of the LVS. The LVS is available at port
       <code class="literal">80</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checktype"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Type of check that should be performed to test if the real servers
       are still alive.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-fallback"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Server onto which to redirect a Web service all real servers for this
       service are down.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-protocol"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Protocol to be used.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-real"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Two real servers defined, both available at port
       <code class="literal">80</code>. The packet forwarding method is
       <code class="literal">gate</code>, meaning that direct routing is used.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-receive"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Regular expression that needs to be matched in the response string
       from the real server.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-request"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       URI to the object that is requested on each real server during the
       check intervals.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-scheduler"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Selected scheduler to be used for load balancing.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-service"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Type of service to monitor.
      </p></td></tr></table></div><p>
     This configuration would lead to the following process flow: The
     <code class="systemitem">ldirectord</code> will connect to each
     real server once every 5 seconds
     (<a class="xref" href="#co-ha-ldirectord-checkintervall"><span class="callout">2</span></a>)
     and request <code class="literal">192.168.0.110:80/test.html</code> or
     <code class="literal">192.168.0.120:80/test.html</code> as specified in
     <a class="xref" href="#co-ha-ldirectord-real"><span class="callout">9</span></a>
     and
     <a class="xref" href="#co-ha-ldirectord-request"><span class="callout">11</span></a>.
     If it does not receive the expected <code class="literal">still alive</code>
     string
     (<a class="xref" href="#co-ha-ldirectord-receive"><span class="callout">10</span></a>)
     from a real server within 3 seconds
     (<a class="xref" href="#co-ha-ldirectord-checktimeout"><span class="callout">3</span></a>)
     of the last check, it will remove the real server from the available
     pool. However, because of the <code class="literal">quiescent=yes</code> setting
     (<a class="xref" href="#co-ha-ldirectord-quiescent"><span class="callout">4</span></a>),
     the real server will not be removed from the LVS table. Instead its
     weight will be set to <code class="literal">0</code> so that no new connections
     to this real server will be accepted. Already established connections
     will be persistent until they time out.
    </p></div></div></div></section><section class="sect2" id="sec-ha-lvs-further" data-id-title="Further Setup"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.2.6 </span><span class="title-name">Further Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-further">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Apart from the configuration of
    <code class="systemitem">ldirectord</code> with YaST, you
    need to make sure the following conditions are fulfilled to complete the
    LVS setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The real servers are set up correctly to provide the needed services.
     </p></li><li class="listitem"><p>
      The load balancing server (or servers) must be able to route traffic
      to the real servers using IP forwarding. The network configuration of
      the real servers depends on which packet forwarding method you have
      chosen.
     </p></li><li class="listitem"><p>
      To prevent the load balancing server (or servers) from becoming a
      single point of failure for the whole system, you need to set up one
      or several backups of the load balancer. In the cluster configuration,
      configure a primitive resource for
      <code class="systemitem">ldirectord</code>, so that
      <code class="systemitem">ldirectord</code> can fail over to
      other servers in case of hardware failure.
     </p></li><li class="listitem"><p>
      As the backup of the load balancer also needs the
      <code class="systemitem">ldirectord</code> configuration file
      to fulfill its task, make sure the
      <code class="filename">/etc/ha.d/ldirectord.cf</code> is available on all
      servers that you want to use as backup for the load balancer. You can
      synchronize the configuration file with Csync2 as described in
      <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a>.
     </p></li></ul></div></section></section><section class="sect1" id="sec-ha-lb-haproxy" data-id-title="Configuring Load Balancing with HAProxy"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.3 </span><span class="title-name">Configuring Load Balancing with HAProxy</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-haproxy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_lb_haproxy.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following section gives an overview of the HAProxy and how to
  set up on High Availability. The load balancer distributes all requests to its
  back-end servers. It is configured as active/passive, meaning if one
  master fails, the slave becomes the master. In such a scenario, the user
  will not notice any interruption.
 </p><p>
  In this section, we will use the following setup:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    A load balancer, with the IP address
    <code class="systemitem">192.168.1.99</code>.
   </p></li><li class="listitem"><p>
    A virtual, floating IP address
    <code class="systemitem">192.168.1.99</code>.
   </p></li><li class="listitem"><p>
    Our servers (usually for Web content)
    <code class="systemitem">www.example1.com</code> (IP:
    <code class="systemitem">192.168.1.200</code>) and
    <code class="systemitem">www.example2.com</code> (IP:
    <code class="systemitem">192.168.1.201</code>)
   </p></li></ul></div><p>
  To configure HAProxy, use the following procedure:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Install the <code class="systemitem">haproxy</code> package.
   </p></li><li class="step"><p>
    Create the file <code class="filename">/etc/haproxy/haproxy.cfg</code> with the
    following contents:
   </p><div class="verbatim-wrap"><pre class="screen">global <span class="callout" id="co-ha-lb-global">1</span>
  maxconn 256
  daemon

defaults <span class="callout" id="co-ha-lb-defaults">2</span>
  log     global
  mode    http
  option  httplog
  option  dontlognull
  retries 3
  option redispatch
  maxconn 2000
  timeout connect   5000  <span class="callout" id="co-ha-lb-timeout-connect">3</span>
  timeout client    50s   <span class="callout" id="co-ha-lb-timeout-client">4</span>
  timeout server    50000 <span class="callout" id="co-ha-lb-timeout-server">5</span>

frontend LB
  bind 192.168.1.99:80 <span class="callout" id="co-ha-lb-listen">6</span>
  reqadd X-Forwarded-Proto:\ http
  default_backend LB

backend LB
  mode http
  stats enable
  stats hide-version
  stats uri /stats
  stats realm Haproxy\ Statistics
  stats auth haproxy:password	<span class="callout" id="co-ha-lb-stats-auth">7</span>
  balance roundrobin	<span class="callout" id="co-ha-lb-balance">8</span>
  option  httpclose
  option forwardfor
  cookie LB insert
  option httpchk GET /robots.txt HTTP/1.0
  server web1-srv 192.168.1.200:80 cookie web1-srv check
  server web2-srv 192.168.1.201:80 cookie web2-srv check</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-global"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which contains process-wide and OS-specific options.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.5.8.2.4.1.2.1"><span class="term"><code class="option">maxconn</code>
       </span></dt><dd><p>
         Maximum per-process number of concurrent connections.
        </p></dd><dt id="id-1.3.4.11.5.8.2.4.1.2.2"><span class="term"><code class="option">daemon</code>
       </span></dt><dd><p>
         Recommended mode, HAProxy runs in the background.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-defaults"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which sets default parameters for all other sections
      following its declaration. Some important lines:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.5.8.2.4.2.2.1"><span class="term"><code class="option">redispatch</code>
       </span></dt><dd><p>
         Enables or disables session redistribution in case of connection
         failure.
        </p></dd><dt id="id-1.3.4.11.5.8.2.4.2.2.2"><span class="term"><code class="option">log</code>
       </span></dt><dd><p>
         Enables logging of events and traffic.
        </p></dd><dt id="id-1.3.4.11.5.8.2.4.2.2.3"><span class="term"><code class="literal">mode http</code>
       </span></dt><dd><p>
         Operates in HTTP mode (recommended mode for HAProxy). In this
         mode, a request will be analyzed before a connection to any server
         is performed. Request that are not RFC-compliant will be rejected.
        </p></dd><dt id="id-1.3.4.11.5.8.2.4.2.2.4"><span class="term"><code class="literal">option forwardfor</code>
       </span></dt><dd><p>
         Adds the HTTP <code class="option">X-Forwarded-For</code> header into the
         request. You need this option if you want to preserve the client's
         IP address.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-connect"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time to wait for a connection attempt to a server
      to succeed.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-client"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time of inactivity on the client side.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-server"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time of inactivity on the server side.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-listen"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which combines front-end and back-end sections in one.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.5.8.2.4.6.2.1"><span class="term"><code class="literal">balance leastconn</code>
       </span></dt><dd><p>
         Defines the load balancing algorithm, see
         <a class="link" href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4-balance" target="_blank">http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4-balance</a>.
        </p></dd><dt id="id-1.3.4.11.5.8.2.4.6.2.2"><span class="term"><code class="literal">stats enable</code>
       , </span><span class="term"><code class="literal">stats auth</code>
       </span></dt><dd><p>
         Enables statistics reporting (by <code class="literal">stats enable</code>).
         The <code class="option">auth</code> option logs statistics with
         authentication to a specific account.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-stats-auth"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Credentials for HAProxy Statistic report page.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-balance"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Load balancing will work in a round-robin process.
      </p></td></tr></table></div></li><li class="step"><p>
    Test your configuration file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">haproxy</code> -f /etc/haproxy/haproxy.cfg -c</pre></div></li><li class="step"><p>
    Add the following line to Csync2's configuration file
    <code class="filename">/etc/csync2/csync2.cfg</code> to make sure the
    HAProxy configuration file is included:
   </p><div class="verbatim-wrap"><pre class="screen">include /etc/haproxy/haproxy.cfg</pre></div></li><li class="step"><p>
    Synchronize it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -f /etc/haproxy/haproxy.cfg
<code class="prompt root"># </code><code class="command">csync2</code> -xv</pre></div><div id="id-1.3.4.11.5.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     The Csync2 configuration part assumes that the HA nodes were
     configured using the bootstrap scripts provided by the crm shell. For details,
     see the Installation and Setup Quick Start.
    </p></div></li><li class="step"><p>
    Make sure HAProxy is disabled on both load balancers
    (<code class="systemitem">alice</code> and
    <code class="systemitem">bob</code>) as it is started by
    Pacemaker:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable haproxy</pre></div></li><li class="step"><p>
    Configure a new CIB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)# </code><code class="command">cib</code> new haproxy-config
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">primitive</code> primitive haproxy systemd:haproxy \
    op start timeout=120 interval=0 \
    op stop timeout=120 interval=0 \
    op monitor timeout=100 interval=5s \
    meta target-role=Started
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">primitive</code> vip IPaddr2 \
    params ip=192.168.1.99 nic=eth0 cidr_netmask=23 broadcast=192.168.1.255 \
    op monitor interval=5s timeout=120 on-fail=restart \
    meta target-role=Started
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">order</code> haproxy-after-IP Mandatory: vip haproxy
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">colocation</code> haproxy-with-public-IPs inf: haproxy vip
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">group</code> g-haproxy vip haproxy-after-IP</pre></div></li><li class="step"><p>
    Verify the new CIB and correct any errors:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(haproxy-config)# </code><code class="command">verify</code></pre></div></li><li class="step"><p>
    Commit the new CIB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(haproxy-config)# </code><code class="command">cib</code> use live
<code class="prompt custom">crm(live)# </code><code class="command">cib</code> commit haproxy-config</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-lb-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="link" href="http://www.haproxy.org" target="_blank">http://www.haproxy.org</a></p></li><li class="listitem"><p>Project home page at <a class="link" href="http://www.linuxvirtualserver.org/" target="_blank">http://www.linuxvirtualserver.org/</a>.
  </p></li><li class="listitem"><p>For more information about <code class="systemitem">ldirectord</code>, refer to its
        comprehensive man page.</p></li><li class="listitem"><p>LVS Knowledge Base: <a class="link" href="http://kb.linuxvirtualserver.org/wiki/Main_Page" target="_blank">http://kb.linuxvirtualserver.org/wiki/Main_Page</a></p></li></ul></div></section></section><section class="chapter" id="cha-ha-geo" data-id-title="Geo Clusters (Multi-Site Clusters)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_geocluster.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Apart from local clusters and metro area clusters, SUSE® Linux Enterprise High Availability
    12 SP4 also supports geographically dispersed clusters (Geo
    clusters, sometimes also called multi-site clusters). That means you can
    have multiple, geographically dispersed sites with a local cluster each.
    Failover between these clusters is coordinated by a higher level entity,
    the so-called <code class="literal">booth</code>. Support for Geo clusters is
    available as a separate extension to Geo Clustering for SUSE Linux Enterprise High Availability. For details on how to
    use and set up Geo clusters, refer to the <em class="citetitle">Geo Clustering Quick Start</em>
    or the <em class="citetitle">Geo Clustering Guide</em>. Both are available from
    <a class="link" href="http://www.suse.com/documentation/sle-ha-geo-12" target="_blank">http://www.suse.com/documentation/sle-ha-geo-12</a>.
   </p></div></div></div></div></section></div><div class="part" id="part-storage" data-id-title="Storage and Data Replication"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part III </span><span class="title-name">Storage and Data Replication </span></span><a title="Permalink" class="permalink" href="#part-storage">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-storage-dlm"><span class="title-number">15 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></span></li><dd class="toc-abstract"><p>
    The Distributed Lock Manager (DLM) in the kernel is the base component used
    by OCFS2, GFS2, Cluster MD, and cLVM to provide active-active
    storage at each respective layer.
   </p></dd><li><span class="chapter"><a href="#cha-ha-ocfs2"><span class="title-number">16 </span><span class="title-name">OCFS2</span></a></span></li><dd class="toc-abstract"><p>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    OCFS2 allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with Corosync and the Distributed
    Lock Manager (DLM).
   </p></dd><li><span class="chapter"><a href="#cha-ha-gfs2"><span class="title-number">17 </span><span class="title-name">GFS2</span></a></span></li><dd class="toc-abstract"><p>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating-mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </p><p>
    SUSE recommends OCFS2 over GFS2 for your cluster environments if
    performance is one of your major requirements. Our tests have revealed
    that OCFS2 performs better as compared to GFS2 in such settings.
   </p></dd><li><span class="chapter"><a href="#cha-ha-drbd"><span class="title-number">18 </span><span class="title-name">DRBD</span></a></span></li><dd class="toc-abstract"><p>
    The <span class="emphasis"><em>distributed replicated block device</em></span> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with Corosync,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </p></dd><li><span class="chapter"><a href="#cha-ha-clvm"><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></a></span></li><dd class="toc-abstract"><p>
    When managing shared storage on a cluster, every node must be informed
    about changes that are done to the storage subsystem. The Logical Volume
    Manager 2 (LVM2), which is widely used to manage local storage,
    has been extended to support transparent management of volume groups
    across the whole cluster. Clustered volume groups can be managed using
    the same commands as local storage.
   </p></dd><li><span class="chapter"><a href="#cha-ha-cluster-md"><span class="title-number">20 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></span></li><dd class="toc-abstract"><p>The cluster multi-device (Cluster MD) is a software based RAID
   storage solution for a cluster. Cluster MD provides the redundancy of
   RAID1 mirroring to the cluster. Currently, only RAID1 is supported now.
   This chapter shows you how to create and use Cluster MD.
   </p></dd><li><span class="chapter"><a href="#cha-ha-samba"><span class="title-number">21 </span><span class="title-name">Samba Clustering</span></a></span></li><dd class="toc-abstract"><p>
    A clustered Samba server provides a High Availability solution in your
    heterogeneous networks. This chapter explains some background
    information and how to set up a clustered Samba server.
   </p></dd><li><span class="chapter"><a href="#cha-ha-rear"><span class="title-number">22 </span><span class="title-name">Disaster Recovery with Relax-and-Recover (ReaR)</span></a></span></li><dd class="toc-abstract"><p>
    Relax-and-Recover (<span class="quote">“<span class="quote">ReaR</span>”</span>) is a disaster recovery framework for use by
    system administrators. It is a collection of Bash scripts that need to
    be adjusted to the specific production environment that is to be
    protected in case of disaster.
   </p><p>
    No disaster recovery solution will  work out-of-the-box. Therefore
    it is essential to take preparations <span class="emphasis"><em>before</em></span> any
    disaster happens.
   </p></dd></ul></div><section class="chapter" id="cha-ha-storage-dlm" data-id-title="Distributed Lock Manager (DLM)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">Distributed Lock Manager (DLM)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-storage-dlm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The Distributed Lock Manager (DLM) in the kernel is the base component used
    by OCFS2, GFS2, Cluster MD, and cLVM to provide active-active
    storage at each respective layer.
   </p></div></div></div></div><section class="sect1" id="sec-ha-storage-dlm-protocol" data-id-title="Protocols for DLM Communication"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">Protocols for DLM Communication</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-dlm-protocol">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To avoid single points of failure, redundant communication paths are important
   for High Availability clusters. This is also true for DLM communication. If network bonding
   (Link Aggregation Control Protocol, LACP) cannot be used for any reason, we
   highly recommend to define a redundant communication channel (a second ring)
   in Corosync. For details, see <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
  </p><p>
   Depending on the configuration in <code class="filename">/etc/corosync/corosync.conf</code>, DLM then decides
   whether to use the TCP or SCTP protocol for its communication:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If <span class="guimenu">rrp_mode</span> is set to <code class="literal">none</code> (which
     means redundant ring configuration is disabled), DLM automatically uses
     TCP. However, without a redundant communication channel, DLM communication
     will fail if the TCP link is down.
   </p></li><li class="listitem"><p>
     If <span class="guimenu">rrp_mode</span> is set to <code class="literal">passive</code> (which
     is the typical setting), and a second communication ring in <code class="filename">/etc/corosync/corosync.conf</code>
     is configured correctly, DLM automatically uses SCTP. In this case, DLM
     messaging has the redundancy capability provided by SCTP.
   </p></li></ul></div></section><section class="sect1" id="sec-ha-storage-generic-dlm-config" data-id-title="Configuring DLM Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Configuring DLM Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-generic-dlm-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   DLM uses the cluster membership services from Pacemaker which run in user
   space. Therefore, DLM needs to be configured as a clone resource that is
   present on each node in the cluster.
  </p><div id="id-1.3.5.3.4.3" data-id-title="DLM Resource for Several Solutions" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: DLM Resource for Several Solutions</div><p>
    As OCFS2, GFS2, Cluster MD, and cLVM all use DLM, it is
    enough to configure one resource for DLM. As the DLM resource runs on all
    nodes in the cluster it is configured as a clone resource.
   </p><p>
    If you have a setup that includes both OCFS2 and cLVM, configuring
    <span class="emphasis"><em>one</em></span> DLM resource for both OCFS2 and cLVM is enough.
   </p></div><div class="procedure" id="pro-dlm-resources" data-id-title="Configuring a Base Group for DLM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.1: </span><span class="title-name">Configuring a Base Group for DLM </span></span><a title="Permalink" class="permalink" href="#pro-dlm-resources">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    The configuration consists of a base group that includes several primitives
    and a base clone. Both base group and base clone can be used in various
    scenarios afterward (for both OCFS2 and cLVM, for example). You only need
    to extend the base group with the respective primitives as needed. As the
    base group has internal colocation and ordering, this simplifies the
    overall setup as you do not need to specify several individual groups,
    clones and their dependencies.
   </p><p>
    Follow the steps below on one node in the cluster:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Enter the following to create the primitive resource for DLM:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dlm ocf:pacemaker:controld \
  op monitor interval="60" timeout="60"</pre></div></li><li class="step"><p>
     Create a base group for the DLM resource and further storage-related
     resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-storage dlm</pre></div></li><li class="step"><p>
     Clone the <code class="literal">g-storage</code> group so that it runs on all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code> <code class="command">clone</code> cl-storage g-storage \
  meta interleave=true target-role=Started</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">exit</code>.
    </p></li></ol></div></div><div id="id-1.3.5.3.4.5" data-id-title="Failure When Disabling STONITH" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Failure When Disabling STONITH</div><p>
    Clusters without STONITH are not supported. If you set the global cluster
    option <code class="varname">stonith-enabled</code> to <code class="literal">false</code> for
    testing or troubleshooting purposes, the DLM resource and all services
    depending on it (such as cLVM, GFS2, and OCFS2) will fail to start.
   </p></div></section></section><section class="chapter" id="cha-ha-ocfs2" data-id-title="OCFS2"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">OCFS2</span></span> <a title="Permalink" class="permalink" href="#cha-ha-ocfs2">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    OCFS2 allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with Corosync and the Distributed
    Lock Manager (DLM).
   </p></div></div></div></div><section class="sect1" id="sec-ha-ocfs2-features" data-id-title="Features and Benefits"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">Features and Benefits</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-features">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   OCFS2 can be used for the following storage solutions for example:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     General applications and workloads.
    </p></li><li class="listitem"><p>
     Xen image store in a cluster. Xen virtual machines and
     virtual servers can be stored on OCFS2 volumes that are mounted by
     cluster servers. This provides quick and easy portability of Xen
     virtual machines between servers.
    </p></li><li class="listitem"><p>
     LAMP (Linux, Apache, MySQL, and PHP | Perl |
     Python) stacks.
    </p></li></ul></div><p>
   As a high-performance, symmetric and parallel cluster file system,
   OCFS2 supports the following functions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </p></li><li class="listitem"><p>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </p></li><li class="listitem"><p>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </p></li><li class="listitem"><p>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </p></li></ul></div><p>
   OCFS2 also provides the following capabilities:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Metadata caching.
    </p></li><li class="listitem"><p>
     Metadata journaling.
    </p></li><li class="listitem"><p>
     Cross-node file data consistency.
    </p></li><li class="listitem"><p>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </p></li><li class="listitem"><p>
     Support for up to 32 cluster nodes.
    </p></li><li class="listitem"><p>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </p></li></ul></div><div id="id-1.3.5.4.3.8" data-id-title="Support for OCFS2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Support for OCFS2</div><p>
    OCFS2 is only supported by SUSE when used with the pcmk (Pacemaker) stack,
    as provided by SUSE Linux Enterprise High Availability. SUSE does not provide support for OCFS2 in
    combination with the o2cb stack.
   </p></div></section><section class="sect1" id="sec-ha-ocfs2-utils" data-id-title="OCFS2 Packages and Management Utilities"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.2 </span><span class="title-name">OCFS2 Packages and Management Utilities</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-utils">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The OCFS2 Kernel module (<code class="literal">ocfs2</code>) is installed
   automatically in SUSE Linux Enterprise High Availability 12 SP4. To use
   OCFS2, make sure the following packages are installed on each node in
   the cluster: <code class="systemitem">ocfs2-tools</code> and
   the matching <code class="systemitem">ocfs2-kmp-*</code>
   packages for your Kernel.
  </p><p>
   The <code class="systemitem">ocfs2-tools</code> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </p><div class="table" id="id-1.3.5.4.4.4" data-id-title="OCFS2 Utilities"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.1: </span><span class="title-name">OCFS2 Utilities </span></span><a title="Permalink" class="permalink" href="#id-1.3.5.4.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        OCFS2 Utility
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        debugfs.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Examines the state of the OCFS file system for debugging.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        fsck.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Checks the file system for errors and optionally repairs errors.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mkfs.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Creates an OCFS2 file system on a device, usually a partition on
        a shared physical or logical disk.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mounted.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Detects and lists all OCFS2 volumes on a clustered system.
        Detects and lists all nodes on the system that have mounted an
        OCFS2 device or lists all OCFS2 devices.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        tunefs.ocfs2
       </p>
      </td><td>
       <p>
        Changes OCFS2 file system parameters, including the volume
        label, number of node slots, journal size for all node slots, and
        volume size.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-ocfs2-create-service" data-id-title="Configuring OCFS2 Services and a STONITH Resource"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.3 </span><span class="title-name">Configuring OCFS2 Services and a STONITH Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-create-service">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM and a STONITH resource.
  </p><p>
   The following procedure uses the <code class="command">crm</code> shell to
   configure the cluster resources. Alternatively, you can also use
   Hawk2 to configure the resources as described in
   <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="16.6. Configuring OCFS2 Resources With Hawk2">Section 16.6, “Configuring OCFS2 Resources With Hawk2”</a>.
  </p><div class="procedure" id="pro-ocfs2-stonith" data-id-title="Configuring a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.1: </span><span class="title-name">Configuring a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.3.5.4.5.4.2" data-id-title="STONITH Device Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: STONITH Device Needed</div><p>
     You need to configure a fencing device. Without a STONITH
     mechanism (like <code class="literal">external/sbd</code>) in place the
     configuration will fail.
    </p></div><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Create an SBD partition as described in
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 10.3, “Initializing the SBD Devices”</a>.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure <code class="literal">external/sbd</code> as fencing device with
     <code class="literal">/dev/sdb2</code> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">exit</code>.
    </p></li></ol></div></div><p>
    For details on configuring the resource group for DLM, see <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a>.
  </p></section><section class="sect1" id="sec-ha-ocfs2-create" data-id-title="Creating OCFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.4 </span><span class="title-name">Creating OCFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured a DLM cluster resource as described in
   <a class="xref" href="#sec-ha-ocfs2-create-service" title="16.3. Configuring OCFS2 Services and a STONITH Resource">Section 16.3, “Configuring OCFS2 Services and a STONITH Resource”</a>, configure your system to
   use OCFS2 and create OCFs2 volumes.
  </p><div id="id-1.3.5.4.6.3" data-id-title="OCFS2 Volumes for Application and Data Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: OCFS2 Volumes for Application and Data Files</div><p>
    We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </p></div><p>
   Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space.
  </p><p>
   Then create and format the OCFS2 volume with the
   <code class="command">mkfs.ocfs2</code> as described in
   <a class="xref" href="#pro-ocfs2-volume" title="Creating and Formatting an OCFS2 Volume">Procedure 16.2, “Creating and Formatting an OCFS2 Volume”</a>. The most important parameters for the
   command are listed in <a class="xref" href="#tab-ha-ofcs2-mkfs-ocfs2-params" title="Important OCFS2 Parameters">Table 16.2, “Important OCFS2 Parameters”</a>.
   For more information and the command syntax, refer to the
   <code class="command">mkfs.ocfs2</code> man page.
  </p><div class="table" id="tab-ha-ofcs2-mkfs-ocfs2-params" data-id-title="Important OCFS2 Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 16.2: </span><span class="title-name">Important OCFS2 Parameters </span></span><a title="Permalink" class="permalink" href="#tab-ha-ofcs2-mkfs-ocfs2-params">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        OCFS2 Parameter
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description and Recommendation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Volume Label (<code class="option">-L</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <code class="command">tunefs.ocfs2</code> utility to modify the label as
        needed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Cluster Size (<code class="option">-C</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Number of Node Slots (<code class="option">-N</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such
        as the journals. Nodes that access the volume
	can be a combination of little-endian architectures (such as AMD64/Intel 64)
        and big-endian architectures (such as IBM Z).
       </p>
       <p>
        Node-specific files are called local files. A node slot
        number is appended to the local file. For example:
        <code class="literal">journal:0000</code> belongs to whatever node is assigned
        to slot number <code class="literal">0</code>.
       </p>
       <p>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <code class="command">tunefs.ocfs2</code> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased, and one node slot will consume about 100 MiB disk space.
       </p>
       <p>
        In case the <code class="option">-N</code> parameter is not specified, the
        number of node slots is decided based on the size of the file system.
        For the default value, refer to the <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Block Size (<code class="option">-b</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <code class="command">mkfs.ocfs2</code> man
        page.

       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Specific Features On/Off (<code class="option">--fs-features</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A comma separated list of feature flags can be provided, and
        <code class="systemitem">mkfs.ocfs2</code> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <code class="literal">no</code> to the name.
       </p>
       <p>
        For an overview of all available flags, refer to the
        <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Pre-Defined Features (<code class="option">--fs-feature-level</code>)
       </p>
      </td><td>
       <p>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr></tbody></table></div></div><p>
   If you do not specify any features when creating and formatting
   the volume with <code class="command">mkfs.ocfs2</code>, the following features are
   enabled by default: <code class="option">backup-super</code>,
   <code class="option">sparse</code>, <code class="option">inline-data</code>,
   <code class="option">unwritten</code>, <code class="option">metaecc</code>,
   <code class="option">indexed-dirs</code>, and <code class="option">xattr</code>.
  </p><div class="procedure" id="pro-ocfs2-volume" data-id-title="Creating and Formatting an OCFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.2: </span><span class="title-name">Creating and Formatting an OCFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-volume">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Execute the following steps only on <span class="emphasis"><em>one</em></span> of the
    cluster nodes.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Create and format the volume using the <code class="command">mkfs.ocfs2</code>
     utility. For information about the syntax for this command, refer to
     the <code class="command">mkfs.ocfs2</code> man page.
    </p><p>
     For example, to create a new OCFS2 file system on
     <code class="filename">/dev/sdb1</code> that supports up to 32 cluster nodes,
     enter the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> mkfs.ocfs2 -N 32 /dev/sdb1</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-mount" data-id-title="Mounting OCFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.5 </span><span class="title-name">Mounting OCFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-mount">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <a class="xref" href="#pro-ocfs2-mount-cluster" title="Mounting an OCFS2 Volume with the Cluster Resource Manager">Procedure 16.4, “Mounting an OCFS2 Volume with the Cluster Resource Manager”</a>.
  </p><div class="procedure" id="pro-ocfs2-mount-manual" data-id-title="Manually Mounting an OCFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.3: </span><span class="title-name">Manually Mounting an OCFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-mount-manual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Mount the volume from the command line, using the
     <code class="command">mount</code> command.
    </p></li></ol></div></div><div id="id-1.3.5.4.7.4" data-id-title="Manually Mounted OCFS2 Devices" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Manually Mounted OCFS2 Devices</div><p>
    If you mount the OCFS2 file system manually for testing purposes,
    make sure to unmount it again before starting to use it by means of
    cluster resources.
   </p></div><div class="procedure" id="pro-ocfs2-mount-cluster" data-id-title="Mounting an OCFS2 Volume with the Cluster Resource Manager"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.4: </span><span class="title-name">Mounting an OCFS2 Volume with the Cluster Resource Manager </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-mount-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To mount an OCFS2 volume with the High Availability software, configure an
    ocfs2 file system resource in the cluster. The following procedure uses
    the <code class="command">crm</code> shell to configure the cluster resources.
    Alternatively, you can also use Hawk2 to configure the resources as
    described in <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="16.6. Configuring OCFS2 Resources With Hawk2">Section 16.6, “Configuring OCFS2 Resources With Hawk2”</a>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure Pacemaker to mount the OCFS2 file system on every node in
     the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" \
  fstype="ocfs2" options="acl" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Stopped"</pre></div></li><li class="step"><p>
     Add the <code class="literal">ocfs2-1</code> primitive
     to the <code class="literal">g-storage</code> group you created in
     <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-storage add ocfs2-1</pre></div><p>The <code class="command">add</code> subcommand appends the new group
     member by default. Because of the base group's internal colocation and ordering, Pacemaker
     will only start the <code class="systemitem">ocfs2-1</code>
     resource on nodes that also have a <code class="literal">dlm</code> resource
     already running.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.

    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">exit</code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-rsc-hawk2" data-id-title="Configuring OCFS2 Resources With Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.6 </span><span class="title-name">Configuring OCFS2 Resources With Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-rsc-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Instead of configuring the DLM and the file system resource for OCFS2
   manually with the crm shell, you can also use the OCFS2 template
   in Hawk2's <span class="guimenu">Setup Wizard</span>.
  </p><div id="id-1.3.5.4.8.3" data-id-title="Differences Between Manual Configuration and Hawk2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Differences Between Manual Configuration and Hawk2</div><p>
    The OCFS2 template in the <span class="guimenu">Setup Wizard</span> does
    <span class="emphasis"><em>not</em></span> include the configuration of a STONITH
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a STONITH resource as
    described in <a class="xref" href="#pro-ocfs2-stonith" title="Configuring a STONITH Resource">Procedure 16.1, “Configuring a STONITH Resource”</a>.
   </p><p>
    Using the OCFS2 template in the Hawk2 <span class="guimenu">Setup
    Wizard</span> also leads to a slightly different resource
    configuration than the manual configuration described in
    <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a> and
    <a class="xref" href="#pro-ocfs2-mount-cluster" title="Mounting an OCFS2 Volume with the Cluster Resource Manager">Procedure 16.4, “Mounting an OCFS2 Volume with the Cluster Resource Manager”</a>.
   </p></div><div class="procedure" id="pro-ha-ocfs2-rsc-hawk" data-id-title="Configuring OCFS2 Resources with Hawk2s Wizard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.5: </span><span class="title-name">Configuring OCFS2 Resources with Hawk2's <span class="guimenu">Wizard</span> </span></span><a title="Permalink" class="permalink" href="#pro-ha-ocfs2-rsc-hawk">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Wizard</span>.
    </p></li><li class="step"><p>
     Expand the <span class="guimenu">File System</span> category and select
     <code class="literal">OCFS2 File System</code>.
    </p></li><li class="step"><p>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in Hawk2. After the last
     configuration step, <span class="guimenu">Verify</span> the values you have entered.
    </p><p>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-wizard-ocfs2-verify.png"><img src="images/hawk2-wizard-ocfs2-verify.png" width="70%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </p><p>
     A message on the screen shows if the action has been successful.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-quota" data-id-title="Using Quotas on OCFS2 File Systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.7 </span><span class="title-name">Using Quotas on OCFS2 File Systems</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-quota">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To use quotas on an OCFS2 file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <code class="literal">ursquota</code> (quota for individual users) or
   <code class="literal">grpquota</code> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <code class="command">tunefs.ocfs2</code>.
  </p><p>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   OCFS2 treats quota information as file system-internal metadata, you
   do not need to run the <code class="command">quotacheck</code>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </p><p>
   To enable enforcement of limits imposed on each user or group, run
   <code class="command">quotaon</code>(8) like you would do for any other file
   system.
  </p><p>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tuneable with
   <code class="command">tunefs.ocfs2</code>, options
   <code class="option">usrquota-sync-interval</code> and
   <code class="option">grpquota-sync-interval</code>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </p></section><section class="sect1" id="sec-ha-ocfs2-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For more information about OCFS2, see the following links:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.10.3.1"><span class="term"><a class="link" href="https://ocfs2.wiki.kernel.org/" target="_blank">https://ocfs2.wiki.kernel.org/</a>
    </span></dt><dd><p>
      The OCFS2 project home page.
     </p></dd><dt id="id-1.3.5.4.10.3.2"><span class="term"><a class="link" href="http://oss.oracle.com/projects/ocfs2/" target="_blank">http://oss.oracle.com/projects/ocfs2/</a>
    </span></dt><dd><p>
      The former OCFS2 project home page at Oracle.
     </p></dd><dt id="id-1.3.5.4.10.3.3"><span class="term"><a class="link" href="http://oss.oracle.com/projects/ocfs2/documentation" target="_blank">http://oss.oracle.com/projects/ocfs2/documentation</a>
    </span></dt><dd><p>
      The project's former documentation home page.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-ha-gfs2" data-id-title="GFS2"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">17 </span><span class="title-name">GFS2</span></span> <a title="Permalink" class="permalink" href="#cha-ha-gfs2">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating-mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </p><p>
    SUSE recommends OCFS2 over GFS2 for your cluster environments if
    performance is one of your major requirements. Our tests have revealed
    that OCFS2 performs better as compared to GFS2 in such settings.
   </p></div></div></div></div><div id="id-1.3.5.5.3" data-id-title="GFS2 support" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: GFS2 support</div><p>
      SUSE only supports GFS2 in read-only mode. Write operations are not supported.
     </p></div><section class="sect1" id="sec-ha-gfs2-utils" data-id-title="GFS2 Packages and Management Utilities"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.1 </span><span class="title-name">GFS2 Packages and Management Utilities</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-utils">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To use GFS2, make sure
   <code class="systemitem">gfs2-utils</code> and a matching
   <code class="systemitem">gfs2-kmp-*</code> package for your
   Kernel is installed on each node of the cluster.
  </p><p>
   The <code class="systemitem">gfs2-utils</code> package provides
   the following utilities for management of GFS2 volumes. For syntax
   information, see their man pages.
  </p><div class="table" id="id-1.3.5.5.4.4" data-id-title="GFS2 Utilities"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 17.1: </span><span class="title-name">GFS2 Utilities </span></span><a title="Permalink" class="permalink" href="#id-1.3.5.5.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        GFS2 Utility
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        fsck.gfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Checks the file system for errors and optionally repairs errors.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        gfs2_jadd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Adds additional journals to a GFS2 file system.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        gfs2_grow
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Grow a GFS2 file system.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mkfs.gfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Create a GFS2 file system on a device, usually a shared device or
        partition.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        tunegfs2
       </p>
      </td><td>
       <p>
        Allows viewing and manipulating the GFS2 file system parameters such
        as <code class="varname">UUID</code>, <code class="varname">label</code>,
        <code class="varname">lockproto</code> and <code class="varname">locktable</code>.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-gfs2-create-service" data-id-title="Configuring GFS2 Services and a STONITH Resource"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.2 </span><span class="title-name">Configuring GFS2 Services and a STONITH Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-create-service">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can create GFS2 volumes, you must configure DLM and a
   STONITH resource.
  </p><div class="procedure" id="pro-gfs2-stonith" data-id-title="Configuring a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.1: </span><span class="title-name">Configuring a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.3.5.5.5.3.2" data-id-title="STONITH Device Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: STONITH Device Needed</div><p>
     You need to configure a fencing device. Without a STONITH
     mechanism (like <code class="literal">external/sbd</code>) in place the
     configuration will fail.
    </p></div><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Create an SBD partition as described in
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 10.3, “Initializing the SBD Devices”</a>.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure <code class="literal">external/sbd</code> as fencing device with
     <code class="literal">/dev/sdb2</code> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
    params pcmk_delay_max=30 meta target-role="Started"</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">exit</code>.
    </p></li></ol></div></div><p>
    For details on configuring the resource group for DLM, see <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a>.
  </p></section><section class="sect1" id="sec-ha-gfs2-create" data-id-title="Creating GFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.3 </span><span class="title-name">Creating GFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured DLM as cluster resources as described in
   <a class="xref" href="#sec-ha-gfs2-create-service" title="17.2. Configuring GFS2 Services and a STONITH Resource">Section 17.2, “Configuring GFS2 Services and a STONITH Resource”</a>, configure your system to
   use GFS2 and create GFS2 volumes.
  </p><div id="id-1.3.5.5.6.3" data-id-title="GFS2 Volumes for Application and Data Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: GFS2 Volumes for Application and Data Files</div><p>
    We recommend that you generally store application files and data files
    on different GFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </p></div><p>
   Before you begin, prepare the block devices you plan to use for your GFS2
   volumes. Leave the devices as free space.
  </p><p>
   Then create and format the GFS2 volume with the
   <code class="command">mkfs.gfs2</code> as described in
   <a class="xref" href="#pro-gfs2-volume" title="Creating and Formatting a GFS2 Volume">Procedure 17.2, “Creating and Formatting a GFS2 Volume”</a>. The most important parameters for the
   command are listed in <a class="xref" href="#tab-ha-gfs2-mkfs-gfs2-params" title="Important GFS2 Parameters">Table 17.2, “Important GFS2 Parameters”</a>. For
   more information and the command syntax, refer to the
   <code class="command">mkfs.gfs2</code> man page.
  </p><div class="table" id="tab-ha-gfs2-mkfs-gfs2-params" data-id-title="Important GFS2 Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 17.2: </span><span class="title-name">Important GFS2 Parameters </span></span><a title="Permalink" class="permalink" href="#tab-ha-gfs2-mkfs-gfs2-params">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        GFS2 Parameter
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description and Recommendation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Lock Protocol Name (<code class="option">-p</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The name of the locking protocol to use. Acceptable locking
        protocols are lock_dlm (for shared storage) or if you are using GFS2
        as a local file system (1 node only), you can specify the
        lock_nolock protocol. If this option is not specified, lock_dlm
        protocol will be assumed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Lock Table Name (<code class="option">-t</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The lock table field appropriate to the lock module you are using.
        It is
        <em class="replaceable">clustername</em>:<em class="replaceable">fsname</em>.
        <em class="replaceable">clustername</em> must match that in the
        cluster configuration file, <code class="filename">/etc/corosync/corosync.conf</code>. Only members of this
        cluster are permitted to use this file system.
        <em class="replaceable">fsname</em> is a unique file system name used
        to distinguish this GFS2 file system from others created (1 to 16
        characters).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Number of Journals (<code class="option">-j</code>)
       </p>
      </td><td>
       <p>
        The number of journals for gfs2_mkfs to create. You need at least
        one journal per machine that will mount the file system. If this
        option is not specified, one journal will be created.
       </p>
      </td></tr></tbody></table></div></div><div class="procedure" id="pro-gfs2-volume" data-id-title="Creating and Formatting a GFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.2: </span><span class="title-name">Creating and Formatting a GFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-volume">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Execute the following steps only on <span class="emphasis"><em>one</em></span> of the
    cluster nodes.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Create and format the volume using the <code class="command">mkfs.gfs2</code>
     utility. For information about the syntax for this command, refer to
     the <code class="command">mkfs.gfs2</code> man page.
    </p><p>
     For example, to create a new GFS2 file system on
     <code class="filename">/dev/sdb1</code> that supports up to 32 cluster nodes,
     use the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkfs.gfs2 -t hacluster:mygfs2 -p lock_dlm -j 32 /dev/sdb1</pre></div><p>
     The <code class="systemitem">hacluster</code> name relates to
     the entry <code class="option">cluster_name</code> in the file
     <code class="filename">/etc/corosync/corosync.conf</code> (this is the default).
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-gfs2-mount" data-id-title="Mounting GFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.4 </span><span class="title-name">Mounting GFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-mount">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can either mount a GFS2 volume manually or with the cluster manager,
   as described in <a class="xref" href="#pro-gfs2-mount-cluster" title="Mounting a GFS2 Volume with the Cluster Manager">Procedure 17.4, “Mounting a GFS2 Volume with the Cluster Manager”</a>.
  </p><div class="procedure" id="pro-gfs2-mount-manual" data-id-title="Manually Mounting a GFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.3: </span><span class="title-name">Manually Mounting a GFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-mount-manual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Mount the volume from the command line, using the
     <code class="command">mount</code> command.
    </p></li></ol></div></div><div id="id-1.3.5.5.7.4" data-id-title="Manually Mounted GFS2 Devices" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Manually Mounted GFS2 Devices</div><p>
    If you mount the GFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it by means of cluster
    resources.
   </p></div><div class="procedure" id="pro-gfs2-mount-cluster" data-id-title="Mounting a GFS2 Volume with the Cluster Manager"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.4: </span><span class="title-name">Mounting a GFS2 Volume with the Cluster Manager </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-mount-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To mount a GFS2 volume with the High Availability software, configure an OCF file
    system resource in the cluster. The following procedure uses the
    <code class="command">crm</code> shell to configure the cluster resources.
    Alternatively, you can also use Hawk2 to configure the resources.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure Pacemaker to mount the GFS2 file system on every node in the
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> gfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" fstype="gfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Stopped"</pre></div></li><li class="step"><p>
     Create a base group that consists of the <code class="literal">dlm</code>
     primitive you created in <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a> and the
     <code class="literal">gfs2-1</code> primitive. Clone the group:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-storage dlm  gfs2-1
     <code class="command">clone</code> cl-storage g-storage \
     meta interleave="true"</pre></div><p>
     Because of the base group's internal colocation and ordering, Pacemaker
     will only start the <code class="systemitem">gfs2-1</code>
     resource on nodes that also have a <code class="literal">dlm</code> resource
     already running.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.

    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">exit</code>.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-drbd" data-id-title="DRBD"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">18 </span><span class="title-name">DRBD</span></span> <a title="Permalink" class="permalink" href="#cha-ha-drbd">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The <span class="emphasis"><em>distributed replicated block device</em></span> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with Corosync,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </p></div></div></div></div><section class="sect1" id="sec-ha-drbd-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   DRBD replicates data on the primary device to the secondary device in a
   way that ensures that both copies of the data remain identical. Think of
   it as a networked RAID 1. It mirrors data in real-time, so its
   replication occurs continuously. Applications do not need to know that in
   fact their data is stored on different disks.

  </p><p>
   DRBD is a Linux Kernel module and sits between the I/O scheduler at the
   lower end and the file system at the upper end, see
   <a class="xref" href="#fig-ha-drbd-concept" title="Position of DRBD within Linux">Figure 18.1, “Position of DRBD within Linux”</a>. To communicate with DRBD, users
   use the high-level command <code class="command">drbdadm</code>. For maximum
   flexibility DRBD comes with the low-level tool
   <code class="command">drbdsetup</code>.
  </p><div class="figure" id="fig-ha-drbd-concept"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_drbd.png"><img src="images/ha_drbd.png" width="80%" alt="Position of DRBD within Linux" title="Position of DRBD within Linux"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 18.1: </span><span class="title-name">Position of DRBD within Linux </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-concept">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><div id="id-1.3.5.6.3.5" data-id-title="Unencrypted Data" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Unencrypted Data</div><p>
    The data traffic between mirrors is not encrypted. For secure data
    exchange, you should deploy a Virtual Private Network (VPN) solution for
    the connection.
   </p></div><p>
   DRBD allows you to use any block device supported by Linux, usually:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     partition or complete hard disk
    </p></li><li class="listitem"><p>
     software RAID
    </p></li><li class="listitem"><p>
     Logical Volume Manager (LVM)
    </p></li><li class="listitem"><p>
     Enterprise Volume Management System (EVMS)
    </p></li></ul></div><p>
   By default, DRBD uses the TCP ports <code class="literal">7788</code> and higher
   for communication between DRBD nodes. Make sure that your firewall does
   not prevent communication on the used ports.
  </p><p>
   You must set up the DRBD devices before creating file systems on them.
   Everything pertaining to user data should be done solely via the
   <code class="filename">/dev/drbd<em class="replaceable">N</em></code> device and
   not on the raw device, as DRBD uses the last part of the raw device for
   metadata. Using the raw device will cause inconsistent data.
  </p><p>
   With udev integration, you will also get symbolic links in the form
   <code class="filename">/dev/drbd/by-res/<em class="replaceable">RESOURCES</em></code>
   which are easier to use and provide safety against misremembering the
   devices' minor number.
  </p><p>
   For example, if the raw device is 1024 MB in size, the DRBD
   device has only 1023 MB available for data, with about
   70 KB hidden and reserved for the metadata. Any attempt to access
   the remaining kilobytes via raw disks fails because
   it is not available for user data.
   
  </p></section><section class="sect1" id="sec-ha-drbd-install" data-id-title="Installing DRBD Services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.2 </span><span class="title-name">Installing DRBD Services</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Install the High Availability pattern on both SUSE Linux Enterprise Server machines in your networked
   cluster as described in <a class="xref" href="#part-install" title="Part I. Installation and Setup">Part I, “Installation and Setup”</a>. Installing
   the pattern also installs the DRBD program files.
  </p><p>
   If you do not need the complete cluster stack but only want to use DRBD,
   install the packages <span class="package">drbd</span>,
    <span class="package">drbd-kmp-<em class="replaceable">FLAVOR</em></span>,
    <span class="package">drbd-utils</span>, and <span class="package">yast2-drbd</span>.
  </p><p>
   To simplify the work with <code class="command">drbdadm</code>, use the Bash
   completion support.
   If you want to enable it in your current shell session, insert the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">source</code> /etc/bash_completion.d/drbdadm.sh</pre></div><p>
   To use it permanently for <code class="systemitem">root</code>, create, or extend a file
   <code class="filename">/root/.bashrc</code> and insert the previous line.
  </p></section><section class="sect1" id="sec-ha-drbd-configure" data-id-title="Setting Up DRBD Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.3 </span><span class="title-name">Setting Up DRBD Service</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.5.6.5.2" data-id-title="Adjustments Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Adjustments Needed</div><p>
    The following procedure uses the server names alice and bob,
    and the cluster resource name <code class="literal">r0</code>. It sets up
    alice as the primary node and <code class="filename">/dev/sda1</code> for
    storage. Make sure to modify the instructions to use your own nodes and
    file names.
   </p></div><p>
   The following sections assumes you have two nodes, alice
   and bob, and that they should use the TCP port <code class="literal">7788</code>.
   Make sure this port is open in your firewall.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Prepare your system:</p><ol type="a" class="substeps"><li class="step"><p>Make sure the block devices in your Linux nodes are ready
            and partitioned (if needed).</p></li><li class="step"><p>
            If your disk already contains a file system that you do not need
            anymore, destroy the file system structure with the following
            command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">dd</code> if=/dev/zero of=<em class="replaceable">YOUR_DEVICE</em> count=16 bs=1M</pre></div><p>If you have more file systems to destroy, repeat this step on
           all devices you want to include into your DRBD setup.</p></li><li class="step"><p>If the cluster is already using DRBD, put your cluster
              in maintenance mode: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=true</pre></div><p> If you skip this step when your cluster uses already
              DRBD, a syntax error in the live configuration will lead
              to a service shutdown. </p><p>As an alternative, you can also use
                <code class="command">drbdadm</code>
              <code class="option">-c <em class="replaceable">FILE</em></code> to
              test a configuration file.</p></li></ol></li><li class="step"><p>Configure DRBD by choosing your method:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="#sec-ha-drbd-configure-manually" title="18.3.1. Configuring DRBD Manually">Section 18.3.1, “Configuring DRBD Manually”</a></p></li><li class="listitem"><p><a class="xref" href="#sec-ha-drbd-configure-yast" title="18.3.2. Configuring DRBD with YaST">Section 18.3.2, “Configuring DRBD with YaST”</a></p></li></ul></div></li><li class="step"><p>
        If you have configured Csync2 (which should be the default), the
        DRBD configuration files are already included in the list of files that
        need to be synchronized. To synchronize them, run the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -xv</pre></div><p>
        If you do not have Csync2 (or do not want to use it), copy the DRBD
        configuration files manually to the other node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.conf bob:/etc/
<code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.d/*  bob:/etc/drbd.d/</pre></div></li><li class="step"><p>Perform the initial synchronization (see <a class="xref" href="#sec-ha-drbd-configure-init" title="18.3.3. Initializing and Formatting DRBD Resource">Section 18.3.3, “Initializing and Formatting DRBD Resource”</a>).</p></li><li class="step"><p>
        Reset the cluster's maintenance mode flag:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=false</pre></div></li></ol></div></div><section class="sect2" id="sec-ha-drbd-configure-manually" data-id-title="Configuring DRBD Manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.1 </span><span class="title-name">Configuring DRBD Manually</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-manually">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.5.6.5.6.2" data-id-title="Restricted Support of Auto Promote Feature" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restricted Support of <span class="quote">“<span class="quote">Auto Promote</span>”</span> Feature</div><p>
      The DRBD9 feature <span class="quote">“<span class="quote">auto promote</span>”</span> can use a clone
      and file system resource instead of a master/slave connection. When using
      this feature while a file system is being mounted, DRBD will change to
      primary mode automatically.
     </p><p>
      The auto promote feature has currently restricted support.
      With DRBD 9, SUSE supports the same use cases that were also supported
 with DRBD 8. Use cases beyond that, such as setups with more than two
 nodes, are not supported.
     </p></div><p>
     To set up DRBD manually, proceed as follows:
    </p><div class="procedure" id="pro-drbd-configure" data-id-title="Manually Configuring DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.1: </span><span class="title-name">Manually Configuring DRBD </span></span><a title="Permalink" class="permalink" href="#pro-drbd-configure">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>Beginning with DRBD version 8.3, the former configuration file is
      split into separate files, located under the directory
      <code class="filename">/etc/drbd.d/</code>.</p><ol class="procedure" type="1"><li class="step"><p>
       Open the file <code class="filename">/etc/drbd.d/global_common.conf</code>. It
       contains already some global, pre-defined values. Go to the
       <code class="literal">startup</code> section and insert these lines:
      </p><div class="verbatim-wrap"><pre class="screen">startup {
    # wfc-timeout degr-wfc-timeout outdated-wfc-timeout
    # wait-after-sb;
    wfc-timeout 100;
    degr-wfc-timeout 120;
}</pre></div><p>
       These options are used to reduce the timeouts when booting, see
       <a class="link" href="https://docs.linbit.com/docs/users-guide-9.0/#ch-configure" target="_blank">https://docs.linbit.com/docs/users-guide-9.0/#ch-configure</a>
       for more details.
      </p></li><li class="step"><p>
       Create the file <code class="filename">/etc/drbd.d/r0.res</code>. Change the
       lines according to your situation and save it:
      </p><div class="verbatim-wrap"><pre class="screen">resource r0 { <span class="callout" id="co-drbd-config-r0">1</span>
  device /dev/drbd0; <span class="callout" id="co-drbd-config-device">2</span>
  disk /dev/sda1; <span class="callout" id="co-drbd-config-disk">3</span>
  meta-disk internal; <span class="callout" id="co-drbd-config-meta-disk">4</span>
  on alice { <span class="callout" id="co-drbd-config-resname">5</span>
    address  192.168.1.10:7788; <span class="callout" id="co-drbd-config-address">6</span>
    node-id 0; <span class="callout" id="co-drbd-config-node-id">7</span>
  }
  on bob { <a class="xref" href="#co-drbd-config-resname"><span class="callout">5</span></a>
    address 192.168.1.11:7788; <a class="xref" href="#co-drbd-config-address"><span class="callout">6</span></a>
    node-id 1; <a class="xref" href="#co-drbd-config-node-id"><span class="callout">7</span></a>
  }
  disk {
    resync-rate 10M; <span class="callout" id="co-drbd-config-syncer-rate">8</span>
  }
  connection-mesh { <span class="callout" id="co-drbd-config-connection-mesh">9</span>
    hosts alice bob;
  }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-r0"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         DRBD resource name that allows some association to the service that needs them.
         For example, <code class="systemitem">nfs</code>,
         <code class="systemitem">http</code>, <code class="systemitem">mysql_0</code>,
         <code class="systemitem">postgres_wal</code>, etc.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The device name for DRBD and its minor number.
        </p><p>
         In the example above, the minor number 0 is used for DRBD. The udev
         integration scripts will give you a symbolic link
         <code class="filename">/dev/drbd/by-res/nfs/0</code>. Alternatively, omit
         the device node name in the configuration and use the following
         line instead:
        </p><p>
         <code class="literal">drbd0 minor 0</code> (<code class="literal">/dev/</code> is
         optional) or <code class="literal">/dev/drbd0</code>
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-disk"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The raw device that is replicated between nodes. Note, in this
         example the devices are the <span class="emphasis"><em>same</em></span> on both nodes.
         If you need different devices, move the <code class="literal">disk</code>
          parameter into the <code class="literal">on</code> host.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-meta-disk"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The meta-disk parameter usually contains the value
         <code class="literal">internal</code>, but it is possible to specify an
         explicit device to hold the meta data. See
         <a class="link" href="https://docs.linbit.com/docs/users-guide-9.0/#s-metadata" target="_blank">https://docs.linbit.com/docs/users-guide-9.0/#s-metadata</a>
         for more information.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-resname"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The <code class="literal">on</code> section states which host this
         configuration statement applies to.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-address"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The IP address and port number of the respective node. Each
         resource needs an individual port, usually starting with
         <code class="literal">7788</code>. Both ports must be the same for a
          DRBD resource.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-node-id"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The node ID is required when configuring more than two nodes. It
        is a unique, non-negative integer to distinguish the different
        nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-syncer-rate"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The synchronization rate. Set it to one third of the lower of the
         disk- and network bandwidth. It only limits the resynchronization,
         not the replication.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-connection-mesh"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines all nodes of a mesh.
       The <code class="option">hosts</code> parameter contains all host names that
       share the same DRBD setup.
    </p></td></tr></table></div></li><li class="step"><p>
      Check the syntax of your configuration file(s). If the following
      command returns an error, verify your files:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> dump all</pre></div></li><li class="step"><p>Continue with <a class="xref" href="#sec-ha-drbd-configure-init" title="18.3.3. Initializing and Formatting DRBD Resource">Section 18.3.3, “Initializing and Formatting DRBD Resource”</a>.</p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-configure-yast" data-id-title="Configuring DRBD with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.2 </span><span class="title-name">Configuring DRBD with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>YaST can be used to start with an initial setup of DRBD.
        After you have created your DRBD setup, you can fine-tune the
        generated files manually.
    </p><p>
        However, when you have changed the configuration files,
        do not use the YaST DRBD module anymore. The DRBD module supports
        only a limited set of basic configuration. If you use it again,
        it is very likely that the module will not show your changes.
    </p><p>
      To set up DRBD with YaST, proceed as follows:</p><div class="procedure" id="pro-drbd-configure-yast" data-id-title="Using YaST to Configure DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 18.2: </span><span class="title-name">Using YaST to Configure DRBD </span></span><a title="Permalink" class="permalink" href="#pro-drbd-configure-yast">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start YaST and select the configuration module <span class="guimenu">High Availability</span> › <span class="guimenu">DRBD</span>. If you already have a DRBD configuration, YaST
     warns you. YaST will change your configuration and will save your
     old DRBD configuration files as <code class="filename">*.YaSTsave</code>.
    </p></li><li class="step"><p>
     Leave the booting flag in <span class="guimenu">Start-up
     Configuration</span> › <span class="guimenu">Booting</span> as it is
     (by default it is <code class="literal">off</code>); do not change that as
     Pacemaker manages this service.
    </p></li><li class="step"><p>If you have a firewall running, enable <span class="guimenu">Open Port in
      Firewall</span>.</p></li><li class="step"><p>Go to the <span class="guimenu">Resource Configuration</span> entry.
      Click <span class="guimenu">Add</span> to create a new resource (see
      <a class="xref" href="#fig-ha-drbd-yast-resconfig" title="Resource Configuration">Figure 18.2, “Resource Configuration”</a>).
    </p><div class="figure" id="fig-ha-drbd-yast-resconfig"><div class="figure-contents"><div class="mediaobject"><a href="images/yast2_drbd-resconfig.png"><img src="images/yast2_drbd-resconfig.png" width="90%" alt="Resource Configuration" title="Resource Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 18.2: </span><span class="title-name">Resource Configuration </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-yast-resconfig">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><p>
      The following parameters need to be set:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.6.5.7.5.5.4.1"><span class="term"><span class="guimenu">Resource Name</span></span></dt><dd><p>The name of the DRBD resource (mandatory)</p></dd><dt id="id-1.3.5.6.5.7.5.5.4.2"><span class="term"><span class="guimenu">Name</span></span></dt><dd><p>The host name of the relevant node</p></dd><dt id="id-1.3.5.6.5.7.5.5.4.3"><span class="term"><span class="guimenu">Address:Port</span></span></dt><dd><p>
          The IP address and port number (default
           <code class="systemitem">7788</code>) for the respective
          node
         </p></dd><dt id="id-1.3.5.6.5.7.5.5.4.4"><span class="term"><span class="guimenu">Device</span></span></dt><dd><p>
          The block device path that is used to access the replicated data.
          If the device contains a minor number, the associated block device
          is usually named <code class="filename">/dev/drbdX</code>, where
          <em class="replaceable">X</em> is the device minor number. If the
          device does not contain a minor number, make sure to add
          <code class="literal">minor 0</code> after the device name.
         </p></dd><dt id="id-1.3.5.6.5.7.5.5.4.5"><span class="term"><span class="guimenu">Disk</span></span></dt><dd><p>
          The raw device that is replicated between both nodes. If you use
          LVM, insert your LVM device name.
         </p></dd><dt id="id-1.3.5.6.5.7.5.5.4.6"><span class="term"><span class="guimenu">Meta-disk</span></span></dt><dd><p>
          The <span class="guimenu">Meta-disk</span> is either set to the value
          <code class="literal">internal</code> or specifies an explicit device
          extended by an index to hold the meta data needed by DRBD.
         </p><p>
          A real device may also be used for multiple drbd resources. For
          example, if your <span class="guimenu">Meta-Disk</span> is
          <code class="filename">/dev/sda6[0]</code> for the first resource, you may
          use <code class="filename">/dev/sda6[1]</code> for the second resource.
          However, there must be at least 128 MB space for each resource
          available on this disk. The fixed metadata size limits the maximum
          data size that you can replicate.
         </p></dd></dl></div><p>
     All of these options are explained in the examples in the
     <code class="filename">/usr/share/doc/packages/drbd/drbd.conf</code> file and in
     the man page of <code class="command">drbd.conf(5)</code>.
    </p></li><li class="step"><p>Click <span class="guimenu">Save</span>.</p></li><li class="step"><p>Click <span class="guimenu">Add</span> to enter the second DRBD resource
     and finish with <span class="guimenu">Save</span>.
    </p></li><li class="step"><p>Close the resource configuration with <span class="guimenu">Ok</span>
      and <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>If you use LVM with DRBD, it is necessary to change some options
       in the LVM configuration file (see the <span class="guimenu">LVM
       Configuration</span> entry). This change can be done by the
       YaST DRBD module automatically.
     </p><p>
       The disk name of localhost for the DRBD resource and the default filter
       will be rejected in the LVM filter. Only <code class="filename">/dev/drbd</code>
       can be scanned for an LVM device.</p><p>
       For example, if <code class="filename">/dev/sda1</code> is used as a DRBD disk,
       the device name will be inserted as the first entry in the LVM filter.
       To change the filter manually, click the
       <span class="guimenu">Modify LVM Device Filter Automatically</span> check box.
     </p></li><li class="step"><p>Save your changes with <span class="guimenu">Finish</span>.</p></li><li class="step"><p>Continue with <a class="xref" href="#sec-ha-drbd-configure-init" title="18.3.3. Initializing and Formatting DRBD Resource">Section 18.3.3, “Initializing and Formatting DRBD Resource”</a>.</p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-configure-init" data-id-title="Initializing and Formatting DRBD Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.3.3 </span><span class="title-name">Initializing and Formatting DRBD Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-init">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>After you have prepared your system and configured DRBD,
      initialize your disk for the first time:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
           On <span class="emphasis"><em>both</em></span> nodes (alice and bob),
           initialize the meta data storage:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> create-md r0
<code class="prompt root"># </code><code class="command">drbdadm</code> up r0</pre></div></li><li class="step"><p>
          To shorten the initial resynchronization of your DRBD resource
          check the following:
         </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            If the DRBD devices on all nodes have the same data (for example,
            by destroying the file system structure with the
            <code class="command">dd</code> command as shown in
            <a class="xref" href="#sec-ha-drbd-configure" title="18.3. Setting Up DRBD Service">Section 18.3, “Setting Up DRBD Service”</a>), then skip the initial
            resynchronization with the following command (on both nodes):
           </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> new-current-uuid --clear-bitmap r0/0</pre></div><p>The state will be <code class="literal">Secondary/Secondary UpToDate/UpToDate</code></p></li><li class="listitem"><p>
            Otherwise, proceed with the next step.
           </p></li></ul></div></li><li class="step"><p>
          On the primary node alice, start the resynchronization process:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary --force r0</pre></div></li><li class="step"><p> Check the status with: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0
r0 role:Primary
  disk:UpToDate
  bob role:Secondary
  peer-disk:UpToDate</pre></div></li><li class="step"><p> Create your file system on top of your DRBD device, for
            example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ext3</code> /dev/drbd0</pre></div></li><li class="step"><p> Mount the file system and use it: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> /dev/drbd0 /mnt/</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-drbd-migrate" data-id-title="Migrating from DRBD 8 to DRBD 9"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.4 </span><span class="title-name">Migrating from DRBD 8 to DRBD 9</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-migrate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Between DRBD 8 (shipped with SUSE Linux Enterprise High Availability 12 SP1) and
   DRBD 9 (shipped with SUSE Linux Enterprise High Availability12 SP2), the metadata format
   has changed. DRBD 9 does not automatically convert previous metadata
   files to the new format.
  </p><p>
   After migrating to 12 SP2 and before starting DRBD, convert the DRBD
   metadata to the version 9 format manually. To do so, use
   <code class="command">drbdadm</code> <code class="option">create-md</code>. No configuration
   needs to be changed.
  </p><div id="id-1.3.5.6.6.4" data-id-title="Restricted Support" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restricted Support</div><p>With DRBD 9, SUSE supports the same use cases that were also supported
 with DRBD 8. Use cases beyond that, such as setups with more than two
 nodes, are not supported.</p></div><p>
   DRBD 9 will fall back to be compatible with version 8.
   For three nodes and more, you need to re-create the metadata
   to use DRBD version 9 specific options.
  </p><p>
   If you have a stacked DRBD resource, refer also to <a class="xref" href="#sec-ha-drbd-resource-stacking" title="18.5. Creating a Stacked DRBD Device">Section 18.5, “Creating a Stacked DRBD Device”</a> for more information.
  </p><p>
   To keep your data and allow to add new nodes without recreating new
   resources, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set one node in standby mode.
    </p></li><li class="step"><p>
     Update all the DRBD packages on all of your nodes, see <a class="xref" href="#sec-ha-drbd-install" title="18.2. Installing DRBD Services">Section 18.2, “Installing DRBD Services”</a>.
    </p></li><li class="step"><p>Add the new node information to your resource configuration:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><em class="parameter">node-id</em> on every <code class="literal">on</code>
       section.
      </p></li><li class="listitem"><p>
       <em class="parameter">connection-mesh</em> section contains all host names
       in the <em class="parameter">hosts</em> parameter.
      </p></li></ul></div><p>
     See the example configuration in <a class="xref" href="#pro-drbd-configure" title="Manually Configuring DRBD">Procedure 18.1, “Manually Configuring DRBD”</a>.
    </p></li><li class="step"><p>
     Enlarge the space of your DRBD disks when using <code class="literal">internal</code>
     as <code class="literal">meta-disk</code> key. Use a device that supports enlarging
     the space like LVM.
     As an alternative, change to an external disk for metadata
     and use <code class="literal">meta-disk <em class="replaceable">DEVICE</em>;</code>.
    </p></li><li class="step"><p>
     Re-create the metadata based on the new configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> create-md <em class="replaceable">RESOURCE</em></pre></div></li><li class="step"><p>
     Cancel the standby mode.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-drbd-resource-stacking" data-id-title="Creating a Stacked DRBD Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.5 </span><span class="title-name">Creating a Stacked DRBD Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-resource-stacking">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A stacked DRBD device contains two other devices of which at least one
   device is also a DRBD resource. In other words, DRBD adds an additional
   node on top of an already existing DRBD resource (see <a class="xref" href="#fig-ha-drbd-resource-stacking" title="Resource Stacking">Figure 18.3, “Resource Stacking”</a>). Such a replication setup
   can be used for backup and disaster recovery purposes.
  </p><div class="figure" id="fig-ha-drbd-resource-stacking"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_stacked_drbd.png"><img src="images/ha_stacked_drbd.png" width="50%" alt="Resource Stacking" title="Resource Stacking"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 18.3: </span><span class="title-name">Resource Stacking </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-resource-stacking">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><p>
   Three-way replication uses asynchronous (DRBD protocol A) and
   synchronous replication (DRBD protocol C). The asynchronous part is used for
   the stacked resource whereas the synchronous part is used for the backup.
  </p><p>
   Your production environment uses the stacked device. For example,
   if you have a DRBD device <code class="filename">/dev/drbd0</code> and a stacked
   device <code class="filename">/dev/drbd10</code> on top, the file system will
   be created on <code class="filename">/dev/drbd10</code>, see <a class="xref" href="#exa-ha-drbd-stacked-drbd" title="Configuration of a Three-Node Stacked DRBD Resource">Example 18.1, “Configuration of a Three-Node Stacked DRBD Resource”</a> for more details.
  </p><div class="example" id="exa-ha-drbd-stacked-drbd" data-id-title="Configuration of a Three-Node Stacked DRBD Resource"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 18.1: </span><span class="title-name">Configuration of a Three-Node Stacked DRBD Resource </span></span><a title="Permalink" class="permalink" href="#exa-ha-drbd-stacked-drbd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># /etc/drbd.d/r0.res
resource r0 {
  protocol C;
  device    /dev/drbd0;
  disk      /dev/sda1;
  meta-disk internal;

  on amsterdam-alice {
    address    192.168.1.1:7900;
  }

  on amsterdam-bob {
    address    192.168.1.2:7900;
  }
}

resource r0-U {
  protocol A;
  device     /dev/drbd10;

  stacked-on-top-of r0 {
    address    192.168.2.1:7910;
  }

  on berlin-charlie {
    disk       /dev/sda10;
    address    192.168.2.2:7910; # Public IP of the backup node
    meta-disk  internal;
  }
}</pre></div></div></div></section><section class="sect1" id="sec-ha-drbd-fencing" data-id-title="Using Resource-Level Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.6 </span><span class="title-name">Using Resource-Level Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-fencing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When a DRBD replication link becomes interrupted, Pacemaker tries to promote
   the DRBD resource to another node. To prevent Pacemaker from starting a service
   with outdated data, enable resource-level fencing in the DRBD configuration
   file as shown in <a class="xref" href="#ex-ha-drbd-fencing" title="Configuration of DRBD with Resource-Level Fencing Using the Cluster Information Base (CIB)">Example 18.2, “Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB)”</a>.
  </p><div class="example" id="ex-ha-drbd-fencing" data-id-title="Configuration of DRBD with Resource-Level Fencing Using the Cluster Information Base (CIB)"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 18.2: </span><span class="title-name">Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB) </span></span><a title="Permalink" class="permalink" href="#ex-ha-drbd-fencing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">resource <em class="replaceable">RESOURCE</em> {
  net {
    fencing resource-only;
    # ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
    # ...
  }
  ...
}</pre></div></div></div><p>If the DRBD replication link becomes disconnected, DRBD does the
  following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>DRBD calls the <code class="command">crm-fence-peer.9.sh</code> script.</p></li><li class="listitem"><p>The script contacts the cluster manager.
    </p></li><li class="listitem"><p>The script determines the Pacemaker resource associated with this
     DRBD resource.</p></li><li class="listitem"><p>The script ensures that the DRBD resource no longer gets
    promoted to any other node. It stays on the currently active one.</p></li><li class="listitem"><p>If the replication link becomes connected again and DRBD
    completes its synchronization process, then the constraint is removed.
    The cluster manager is now free to promote the resource.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-drbd-test" data-id-title="Testing the DRBD Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.7 </span><span class="title-name">Testing the DRBD Service</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-test">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If the install and configuration procedures worked as expected, you are
   ready to run a basic test of the DRBD functionality. This test also helps
   with understanding how the software works.
  </p><div class="procedure" id="pro-drbd-test"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Test the DRBD service on alice.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a terminal console, then log in as
       <code class="systemitem">root</code>.
      </p></li><li class="step"><p>
       Create a mount point on alice, such as
       <code class="filename">/srv/r0</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> -p /srv/r0</pre></div></li><li class="step"><p>
       Mount the <code class="command">drbd</code> device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -o rw /dev/drbd0 /srv/r0</pre></div></li><li class="step"><p>
       Create a file from the primary node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">touch</code> /srv/r0/from_alice</pre></div></li><li class="step"><p>
       Unmount the disk on alice:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">umount</code> /srv/r0</pre></div></li><li class="step"><p>
       Downgrade the DRBD service on alice by typing the following
       command on alice:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary r0</pre></div></li></ol></li><li class="step"><p>
     Test the DRBD service on bob.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a terminal console, then log in as <code class="systemitem">root</code>
       on bob.
      </p></li><li class="step"><p>
       On bob, promote the DRBD service to primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
       On bob, check to see if bob is primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li><li class="step"><p> On bob, create a mount point such as
        <code class="filename">/srv/r0</code>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> /srv/r0</pre></div></li><li class="step"><p>
       On bob, mount the DRBD device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -o rw /dev/drbd0 /srv/r0</pre></div></li><li class="step"><p>
       Verify that the file you created on alice exists:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ls</code> /srv/r0/from_alice</pre></div><p> The <code class="filename">/srv/r0/from_alice</code> file should be
       listed. </p></li></ol></li><li class="step"><p>
     If the service is working on both nodes, the DRBD setup is complete.
    </p></li><li class="step"><p>
     Set up alice as the primary again.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Dismount the disk on bob by typing the following command on
       bob:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">umount</code> /srv/r0</pre></div></li><li class="step"><p>
       Downgrade the DRBD service on bob by typing the following
       command on bob:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary</pre></div></li><li class="step"><p>
       On alice, promote the DRBD service to primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary</pre></div></li><li class="step"><p>
       On alice, check to see if alice is primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li></ol></li><li class="step"><p>
     To get the service to automatically start and fail over if the server
     has a problem, you can set up DRBD as a high availability service with
     Pacemaker/Corosync. For information about installing and
     configuring for SUSE Linux Enterprise 12 SP4 see
     <a class="xref" href="#part-config" title="Part II. Configuration and Administration">Part II, “Configuration and Administration”</a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-drbd-tuning" data-id-title="Tuning DRBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.8 </span><span class="title-name">Tuning DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-tuning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are several ways to tune DRBD:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Use an external disk for your metadata. This might help, at the cost of
     maintenance ease.
    </p></li><li class="listitem"><p>
     Tune your network connection, by changing the receive and send buffer
     settings via <code class="command">sysctl</code>.
    </p></li><li class="listitem"><p>
     Change the <code class="systemitem">max-buffers</code>,
     <code class="systemitem">max-epoch-size</code> or both in the DRBD
     configuration.
    </p></li><li class="listitem"><p>
     Increase the <code class="systemitem">al-extents</code> value, depending on
     your IO patterns.
    </p></li><li class="listitem"><p>
     If you have a hardware RAID controller with a BBU (<span class="emphasis"><em>Battery
     Backup Unit</em></span>), you might benefit from setting
     <code class="systemitem">no-disk-flushes</code>,
     <code class="systemitem">no-disk-barrier</code> and/or
     <code class="systemitem">no-md-flushes</code>.
    </p></li><li class="listitem"><p>
     Enable read-balancing depending on your workload. See
     <a class="link" href="https://www.linbit.com/en/read-balancing/" target="_blank">https://www.linbit.com/en/read-balancing/</a> for
     more details.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-drbd-trouble" data-id-title="Troubleshooting DRBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.9 </span><span class="title-name">Troubleshooting DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The DRBD setup involves many components and problems may arise
   from different sources. The following sections cover several common
   scenarios and recommend various solutions.
  </p><section class="sect2" id="sec-ha-drbd-trouble-config" data-id-title="Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.1 </span><span class="title-name">Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If the initial DRBD setup does not work as expected, there is probably
    something wrong with your configuration.
   </p><p>
    To get information about the configuration:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal console, then log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Test the configuration file by running <code class="command">drbdadm</code> with
      the <code class="command">-d</code> option. Enter the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> -d adjust r0</pre></div><p>
      In a dry run of the <code class="command">adjust</code> option,
      <code class="command">drbdadm</code> compares the actual configuration of the
      DRBD resource with your DRBD configuration file, but it does not
      execute the calls. Review the output to make sure you know the source
      and cause of any errors.
     </p></li><li class="step"><p>
      If there are errors in the <code class="filename">/etc/drbd.d/*</code> and
      <code class="filename">drbd.conf</code> files, correct them before continuing.
     </p></li><li class="step"><p>
      If the partitions and settings are correct, run
      <code class="command">drbdadm</code> again without the <code class="command">-d</code>
      option.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> adjust r0</pre></div><p>
      This applies the configuration file to the DRBD resource.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-hostnames" data-id-title="Host Names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.2 </span><span class="title-name">Host Names</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-hostnames">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For DRBD, host names are case-sensitive (<code class="systemitem">Node0</code>
    would be a different host than <code class="systemitem">node0</code>), and
    compared to the host name as stored in the Kernel (see the
    <code class="command">uname -n</code> output).
   </p><p>
    If you have several network devices and want to use a dedicated network
    device, the host name will likely not resolve to the used IP address. In
    this case, use the parameter <code class="literal">disable-ip-verification</code>.
   </p></section><section class="sect2" id="sec-ha-drbd-port" data-id-title="TCP Port 7788"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.3 </span><span class="title-name">TCP Port 7788</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-port">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If your system cannot connect to the peer, this might be a problem with
    your local firewall. By default, DRBD uses the TCP port
    <code class="literal">7788</code> to access the other node. Make sure that this
    port is accessible on both nodes.
   </p></section><section class="sect2" id="sec-ha-drbd-trouble-broken" data-id-title="DRBD Devices Broken after Reboot"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">18.9.4 </span><span class="title-name">DRBD Devices Broken after Reboot</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble-broken">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In cases when DRBD does not know which of the real devices holds the
    latest data, it changes to a split brain condition. In this case, the
    respective DRBD subsystems come up as secondary and do not connect to
    each other. In this case, the following message can be found in the
    logging data:
   </p><div class="verbatim-wrap"><pre class="screen">Split-Brain detected, dropping connection!</pre></div><p>
    To resolve this situation, enter the following commands on the node which has
    data to be discarded:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary r0</pre></div><p>
    If the state is in <code class="literal">WFconnection</code>, disconnect first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> disconnect r0</pre></div><p>
    On the node which has the latest data enter the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> connect  --discard-my-data r0</pre></div><p>
    That resolves the issue by overwriting one node's data with the peer's
    data, therefore getting a consistent view on both nodes.
   </p></section></section><section class="sect1" id="sec-ha-drbd-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">18.10 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following open source resources are available for DRBD:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The project home page <a class="link" href="http://www.drbd.org" target="_blank">http://www.drbd.org</a>.
    </p></li><li class="listitem"><p>
     See <span class="intraxref">Article “Highly Available NFS Storage with DRBD and Pacemaker”</span>.
    </p></li><li class="listitem"><p>
     <a class="link" href="http://clusterlabs.org/wiki/DRBD_HowTo_1.0" target="_blank">http://clusterlabs.org/wiki/DRBD_HowTo_1.0</a> by the
     Linux Pacemaker Cluster Stack Project.
    </p></li><li class="listitem"><p>
     The following man pages for DRBD are available in the distribution:
     <code class="command">drbd(8)</code>, <code class="command">drbdmeta(8)</code>,
     <code class="command">drbdsetup(8)</code>, <code class="command">drbdadm(8)</code>,
     <code class="command">drbd.conf(5)</code>.
    </p></li><li class="listitem"><p>
     Find a commented example configuration for DRBD at
     <code class="filename">/usr/share/doc/packages/drbd-utils/drbd.conf.example</code>.
    </p></li><li class="listitem"><p>
     Furthermore, for easier storage administration across your cluster, see
     the recent announcement about the <em class="citetitle">DRBD-Manager</em>
     at <a class="link" href="https://www.linbit.com/en/drbd-manager/" target="_blank">https://www.linbit.com/en/drbd-manager/</a>.
    </p></li></ul></div></section></section><section class="chapter" id="cha-ha-clvm" data-id-title="Cluster Logical Volume Manager (cLVM)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-clvm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    When managing shared storage on a cluster, every node must be informed
    about changes that are done to the storage subsystem. The Logical Volume
    Manager 2 (LVM2), which is widely used to manage local storage,
    has been extended to support transparent management of volume groups
    across the whole cluster. Clustered volume groups can be managed using
    the same commands as local storage.
   </p></div></div></div></div><section class="sect1" id="sec-ha-clvm-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Clustered LVM2 is coordinated with different tools:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.3.3.1"><span class="term">Distributed Lock Manager (DLM)</span></dt><dd><p> Coordinates disk access for cLVM and mediates metadata access
      through locking.</p></dd><dt id="id-1.3.5.7.3.3.2"><span class="term">Logical Volume Manager2 (LVM2)</span></dt><dd><p>
      Enables flexible distribution of one file system over several disks.
      LVM2 provides a virtual pool of disk space.
     </p></dd><dt id="id-1.3.5.7.3.3.3"><span class="term">Clustered Logical Volume Manager (cLVM)</span></dt><dd><p>
      Coordinates access to the LVM2 metadata so every node knows about
      changes. cLVM does not coordinate access to the shared data itself; to
      enable cLVM to do so, you must configure OCFS2 or other cluster-aware
      applications on top of the cLVM-managed storage.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-clvm-config" data-id-title="Configuration of cLVM"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.2 </span><span class="title-name">Configuration of cLVM</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Depending on your scenario it is possible to create a RAID 1
   device with cLVM with the following layers:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">LVM2. </span>
      This is a very flexible solution if you want to increase or decrease
      your file system size, add more physical storage, or create snapshots
      of your file systems. This method is described in
      <a class="xref" href="#sec-ha-clvm-scenario-iscsi" title="19.2.3. Scenario: cLVM with iSCSI on SANs">Section 19.2.3, “Scenario: cLVM with iSCSI on SANs”</a>.
     </p></li><li class="listitem"><p><span class="formalpara-title">DRBD. </span>
      This solution only provides RAID 0 (striping) and
      RAID 1 (mirroring). The last method is described in
      <a class="xref" href="#sec-ha-clvm-scenario-drbd" title="19.2.4. Scenario: cLVM With DRBD">Section 19.2.4, “Scenario: cLVM With DRBD”</a>.
     </p></li></ul></div><p>
   Make sure you have fulfilled the following prerequisites:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A shared storage device is available, such as provided by a Fibre
     Channel, FCoE, SCSI, iSCSI SAN, or DRBD*.
    </p></li><li class="listitem"><p>
     In case of DRBD, both nodes must be primary (as described in the
     following procedure).
    </p></li><li class="listitem"><p>
     Check if the locking type of LVM2 is cluster-aware. The keyword
     <code class="literal">locking_type</code> in
     <code class="filename">/etc/lvm/lvm.conf</code> must contain the value
     <code class="literal">3</code> (the default is <code class="literal">1</code>). Copy the configuration to all nodes, if necessary.
    </p></li><li class="listitem"><p>
     Check if the <code class="systemitem">lvmetad</code> daemon is
     disabled, because it cannot work with cLVM. In <code class="filename">/etc/lvm/lvm.conf</code>,
     the keyword <code class="literal">use_lvmetad</code> must be set to <code class="literal">0</code>
     (the default is <code class="literal">1</code>).
     Copy the configuration to all nodes, if necessary.
    </p></li></ul></div><section class="sect2" id="sec-ha-clvm-config-resources" data-id-title="Creating the Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.1 </span><span class="title-name">Creating the Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-config-resources">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Preparing the cluster for use of cLVM includes the following basic
    steps:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-dlmresource" title="Creating a DLM Resource">Creating a DLM Resource</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-config-cmirrord" title="Configuring DLM, CLVM, and STONITH">Configuring DLM, CLVM, and STONITH</a>
     </p></li></ul></div><div class="procedure" id="pro-ha-clvm-dlmresource" data-id-title="Creating a DLM Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.1: </span><span class="title-name">Creating a DLM Resource </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-dlmresource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Check the current configuration of the cluster resources:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure show</pre></div></li><li class="step"><p>
      If you have already configured a DLM resource (and a corresponding
      base group and base clone), continue with
      <a class="xref" href="#pro-ha-clvm-config-cmirrord" title="Configuring DLM, CLVM, and STONITH">Procedure 19.2, “Configuring DLM, CLVM, and STONITH”</a>.
     </p><p>
      Otherwise, configure a DLM resource and a corresponding base group and
      base clone as described in <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a>.
     </p></li><li class="step"><p>
      Leave the crm live configuration with <code class="command">exit</code>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-clvm-config-cmirrord" data-id-title="Scenario: Configuring Cmirrord"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.2 </span><span class="title-name">Scenario: Configuring Cmirrord</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-config-cmirrord">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To track mirror log information in a cluster, the
    <code class="systemitem">cmirrord</code> daemon is used. Cluster
    mirrors are not possible without this daemon running.
   </p><p> We assume that <code class="filename">/dev/sda</code> and
     <code class="filename">/dev/sdb</code> are the shared storage devices as with
    DRBD, iSCSI, and others. Replace these with your own device name(s), if
    necessary. Proceed as follows: </p><div class="procedure" id="pro-ha-clvm-config-cmirrord" data-id-title="Configuring DLM, CLVM, and STONITH"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.2: </span><span class="title-name">Configuring DLM, CLVM, and STONITH </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-config-cmirrord">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Create a cluster with at least two nodes as described in
       Installation and Setup Quick Start.</p></li><li class="step"><p>
      Configure your cluster to run <code class="command">dlm</code>,
      <code class="command">clvmd</code>, and STONITH:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> clvmd ocf:heartbeat:clvm \
        params with_cmirrord=1 \
        op stop interval=0 timeout=100 \
	       op start interval=0 timeout=90 \
	       op monitor interval=20 timeout=20
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dlm ocf:pacemaker:controld \
        op start timeout="90" \
        op stop timeout="100" \
        op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
        params pcmk_delay_max=30
<code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-storage dlm clvmd
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-storage g-storage \
        meta interleave="true" ordered=true</pre></div></li><li class="step"><p>
      Leave crmsh with <code class="command">exit</code> and commit your changes.
     </p></li></ol></div></div><p>Continue configuring your disks with <a class="xref" href="#pro-ha-clvm-config-cmirrord-disks" title="Configuring the Disks for cLVM">Procedure 19.3</a>.
   </p><div class="procedure" id="pro-ha-clvm-config-cmirrord-disks" data-id-title="Configuring the Disks for cLVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.3: </span><span class="title-name">Configuring the Disks for cLVM </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-config-cmirrord-disks">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a clustered volume group (VG):

     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sda /dev/sdb
<code class="prompt root"># </code><code class="command">vgcreate</code> -cy vg1 /dev/sda /dev/sdb</pre></div></li><li class="step"><p>
      Create a mirrored-log logical volume (LV) in your cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -n lv1 -m1 -l10%VG vg1 --mirrorlog mirrored</pre></div></li><li class="step"><p>
      Use <code class="command">lvs</code> to show the progress. If the percentage
      number has reached 100%, the mirrored disk is successfully
      synchronized.
     </p></li><li class="step" id="st-ha-clvm-config-cmirrord-test"><p>
      To test the clustered volume <code class="filename">/dev/vg1/lv1</code>, use the
      following steps:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Read or write to <code class="filename">/dev/vg1/lv1</code>.
       </p></li><li class="step"><p>
        Deactivate your LV with <code class="command">lvchange</code>
        <code class="option">-an</code>.
       </p></li><li class="step"><p>
        Activate your LV with <code class="command">lvchange</code>
        <code class="option">-ay</code>.
       </p></li><li class="step"><p>
        Use <code class="command">lvconvert</code> to convert a mirrored log to a disk
        log.
       </p></li></ol></li><li class="step"><p>
      Create a mirrored-log LV in another cluster VG. This is a different
      volume group from the previous one.
     </p></li></ol></div></div><p>
    The current cLVM can only handle one physical volume (PV) per mirror
    side. If one mirror is actually made up of several PVs that need to be
    concatenated or striped, <code class="command">lvcreate</code> does not understand
    this. For this reason, <code class="command">lvcreate</code> and
    <code class="systemitem">cmirrord</code> metadata needs to understand
    <span class="quote">“<span class="quote">grouping</span>”</span> of PVs into one side, effectively supporting
    RAID10.
   </p><p>
    To support RAID10 for <code class="systemitem">cmirrord</code>,
    use the following procedure (assuming that <code class="filename">/dev/sda</code>,
    <code class="filename">/dev/sdb</code>, <code class="filename">/dev/sdc</code>, and
    <code class="filename">/dev/sdd</code> are the shared storage devices):
   </p><div class="procedure" id="pro-ha-clvm-config-raid10"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a volume group (VG):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sda /dev/sdb /dev/sdc /dev/sdd
  Physical volume "/dev/sda" successfully created
  Physical volume "/dev/sdb" successfully created
  Physical volume "/dev/sdc" successfully created
  Physical volume "/dev/sdd" successfully created
<code class="prompt root"># </code><code class="command">vgcreate</code> vgtest /dev/sda /dev/sdb /dev/sdc /dev/sdd
  Clustered volume group "vgtest" successfully created</pre></div></li><li class="step"><p>
      Open the file <code class="filename">/etc/lvm/lvm.conf</code> and go to the
      section <code class="literal">allocation</code>. Set the following line and save
      the file:
     </p><div class="verbatim-wrap"><pre class="screen">mirror_logs_require_separate_pvs = 1</pre></div></li><li class="step"><p>
      Add your tags to your PVs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvchange</code> --addtag @a /dev/sda /dev/sdb
<code class="prompt root"># </code><code class="command">pvchange</code> --addtag @b /dev/sdc /dev/sdd</pre></div><p>
      A tag is an unordered keyword or term assigned to the metadata of a
      storage object. Tagging allows you to classify collections of LVM2
      storage objects in ways that you find useful by attaching an unordered
      list of tags to their metadata.
     </p></li><li class="step"><p>
      List your tags:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvs</code> -o pv_name,vg_name,pv_tags /dev/sd{a,b,c,d}</pre></div><p>
      You should receive this output:
     </p><div class="verbatim-wrap"><pre class="screen">PV        VG   PV Tags
/dev/sda  vgtest   a
/dev/sdb  vgtest   a
/dev/sdc  vgtest   b
/dev/sdd  vgtest   b</pre></div></li></ol></div></div><p>
    If you need further information regarding LVM2, refer to the SUSE Linux Enterprise Server
    12 SP4 Storage Administration Guide, chapter
    <em class="citetitle">LVM2 Configuration</em>. It is available from
    <a class="link" href="http://www.suse.com/documentation/" target="_blank">http://www.suse.com/documentation/</a>.
   </p></section><section class="sect2" id="sec-ha-clvm-scenario-iscsi" data-id-title="Scenario: cLVM with iSCSI on SANs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.3 </span><span class="title-name">Scenario: cLVM with iSCSI on SANs</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-scenario-iscsi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenario uses two SAN boxes which export their iSCSI
    targets to several clients. The general idea is displayed in
    <a class="xref" href="#fig-ha-clvm-scenario-iscsi" title="Setup of iSCSI with cLVM">Figure 19.1, “Setup of iSCSI with cLVM”</a>.
   </p><div class="figure" id="fig-ha-clvm-scenario-iscsi"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_clvm.png"><img src="images/ha_clvm.png" width="45%" alt="Setup of iSCSI with cLVM" title="Setup of iSCSI with cLVM"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 19.1: </span><span class="title-name">Setup of iSCSI with cLVM </span></span><a title="Permalink" class="permalink" href="#fig-ha-clvm-scenario-iscsi">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div><div id="id-1.3.5.7.4.9.4" data-id-title="Data Loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Data Loss</div><p>
     The following procedures will destroy any data on your disks!
    </p></div><p>
    Configure only one SAN box first. Each SAN box needs to export its own
    iSCSI target. Proceed as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenario-iscsi-targets" data-id-title="Configuring iSCSI Targets (SAN)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.4: </span><span class="title-name">Configuring iSCSI Targets (SAN) </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenario-iscsi-targets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI LIO Target</span> to
      start the iSCSI Server module.
     </p></li><li class="step"><p>
      If you want to start the iSCSI target whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise choose
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      If you have a firewall running, enable <span class="guimenu">Open Port in
      Firewall</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Global</span> tab. If you need
      authentication enable incoming or outgoing authentication or both. In
      this example, we select <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Add a new iSCSI target:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Switch to the <span class="guimenu">Targets</span> tab.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step" id="st-ha-clvm-iscsi-iqn"><p>
        Enter a target name. The name needs to be formatted like this:
       </p><div class="verbatim-wrap"><pre class="screen">iqn.<em class="replaceable">DATE</em>.<em class="replaceable">DOMAIN</em></pre></div><p>
        For more information about the format, refer to <em class="citetitle">Section
        3.2.6.3.1. Type "iqn." (iSCSI Qualified Name) </em> at
        <a class="link" href="http://www.ietf.org/rfc/rfc3720.txt" target="_blank">http://www.ietf.org/rfc/rfc3720.txt</a>.
       </p></li><li class="step"><p>
        If you want a more descriptive name, you can change it as long as
        your identifier is unique for your different targets.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step"><p>
        Enter the device name in <span class="guimenu">Path</span> and use a
        <span class="guimenu">Scsiid</span>.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Next</span> twice.
       </p></li></ol></li><li class="step"><p>
      Confirm the warning box with <span class="guimenu">Yes</span>.
     </p></li><li class="step"><p>
      Open the configuration file <code class="filename">/etc/iscsi/iscsid.conf</code>
      and change the parameter <code class="literal">node.startup</code> to
      <code class="literal">automatic</code>.
     </p></li></ol></div></div><p>
    Now set up your iSCSI initiators as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-initiator" data-id-title="Configuring iSCSI Initiators"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.5: </span><span class="title-name">Configuring iSCSI Initiators </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenarios-iscsi-initiator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI Initiator</span>.
     </p></li><li class="step"><p>
      If you want to start the iSCSI initiator whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise set
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      Change to the <span class="guimenu">Discovery</span> tab and click the
      <span class="guimenu">Discovery</span> button.
     </p></li><li class="step"><p>
      Add your IP address and your port of your iSCSI target (see
      <a class="xref" href="#pro-ha-clvm-scenario-iscsi-targets" title="Configuring iSCSI Targets (SAN)">Procedure 19.4, “Configuring iSCSI Targets (SAN)”</a>). Normally, you
      can leave the port as it is and use the default value.
     </p></li><li class="step"><p>
      If you use authentication, insert the incoming and outgoing user name
      and password, otherwise activate <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Select <span class="guimenu">Next</span>. The found connections are displayed in
      the list.
     </p></li><li class="step"><p>
      Proceed with <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>
      Open a shell, log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Test if the iSCSI initiator has been started successfully:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m discovery -t st -p 192.168.3.100
192.168.3.100:3260,1 iqn.2010-03.de.jupiter:san1</pre></div></li><li class="step"><p>
      Establish a session:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m node -l -p 192.168.3.100 -T iqn.2010-03.de.jupiter:san1
Logging in to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]
Login to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]: successful</pre></div><p>
      See the device names with <code class="command">lsscsi</code>:
     </p><div class="verbatim-wrap"><pre class="screen">...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</pre></div><p>
      Look for entries with <code class="literal">IET</code> in their third column. In
      this case, the devices are <code class="filename">/dev/sdd</code> and
      <code class="filename">/dev/sde</code>.
     </p></li></ol></div></div><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-lvm" data-id-title="Creating the LVM2 Volume Groups"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.6: </span><span class="title-name">Creating the LVM2 Volume Groups </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenarios-iscsi-lvm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a <code class="systemitem">root</code> shell on one of the nodes you have run the iSCSI
      initiator from
      <a class="xref" href="#pro-ha-clvm-scenarios-iscsi-initiator" title="Configuring iSCSI Initiators">Procedure 19.5, “Configuring iSCSI Initiators”</a>.
     </p></li><li class="step"><p>
      Prepare the physical volume for LVM2 with the command
      <code class="command">pvcreate</code> on the disks <code class="filename">/dev/sdd</code>
      and <code class="filename">/dev/sde</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sdd
<code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sde</pre></div></li><li class="step"><p>
      Create the cluster-aware volume group on both disks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate</code> --clustered y clustervg /dev/sdd /dev/sde</pre></div></li><li class="step"><p>
      Create logical volumes as needed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -m1 --name clusterlv --size 500M clustervg</pre></div></li><li class="step"><p>
      Check the physical volume with <code class="command">pvdisplay</code>:
     </p><div class="verbatim-wrap"><pre class="screen">  --- Physical volume ---
      PV Name               /dev/sdd
      VG Name               clustervg
      PV Size               509,88 MB / not usable 1,88 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               52okH4-nv3z-2AUL-GhAN-8DAZ-GMtU-Xrn9Kh

      --- Physical volume ---
      PV Name               /dev/sde
      VG Name               clustervg
      PV Size               509,84 MB / not usable 1,84 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               Ouj3Xm-AI58-lxB1-mWm2-xn51-agM2-0UuHFC</pre></div></li><li class="step"><p>
      Check the volume group with <code class="command">vgdisplay</code>:
     </p><div class="verbatim-wrap"><pre class="screen">  --- Volume group ---
      VG Name               clustervg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      Clustered             yes
      Shared                no
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</pre></div></li></ol></div></div><p>
    After you have created the volumes and started your resources you should
    have a new device named
    <code class="filename">/dev/dm-<em class="replaceable">*</em></code>.

    It is recommended to use a clustered file system on top of your LVM2
    resource, for example OCFS. For more information, see
    <a class="xref" href="#cha-ha-ocfs2" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a>.
   </p></section><section class="sect2" id="sec-ha-clvm-scenario-drbd" data-id-title="Scenario: cLVM With DRBD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.4 </span><span class="title-name">Scenario: cLVM With DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-scenario-drbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenarios can be used if you have data centers located in
    different parts of your city, country, or continent.
   </p><div class="procedure" id="pro-ha-clvm-withdrbd" data-id-title="Creating a Cluster-Aware Volume Group With DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.7: </span><span class="title-name">Creating a Cluster-Aware Volume Group With DRBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-withdrbd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primary/primary DRBD resource:
     </p><ol type="a" class="substeps"><li class="step"><p>
        First, set up a DRBD device as primary/secondary as described in
        <a class="xref" href="#pro-drbd-configure" title="Manually Configuring DRBD">Procedure 18.1, “Manually Configuring DRBD”</a>. Make sure the disk state is
        <code class="literal">up-to-date</code> on both nodes. Check this with
        <code class="command">drbdadm status</code>.
       </p></li><li class="step"><p>
        Add the following options to your configuration file (usually
        something like <code class="filename">/etc/drbd.d/r0.res</code>):
       </p><div class="verbatim-wrap"><pre class="screen">resource r0 {
  net {
     allow-two-primaries;
  }
  ...
}</pre></div></li><li class="step"><p>
        Copy the changed configuration file to the other node, for example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.d/r0.res venus:/etc/drbd.d/</pre></div></li><li class="step"><p>
        Run the following commands on <span class="emphasis"><em>both</em></span> nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> disconnect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> connect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
        Check the status of your nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li></ol></li><li class="step"><p>
      Include the clvmd resource as a clone in the pacemaker configuration,
      and make it depend on the DLM clone resource. See
      <a class="xref" href="#pro-ha-clvm-dlmresource" title="Creating a DLM Resource">Procedure 19.1, “Creating a DLM Resource”</a> for detailed instructions.
      Before proceeding, confirm that these resources have started
      successfully on your cluster. You may use <code class="command">crm status</code>
      or the Web interface to check the running services.
     </p></li><li class="step"><p>
      Prepare the physical volume for LVM2 with the command
      <code class="command">pvcreate</code>. For example, on the device
      <code class="filename">/dev/drbd_r0</code> the command would look like this:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create a cluster-aware volume group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate</code> --clustered y myclusterfs /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create logical volumes as needed. You may probably want to change the
      size of the logical volume. For example, create a 4 GB logical
      volume with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -m1 --name testlv -L 4G myclusterfs</pre></div></li><li class="step"><p>
      
      The logical volumes within the VG are now available as file system
      mounts or raw usage. Ensure that services using them have proper
      dependencies to collocate them with and order them after the VG has
      been activated.
     </p></li></ol></div></div><p>
    After finishing these configuration steps, the LVM2 configuration can be
    done like on any stand-alone workstation.
   </p></section></section><section class="sect1" id="sec-ha-clvm-drbd" data-id-title="Configuring Eligible LVM2 Devices Explicitly"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.3 </span><span class="title-name">Configuring Eligible LVM2 Devices Explicitly</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-drbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or DRBD), it is recommended to
   explicitly configure the devices which LVM2 scans for PVs.
  </p><p>
   For example, if the command <code class="command">vgcreate</code> uses the physical
   device instead of using the mirrored block device, DRBD will be confused
   which may result in a split brain condition for DRBD.
  </p><p>
   To deactivate a single device for LVM2, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit the file <code class="filename">/etc/lvm/lvm.conf</code> and search for the
     line starting with <code class="literal">filter</code>.
    </p></li><li class="step"><p>
     The patterns there are handled as regular expressions. A leading
     <span class="quote">“<span class="quote">a</span>”</span> means to accept a device pattern to the scan, a
     leading <span class="quote">“<span class="quote">r</span>”</span> rejects the devices that follow the device
     pattern.
    </p></li><li class="step"><p>
     To remove a device named <code class="filename">/dev/sdb1</code>, add the
     following expression to the filter rule:
    </p><div class="verbatim-wrap"><pre class="screen">"r|^/dev/sdb1$|"</pre></div><p>
     The complete filter line will look like the following:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|", "r|/dev/.*/by-id/.*|", "a/.*/" ]</pre></div><p>
     A filter line, that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/" ]</pre></div></li><li class="step"><p>
     Write the configuration file and copy it to all cluster nodes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-clvm-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Thorough information is available from the pacemaker mailing list,
   available at
   <a class="link" href="http://www.clusterlabs.org/wiki/Help:Contents" target="_blank">http://www.clusterlabs.org/wiki/Help:Contents</a>.
  </p><p>
   The official cLVM FAQ can be found at
   <a class="link" href="http://sources.redhat.com/cluster/wiki/FAQ/CLVM" target="_blank">http://sources.redhat.com/cluster/wiki/FAQ/CLVM</a>.
  </p></section></section><section class="chapter" id="cha-ha-cluster-md" data-id-title="Cluster Multi-device (Cluster MD)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">20 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-cluster-md">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>The cluster multi-device (Cluster MD) is a software based RAID
   storage solution for a cluster. Cluster MD provides the redundancy of
   RAID1 mirroring to the cluster. Currently, only RAID1 is supported now.
   This chapter shows you how to create and use Cluster MD.
   </p></div></div></div></div><section class="sect1" id="sec-ha-cluster-md-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>The Cluster MD provides support for use of RAID1 across a cluster
   environment. The disks or devices used by Cluster MD are accessed by each node.
   If one device of the Cluster MD fails, it can be
   replaced at runtime by another device and it is re-synced to provide
   the same amount of redundancy. The Cluster MD requires Corosync
   and Distributed Lock Manager (DLM) for co-ordination and messaging.
  </p><p>
   A Cluster MD device is not automatically started on boot like the rest of
   the regular MD devices. A clustered device needs to be started using
   resource agents to ensure the DLM resource has been started.
  </p></section><section class="sect1" id="sec-ha-cluster-md-create" data-id-title="Creating a Clustered MD RAID Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.2 </span><span class="title-name">Creating a Clustered MD RAID Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.3.5.8.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>A running cluster with pacemaker.</p></li><li class="listitem"><p>A resource agent for DLM (see <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a> on how to configure DLM).</p></li><li class="listitem"><p>At least two shared disk devices. You can use an additional device as
    a spare which will fail over automatically in case of device failure.</p></li><li class="listitem"><p>An installed package <span class="package">cluster-md-kmp-default</span>.</p></li></ul></div><p>
   This procedure uses <code class="literal">/dev/sd<em class="replaceable">X</em></code>
   device names as examples. For better stability, use persistent device names
   such as <code class="literal">/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the DLM resource is up and running on every node of the cluster
     and check the resource status with the command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_resource</code> -r dlm -W</pre></div></li><li class="step"><p>Create the Cluster MD device:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If you do not have an existing normal RAID device, create the Cluster MD
       device on the node running the DLM resource with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --create /dev/md0 --bitmap=clustered \
   --metadata=1.2 --raid-devices=2 --level=mirror /dev/sda /dev/sdb</pre></div><p>
       As Cluster MD only works with version 1.2 of the metadata, it is
       recommended to specify the version using the <code class="option">--metadata</code>
       option.
       For other useful options, refer to the man page of
        <code class="command">mdadm</code>. Monitor the progress of the re-sync in
        <code class="filename">/proc/mdstat</code>. </p></li><li class="listitem"><p>
       If you already have an existing normal RAID, first clear the existing
       bitmap and then create the clustered bitmap:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --grow /dev/md<em class="replaceable">X</em> --bitmap=none
<code class="prompt root"># </code><code class="command">mdadm</code> --grow /dev/md<em class="replaceable">X</em> --bitmap=clustered</pre></div></li><li class="listitem"><p>Optionally, to create a Cluster MD device with a
       spare device for automatic failover, run the following command on one
       cluster node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --create /dev/md0 --bitmap=clustered --raid-devices=2 \
      --level=mirror --spare-devices=1 /dev/sda /dev/sdb /dev/sdc --metadata=1.2</pre></div></li></ul></div></li><li class="step"><p>Get the UUID and the related md path:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --detail --scan</pre></div><p>The UUID must match the UUID stored in the superblock. For details on
      the UUID, refer to the <code class="command">mdadm.conf</code> man page.
     </p></li><li class="step"><p>Open <code class="filename">/etc/mdadm.conf</code> and add the md device name
     and the devices associated with it. Use the UUID from the previous step: </p><div class="verbatim-wrap"><pre class="screen">DEVICE /dev/sda /dev/sdb
ARRAY /dev/md0 UUID=1d70f103:49740ef1:af2afce5:fcf6a489</pre></div></li><li class="step"><p>Open Csync2's configuration file <code class="filename">/etc/csync2/csync2.cfg</code>
     and add <code class="filename">/etc/mdadm.conf</code>:</p><div class="verbatim-wrap"><pre class="screen">group ha_group
{
   # ... list of files pruned ...
   include /etc/mdadm.conf
}</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-ra" data-id-title="Configuring a Resource Agent"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.3 </span><span class="title-name">Configuring a Resource Agent</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-ra">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Configure a CRM resource as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Create a <code class="systemitem">Raid1</code> primitive:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> raider Raid1 \
  params raidconf="/etc/mdadm.conf" raiddev=/dev/md0 \
  force_clones=true \
  op monitor timeout=20s interval=10 \
  op start timeout=20s interval=0 \
  op stop timeout=20s interval=0</pre></div></li><li class="step"><p>Add the <code class="systemitem">raider</code> resource to the base group for storage that you have created for
     DLM:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-storage add raider</pre></div><p>The <code class="command">add</code> sub-command appends the new group
     member by default. </p><p>
    If not already done, clone the <code class="literal">g-storage</code> group so that it runs on all nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-storage g-storage \
    meta interleave=true target-role=Started</pre></div></li><li class="step"><p>Review your changes with
     <code class="command">show</code>.</p></li><li class="step"><p>If everything seems correct, submit your changes with
     <code class="command">commit</code>.</p></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-dev-add" data-id-title="Adding a Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.4 </span><span class="title-name">Adding a Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-add">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>To add a device to an existing, active Cluster MD device, first ensure that
   the device is <span class="quote">“<span class="quote">visible</span>”</span> on each node with the command
   <code class="command">cat /proc/mdstat</code>.
   If the device is not visible, the command will fail.
  </p><p>
   Use the following command on one cluster node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --manage /dev/md0 --add /dev/sdc</pre></div><p>The behavior of the new device added depends on the state of the Cluster
   MD device:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If only one of the mirrored devices is active, the new device becomes
     the second device of the mirrored devices and a recovery is initiated.</p></li><li class="listitem"><p>If both devices of the Cluster MD device are active, the new
     added device becomes a spare device.</p></li></ul></div></section><section class="sect1" id="sec-ha-cluster-md-dev-readd" data-id-title="Re-adding a Temporarily Failed Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.5 </span><span class="title-name">Re-adding a Temporarily Failed Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-readd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Quite often the failures are transient and limited to a single node. If
   any of the nodes encounters a failure during an I/O operation, the device will
   be marked as failed for the entire cluster.
  </p><p> This could happen, for example, because of a cable failure on one of the
   nodes. After correcting the problem, you can re-add the device. Only the
   outdated parts will be synchronized as opposed to synchronizing the entire device by
   adding a new one.
  </p><p>
   To re-add the device, run the following command on one cluster node: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --manage /dev/md0 --re-add /dev/sdb</pre></div></section><section class="sect1" id="sec-ha-cluster-md-dev-remove" data-id-title="Removing a Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.6 </span><span class="title-name">Removing a Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-remove">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Before removing a device at runtime for replacement, do the following:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Make sure the device is failed by introspecting <code class="filename">/proc/mdstat</code>.
    Look for an <code class="literal">(F)</code> before the device.</p></li><li class="step"><p>Run the following command on one cluster node to make a device fail:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --manage /dev/md0 --fail /dev/sda</pre></div></li><li class="step"><p>Remove the failed device using the command on one cluster node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --manage /dev/md0 --remove /dev/sda</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-convert-raid" data-id-title="Assembling Cluster MD as normal RAID at the disaster recovery site"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.7 </span><span class="title-name">Assembling Cluster MD as normal RAID at the disaster recovery site</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-convert-raid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the event of disaster recovery, you might face the situation that you do not have
   a Pacemaker cluster stack in the infrastructure on the disaster recovery site, but
   applications still need to access the data on the existing Cluster MD disks,
   or from the backups.
  </p><p>
   You can convert a Cluster MD RAID to a normal RAID by using the <code class="option">--assemble</code>
   operation with the <code class="option">-U no-bitmap</code> option to change the metadata
   of the RAID disks accordingly.
   </p><p>
    Find an example below of how to assemble all arrays on the data recovery site:
   </p><div class="verbatim-wrap"><pre class="screen">while read i; do
   NAME=`echo $i | sed 's/.*name=//'|awk '{print $1}'|sed 's/.*://'`
   UUID=`echo $i | sed 's/.*UUID=//'|awk '{print $1}'`
   mdadm -AR "/dev/md/$NAME" -u $UUID -U no-bitmap
   echo "NAME =" $NAME ", UUID =" $UUID ", assembled."
done &lt; &lt;(mdadm -Es)</pre></div></section></section><section class="chapter" id="cha-ha-samba" data-id-title="Samba Clustering"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">21 </span><span class="title-name">Samba Clustering</span></span> <a title="Permalink" class="permalink" href="#cha-ha-samba">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    A clustered Samba server provides a High Availability solution in your
    heterogeneous networks. This chapter explains some background
    information and how to set up a clustered Samba server.
   </p></div></div></div></div><section class="sect1" id="sec-ha-samba-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Trivial Database (TDB) has been used by Samba for many years. It allows
   multiple applications to write simultaneously. To make sure all write
   operations are successfully performed and do not collide with each other,
   TDB uses an internal locking mechanism.
  </p><p>
   Cluster Trivial Database (CTDB) is a small extension of the existing TDB.
   CTDB is described by the project as a <span class="quote">“<span class="quote">cluster implementation of
   the TDB database used by Samba and other projects to store temporary
   data</span>”</span>.
  </p><p>
   Each cluster node runs a local CTDB daemon. Samba communicates with its
   local CTDB daemon instead of writing directly to its TDB. The daemons
   exchange metadata over the network, but actual write and read operations
   are done on a local copy with fast storage. The concept of CTDB is
   displayed in <a class="xref" href="#fig-ha-samba-overview" title="Structure of a CTDB Cluster">Figure 21.1, “Structure of a CTDB Cluster”</a>.
  </p><div id="id-1.3.5.9.3.5" data-id-title="CTDB For Samba Only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: CTDB For Samba Only</div><p>
    The current implementation of the CTDB Resource Agent configures CTDB to
    only manage Samba. Everything else, including IP failover, should be
    configured with Pacemaker.
   </p><p>
    CTDB is only supported for completely homogeneous clusters. For example,
    all nodes in the cluster need to have the same architecture. You cannot
    mix x86 with AMD64/Intel 64.
   </p></div><div class="figure" id="fig-ha-samba-overview"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_samba.png"><img src="images/ha_samba.png" width="80%" alt="Structure of a CTDB Cluster" title="Structure of a CTDB Cluster"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 21.1: </span><span class="title-name">Structure of a CTDB Cluster </span></span><a title="Permalink" class="permalink" href="#fig-ha-samba-overview">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div><p>
   A clustered Samba server must share certain data:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Mapping table that associates Unix user and group IDs to Windows users
     and groups.
    </p></li><li class="listitem"><p>
     The user database must be synchronized between all nodes.
    </p></li><li class="listitem"><p>
     Join information for a member server in a Windows domain must be
     available on all nodes.
    </p></li><li class="listitem"><p>
     Metadata needs to be available on all nodes, like active SMB sessions,
     share connections, and various locks.
    </p></li></ul></div><p>
   The goal is that a clustered Samba server with N+1 nodes is faster than
   with only N nodes. One node is not slower than an unclustered Samba
   server.
  </p></section><section class="sect1" id="sec-ha-samba-basicconf" data-id-title="Basic Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.2 </span><span class="title-name">Basic Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-basicconf">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.5.9.4.3" data-id-title="Changed Configuration Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changed Configuration Files</div><p>
    The CTDB Resource Agent automatically changes
    <code class="filename">/etc/sysconfig/ctdb</code>. Use <code class="command">crm
    ra</code> <code class="option">info CTDB</code> to list all parameters
    that can be specified for the CTDB resource.
   </p></div><p>
   To set up a clustered Samba server, proceed as follows:
  </p><div class="procedure" id="pro-ha-samba-basicconf" data-id-title="Setting Up a Basic Clustered Samba Server"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.1: </span><span class="title-name">Setting Up a Basic Clustered Samba Server </span></span><a title="Permalink" class="permalink" href="#pro-ha-samba-basicconf">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Prepare your cluster:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Make sure the following packages are installed before you proceed:
       <code class="systemitem">ctdb</code>,
       <code class="systemitem">tdb-tools</code>, and
       <code class="systemitem">samba</code> (needed for
       <code class="literal">smb</code> and <code class="literal">nmb</code> resources).
      </p></li><li class="step"><p>
       Configure your cluster (Pacemaker, OCFS2) as described in this guide
       in <a class="xref" href="#part-config" title="Part II. Configuration and Administration">Part II, “Configuration and Administration”</a>.
      </p></li><li class="step"><p>
       Configure a shared file system, like OCFS2, and mount it, for
       example, on <code class="filename">/srv/clusterfs</code>.
        See <a class="xref" href="#cha-ha-ocfs2" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a> for more information.
      </p></li><li class="step"><p>
       If you want to turn on POSIX ACLs, enable it:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         For a new OCFS2 file system use:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ocfs2</code> --fs-features=xattr ...</pre></div></li><li class="listitem"><p>
         For an existing OCFS2 file system use:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">tunefs.ocfs2</code> --fs-feature=xattr <em class="replaceable">DEVICE</em></pre></div><p>
         Make sure the <code class="option">acl</code> option is specified in the file
         system resource. Use the <code class="command">crm</code> shell as follows:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-3 ocf:heartbeat:Filesystem params options="acl" ...</pre></div></li></ul></div></li><li class="step"><p>
       Make sure the services <code class="systemitem">ctdb</code>,
       <code class="systemitem">smb</code>, and
       <code class="systemitem">nmb</code> are disabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable ctdb
<code class="prompt root"># </code><code class="command">systemctl</code> disable smb
<code class="prompt root"># </code><code class="command">systemctl</code> disable nmb</pre></div></li><li class="step"><p>
       Open port <code class="literal">4379</code> of your firewall on all nodes. This
       is needed for CTDB to communicate with other cluster nodes.
      </p></li></ol></li><li class="step"><p>
     Create a directory for the CTDB lock on the shared file system:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> -p /srv/clusterfs/samba/</pre></div></li><li class="step"><p>
     In <code class="filename">/etc/ctdb/nodes</code> insert all nodes which contain
     all private IP addresses of each node in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen">192.168.1.10
192.168.1.11</pre></div></li><li class="step"><p>
     Configure Samba. Add the following lines in the
     <code class="literal">[global]</code> section of
     <code class="filename">/etc/samba/smb.conf</code>. Use the host name of your
     choice in place of "CTDB-SERVER" (all nodes in the cluster will appear
     as one big node with this name, effectively):
    </p><div class="verbatim-wrap"><pre class="screen">[global]
    # ...
    # settings applicable for all CTDB deployments
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket
    # settings necessary for CTDB on OCFS2
    fileid:algorithm = fsid
    vfs objects = fileid
    # ...</pre></div></li><li class="step"><p>
     Copy the configuration file to all of your nodes by using
     <code class="command">csync2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p>
     For more information, see
     <a class="xref" href="#pro-ha-installation-setup-csync2-start" title="Synchronizing the Configuration Files with Csync2">Procedure 4.9, “Synchronizing the Configuration Files with Csync2”</a>.
    </p></li><li class="step"><p>
     Add a CTDB resource to the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ctdb CTDB params \
    ctdb_manages_winbind="false" \ 
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="/srv/clusterfs/samba/ctdb.lock" \
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \ 
      op monitor interval="10" timeout="20" \
      op start interval="0" timeout="90" \
      op stop interval="0" timeout="100"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-ctdb ctdb nmb smb
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ctdb g-ctdb meta interleave="true"
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-ctdb-with-clusterfs inf: cl-ctdb cl-clusterfs
<code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-clusterfs-then-ctdb inf: cl-clusterfs cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Add a clustered IP address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ip IPaddr2 params ip=192.168.2.222 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-ip-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-ip-then-ctdb 0: cl-ip cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
     If <code class="literal">unique_clone_address</code> is set to
     <code class="literal">true</code>, the IPaddr2 resource agent adds a clone ID to
     the specified address, leading to three different IP addresses. These
     are usually not needed, but help with load balancing. For further
     information about this topic, see <a class="xref" href="#sec-ha-lb-lvs" title="13.2. Configuring Load Balancing with Linux Virtual Server">Section 13.2, “Configuring Load Balancing with Linux Virtual Server”</a>.
    </p></li><li class="step"><p>
     Commit your change:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Check the result:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> status
Clone Set: cl-storage [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
Clone Set: cl-clusterfs [clusterfs]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</pre></div></li><li class="step"><p>
     Test from a client machine. On a Linux client, run the following
     command to see if you can copy files from and to the system:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">smbclient</code> <code class="option">//192.168.2.222/myshare</code></pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-ad" data-id-title="Joining an Active Directory Domain"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.3 </span><span class="title-name">Joining an Active Directory Domain</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-ad">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Active Directory (AD) is a directory service for Windows server systems.
  </p><p>
   The following instructions outline how to join a CTDB cluster to an
   Active Directory domain:
  </p><div class="procedure" id="pro-ha-samba-ad-join"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a CTDB resource as described in
     <a class="xref" href="#pro-ha-samba-basicconf" title="Setting Up a Basic Clustered Samba Server">Procedure 21.1, “Setting Up a Basic Clustered Samba Server”</a>.
    </p></li><li class="step"><p>
     Install the <code class="systemitem">samba-winbind</code>
     package.
    </p></li><li class="step"><p>
     Disable the <code class="systemitem">winbind</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable winbind</pre></div></li><li class="step"><p>
     Define a winbind cluster resource:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> winbind systemd:winbind \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Edit the <code class="literal">g-ctdb</code> group and insert
     <code class="literal">winbind</code> between the <code class="literal">nmb</code> and
     <code class="literal">smb</code> resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">edit</code> g-ctdb</pre></div><p>
     Save and close the editor with <span class="keycap">:</span><span class="key-connector">–</span><span class="keycap">w</span> (<code class="command">vim</code>).
    </p></li><li class="step"><p>
     Consult your Windows Server documentation for instructions on how to
     set up an Active Directory domain. In this example, we use the
     following parameters:
    </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          AD and DNS server
         </p>
        </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">win2k3.2k3test.example.com</pre></div>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          AD domain
         </p>
        </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">2k3test.example.com</pre></div>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Cluster AD member NetBIOS name
         </p>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">CTDB-SERVER</pre></div>
        </td></tr></tbody></table></div></li><li class="step"><p>
     <a class="xref" href="#pro-ha-samba-config-join-ad" title="Joining Active Directory">Procedure 21.2, “Joining Active Directory”</a>
    </p></li></ol></div></div><p>
   Finally, join your cluster to the Active Directory server:
  </p><div class="procedure" id="pro-ha-samba-config-join-ad" data-id-title="Joining Active Directory"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.2: </span><span class="title-name">Joining Active Directory </span></span><a title="Permalink" class="permalink" href="#pro-ha-samba-config-join-ad">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the following files are included in Csync2's
     configuration to become installed on all cluster hosts:
    </p><div class="verbatim-wrap"><pre class="screen">/etc/samba/smb.conf
/etc/security/pam_winbind.conf
/etc/krb5.conf
/etc/nsswitch.conf
/etc/security/pam_mount.conf.xml
/etc/pam.d/common-session</pre></div><p>
     You can also use YaST's <span class="guimenu">Configure Csync2</span>
     module for this task, see
     <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a>.
    </p></li><li class="step"><p>
     Run YaST and open the <span class="guimenu">Windows Domain Membership</span>
     module from the <span class="guimenu">Network Services</span> entry.
    </p></li><li class="step"><p>
     Enter your domain or workgroup settings and finish with
     <span class="guimenu">Ok</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-testing" data-id-title="Debugging and Testing Clustered Samba"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.4 </span><span class="title-name">Debugging and Testing Clustered Samba</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-testing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To debug your clustered Samba server, the following tools which operate
   on different levels are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.9.6.3.1"><span class="term"><code class="command">ctdb_diagnostics</code>
    </span></dt><dd><p>
      Run this tool to diagnose your clustered Samba server. Detailed debug
      messages should help you track down any problems you might have.
     </p><p>
      The <code class="command">ctdb_diagnostics</code> command searches for the
      following files which must be available on all nodes:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/krb5.conf
/etc/hosts
/etc/ctdb/nodes
/etc/sysconfig/ctdb
/etc/resolv.conf
/etc/nsswitch.conf
/etc/sysctl.conf
/etc/samba/smb.conf
/etc/fstab
/etc/multipath.conf
/etc/pam.d/system-auth
/etc/sysconfig/nfs
/etc/exports
/etc/vsftpd/vsftpd.conf</pre></div><p>
      If the files <code class="filename">/etc/ctdb/public_addresses</code> and
      <code class="filename">/etc/ctdb/static-routes</code> exist, they will be
      checked as well.
     </p></dd><dt id="id-1.3.5.9.6.3.2"><span class="term"><code class="command">ping_pong</code>
    </span></dt><dd><p>
      Check whether your file system is suitable for CTDB with
      <code class="command">ping_pong</code>. It performs certain tests of your
      cluster file system like coherence and performance (see
      <a class="link" href="http://wiki.samba.org/index.php/Ping_pong" target="_blank">http://wiki.samba.org/index.php/Ping_pong</a>) and
      gives some indication how your cluster may behave under high load.
     </p></dd><dt id="id-1.3.5.9.6.3.3"><span class="term"><code class="command">send_arp</code> Tool and <code class="systemitem">SendArp</code> Resource Agent</span></dt><dd><p>
      The <code class="systemitem">SendArp</code> resource agent
      is located in <code class="filename">/usr/lib/heartbeat/send_arp</code> (or
      <code class="filename">/usr/lib64/heartbeat/send_arp</code>). The
      <code class="command">send_arp</code> tool sends out a gratuitous ARP (Address
      Resolution Protocol) packet and can be used for updating other
      machines' ARP tables. It can help to identify communication problems
      after a failover process. If you cannot connect to a node or ping it
      although it shows the clustered IP address for Samba, use the
      <code class="command">send_arp</code> command to test if the nodes only need an
      ARP table update.
     </p><p>
      For more information, refer to
      <a class="link" href="http://wiki.wireshark.org/Gratuitous_ARP" target="_blank">http://wiki.wireshark.org/Gratuitous_ARP</a>.
     </p></dd></dl></div><p>
   To test certain aspects of your cluster file system proceed as follows:
  </p><div class="procedure" id="id-1.3.5.9.6.5" data-id-title="Test Coherence and Performance of Your Cluster File System"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.3: </span><span class="title-name">Test Coherence and Performance of Your Cluster File System </span></span><a title="Permalink" class="permalink" href="#id-1.3.5.9.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start the command <code class="command">ping_pong</code> on one node and replace
     the placeholder <em class="replaceable">N</em> with the amount of nodes
     plus one. The file
     <code class="filename"><em class="replaceable">ABSPATH</em>/data.txt</code> is
     available in your shared storage and is therefore accessible on all
     nodes (<em class="replaceable">ABSPATH </em> indicates an absolute path):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="command">ping_pong</code> <em class="replaceable">ABSPATH</em>/data.txt <em class="replaceable">N</em></pre></div><p>
     Expect a very high locking rate as you are running only one node. If
     the program does not print a locking rate, replace your cluster file
     system.
    </p></li><li class="step"><p>
     Start a second copy of <code class="command">ping_pong</code> on another node
     with the same parameters.
    </p><p>
     Expect to see a dramatic drop in the locking rate. If any of the
     following applies to your cluster file system, replace it:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="command">ping_pong</code> does not print a locking rate per
       second,
      </p></li><li class="listitem"><p>
       the locking rates in the two instances are not almost equal,
      </p></li><li class="listitem"><p>
       the locking rate did not drop after you started the second instance.
      </p></li></ul></div></li><li class="step"><p>
     Start a third copy of <code class="command">ping_pong</code>. Add another node
     and note how the locking rates change.
    </p></li><li class="step"><p>
     Kill the <code class="command">ping_pong</code> commands one after the other. You
     should observe an increase of the locking rate until you get back to
     the single node case. If you did not get the expected behavior, find
     more information in <a class="xref" href="#cha-ha-ocfs2" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.5 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="http://wiki.samba.org/index.php/CTDB_Setup" target="_blank">http://wiki.samba.org/index.php/CTDB_Setup</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://ctdb.samba.org" target="_blank">http://ctdb.samba.org</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://wiki.samba.org/index.php/Samba_%26_Clustering" target="_blank">http://wiki.samba.org/index.php/Samba_%26_Clustering</a>
    </p></li></ul></div></section></section><section class="chapter" id="cha-ha-rear" data-id-title="Disaster Recovery with Relax-and-Recover (ReaR)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">22 </span><span class="title-name">Disaster Recovery with Relax-and-Recover (ReaR)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-rear">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Relax-and-Recover (<span class="quote">“<span class="quote">ReaR</span>”</span>) is a disaster recovery framework for use by
    system administrators. It is a collection of Bash scripts that need to
    be adjusted to the specific production environment that is to be
    protected in case of disaster.
   </p><p>
    No disaster recovery solution will  work out-of-the-box. Therefore
    it is essential to take preparations <span class="emphasis"><em>before</em></span> any
    disaster happens.
   </p></div></div></div></div><section class="sect1" id="sec-ha-rear-concept" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following sections describe the general disaster recovery concept and
   the basic steps you need to execute for successful recovery with
   ReaR. They also provide some guidance on ReaR requirements,

   some limitations to be aware of, and scenarios and backup tools.
  </p><div id="id-1.3.5.10.3.3" data-id-title="Understanding ReaR" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Understanding ReaR</div><p>
     Understanding ReaR's complex functionality is essential for making the
     tool work as intended. Therefore, read this chapter carefully and
     familiarize yourself with ReaR before a disaster strikes. You should also be
     aware of ReaR's known limitations and test your system in advance.
    </p></div><section class="sect2" id="sec-ha-rear-concept-drp" data-id-title="Creating a Disaster Recovery Plan"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.1 </span><span class="title-name">Creating a Disaster Recovery Plan</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-drp">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <span class="emphasis"><em>Before</em></span> the worst scenario happens, take action:
    analyze your IT infrastructure for any substantial risks, evaluate your
    budget, and create a disaster recovery plan. If you do not already have
    a disaster recovery plan at hand, find some information on each step
    below:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Risk Analysis. </span>
       Conduct a solid risk analysis of your infrastructure. List all the
       possible threats and evaluate how serious they are. Determine how
       likely these threats are and prioritize them. It is recommended to
       use a simple categorization: probability and impact.
      </p></li><li class="listitem"><p><span class="formalpara-title">Budget Planning. </span>
       The outcome of the analysis is an overview, which risks can be
       tolerated and which are critical for your business. Ask yourself how
       you can minimize risks and how much will it cost. Depending on how
       big your company is, spend two to fifteen percent of the overall IT
       budget on disaster recovery.
      </p></li><li class="listitem"><p><span class="formalpara-title">Disaster Recovery Plan Development. </span>
       Make checklists, test procedures, establish and assign priorities,
       and inventory your IT infrastructure. Define how to deal with a
       problem when some services in your infrastructure fail.
      </p></li><li class="listitem"><p><span class="formalpara-title">Test. </span>
       After defining an elaborate plan, test it. Test it at least once a
       year. Use the same testing hardware as your main IT infrastructure.
      </p></li></ul></div></section><section class="sect2" id="sec-ha-rear-concept-recovery" data-id-title="What Does Disaster Recovery Mean?"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.2 </span><span class="title-name">What Does Disaster Recovery Mean?</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-recovery">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If a system in a production environment has been destroyed (for whatever
    reasons—be it broken hardware, a misconfiguration or software
    problems), you need to re-create the system. The recreation can be done
    either on the same hardware or on compatible replacement hardware.
    Re-creating a system means more than restoring files from a backup.
    It also includes preparing the system's storage (with regard to partitioning, file
    systems, and mount points), and reinstalling the boot
    loader.
   </p></section><section class="sect2" id="sec-ha-rear-concept-basics" data-id-title="How Does Disaster Recovery With ReaR Work?"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.3 </span><span class="title-name">How Does Disaster Recovery With ReaR Work?</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-basics">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    While the system is up and running, create a backup of the files and
    create a recovery system on a recovery medium. The recovery system
    contains a recovery installer.
   </p><p>
    In case the system has been destroyed, replace broken hardware (if
    needed), boot the recovery system from the recovery medium and launch
    the recovery installer. The recovery installer re-creates the system:
    First, it prepares the storage (partitioning, file systems, mount
    points), then it restores the files from the backup. Finally, it
    reinstalls the boot loader.
   </p></section><section class="sect2" id="sec-ha-rear-concept-requirements" data-id-title="ReaR Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.4 </span><span class="title-name">ReaR Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-requirements">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To use ReaR you need at least two identical systems: the machine
    that runs your production environment and an identical test machine.
    <span class="quote">“<span class="quote">Identical</span>”</span> in this context means that you can, for
    example, replace a network card with another one using the same Kernel
    driver.
   </p><div id="id-1.3.5.10.3.7.3" data-id-title="Identical Drivers Required" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Identical Drivers Required</div><p>
     If a hardware component does not use the same driver as the one in
     your production environment, it is not considered identical by
     ReaR.
    </p></div></section><section class="sect2" id="sec-ha-rear-concept-versions" data-id-title="ReaR Version Updates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.5 </span><span class="title-name">ReaR Version Updates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-versions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To be compatible with older service packs, SUSE Linux Enterprise High Availability 12 SP4
    ships with different ReaR versions: 1.16 (included in RPM package
    <span class="package">rear116</span>), 1.17.2.a (<span class="package">rear1172a</span>),
    1.18.a (<span class="package">rear118a</span>), and 2.4 (<span class="package">rear23a</span>).
    The latest version contains some
    later enhancements from the upstream GitHub project.
   </p><div id="id-1.3.5.10.3.8.3" data-id-title="Find Important Information in Changelogs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Find Important Information in Changelogs</div><p>
     Any information about bugfixes, incompatibilities, and other issues can
     be found in the changelogs of the packages. It is recommended to review
     also later package versions of ReaR in case you need to re-validate
     your disaster recovery procedure.
    </p></div><p>
     Be aware of the following issues with ReaR:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      To allow disaster recover on UEFI systems, you need version 1.18.a
      and the package <span class="package">ebiso</span>. Only this version supports the
      new helper tool <code class="filename">/usr/bin/ebiso</code>. This helper tool is
      used to create a UEFI-bootable ReaR system ISO image.
     </p></li><li class="listitem"><p>
      If you have a tested and fully functional disaster recovery procedure
      with one ReaR version, do not update ReaR. Keep the ReaR package
      and do not change your disaster recovery method!
     </p></li><li class="listitem"><p>
      Version updates for ReaR are provided as separate packages that
      intentionally conflict with each other to prevent your installed version
      getting accidentally replaced with another version.
     </p></li></ul></div><p>
    In the following cases you need to completely re-validate your existing
    disaster recovery procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      For each ReaR version update.
     </p></li><li class="listitem"><p>When you update ReaR manually.</p></li><li class="listitem"><p>
      For each software that is used by ReaR.
     </p></li><li class="listitem"><p>
      If you update low-level system components such as <code class="command">parted</code>,
       <code class="command">btrfs</code> and similar.
     </p></li></ul></div></section><section class="sect2" id="sec-ha-rear-concept-limit" data-id-title="Limitations with Btrfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.6 </span><span class="title-name">Limitations with Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-limit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following limitations apply if you use Btrfs.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.3.9.3.1"><span class="term">Your System Includes Subvolumes, but No Snapshots Subvolumes</span></dt><dd><p>
       At least ReaR version 1.17.2.a is required. This version supports re-creating
       <span class="quote">“<span class="quote">normal</span>”</span> Btrfs subvolume structure (no snapshot
       subvolumes).
      </p></dd><dt id="id-1.3.5.10.3.9.3.2"><span class="term">Your System Includes Snapshot Subvolumes</span></dt><dd><div id="id-1.3.5.10.3.9.3.2.2.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
        Btrfs snapshot subvolumes <span class="emphasis"><em>cannot</em></span> be backed up
        and restored as usual with file-based backup software.
       </p></div><p>
       While recent snapshot subvolumes on Btrfs file systems need
       almost no disk space (because of Btrfs's copy-on-write functionality),
       those files would be backed up as complete files when using
       file-based backup software. They would end up twice in the backup
       with their original file size. Therefore, it is impossible to
       restore the snapshots as they have been before on the original
       system.
      </p></dd><dt id="id-1.3.5.10.3.9.3.3"><span class="term">Your SLE12 System Needs Matching ReaR Configuration</span></dt><dd><p>
       The setup in SLE12 GA, SLE12 SP1, and SLE12 SP2 have several
       incompatible Btrfs default structures. As such, it is
       crucial to use a matching ReaR configuration file. See the example
       files <code class="filename">/usr/share/rear/conf/examples/SLE12*-btrfs-example.conf</code>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-rear-concept-scenarios" data-id-title="Scenarios and Backup Tools"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.7 </span><span class="title-name">Scenarios and Backup Tools</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    ReaR can create a disaster recovery system (including a system-specific
    recovery installer) that can be booted from a local medium (like a hard
    disk, a flash disk, a DVD/CD-R) or via PXE. The backup data can be
    stored on a network file system, for example NFS, as described in
    <a class="xref" href="#ex-ha-rear-nfs-server-backup" title="Using an NFS Server to Store the File Backup">Example 22.1</a>.
   </p><p>
    ReaR does not replace a file backup, but complements it. By
    default, ReaR supports the generic <code class="command">tar</code> command,
    and several third-party backup tools (such as Tivoli Storage Manager,
    QNetix Galaxy, Symantec NetBackup, EMC NetWorker, or HP DataProtector).
    Refer to
    <a class="xref" href="#ex-ha-rear-config-EMC" title="Using Third-Party Backup Tools Like EMC NetWorker">Example 22.2</a> for an
    example configuration of using ReaR with EMC NetWorker as backup
    tool.
   </p></section><section class="sect2" id="sec-ha-rear-concept-overview" data-id-title="Basic Steps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.1.8 </span><span class="title-name">Basic Steps</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-overview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For a successful recovery with ReaR in case of disaster, you need
    to execute the following basic steps:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.3.11.3.1"><span class="term"><a class="xref" href="#sec-ha-rear-config" title="22.2. Setting Up ReaR and Your Backup Solution">Setting Up ReaR and Your Backup Solution</a>
     </span></dt><dd><p>
       This includes tasks like editing the ReaR configuration file,
       adjusting the Bash scripts, and configuring the backup solution that
       you want to use.
      </p></dd><dt id="id-1.3.5.10.3.11.3.2"><span class="term"><a class="xref" href="#sec-ha-rear-mkbackup" title="22.3. Creating the Recovery Installation System">Creating the Recovery Installation System</a>
     </span></dt><dd><p>
       While the system to be protected is up and running use the
       <code class="command">rear mkbackup</code> command to create a file backup and
       to generate a recovery system that contains a system-specific
       ReaR recovery installer.
      </p></dd><dt id="id-1.3.5.10.3.11.3.3"><span class="term"><a class="xref" href="#sec-ha-rear-testing" title="22.4. Testing the Recovery Process">Testing the Recovery Process</a>
     </span></dt><dd><p>
       Whenever you have created a disaster recovery medium with ReaR,
       test the disaster recovery process thoroughly. It is essential to use
       a test machine that has <span class="emphasis"><em>identical</em></span> hardware like
       the one that is part of your production environment. For details,
       refer to <a class="xref" href="#sec-ha-rear-concept-requirements" title="22.1.4. ReaR Requirements">Section 22.1.4, “ReaR Requirements”</a>.
      </p></dd><dt id="id-1.3.5.10.3.11.3.4"><span class="term"><a class="xref" href="#sec-ha-rear-recover" title="22.5. Recovering from Disaster">Recovering from Disaster</a>
     </span></dt><dd><p>
       After a disaster has occurred, replace any broken hardware (if
       necessary). Then boot the ReaR recovery system and start the
       recovery installer with the <code class="command">rear recover</code> command.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-rear-config" data-id-title="Setting Up ReaR and Your Backup Solution"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.2 </span><span class="title-name">Setting Up ReaR and Your Backup Solution</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To set up ReaR, you need to edit at least the ReaR
   configuration file <code class="filename">/etc/rear/local.conf</code> and, if
   needed, the Bash scripts that are part of the ReaR framework.
  </p><p>
   In particular, you need to define the following tasks that ReaR
   should do:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">When your system is booted with UEFI. </span>
      If your system boots with a UEFI boot loader, install the package
      <span class="package">ebiso</span> and add the following line to
      <code class="filename">/etc/rear/local.conf</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ISO_MKISOFS_BIN="/usr/bin/ebiso"</pre></div><p>
     If your system boots with UEFI Secure Boot, you must also add the following line:
    </p><div class="verbatim-wrap"><pre class="screen">SECURE_BOOT_BOOTLOADER="/boot/efi/EFI/BOOT/shim.efi"</pre></div><p>
     For more information about ReaR configuration variables for UEFI, see the
     <code class="filename">/usr/share/rear/conf/default.conf</code> file.
    </p></li><li class="listitem"><p><span class="formalpara-title">How to back up files and how to create and store the disaster recovery system. </span>
      This needs to be configured in
      <code class="filename">/etc/rear/local.conf</code>.
     </p></li><li class="listitem"><p><span class="formalpara-title">What to re-create exactly (partitioning, file systems, mount points, etc.). </span>
      This can be defined in <code class="filename">/etc/rear/local.conf</code> (for
      example, what to exclude). To re-create non-standard systems, you may
      need to enhance the Bash scripts.
     </p></li><li class="listitem"><p><span class="formalpara-title">How the recovery process works. </span>
      To change how ReaR generates the recovery installer, or to adapt
      what the ReaR recovery installer does, you need to edit the Bash
      scripts.
     </p></li></ul></div><p>
   To configure ReaR, add your options to the
   <code class="filename">/etc/rear/local.conf</code> configuration file. (The former
   configuration file <code class="filename">/etc/rear/sites.conf</code> has been
   removed from the package. However, if you have such a file from your last
   setup, ReaR will still use it.)
  </p><p>
   All ReaR configuration variables and their default values are set in
   <code class="filename">/usr/share/rear/conf/default.conf</code>.
   Some example files (<code class="filename">*example.conf</code>) for user configurations
   (for example, what is set in <code class="filename">/etc/rear/local.conf</code>)
   are available in the <code class="filename">examples</code> subdirectory.
   Find more information in the ReaR man page.
  </p><p>
   You should start with a matching example configuration file as template
   and adapt it as needed to create your particular configuration file.
   Copy various options from several example configuration files and paste them
   into your specific <code class="filename">/etc/rear/local.conf</code> file that
   matches your particular system.
   Do not use original example configuration files, because they provide an
   overview of variables that can be used for specific setups.
  </p><p>
   After you have changed the ReaR configuration file, run the
   following command and check its output:
  </p><div class="verbatim-wrap"><pre class="screen">rear dump</pre></div><div class="complex-example"><div class="example" id="ex-ha-rear-nfs-server-backup" data-id-title="Using an NFS Server to Store the File Backup"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 22.1: </span><span class="title-name">Using an NFS Server to Store the File Backup </span></span><a title="Permalink" class="permalink" href="#ex-ha-rear-nfs-server-backup">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    ReaR can be used in different scenarios. The following example uses
    an NFS server as storage for the file backup.
   </p></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up an NFS server with YaST as described in the SUSE Linux Enterprise Server
     12 SP4 Administration Guide, chapter <em class="citetitle">Sharing File Systems
     with NFS</em>. It is available from <a class="link" href="http://www.suse.com/documentation/" target="_blank">http://www.suse.com/documentation/</a>.
    </p></li><li class="step"><p>
     Define the configuration for your NFS server in the
     <code class="filename">/etc/exports</code> file. Make sure the directory on the
     NFS server (where you want the backup data to be available), has the
     right mount options. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">/srv/nfs</em> *([...],rw,no_root_squash,[...])</pre></div><p>
     Replace <code class="filename">/srv/nfs</code> with the path to your
     backup data on the NFS server and adjust the mount options. You will
     probably need <code class="literal">no_root_squash</code> to access the backup
     data as the <code class="command">rear mkbackup</code> command runs as
     <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Adjust the various <code class="varname">BACKUP</code> parameters in the
     configuration file <code class="filename">/etc/rear/local.conf</code> to make
     ReaR store the file backup on the respective NFS server. Find
     examples in your installed system under
     <code class="filename">/usr/share/rear/conf/examples/SLE12-*-example.conf</code>.
    </p></li></ol></div></div><div class="complex-example"><div class="example" id="ex-ha-rear-config-EMC" data-id-title="Using Third-Party Backup Tools Like EMC NetWorker"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 22.2: </span><span class="title-name">Using Third-Party Backup Tools Like EMC NetWorker </span></span><a title="Permalink" class="permalink" href="#ex-ha-rear-config-EMC">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    Using third-party backup tools instead of <code class="command">tar</code>
    requires appropriate settings in the ReaR configuration file.
   </p><p>
    The following is an example configuration for EMC NetWorker. Add this
    configuration snippet to <code class="filename">/etc/rear/local.conf</code> and
    adjust it according to your setup:
   </p><div class="verbatim-wrap"><pre class="screen">BACKUP=NSR
    OUTPUT=ISO
    BACKUP_URL=nfs://<em class="replaceable">host.example.com/path/to/rear/backup</em>
    OUTPUT_URL=nfs://<em class="replaceable">host.example.com/path/to/rear/backup</em>
    NSRSERVER=<em class="replaceable">backupserver.example.com</em>
    RETENTION_TIME="Month"</pre></div></div></div></div></section><section class="sect1" id="sec-ha-rear-mkbackup" data-id-title="Creating the Recovery Installation System"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.3 </span><span class="title-name">Creating the Recovery Installation System</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-mkbackup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured ReaR as described in
   <a class="xref" href="#sec-ha-rear-config" title="22.2. Setting Up ReaR and Your Backup Solution">Section 22.2</a>, create the
   recovery installation system (including the ReaR recovery installer)
   plus the file backup with the following command:
  </p><div class="verbatim-wrap"><pre class="screen">rear -d  -D mkbackup</pre></div><p>
   It executes the following steps:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Analyzing the target system and gathering information, in particular
     about the disk layout (partitioning, file systems, mount points) and
     about the boot loader.
    </p></li><li class="listitem"><p>
     Creating a bootable recovery system with the information gathered in
     the first step. The resulting ReaR recovery installer is
     <span class="emphasis"><em>specific</em></span> to the system that you want to protect
     from disaster. It can only be used to re-create this specific system.
    </p></li><li class="listitem"><p>
     Calling the configured backup tool to back up system and user files.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-rear-testing" data-id-title="Testing the Recovery Process"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.4 </span><span class="title-name">Testing the Recovery Process</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-testing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After having created the recovery system, test the recovery process on a
   test machine with identical hardware. See also
   <a class="xref" href="#sec-ha-rear-concept-requirements" title="22.1.4. ReaR Requirements">Section 22.1.4, “ReaR Requirements”</a>. Make sure the test
   machine is correctly set up and can serve as a replacement for your main
   machine.
  </p><div id="id-1.3.5.10.6.3" data-id-title="Extensive Testing on Identical Hardware" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Extensive Testing on Identical Hardware</div><p>
    Thorough testing of the disaster recovery process on machines is
    required. Test the recovery procedure on a regular basis to ensure
    everything works as expected.
   </p></div><div class="procedure" id="pro-ha-rear-testing" data-id-title="Performing a Disaster Recovery on a Test Machine"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 22.1: </span><span class="title-name">Performing a Disaster Recovery on a Test Machine </span></span><a title="Permalink" class="permalink" href="#pro-ha-rear-testing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a recovery medium by burning the recovery system that you have
     created in
     <a class="xref" href="#sec-ha-rear-mkbackup" title="22.3. Creating the Recovery Installation System">Section 22.3</a> to a
     DVD or CD. Alternatively, you can use a network boot via PXE.
    </p></li><li class="step"><p>
     Boot the test machine from the recovery medium.
    </p></li><li class="step"><p>
     From the menu, select <span class="guimenu">Recover</span>.
    </p></li><li class="step"><p>
     Log in as <code class="systemitem">root</code> (no password needed).
    </p></li><li class="step"><p>
     Enter the following command to start the recovery installer:
    </p><div class="verbatim-wrap"><pre class="screen">rear -d -D recover</pre></div><p>
     For details about the steps that ReaR takes during the process,
     see <a class="xref" href="#ol-ha-rear-recovers-steps" title="Recovery Process">Recovery Process</a>.
    </p></li><li class="step"><p>
     After the recovery process has finished, check whether the system has been
     successfully re-created and can serve as a replacement for your original
     system in the production environment.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-rear-recover" data-id-title="Recovering from Disaster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.5 </span><span class="title-name">Recovering from Disaster</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-recover">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case a disaster has occurred, replace any broken hardware if
   necessary. Then proceed as described in
   <a class="xref" href="#pro-ha-rear-testing" title="Performing a Disaster Recovery on a Test Machine">Procedure 22.1</a>, using
   either the repaired machine (or a tested, identical machine that can
   serve as a replacement for your original system).
  </p><p>
   The <code class="command">rear recover</code> command will execute the following
   steps:
  </p><div class="orderedlist" id="ol-ha-rear-recovers-steps" data-id-title="Recovery Process"><div class="title-container"><div class="orderedlist-title-wrap"><div class="orderedlist-title"><span class="title-number-name"><span class="title-name">Recovery Process </span></span><a title="Permalink" class="permalink" href="#ol-ha-rear-recovers-steps">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div><ol class="orderedlist" type="1"><li class="listitem"><p>
     Restoring the disk layout (partitions, file systems, and mount points).
    </p></li><li class="listitem"><p>
     Restoring the system and user files from the backup.
    </p></li><li class="listitem"><p>
     Restoring the boot loader.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-rear-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.6 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="http://en.opensuse.org/SDB:Disaster_Recovery" target="_blank">http://en.opensuse.org/SDB:Disaster_Recovery</a>
    </p></li><li class="listitem"><p>
     <code class="command">rear</code> man page
    </p></li><li class="listitem"><p>
     <code class="filename">/usr/share/doc/packages/rear/README</code>
    </p></li></ul></div></section></section></div><div class="part" id="part-maintenance" data-id-title="Maintenance and Upgrade"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part IV </span><span class="title-name">Maintenance and Upgrade </span></span><a title="Permalink" class="permalink" href="#part-maintenance">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-maintenance"><span class="title-number">23 </span><span class="title-name">Executing Maintenance Tasks</span></a></span></li><dd class="toc-abstract"><p>
    To perform maintenance tasks on the cluster nodes, you might need to stop
    the resources running on that node, to move them, or to shut down or reboot
    the node. It might also be necessary to temporarily take over the control of
    resources from the cluster, or even to stop the cluster service while resources
    remain running.
   </p><p>
    This chapter explains how to manually take down a cluster node without
    negative side effects. It also gives an overview of different options the
    cluster stack provides for executing maintenance tasks.
   </p></dd><li><span class="chapter"><a href="#cha-ha-migration"><span class="title-number">24 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></span></li><dd class="toc-abstract"><p>
    This chapter covers two different scenarios: upgrading a cluster to
    another version of SUSE Linux Enterprise High Availability (either a major release or a service
    pack) as opposed to updating individual packages on cluster nodes. See
    <a class="xref" href="#sec-ha-migration-upgrade" title="24.2. Upgrading your Cluster to the Latest Product Version">Section 24.2, “Upgrading your Cluster to the Latest Product Version”</a> versus
    <a class="xref" href="#sec-ha-migration-update" title="24.3. Updating Software Packages on Cluster Nodes">Section 24.3, “Updating Software Packages on Cluster Nodes”</a>.
   </p><p>
    If you want to upgrade your cluster, check
    <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 24.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a> and
    <a class="xref" href="#sec-ha-migration-upgrade-require" title="24.2.2. Required Preparations Before Upgrading">Section 24.2.2, “Required Preparations Before Upgrading”</a> before starting
    to upgrade.
   </p></dd></ul></div><section class="chapter" id="cha-ha-maintenance" data-id-title="Executing Maintenance Tasks"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">23 </span><span class="title-name">Executing Maintenance Tasks</span></span> <a title="Permalink" class="permalink" href="#cha-ha-maintenance">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    To perform maintenance tasks on the cluster nodes, you might need to stop
    the resources running on that node, to move them, or to shut down or reboot
    the node. It might also be necessary to temporarily take over the control of
    resources from the cluster, or even to stop the cluster service while resources
    remain running.
   </p><p>
    This chapter explains how to manually take down a cluster node without
    negative side effects. It also gives an overview of different options the
    cluster stack provides for executing maintenance tasks.
   </p></div></div></div></div><section class="sect1" id="sec-ha-maint-shutdown-node" data-id-title="Implications of Taking Down a Cluster Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.1 </span><span class="title-name">Implications of Taking Down a Cluster Node</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-shutdown-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When you shut down or reboot a cluster node (or stop the Pacemaker service on a
   node), the following processes will be triggered:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The resources that are running on the node will be stopped or moved off
     the node.
    </p></li><li class="listitem"><p>
     If stopping the resources should fail or time out, the STONITH mechanism
     will fence the node and shut it down.
    </p></li></ul></div><div class="procedure" id="pro-ha-maint-shutdown-node" data-id-title="Manually Rebooting a Cluster Node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.1: </span><span class="title-name">Manually Rebooting a Cluster Node </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-shutdown-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    If your aim is to move the services off the node in an orderly fashion
    before shutting down or rebooting the node, proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     On the node you want to reboot or shut down, log in as <code class="systemitem">root</code> or
     equivalent.
    </p></li><li class="step"><p>
     Put the node into <code class="literal">standby</code> mode:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm node standby</pre></div><p>
     That way, services can migrate off the node without being limited by the
     shutdown timeout of Pacemaker.
    </p></li><li class="step"><p>
     Check the cluster status with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status</pre></div><p>
     It shows the respective node in <code class="literal">standby</code> mode:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
Node <em class="replaceable">bob</em>: standby
[...]</pre></div></li><li class="step"><p>
     Stop the Pacemaker service on that node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl stop pacemaker.service</pre></div></li><li class="step"><p>
     Reboot the node.
    </p></li></ol></div></div><p>
   To check if the node joins the cluster again:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to the node as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Check if the Pacemaker service has started:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl status pacemaker.service</pre></div></li><li class="step"><p>
     If not, start it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl start pacemaker.service</pre></div></li><li class="step"><p>
     Check the cluster status with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status</pre></div><p>
     It should show the node coming online again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-overview" data-id-title="Different Options for Maintenance Tasks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.2 </span><span class="title-name">Different Options for Maintenance Tasks</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Pacemaker offers a variety of options for performing system maintenance:
  </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-maint-mode-cluster"><span class="term"><a class="xref" href="#sec-ha-maint-mode-cluster" title="23.4. Putting the Cluster into Maintenance Mode">Putting the Cluster into Maintenance Mode</a></span></dt><dd><p>
      The global cluster property <code class="literal">maintenance-mode</code> puts all
      resources into maintenance state at once. The cluster stops monitoring them
      and becomes oblivious to their status. Note that only the resource management
      by Pacemaker is disabled. Corosync and SBD are still functional. Use maintenance
      mode for any tasks involving cluster resources. For any tasks involving
      infrastructure such as storage or networking, the safest method is to stop
      the cluster services completely. See <a class="xref" href="#sec-ha-maint-shutdown-node" title="23.1. Implications of Taking Down a Cluster Node">Section 23.1, “Implications of Taking Down a Cluster Node”</a>.
     </p></dd><dt id="vle-ha-maint-mode-node"><span class="term"><a class="xref" href="#sec-ha-maint-mode-node" title="23.5. Putting a Node into Maintenance Mode">Putting a Node into Maintenance Mode</a></span></dt><dd><p>
      This option allows you to put all resources running on a specific node into
      maintenance state at once. The cluster will cease monitoring them and thus
      become oblivious to their status.
     </p></dd><dt id="vle-ha-maint-node-standby"><span class="term"><a class="xref" href="#sec-ha-maint-node-standby" title="23.6. Putting a Node into Standby Mode">Putting a Node into Standby Mode</a></span></dt><dd><p>
      A node that is in standby mode can no longer run resources. Any resources
      running on the node will be moved away or stopped (in case no other node
      is eligible to run the resource). Also, all monitoring operations will be
      stopped on the node (except for those with
      <code class="literal">role="Stopped"</code>).
     </p><p>
      You can use this option if you need to stop a node in a cluster while
      continuing to provide the services running on another node.
     </p></dd><dt id="vle-ha-maint-mode-rsc"><span class="term"><a class="xref" href="#sec-ha-maint-mode-rsc" title="23.7. Putting a Resource into Maintenance Mode">Putting a Resource into Maintenance Mode</a></span></dt><dd><p>
      When this mode is enabled for a resource, no monitoring operations will be
      triggered for the resource.
     </p><p>
      Use this option if you need to manually touch the service that is managed
      by this resource and do not want the cluster to run any monitoring
      operations for the resource during that time.
     </p></dd><dt id="vle-ha-maint-rsc-unmanaged"><span class="term"><a class="xref" href="#sec-ha-maint-rsc-unmanaged" title="23.8. Putting a Resource into Unmanaged Mode">Putting a Resource into Unmanaged Mode</a></span></dt><dd><p>
      The <code class="option">is-managed</code> meta attribute allows you to temporarily
      <span class="quote">“<span class="quote">release</span>”</span> a resource from being managed by the cluster
      stack. This means you can manually touch the service that is managed by
      this resource (for example, to adjust any components). However, the
      cluster will continue to <span class="emphasis"><em>monitor</em></span> the resource and to
      report any failures.
     </p><p>
      If you want the cluster to also cease <span class="emphasis"><em>monitoring</em></span> the
      resource, use the per-resource maintenance mode instead (see
      <a class="xref" href="#vle-ha-maint-mode-rsc">Putting a Resource into Maintenance Mode</a>).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-maint-outline" data-id-title="Preparing and Finishing Maintenance Work"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.3 </span><span class="title-name">Preparing and Finishing Maintenance Work</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-outline">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.6.3.5.2" data-id-title="Risk of Data Loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Risk of Data Loss</div><p>
    If you need to do testing or maintenance work, follow the general steps
    below.
   </p><p>
    Otherwise you risk unwanted side effects, like resources not starting in an
    orderly fashion, unsynchronized CIBs across the cluster nodes, or even data loss.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Before you start, choose which of the options outlined in
     <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2</a> is appropriate for your situation.
    </p></li><li class="step"><p>
     Apply this option with Hawk2 or crmsh.
    </p></li><li class="step"><p>
     Execute your maintenance task or tests.
    </p></li><li class="step"><p>
     After you have finished, put the resource, node or cluster back to
     <span class="quote">“<span class="quote">normal</span>”</span> operation.
    </p></li></ol></div></div></div></section><section class="sect1" id="sec-ha-maint-mode-cluster" data-id-title="Putting the Cluster into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.4 </span><span class="title-name">Putting the Cluster into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-cluster">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.6.3.6.2" data-id-title="Maintenance mode only disables Pacemaker" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Maintenance mode only disables Pacemaker</div><p>
    When putting a cluster into maintenance mode, only the resource management
    by Pacemaker is disabled. Corosync and SBD are still functional. Depending
    on your maintainence tasks, this might lead to fence operations.
   </p><p>
    Use maintenance mode for any tasks involving cluster resources. For any tasks
    involving infrastructure such as storage or networking, the safest method is to stop
    the cluster services completely. See <a class="xref" href="#sec-ha-maint-shutdown-node" title="23.1. Implications of Taking Down a Cluster Node">Section 23.1, “Implications of Taking Down a Cluster Node”</a>.
   </p></div><p>
    To put the cluster into maintenance mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=true</pre></div><p>
    To put the cluster back into normal mode after your maintenance work is done, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=false</pre></div><div class="procedure" id="pro-ha-maint-mode-cluster-hawk2" data-id-title="Putting the Cluster into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.2: </span><span class="title-name">Putting the Cluster into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-cluster-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="6.2. Logging In">Section 6.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster
     Configuration</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">CRM Configuration</span> group, select the
     <span class="guimenu">maintenance-mode</span> attribute from the empty drop-down box
     and click the plus icon to add it.
    </p></li><li class="step"><p>
     To set <code class="literal">maintenance-mode=true</code>, activate the check box
     next to <code class="literal">maintenance-mode</code> and confirm your changes.
    </p></li><li class="step"><p>
     After you have finished the maintenance task for the whole cluster,
     deactivate the check box next to the <code class="literal">maintenance-mode</code>
     attribute.
    </p><p>
     From this point on, High Availability will take over cluster management again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-mode-node" data-id-title="Putting a Node into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.5 </span><span class="title-name">Putting a Node into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a node into maintenance mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node maintenance <em class="replaceable">NODENAME</em></pre></div><p>
    To put the node back into normal mode after your maintenance work is done, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node ready <em class="replaceable">NODENAME</em></pre></div><div class="procedure" id="pro-ha-maint-mode-nodes-hawk2" data-id-title="Putting a Node into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.3: </span><span class="title-name">Putting a Node into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-nodes-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="6.2. Logging In">Section 6.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Status</span>.
    </p></li><li class="step"><p>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <span class="guimenu">Maintenance</span>.
    </p></li><li class="step"><p>
      After you have finished your maintenance task, click the wrench icon next to the node
     and select <span class="guimenu">Ready</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-node-standby" data-id-title="Putting a Node into Standby Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.6 </span><span class="title-name">Putting a Node into Standby Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-node-standby">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a node into standby mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm node standby <em class="replaceable">NODENAME</em></pre></div><p>
    To bring the node back online after your maintenance work is done, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm node online <em class="replaceable">NODENAME</em></pre></div><div class="procedure" id="pro-ha-maint-node-standby-hawk2" data-id-title="Putting a Node into Standby Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.4: </span><span class="title-name">Putting a Node into Standby Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-node-standby-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="6.2. Logging In">Section 6.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Status</span>.
    </p></li><li class="step"><p>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <span class="guimenu">Standby</span>.
    </p></li><li class="step"><p>
     Finish the maintenance task for the node.
    </p></li><li class="step"><p>
     To deactivate the standby mode, click the wrench icon next to the node
     and select <span class="guimenu">Ready</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-mode-rsc" data-id-title="Putting a Resource into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.7 </span><span class="title-name">Putting a Resource into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-rsc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a resource into maintenance mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource maintenance <em class="replaceable">RESOURCE_ID</em> true</pre></div><p>
    To put the resource back into normal mode after your maintenance work is done, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource maintenance <em class="replaceable">RESOURCE_ID</em> false</pre></div><div class="procedure" id="pro-ha-maint-mode-rsc-hawk2" data-id-title="Putting a Resource into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.5: </span><span class="title-name">Putting a Resource into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-rsc-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="6.2. Logging In">Section 6.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Resources</span>.
    </p></li><li class="step"><p>
     Select the resource you want to put in maintenance mode or unmanaged mode,
     click the wrench icon next to the resource and select <span class="guimenu">Edit
     Resource</span>.
    </p></li><li class="step"><p>
     Open the <span class="guimenu">Meta Attributes</span> category.
    </p></li><li class="step"><p>
     From the empty drop-down box, select the <span class="guimenu">maintenance</span>
     attribute and click the plus icon to add it.
    </p></li><li class="step"><p>
     Activate the check box next to <code class="literal">maintenance</code> to set the
     maintenance attribute to <code class="literal">yes</code>.
    </p></li><li class="step"><p>
     Confirm your changes.
    </p></li><li class="step"><p>
     After you have finished the maintenance task for that resource, deactivate
     the check box next to the <code class="literal">maintenance</code> attribute for
     that resource.
    </p><p>
     From this point on, the resource will be managed by the High Availability software
     again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-rsc-unmanaged" data-id-title="Putting a Resource into Unmanaged Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.8 </span><span class="title-name">Putting a Resource into Unmanaged Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-rsc-unmanaged">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a resource into unmanaged mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource unmanage <em class="replaceable">RESOURCE_ID</em></pre></div><p>
    To put it into managed mode again after your maintenance work is done, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource manage <em class="replaceable">RESOURCE_ID</em></pre></div><div class="procedure" id="pro-ha-maint-rsc-unmanaged-hawk2" data-id-title="Putting a Resource into Unmanaged Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.6: </span><span class="title-name">Putting a Resource into Unmanaged Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-rsc-unmanaged-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="6.2. Logging In">Section 6.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Status</span> and go to
     the <span class="guimenu">Resources</span> list.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column, click the arrow down icon
     next to the resource you want to modify and select
     <span class="guimenu">Edit</span>.
    </p><p>
     The resource configuration screen opens.
    </p></li><li class="step"><p>
     Below <span class="guimenu">Meta Attributes</span>, select the
     <span class="guimenu">is-managed</span> entry from the empty drop-down box.
    </p></li><li class="step"><p>
     Set its value to <code class="literal">No</code> and click <span class="guimenu">Apply</span>.
    </p></li><li class="step"><p>
     After you have finished your maintenance task, set
     <span class="guimenu">is-managed</span> to <code class="literal">Yes</code> (which is the
     default value) and apply your changes.
    </p><p>
     From this point on, the resource will be managed by the High Availability software
     again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-shutdown-node-maint-mode" data-id-title="Rebooting a Cluster Node While In Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.9 </span><span class="title-name">Rebooting a Cluster Node While In Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-shutdown-node-maint-mode">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.6.3.11.2" data-id-title="Implications" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Implications</div><p>
       If the cluster or a node is in maintenance mode, you can stop or restart cluster
       resources at will—the High Availability software will not attempt to restart them. If
       you stop the Pacemaker service on a node, all daemons and processes
       (originally started as Pacemaker-managed cluster resources) will continue
       to run.
      </p><p>
       If you attempt to start Pacemaker services on a node while the cluster or
       node is in maintenance mode, Pacemaker will initiate a single one-shot monitor
       operation (a <span class="quote">“<span class="quote">probe</span>”</span>) for every resource to evaluate which
       resources are currently running on that node. However, it will take no
       further action other than determining the resources' status.
      </p></div><div class="procedure"><div class="procedure-contents"><p>
    If you want to take down a node while either the cluster or the node is in
    <code class="literal">maintenance mode</code>, proceed as follows:
    </p><ol class="procedure" type="1"><li class="step"><p>
     On the node you want to reboot or shut down, log in as <code class="systemitem">root</code> or
     equivalent.
    </p></li><li class="step"><p>
     Check if you have resources of the type <code class="literal">ocf:pacemaker:controld</code>
     or any dependencies on this type of resource. Resources of the type
     <code class="literal">ocf:pacemaker:controld</code> are DLM resources.
    </p><ol type="a" class="substeps"><li class="step"><p>
       If yes, explicitly stop the DLM resources and any resources
       depending on them:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code>stop <em class="replaceable">RESOURCE_ID</em></pre></div><p>
       The reason is that stopping Pacemaker also stops the Corosync service, on
       whose membership and messaging services DLM depends. If Corosync stops,
       the DLM resource will assume a split brain scenario and trigger a fencing
       operation.
      </p></li><li class="step"><p>
        If no, continue with <a class="xref" href="#step-stop-pcmkr-service" title="Step 3">Step 3</a>.
       </p></li></ol></li><li class="step" id="step-stop-pcmkr-service"><p>
     Stop the Pacemaker service on that node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl stop pacemaker.service</pre></div></li><li class="step"><p>
     Shut down or reboot the node.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-migration" data-id-title="Upgrading Your Cluster and Updating Software Packages"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">24 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></span> <a title="Permalink" class="permalink" href="#cha-ha-migration">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter covers two different scenarios: upgrading a cluster to
    another version of SUSE Linux Enterprise High Availability (either a major release or a service
    pack) as opposed to updating individual packages on cluster nodes. See
    <a class="xref" href="#sec-ha-migration-upgrade" title="24.2. Upgrading your Cluster to the Latest Product Version">Section 24.2, “Upgrading your Cluster to the Latest Product Version”</a> versus
    <a class="xref" href="#sec-ha-migration-update" title="24.3. Updating Software Packages on Cluster Nodes">Section 24.3, “Updating Software Packages on Cluster Nodes”</a>.
   </p><p>
    If you want to upgrade your cluster, check
    <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 24.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a> and
    <a class="xref" href="#sec-ha-migration-upgrade-require" title="24.2.2. Required Preparations Before Upgrading">Section 24.2.2, “Required Preparations Before Upgrading”</a> before starting
    to upgrade.
   </p></div></div></div></div><section class="sect1" id="sec-ha-migration-terminology" data-id-title="Terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.1 </span><span class="title-name">Terminology</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-terminology">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the following, find definitions of the most important terms used in
   this chapter:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.3.3.1"><span class="term">Major Release, </span><span class="term">General Availability (GA) Version</span></dt><dd><p>
      The Major Release of SUSE Linux Enterprise (or any software product) is a new version
      that brings new features and tools, decommissions previously
      deprecated components and comes with backward incompatible changes.
     </p></dd><dt id="vle-upgrade-offline"><span class="term">Cluster Offline Upgrade</span></dt><dd><p>
      If a new product version includes major changes that are backward
      incompatible, the cluster needs to be upgraded by a cluster offline
      upgrade. You need to take all nodes offline and upgrade the cluster as a whole,
      before you can bring all nodes back online.
     </p></dd><dt id="vle-upgrade-rolling"><span class="term">Cluster Rolling Upgrade</span></dt><dd><p>
      In a cluster rolling upgrade, one cluster node at a time is upgraded while the
      rest of the cluster is still running. You take the first node offline,
      upgrade it and bring it back online to join the cluster. Then you
      continue one by one until all cluster nodes are upgraded to a major
      version.
     </p></dd><dt id="id-1.3.6.4.3.3.4"><span class="term">Service Pack (SP)</span></dt><dd><p>
      Combines several patches into a form that is easy to install or
      deploy. Service packs are numbered and usually contain security fixes,
      updates, upgrades, or enhancements of programs.
     </p></dd><dt id="id-1.3.6.4.3.3.5"><span class="term">Update</span></dt><dd><p>
      Installation of a newer <span class="emphasis"><em>minor</em></span> version of a
      package.
     </p></dd><dt id="id-1.3.6.4.3.3.6"><span class="term">Upgrade</span></dt><dd><p>
      Installation of a newer <span class="emphasis"><em>major</em></span> version of a
      package or distribution, which brings <span class="emphasis"><em>new
      features</em></span>. See also <a class="xref" href="#vle-upgrade-offline">Cluster Offline Upgrade</a>
      versus <a class="xref" href="#vle-upgrade-rolling">Cluster Rolling Upgrade</a>.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-migration-upgrade" data-id-title="Upgrading your Cluster to the Latest Product Version"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.2 </span><span class="title-name">Upgrading your Cluster to the Latest Product Version</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Which upgrade path is supported, and how to perform the upgrade depends
   both on the current product version and on the target version you want to
   migrate to.
  </p><p>
   SUSE Linux Enterprise High Availability has the same supported upgrade paths as the underlying base system. For a complete
   overview, see the section <a class="link" href="https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-update-sle.html#sec-update-sle-paths" target="_blank"><em class="citetitle">Supported Upgrade Paths to SUSE Linux Enterprise Server 12 SP4</em></a> in the
   SUSE Linux Enterprise Server Deployment Guide.
  </p><p>
   In addition, the following rules apply, as the High Availability cluster stack offers two methods for
   upgrading the cluster:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title"><a class="xref" href="#vle-upgrade-rolling">Cluster Rolling Upgrade</a>. </span>A cluster rolling upgrade is only supported within the same major release
      (from one service pack to the next, or from the GA version of a product to SP1).</p></li><li class="listitem"><p><span class="formalpara-title"><a class="xref" href="#vle-upgrade-offline">Cluster Offline Upgrade</a>. </span>A cluster offline upgrade is required to upgrade from one major release to
      the next (for example, from SLE HA 11 to
      SLE HA 12) or from a service pack within one major
      release to the next major release (for example, from
      SLE HA 11 SP3 to SLE HA 12).
     </p></li></ul></div><p>
   <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 24.2.1</a>
   list the supported upgrade paths and methods for SLE HA (Geo),
   moving from one version to the next. The column <em class="citetitle">For Details</em> lists
   the specific upgrade documentation you should refer to (including also the base
   system and Geo Clustering for SUSE Linux Enterprise High Availability). This documentation is available from:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="link" href="https://documentation.suse.com/sles" target="_blank">https://documentation.suse.com/sles</a>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://documentation.suse.com/sle-ha" target="_blank">https://documentation.suse.com/sle-ha</a>
     </p></li></ul></div><div id="id-1.3.6.4.4.8" data-id-title="No Support for Mixed Clusters and Reversion After Upgrade" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support for Mixed Clusters and Reversion After Upgrade</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Mixed clusters running on SUSE Linux Enterprise High Availability 11/SUSE Linux Enterprise High Availability 12 are
      <span class="emphasis"><em>not</em></span> supported.
     </p></li><li class="listitem"><p>
      After the upgrade process to product version 12, reverting back to
      product version 11 is <span class="emphasis"><em>not</em></span> supported.
     </p></li></ul></div></div><section class="sect2" id="sec-ha-migration-upgrade-oview" data-id-title="Supported Upgrade Paths for SLE HA and SLE HA Geo"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">24.2.1 </span><span class="title-name">Supported Upgrade Paths for SLE HA and SLE HA Geo</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-oview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Upgrade From ... To
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Upgrade Path
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         For Details
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA 11 SP3 to SLE HA (Geo) 12
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Offline Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 Deployment Guide, part
           <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12
            Geo Clustering Quick Start, section <em class="citetitle">Upgrading from
             SLE HA (Geo) 11 SP3 to
             SLE HA Geo 12</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA (Geo) 11 SP4 to
         SLE HA (Geo) 12 SP1
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Offline Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 SP1 Deployment Guide,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP1 Geo Clustering Quick Start,
           section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA (Geo) 12 to
         SLE HA (Geo) 12 SP1
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Rolling Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 SP1
            Deployment Guide, part <em class="citetitle">Updating and Upgrading
             SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP1
            Geo Clustering Quick Start, section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP1 to
         SLE HA (Geo) 12 SP2</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 SP2
            Deployment Guide, part <em class="citetitle">Updating and Upgrading
             SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP2
            Geo Clustering Quick Start, section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li><li class="listitem"><p>DRBD 8 to DRBD 9: <a class="xref" href="#sec-ha-drbd-migrate" title="18.4. Migrating from DRBD 8 to DRBD 9">Migrating from DRBD 8 to DRBD 9</a></p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP2 to
         SLE HA (Geo) 12 SP3</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 SP3
            Deployment Guide, part <em class="citetitle">Updating and Upgrading
             SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP3
            Geo Clustering Guide, section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP3 to
         SLE HA (Geo) 12 SP4</p>
       </td><td style="border-right: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: SUSE Linux Enterprise Server 12 SP4
            Deployment Guide, part <em class="citetitle">Updating and Upgrading
             SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering for SUSE Linux Enterprise High Availability 12 SP4
            Geo Clustering Guide, section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li></ul></div>
       </td></tr></tbody></table></div></section><section class="sect2" id="sec-ha-migration-upgrade-require" data-id-title="Required Preparations Before Upgrading"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">24.2.2 </span><span class="title-name">Required Preparations Before Upgrading</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-require">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.4.10.2.1"><span class="term">Backup</span></dt><dd><p>
       Ensure that your system backup is up to date and restorable.
      </p></dd><dt id="id-1.3.6.4.4.10.2.2"><span class="term">Testing</span></dt><dd><p>
       Test the upgrade procedure on a staging instance of your cluster
       setup first, before performing it in a production environment.
      </p><p>
       This gives you an estimation of the time frame required for the
       maintenance window. It also helps to detect and solve any unexpected
       problems that might arise.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-migration-upgrade-offline" data-id-title="Cluster Offline Upgrade"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">24.2.3 </span><span class="title-name">Cluster Offline Upgrade</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-offline">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section applies to the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Upgrading from SLE HA 11 SP3 to SLE HA 12
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 11 SP4 to
      SLE HA 12 SP1
     </p></li></ul></div><p>
    If your cluster is still based on an older product version than the ones
    listed above, first upgrade it to a version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability
    that can be used as a source for upgrading to the desired target
    version.
   </p><div class="procedure" id="pro-ha-migration-offline" data-id-title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 24.1: </span><span class="title-name">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-offline">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     SUSE Linux Enterprise High Availability 12 cluster stack comes with major changes in various
     components (for example, <code class="filename">/etc/corosync/corosync.conf</code>, disk formats of OCFS2).
     Therefore, a <code class="literal">cluster rolling upgrade</code> from any SUSE Linux Enterprise High Availability
    11 version is not supported. Instead, all cluster nodes must be offline
     and the cluster needs to be upgraded as a whole as described below.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Log in to each cluster node and stop the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rcopenais</code> stop</pre></div></li><li class="step"><p>
      For each cluster node, perform an upgrade to the desired target
      version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability. If you have an existing Geo
      cluster setup and want to upgrade it, see the additional instructions
      in the Geo Clustering for SUSE Linux Enterprise High Availability Geo Clustering Quick Start. To find the
      details for the individual upgrade processes, see
      <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 24.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a>.
     </p></li><li class="step"><p>
      After the upgrade process has finished, reboot each node with the
      upgraded version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability.
     </p></li><li class="step"><p>
      If you use OCFS2 in your cluster setup, update the on-device structure
      by executing the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">o2cluster</code> --update <em class="replaceable">PATH_TO_DEVICE</em></pre></div><p>
      It adds additional parameters to the disk which are needed for the
      updated OCFS2 version that is shipped with SUSE Linux Enterprise High Availability 12 and
      12 SPx.
     </p></li><li class="step"><p>
      To update <code class="filename">/etc/corosync/corosync.conf</code> for Corosync version 2:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Log in to one node and start the YaST cluster module.
       </p></li><li class="step"><p>
        Switch to the <span class="guimenu">Communication Channels</span> category and
        enter values for the following new parameters: <span class="guimenu">Cluster
        Name</span> and <span class="guimenu">Expected Votes</span>. For details,
        see <a class="xref" href="#pro-ha-installation-setup-channel1-udp" title="Defining the First Communication Channel (Multicast)">Procedure 4.1, “Defining the First Communication Channel (Multicast)”</a>
        or <a class="xref" href="#pro-ha-installation-setup-channel1-udpu" title="Defining the First Communication Channel (Unicast)">Procedure 4.2, “Defining the First Communication Channel (Unicast)”</a>,
        respectively.
       </p><p>
        If YaST should detect any other options that are invalid or
        missing according to Corosync version 2, it will prompt you to
        change them.
       </p></li><li class="step"><p>
        Confirm your changes in YaST. YaST will write them to
        <code class="filename">/etc/corosync/corosync.conf</code>.
       </p></li><li class="step"><p>
        If Csync2 is configured for your cluster, use the following command
        to push the updated Corosync configuration to the other cluster
        nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p>
        For details on Csync2, see
        <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a>.
       </p><p>
        Alternatively, synchronize the updated Corosync configuration by
        manually copying <code class="filename">/etc/corosync/corosync.conf</code> to all cluster nodes.
       </p></li></ol></li><li class="step"><p>
      Log in to each node and start the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></li><li class="step"><p>
      Check the cluster status with <code class="command">crm status</code> or with
      Hawk2.
     </p></li><li class="step"><p>Configure the following services to start at boot time:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl enable pacemaker
<code class="prompt root"># </code>systemctl enable hawk
<code class="prompt root"># </code>systemctl enable sbd</pre></div></li></ol></div></div><div id="note-ha-cib-upgrade" data-id-title="Upgrading the CIB Syntax Version" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Upgrading the CIB Syntax Version</div><p>
     Tags (for grouping resources) and some ACL features only work with the
     CIB syntax version <code class="literal">pacemaker-2.0</code> or higher. (To
     check your version, use the <code class="command">cibadmin -Q |grep
     validate-with</code> command.) If you have upgraded from SUSE Linux Enterprise High Availability 11
     SPx, your CIB version will <span class="emphasis"><em>not</em></span> be upgraded by
     default. To manually upgrade to the latest CIB version use one of the
     following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">cibadmin</code> --upgrade --force</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure upgrade force</pre></div></div></section><section class="sect2" id="sec-ha-migration-upgrade-rolling" data-id-title="Cluster Rolling Upgrade"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">24.2.4 </span><span class="title-name">Cluster Rolling Upgrade</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-rolling">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section applies to the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Upgrading from SLE HA 12 to SLE HA 12 SP1
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP1 to
      SLE HA 12 SP2
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA (Geo) 12 SP2 to
      SLE HA (Geo) 12 SP3
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA (Geo) 12 SP3 to
      SLE HA (Geo) 12 SP4
     </p></li></ul></div><p>Use one of the following procedures for your scenario:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      For a more general rolling upgrade, refer to <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Procedure 24.2</a>.
     </p></li><li class="listitem"><p>
      For a specific rolling upgrade, refer to <a class="xref" href="#pro-ha-migration-rolling-upgrade-newsp-freshinstall" title="Performing a Cluster-wide Fresh Installation of a New Service Pack">Procedure 24.3</a>.
     </p></li></ul></div><div id="id-1.3.6.4.4.12.6" data-id-title="Active Cluster Stack" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Active Cluster Stack</div><p>
     Before starting an upgrade for a node, <span class="emphasis"><em>stop</em></span> the
     cluster stack <span class="emphasis"><em>on that node</em></span>.
    </p><p>
     If the cluster resource manager on a node is active during the software
     update, this can lead to results such as fencing of active
     nodes.
    </p></div><div id="id-1.3.6.4.4.12.7" data-id-title="Time Limit for Cluster Rolling Upgrade" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Time Limit for Cluster Rolling Upgrade</div><p>
     The new features shipped with the latest product version will only be
     available after <span class="emphasis"><em>all</em></span> cluster nodes have been
     upgraded to the latest product version. Mixed version clusters are only
     supported for a short time frame during the cluster rolling upgrade. Complete
     the cluster rolling upgrade within one week.
    </p></div><div class="procedure" id="pro-ha-migration-rolling-upgrade" data-id-title="Performing a Cluster Rolling Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 24.2: </span><span class="title-name">Performing a Cluster Rolling Upgrade </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-rolling-upgrade">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> on the node that you want to upgrade and stop the
      cluster stack:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> stop pacemaker</pre></div></li><li class="step" id="step-ha-migration-upgrade-tolatest"><p>
      Perform an upgrade to the desired target version of SUSE Linux Enterprise Server and
      SUSE Linux Enterprise High Availability. To find the details for the individual upgrade
      processes, see
      <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 24.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a>.
     </p></li><li class="step" id="step-ha-migration-upgrade-ais"><p>
      Restart the cluster stack on the upgraded node to make the node rejoin
      the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></li><li class="step"><p>
      Take the next node offline and repeat the procedure for that node.
     </p></li><li class="step"><p>
      Check the cluster status with <code class="command">crm status</code> or with
      Hawk2.
     </p></li></ol></div></div><p>The Hawk2 <span class="guimenu">Status</span> screen also shows a warning if
    different CRM versions are detected for your cluster nodes.</p><p>Beside an in-place upgrade, many customers prefer a fresh installation
    even for moving to the next service pack. The following procedure shows a
    scenario where a two-node cluster with the nodes alice and bob is
    upgraded to the next service pack (SP):
   </p><div class="procedure" id="pro-ha-migration-rolling-upgrade-newsp-freshinstall" data-id-title="Performing a Cluster-wide Fresh Installation of a New Service Pack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 24.3: </span><span class="title-name">Performing a Cluster-wide Fresh Installation of a New Service Pack </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-rolling-upgrade-newsp-freshinstall">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="st-ha-migration-rolling-up-sp-backup"><p>Make a backup of your cluster configuration. A minimum set of files
      are shown in the following list:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/corosync/corosync.conf
/etc/corosync/authkey
/etc/sysconfig/sbd
/etc/modules-load.d/watchdog.conf
/etc/hosts
/etc/ntp.conf</pre></div><p>Depending on your resources, you may also need the following files:</p><div class="verbatim-wrap"><pre class="screen">/etc/services
/etc/passwd
/etc/shadow
/etc/groups
/etc/drbd/*
/etc/lvm/lvm.conf
/etc/mdadm.conf
/etc/mdadm.SID.conf</pre></div></li><li class="step" id="st-ha-migration-rolling-alice"><p>Start with node alice.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Put the node into standby node. That way, resources can move off the node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> --wait node standby alice reboot</pre></div><p>
        With the option <code class="option">--wait</code>, the command returns only when
        the cluster finishes the transition and becomes idle.
        The <code class="option">reboot</code> option has the effect that the node will
        be already out of standby mode when it is online again.
        Despite its name, the <code class="option">reboot</code> option works as long
        as the node goes offline and online.
       </p></li><li class="step"><p>Stop the cluster services on node alice:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="step"><p>
        At this point, alice does not have running resources anymore.
        Upgrade the node alice and reboot it afterward.
        Cluster services are assumed not to start on boot.
       </p></li><li class="step"><p>
        Copy your backup files from <a class="xref" href="#st-ha-migration-rolling-up-sp-backup" title="Step 1">Step 1</a>
        to the original places.
       </p></li><li class="step"><p>Bring back node alice into cluster:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>Check that resources are fine.</p></li></ol></li><li class="step"><p>Repeat <a class="xref" href="#st-ha-migration-rolling-alice" title="Step 2">Step 2</a> for node
      bob.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-migration-update" data-id-title="Updating Software Packages on Cluster Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.3 </span><span class="title-name">Updating Software Packages on Cluster Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.6.4.5.2" data-id-title="Active Cluster Stack" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Active Cluster Stack</div><p>
    Before starting an update for a node, either <span class="emphasis"><em>stop</em></span>
    the cluster stack <span class="emphasis"><em>on that node</em></span> or put the
    <span class="emphasis"><em>node into maintenance mode</em></span>, depending on whether the
    cluster stack is affected or not. See <a class="xref" href="#step-update-check" title="Step 1">Step 1</a>
    for details.
   </p><p>
    If the cluster resource manager on a node is active during the software
    update, this can lead to results such as fencing of active
    nodes.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="step-update-check"><p>
     Before installing any package updates on a node, check the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Does the update affect any packages belonging to SUSE Linux Enterprise High Availability or the
       Geo clustering extension? If <code class="literal">yes</code>: Stop the cluster stack on
       the node before starting the software update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> stop pacemaker</pre></div></li><li class="listitem"><p>
       Does the package update require a reboot? If <code class="literal">yes</code>:
       Stop the cluster stack on the node before starting the software
       update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> stop pacemaker</pre></div></li><li class="listitem"><p>
       If none of the situations above apply, you do not need to stop the
       cluster stack. In that case, put the node into maintenance mode
       before starting the software update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node maintenance <em class="replaceable">NODE_NAME</em></pre></div><p>
       For more details on maintenance mode, see
       <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a>.
      </p></li></ul></div></li><li class="step"><p>
     Install the package update using either YaST or Zypper.
    </p></li><li class="step"><p>
     After the update has been successfully installed:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Either start the cluster stack on the respective node (if you
       stopped it in <a class="xref" href="#step-update-check" title="Step 1">Step 1</a>):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></li><li class="listitem"><p>
       or remove the maintenance flag to bring the node back to
       normal mode:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node ready <em class="replaceable">NODE_NAME</em></pre></div></li></ul></div></li><li class="step"><p>
     Check the cluster status with <code class="command">crm status</code> or with
     Hawk2.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-migration-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For detailed information about any changes and new features of the
   product you are upgrading to, refer to its release notes. They are
   available from <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section></section></div><div class="part" id="part-appendix" data-id-title="Appendix"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part V </span><span class="title-name">Appendix </span></span><a title="Permalink" class="permalink" href="#part-appendix">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/book_sle_haguide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="appendix"><a href="#app-ha-troubleshooting"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></span></li><dd class="toc-abstract"><p>
    Strange problems may occur that are not easy to understand, especially
    when starting to experiment with High Availability. However, there are several
    utilities that allow you to take a closer look at the High Availability internal
    processes. This chapter recommends various solutions.
   </p></dd><li><span class="appendix"><a href="#app-naming"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></span></li><dd class="toc-abstract"><p>
  This guide uses the following naming conventions for cluster nodes and
  names, cluster resources, and constraints.
 </p></dd><li><span class="appendix"><a href="#app-ha-management"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></span></li><dd class="toc-abstract"><p>SUSE Linux Enterprise High Availability ships with a comprehensive set of tools to assists you in managing your cluster from the command line. This chapter introduces the tools needed for managing the cluster configuration in the CIB and the cluster resources. Other command line tools for managing r…</p></dd><li><span class="appendix"><a href="#app-crmreport-nonroot"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></span></li><dd class="toc-abstract"><p>
  All cluster nodes must be able to access each other via SSH. Tools like
  <code class="command">crm report</code> (for
  troubleshooting) and Hawk2's <span class="guimenu">History Explorer</span> require
  passwordless SSH access between the nodes, otherwise they can only collect
  data from the current node.
 </p></dd><li><span class="appendix"><a href="#app-ha-docupdates"><span class="title-number">E </span><span class="title-name">Documentation Updates</span></a></span></li><dd class="toc-abstract"><p>
  This chapter lists content changes for this document.
 </p></dd></ul></div><section class="appendix" id="app-ha-troubleshooting" data-id-title="Troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></span> <a title="Permalink" class="permalink" href="#app-ha-troubleshooting">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Strange problems may occur that are not easy to understand, especially
    when starting to experiment with High Availability. However, there are several
    utilities that allow you to take a closer look at the High Availability internal
    processes. This chapter recommends various solutions.
   </p></div></div></div></div><section class="sect1" id="sec-ha-troubleshooting-install" data-id-title="Installation and First Steps"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.1 </span><span class="title-name">Installation and First Steps</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Troubleshooting difficulties when installing the packages or bringing the
   cluster online.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.3.3.1"><span class="term">Are the HA packages installed?</span></dt><dd><p>
      The packages needed for configuring and managing a cluster are
      included in the <code class="literal">High Availability</code> installation
      pattern, available with SUSE Linux Enterprise High Availability.
     </p><p>
      Check if SUSE Linux Enterprise High Availability is installed on each of the cluster nodes and if the
      <span class="guimenu">High Availability</span> pattern is installed on each of
      the machines as described in the Installation and Setup Quick Start.
     </p></dd><dt id="id-1.3.7.2.3.3.2"><span class="term">Is the initial configuration the same for all cluster nodes?</span></dt><dd><p>
      To communicate with each other, all nodes belonging to the same
      cluster need to use the same <code class="literal">bindnetaddr</code>,
      <code class="literal">mcastaddr</code> and <code class="literal">mcastport</code> as
      described in <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
     </p><p>
      Check if the communication channels and options configured in
      <code class="filename">/etc/corosync/corosync.conf</code> are the same for all
      cluster nodes.
     </p><p>
      In case you use encrypted communication, check if the
      <code class="filename">/etc/corosync/authkey</code> file is available on all
      cluster nodes.
     </p><p>
      All <code class="filename">corosync.conf</code> settings except for
      <code class="literal">nodeid</code> must be the same;
      <code class="filename">authkey</code> files on all nodes must be identical.
     </p></dd><dt id="id-1.3.7.2.3.3.3"><span class="term">Does the Firewall allow communication via the
            <code class="literal">mcastport</code>?</span></dt><dd><p>
      If the mcastport used for communication between the cluster nodes is
      blocked by the firewall, the nodes cannot see each other. When
      configuring the initial setup with YaST or the bootstrap scripts
      as described in <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a> or
      the Installation and Setup Quick Start, respectively, the firewall
      settings are usually automatically adjusted.
     </p><p>
      To make sure the mcastport is not blocked by the firewall, check the
      settings in <code class="filename">/etc/sysconfig/SuSEfirewall2</code> on each
      node. Alternatively, start the YaST firewall module on each
      cluster node. After clicking <span class="guimenu">Allowed
      Service</span> › <span class="guimenu">Advanced</span>, add the
      mcastport to the list of allowed <span class="guimenu">UDP Ports</span> and
      confirm your changes.
     </p></dd><dt id="id-1.3.7.2.3.3.4"><span class="term">Are Pacemaker and Corosync started on each cluster node?</span></dt><dd><p>
      Usually, starting Pacemaker also starts the Corosync service. To
      check if both services are running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status pacemaker corosync</pre></div><p>
      In case they are not running, start them by executing the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start pacemaker</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-log" data-id-title="Logging"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.2 </span><span class="title-name">Logging</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-log">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.4.2.1"><span class="term">Where to find the log files? </span></dt><dd><p>
      For the Pacemaker log files, see the settings configured in the
      <code class="literal">logging</code> section of <code class="filename">/etc/corosync/corosync.conf</code>. In case the
      log file specified there should be ignored by Pacemaker, check the
      logging settings in <code class="filename">/etc/sysconfig/pacemaker</code>,
      Pacemaker's own configuration file. In case
      <code class="literal">PCMK_logfile</code> is configured there, Pacemaker will
      use the path that is defined by this parameter.
     </p><p>
      If you need a cluster-wide report showing all relevant log files, see
      <a class="xref" href="#vle-ha-crmreport">How can I create a report with an analysis of all my cluster nodes?</a> for more information.
     </p></dd><dt id="id-1.3.7.2.4.2.2"><span class="term">I enabled monitoring but there is no trace of monitoring operations in
          the log files?</span></dt><dd><p>
      The <code class="systemitem">lrmd</code> daemon does not log
      recurring monitor operations unless an error occurred. Logging all
      recurring operations would produce too much noise. Therefore recurring
      monitor operations are logged only once an hour.
     </p></dd><dt id="id-1.3.7.2.4.2.3"><span class="term">I only get a <code class="literal">failed</code> message. Is it possible to get more
          information?</span></dt><dd><p>
      Add the <code class="literal">--verbose</code> parameter to your commands. If
      you do that multiple times, the debug output becomes quite verbose.
      See the logging data (<code class="command">sudo journalctl -n</code>) for
      useful hints.
     </p></dd><dt id="id-1.3.7.2.4.2.4"><span class="term">How can I get an overview of all my nodes and resources?</span></dt><dd><p>
      Use the <code class="command">crm_mon</code> command. The following displays the
      resource operation history (option <code class="option">-o</code>) and inactive
      resources (<code class="option">-r</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_mon</code> -o -r</pre></div><p>
      The display is refreshed when the status changes (to cancel this press
      <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>). An example may look like:
     </p><div class="example" id="id-1.3.7.2.4.2.4.2.4" data-id-title="Stopped Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example A.1: </span><span class="title-name">Stopped Resources </span></span><a title="Permalink" class="permalink" href="#id-1.3.7.2.4.2.4.2.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">Last updated: Fri Aug 15 10:42:08 2014
Last change: Fri Aug 15 10:32:19 2014
 Stack: corosync
Current DC: bob (175704619) - partition with quorum
Version: 1.1.12-ad083a8
2 Nodes configured
3 Resources configured

Online: [ alice bob ]

Full list of resources:

my_ipaddress    (ocf:heartbeat:Dummy): Started bob
my_filesystem   (ocf:heartbeat:Dummy): Stopped
my_webserver    (ocf:heartbeat:Dummy): Stopped

Operations:
* Node bob:
    my_ipaddress: migration-threshold=3
      + (14) start: rc=0 (ok)
      + (15) monitor: interval=10000ms rc=0 (ok)
      * Node alice:</pre></div></div></div><p>
      The  <em class="citetitle">Pacemaker Explained</em> PDF, available at <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>, covers three
      different recovery types in the <em class="citetitle">How are OCF Return Codes
      Interpreted?</em> section.
     </p></dd><dt id="id-1.3.7.2.4.2.5"><span class="term">How to view logs?</span></dt><dd><p>For a more detailed view of what is happening in your
            cluster, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history log [<em class="replaceable">NODE</em>]</pre></div><p>Replace <em class="replaceable">NODE</em> with the node you
            want to examine, or leave it empty. See <a class="xref" href="#sec-ha-troubleshooting-history" title="A.5. History">Section A.5, “History”</a> for further
            information.</p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-resource" data-id-title="Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.3 </span><span class="title-name">Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-resource">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.5.2.1"><span class="term">How can I clean up my resources?</span></dt><dd><p>
      Use the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource list
crm resource cleanup <em class="replaceable">rscid</em> [<em class="replaceable">node</em>]</pre></div><p>
      If you leave out the node, the resource is cleaned on all nodes. More
      information can be found in
      <a class="xref" href="#sec-ha-manual-config-cleanup" title="7.5.3. Cleaning Up Resources">Section 7.5.3, “Cleaning Up Resources”</a>.
     </p></dd><dt id="id-1.3.7.2.5.2.2"><span class="term">How can I list my currently known resources?</span></dt><dd><p>
      Use the command <code class="command">crm resource list</code> to display your
      current resources.
     </p></dd><dt id="id-1.3.7.2.5.2.3"><span class="term">I configured a resource, but it always fails. Why?</span></dt><dd><p>
      To check an OCF script use <code class="command">ocf-tester</code>, for
      instance:
     </p><div class="verbatim-wrap"><pre class="screen">ocf-tester -n ip1 -o ip=<em class="replaceable">YOUR_IP_ADDRESS</em> \
  /usr/lib/ocf/resource.d/heartbeat/IPaddr</pre></div><p>
      Use <code class="option">-o</code> multiple times for more parameters. The list
      of required and optional parameters can be obtained by running
      <code class="command">crm</code> <code class="option">ra</code> <code class="option">info</code>
      <em class="replaceable">AGENT</em>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info ocf:heartbeat:IPaddr</pre></div><p>
      Before running ocf-tester, make sure the resource is not managed by
      the cluster.
     </p></dd><dt id="id-1.3.7.2.5.2.4"><span class="term">Why do resources not fail over and why are there no errors?</span></dt><dd><p>
      The terminated node might be considered unclean.
      Then it is necessary to fence it. If the STONITH resource is not
      operational or does not exist, the remaining node will waiting for the
      fencing to happen. The fencing timeouts are typically high, so it may
      take quite a while to see any obvious sign of problems (if ever).
     </p><p>
      Yet another possible explanation is that a resource is simply not
      allowed to run on this node. That may be because of a failure which
      happened in the past and which was not <span class="quote">“<span class="quote">cleaned</span>”</span>. Or it
      may be because of an earlier administrative action, that is a location
      constraint with a negative score. Such a location constraint is for
      instance inserted by the <code class="command">crm resource migrate</code>
      command.
     </p></dd><dt id="id-1.3.7.2.5.2.5"><span class="term">Why can I never tell where my resource will run?</span></dt><dd><p>
      If there are no location constraints for a resource, its placement is
      subject to an (almost) random node choice. You are well advised to
      always express a preferred node for resources. That does not mean that
      you need to specify location preferences for <span class="emphasis"><em>all</em></span>
      resources. One preference suffices for a set of related (collocated)
      resources. A node preference looks like this:
     </p><div class="verbatim-wrap"><pre class="screen">location rsc-prefers-alice rsc 100: alice</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-stonith" data-id-title="STONITH and Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.4 </span><span class="title-name">STONITH and Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-stonith">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.6.3.1"><span class="term">Why does my STONITH resource not start?</span></dt><dd><p>
      Start (or enable) operation includes checking the status of the
      device. If the device is not ready, the STONITH resource will
      fail to start.
     </p><p>
      At the same time the STONITH plugin will be asked to produce a
      host list. If this list is empty, there is no point in running a
      STONITH resource which cannot shoot anything. The name of the
      host on which STONITH is running is filtered from the list, since
      the node cannot shoot itself.
     </p><p>
      To use single-host management devices such as lights-out
      devices, make sure that the STONITH resource is
      <span class="emphasis"><em>not</em></span> allowed to run on the node which it is
      supposed to fence. Use an infinitely negative location node preference
      (constraint). The cluster will move the STONITH resource to
      another place where it can start, but not before informing you.
     </p></dd><dt id="id-1.3.7.2.6.3.2"><span class="term">Why does fencing not happen, although I have the STONITH resource?</span></dt><dd><p>
      Each STONITH resource must provide a host list. This list may be
      inserted by hand in the STONITH resource configuration or
      retrieved from the device itself, for example, from outlet names. That
      depends on the nature of the STONITH plugin.
      <code class="systemitem">stonithd</code> uses the list to find out which
      STONITH resource can fence the target node. Only if the node
      appears in the list can the STONITH resource shoot (fence) the
      node.
     </p><p>
      If <code class="systemitem">stonithd</code> does not find the node in any of
      the host lists provided by running STONITH resources, it will ask
      <code class="systemitem">stonithd</code> instances on other nodes. If the
      target node does not show up in the host lists of other
      <code class="systemitem">stonithd</code> instances, the fencing request ends
      in a timeout at the originating node.
     </p></dd><dt id="id-1.3.7.2.6.3.3"><span class="term">Why does my STONITH resource fail occasionally?</span></dt><dd><p>
      Power management devices may give up if there is too much broadcast
      traffic. Space out the monitor operations. Given that fencing is
      necessary only once in a while (and hopefully never), checking the
      device status once a few hours is more than enough.
     </p><p>
      Also, some of these devices may refuse to talk to more than one party
      at the same time. This may be a problem if you keep a terminal or
      browser session open while the cluster tries to test the status.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-history" data-id-title="History"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.5 </span><span class="title-name">History</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-history">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.7.2.1"><span class="term">How to retrieve status information or a log from a failed resource?</span></dt><dd><p>Use the <code class="command">history</code> command and its subcommand
         <code class="command">resource</code>:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history resource <em class="replaceable">NAME1</em></pre></div><p>This gives you a full transition log for the given resource only.
          However, it is possible to investigate more than one resource. Append
          the resource names after the first.
         </p><p>If you followed some naming conventions (see <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>), the
         <code class="command">resource</code> command makes it easier to investigate
          a group of resources. For example, this command investigates all
          primitives starting with <code class="literal">db</code>:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history resource db*</pre></div><p>View the log file in
           <code class="filename">/var/cache/crm/history/live/alice/ha-log.txt</code>.</p></dd><dt id="id-1.3.7.2.7.2.2"><span class="term">How can I reduce the history output?</span></dt><dd><p>There are two options for the <code class="command">history</code> command:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Use <code class="command">exclude</code></p></li><li class="listitem"><p>Use <code class="command">timeframe</code></p></li></ul></div><p>The <code class="command">exclude</code> command let you set an
            additive regular expression that excludes certain patterns
            from the log. For example, the following command excludes
            all SSH, <code class="systemitem">systemd</code>, and kernel messages: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history exclude ssh|systemd|kernel.</pre></div><p>With the <code class="command">timeframe</code> command you limit
            the output to a certain range. For example, the following
            command shows all the events on August 23rd from 12:00 to
            12:30:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history timeframe "Aug 23 12:00" "Aug 23 12:30"</pre></div></dd><dt id="id-1.3.7.2.7.2.3"><span class="term">How can I store a <span class="quote">“<span class="quote">session</span>”</span> for later inspection?</span></dt><dd><p>When you encounter a bug or an event that needs further
            examination, it is useful to store all the current settings.
            This file can be sent to support or viewed with
              <code class="command">bzless</code>. For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">timeframe</code> "Oct 13 15:00" "Oct 13 16:00"
<code class="prompt custom">crm(live)history# </code><code class="command">session</code> save tux-test
<code class="prompt custom">crm(live)history# </code><code class="command">session</code> pack
Report saved in '/root/tux-test.tar.bz2'</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-hawk2" data-id-title="Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.6 </span><span class="title-name">Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-trouble-hawk2-cert"><span class="term">Replacing the Self-Signed Certificate</span></dt><dd><p> To avoid the warning about the self-signed certificate on first
      Hawk2 start-up, replace the automatically created certificate with
      your own certificate (or a certificate that was signed by an official
      Certificate Authority, CA):</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Replace <code class="filename">/etc/hawk/hawk.key</code> with the private
        key.</p></li><li class="step"><p>Replace <code class="filename">/etc/hawk/hawk.pem</code> with the
        certificate that Hawk2 should present.</p></li><li class="step"><p>
        Restart the Hawk2 service to reload the new certificate:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> restart hawk</pre></div></li></ol></div></div><p>
      Change ownership of the files to <code class="literal">root:haclient</code>
      and make the files accessible to the group:</p><div class="verbatim-wrap"><pre class="screen">chown root:haclient /etc/hawk/hawk.key /etc/hawk/hawk.pem
chmod 640 /etc/hawk/hawk.key /etc/hawk/hawk.pem</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-misc" data-id-title="Miscellaneous"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.7 </span><span class="title-name">Miscellaneous</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-misc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.9.2.1"><span class="term">How can I run commands on all cluster nodes?</span></dt><dd><p>
      Use the command <code class="command">pssh</code> for this task. If necessary,
      install <code class="systemitem">pssh</code>. Create a file
      (for example <code class="filename">hosts.txt</code>) where you collect all
      your IP addresses or host names you want to visit. Make sure you can
      log in with <code class="command">ssh</code> to each host listed in your
      <code class="filename">hosts.txt</code> file. If everything is correctly
      prepared, execute <code class="command">pssh</code> and use the
      <code class="filename">hosts.txt</code> file (option <code class="option">-h</code>) and
      the interactive mode (option <code class="option">-i</code>) as shown in this
      example:
     </p><div class="verbatim-wrap"><pre class="screen">pssh -i -h hosts.txt "ls -l /corosync/*.conf"
[1] 08:28:32 [SUCCESS] root@venus.example.com
-rw-r--r-- 1 root root 1480 Nov 14 13:37 /etc/corosync/corosync.conf
[2] 08:28:32 [SUCCESS] root@192.168.2.102
-rw-r--r-- 1 root root 1480 Nov 14 13:37 /etc/corosync/corosync.conf</pre></div></dd><dt id="id-1.3.7.2.9.2.2"><span class="term">What is the state of my cluster?</span></dt><dd><p>
      To check the current state of your cluster, use one of the programs
      <code class="literal">crm_mon</code> or <code class="command">crm</code>
      <code class="option">status</code>. This displays the current DC and all the
      nodes and resources known by the current node.
     </p></dd><dt id="id-1.3.7.2.9.2.3"><span class="term">Why can several nodes of my cluster not see each other?</span></dt><dd><p>
      There could be several reasons:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Look first in the configuration file
        <code class="filename">/etc/corosync/corosync.conf</code>. Check if the
        multicast or unicast address is the same for every node in the
        cluster (look in the <code class="literal">interface</code> section with the
        key <code class="literal">mcastaddr</code>).
       </p></li><li class="listitem"><p>
        Check your firewall settings.
       </p></li><li class="listitem"><p>
        Check if your switch supports multicast or unicast addresses.
       </p></li><li class="listitem"><p>
        Check if the connection between your nodes is broken. Most often,
        this is the result of a badly configured firewall. This also may be
        the reason for a <span class="emphasis"><em>split brain</em></span> condition, where
        the cluster is partitioned.
       </p></li></ul></div></dd><dt id="id-1.3.7.2.9.2.4"><span class="term">Why can an OCFS2 device not be mounted?</span></dt><dd><p>
      Check the log messages (<code class="command">sudo journalctl -n</code>) for the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">Jan 12 09:58:55 alice lrmd: [3487]: info: RA output: [...]
  ERROR: Could not load ocfs2_stackglue
Jan 12 16:04:22 alice modprobe: FATAL: Module ocfs2_stackglue not found.</pre></div><p>
      In this case the Kernel module <code class="filename">ocfs2_stackglue.ko</code>
      is missing. Install the package
      <code class="filename">ocfs2-kmp-default</code>,
      <code class="filename">ocfs2-kmp-pae</code> or
      <code class="filename">ocfs2-kmp-xen</code>, depending on the installed Kernel.
     </p></dd><dt id="vle-ha-crmreport"><span class="term">How can I create a report with an analysis of all my cluster nodes?</span></dt><dd><p> On the crm shell, use <code class="command">crm report</code> to
            create a report. This tool compiles: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Cluster-wide log files,
       </p></li><li class="listitem"><p>
        Package states,
       </p></li><li class="listitem"><p>
        DLM/OCFS2 states,
       </p></li><li class="listitem"><p>
        System information,
       </p></li><li class="listitem"><p>
        CIB history,
       </p></li><li class="listitem"><p>
        Parsing of core dump reports, if a debuginfo package is installed.
       </p></li></ul></div><p>
      Usually run <code class="command">crm report</code> with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm report</code> -f 0:00 -n alice -n bob</pre></div><p>
      The command extracts all information since 0am on the hosts alice
      and bob and creates a <code class="literal">*.tar.bz2</code> archive named
      <code class="filename">crm_report-<em class="replaceable">DATE</em>.tar.bz2</code>
      in the current directory, for example,
      <code class="filename">crm_report-Wed-03-Mar-2012</code>. If you are only
      interested in a specific time frame, add the end time with the
      <code class="option">-t</code> option.
     </p><div id="id-1.3.7.2.9.2.5.2.6" data-id-title="Remove Sensitive Information" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Remove Sensitive Information</div><p>
       The <code class="command">crm report</code> tool tries to remove any sensitive
       information from the CIB and the peinput files, however, it cannot do
       everything. If you have more sensitive information, supply additional
       patterns. The log files and the <code class="command">crm_mon</code>,
       <code class="command">ccm_tool</code>, and <code class="command">crm_verify</code> output
       are <span class="emphasis"><em>not</em></span> sanitized.
      </p><p>
       Before sharing your data in any way, check the archive and remove all
       information you do not want to expose.
      </p></div><p>
      Customize the command execution with further options. For example, if
      you have a Pacemaker cluster, you certainly want to add the option
      <code class="option">-A</code>. In case you have another user who has permissions
      to the cluster, use the <code class="option">-u</code> option and specify this
      user (in addition to <code class="systemitem">root</code> and
      <code class="systemitem">hacluster</code>). In case you have
      a non-standard SSH port, use the <code class="option">-X</code> option to add the
      port (for example, with the port 3479, use <code class="literal">-X "-p
      3479"</code>). Further options can be found in the man page of
      <code class="command">crm report</code>.
     </p><p>
      After <code class="command">crm report</code> has analyzed all the relevant log
      files and created the directory (or archive), check the log files for
      an uppercase <code class="literal">ERROR</code> string. The most important files
      in the top level directory of the report are:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.2.9.2.5.2.9.1"><span class="term"><code class="filename">analysis.txt</code>
       </span></dt><dd><p>
         Compares files that should be identical on all nodes.
        </p></dd><dt id="id-1.3.7.2.9.2.5.2.9.2"><span class="term"><code class="filename">corosync.txt</code>
         </span></dt><dd><p>
             Contains a copy of the Corosync configuration file.
           </p></dd><dt id="id-1.3.7.2.9.2.5.2.9.3"><span class="term"><code class="filename">crm_mon.txt</code>
       </span></dt><dd><p>
         Contains the output of the <code class="command">crm_mon</code> command.
        </p></dd><dt id="id-1.3.7.2.9.2.5.2.9.4"><span class="term"><code class="filename">description.txt</code>
       </span></dt><dd><p>
         Contains all cluster package versions on your nodes. There is also
         the <code class="filename">sysinfo.txt</code> file which is node specific.
         It is linked to the top directory.
        </p><p>This file can be used as a template to describe the issue
        you encountered and post it to <a class="link" href="https://github.com/ClusterLabs/crmsh/issues" target="_blank">https://github.com/ClusterLabs/crmsh/issues</a>.</p></dd><dt id="id-1.3.7.2.9.2.5.2.9.5"><span class="term"><code class="filename">members.txt</code></span></dt><dd><p>A list of all nodes</p></dd><dt id="id-1.3.7.2.9.2.5.2.9.6"><span class="term"><code class="filename">sysinfo.txt</code></span></dt><dd><p>Contains a list of all relevant package names and their
            versions. Additionally, there is also a list of configuration
            files which are different from the original RPM package.</p></dd></dl></div><p>
      Node-specific files are stored in a subdirectory named by the node's
      name. It contains a copy of the directory <code class="filename">/etc</code>
      of the respective node.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For additional information about high availability on Linux, including
   configuring cluster resources and managing and customizing a High Availability
   cluster, see
   <a class="link" href="http://clusterlabs.org/wiki/Documentation" target="_blank">http://clusterlabs.org/wiki/Documentation</a>.
  </p></section></section><section class="appendix" id="app-naming" data-id-title="Naming Conventions"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></span> <a title="Permalink" class="permalink" href="#app-naming">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_naming.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This guide uses the following naming conventions for cluster nodes and
  names, cluster resources, and constraints.
 </p><div class="variablelist" id="vl-naming-cluster-names-nodes"><dl class="variablelist"><dt id="id-1.3.7.3.4.1"><span class="term">Cluster Nodes</span></dt><dd><p>
     Cluster nodes use first names:
    </p><p>
     alice, bob, charlie, doro, and eris
    </p></dd><dt id="id-1.3.7.3.4.2"><span class="term">Cluster Site Names</span></dt><dd><p>
     Clusters sites are named after cities:
    </p><p>
     amsterdam, berlin, canberra, dublin,
     fukuoka, gizeh, hanoi, and istanbul
    </p></dd><dt id="id-1.3.7.3.4.3"><span class="term">Cluster Resources</span></dt><dd><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Primitives
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          No prefix
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Groups
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">g-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Clones
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">cl-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Multi-state resources
         </p>
        </td><td>
         <p>
          Prefix <code class="literal">ms-</code>
         </p>
        </td></tr></tbody></table></div></dd><dt id="id-1.3.7.3.4.4"><span class="term">Constraints</span></dt><dd><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Ordering constraints
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">o-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Location constraints
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">loc-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Colocation constraints
         </p>
        </td><td>
         <p>
          Prefix <code class="literal">col-</code>
         </p>
        </td></tr></tbody></table></div></dd></dl></div></section><section class="appendix" id="app-ha-management" data-id-title="Cluster Management Tools (Command Line)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></span> <a title="Permalink" class="permalink" href="#app-ha-management">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_management.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  SUSE Linux Enterprise High Availability ships with a comprehensive set of tools to assists you in
  managing your cluster from the command line. This chapter introduces the
  tools needed for managing the cluster configuration in the CIB and the
  cluster resources. Other command line tools for managing resource agents
  or tools used for debugging (and troubleshooting) your setup are covered
  in <a class="xref" href="#app-ha-troubleshooting" title="Appendix A. Troubleshooting">Appendix A, <em>Troubleshooting</em></a>.
 </p><div id="id-1.3.7.4.4" data-id-title="Use crmsh" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use crmsh</div><p>
   This tool is for experts only. Usually the crm shell (crmsh) is
   the recommended way of managing your cluster.
  </p></div><p>
  The following list presents several tasks related to cluster management
  and briefly introduces the tools to use to accomplish these tasks:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.4.6.1"><span class="term">Monitoring the Cluster's Status</span></dt><dd><p>
     The <code class="command">crm_mon</code> command allows you to monitor your
     cluster's status and configuration. Its output includes the number of
     nodes, uname, uuid, status, the resources configured in your cluster,
     and the current status of each. The output of
     <code class="command">crm_mon</code> can be displayed at the console or printed
     into an HTML file. When provided with a cluster configuration file
     without the status section, <code class="command">crm_mon</code> creates an
     overview of nodes and resources as specified in the file. See the
     <code class="command">crm_mon</code> man page for a detailed introduction to this
     tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.2"><span class="term">Managing the CIB</span></dt><dd><p>
     The <code class="command">cibadmin</code> command is the low-level administrative
     command for manipulating the CIB. It can be used to dump all or part of
     the CIB, update all or part of it, modify all or part of it, delete the
     entire CIB, or perform miscellaneous CIB administrative operations. See
     the <code class="command">cibadmin</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.3"><span class="term">Managing Configuration Changes</span></dt><dd><p>
     The <code class="command">crm_diff</code> command assists you in creating and
     applying XML patches. This can be useful for visualizing the changes
     between two versions of the cluster configuration or saving changes so
     they can be applied at a later time using <code class="command">cibadmin</code>.
     See the <code class="command">crm_diff</code> man page for a detailed
     introduction to this tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.4"><span class="term">Manipulating CIB Attributes</span></dt><dd><p>
     The <code class="command">crm_attribute</code> command lets you query and
     manipulate node attributes and cluster configuration options that are
     used in the CIB. See the <code class="command">crm_attribute</code> man page for
     a detailed introduction to this tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.5"><span class="term">Validating the Cluster Configuration</span></dt><dd><p>
     The <code class="command">crm_verify</code> command checks the configuration
     database (CIB) for consistency and other problems. It can check a file
     containing the configuration or connect to a running cluster. It
     reports two classes of problems. Errors must be fixed before the
     High Availability software can work properly while warning resolution is up to the
     administrator. <code class="command">crm_verify</code> assists in creating new or
     modified configurations. You can take a local copy of a CIB in the
     running cluster, edit it, validate it using
     <code class="command">crm_verify</code>, then put the new configuration into
     effect using <code class="command">cibadmin</code>. See the
     <code class="command">crm_verify</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.6"><span class="term">Managing Resource Configurations</span></dt><dd><p>
     The <code class="command">crm_resource</code> command performs various
     resource-related actions on the cluster. It lets you modify the
     definition of configured resources, start and stop resources, or delete
     and migrate resources between nodes. See the
     <code class="command">crm_resource</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.3.7.4.6.7"><span class="term">Managing Resource Fail Counts</span></dt><dd><p>
     The <code class="command">crm_failcount</code> command queries the number of
     failures per resource on a given node. This tool can also be used to
     reset the failcount, allowing the resource to again run on nodes where
     it had failed too often. See the <code class="command">crm_failcount</code> man
     page for a detailed introduction to this tool's usage and command
     syntax.
    </p></dd><dt id="id-1.3.7.4.6.8"><span class="term">Managing a Node's Standby Status</span></dt><dd><p>
     The <code class="command">crm_standby</code> command can manipulate a node's
     standby attribute. Any node in standby mode is no longer eligible to
     host resources and any resources that are there must be moved. Standby
     mode can be useful for performing maintenance tasks, such as Kernel
     updates. Remove the standby attribute from the node for it to become a
     fully active member of the cluster again. See the
     <code class="command">crm_standby</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd></dl></div></section><section class="appendix" id="app-crmreport-nonroot" data-id-title="Running Cluster Reports Without root Access"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></span> <a title="Permalink" class="permalink" href="#app-crmreport-nonroot">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  All cluster nodes must be able to access each other via SSH. Tools like
  <code class="command">crm report</code> (for
  troubleshooting) and Hawk2's <span class="guimenu">History Explorer</span> require
  passwordless SSH access between the nodes, otherwise they can only collect
  data from the current node.
 </p><p>
  If passwordless SSH <code class="systemitem">root</code> access does not comply with regulatory
  requirements, you can use a work-around for running cluster reports. It
  consists of the following basic steps:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Creating a dedicated local user account (for running
    <code class="command">crm report</code>).
   </p></li><li class="listitem"><p>
    Configuring passwordless SSH access for that user account, ideally by
    using a non-standard SSH port.
   </p></li><li class="listitem"><p>
    Configuring <code class="command">sudo</code> for that user.
   </p></li><li class="listitem"><p>
    Running <code class="command">crm report</code> as
    that user.
   </p></li></ol></div><p>
  By default when <code class="command">crm report</code> is run, it attempts to
  log in to
  remote nodes first as <code class="systemitem">root</code>, then as user
  <code class="systemitem">hacluster</code>. However, if your
  local security policy prevents <code class="systemitem">root</code> login using SSH, the script
  execution will fail on all remote nodes. Even attempting to run the script
  as user <code class="systemitem">hacluster</code> will fail
  because this is a service account, and its shell is set to
  <code class="filename">/bin/false</code>, which prevents login. Creating a
  dedicated local user is the only option to successfully run the
  <code class="command">crm report</code> script on
  all nodes in the High Availability cluster.
 </p><section class="sect1" id="sec-crmreport-nonroot-user" data-id-title="Creating a Local User Account"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.1 </span><span class="title-name">Creating a Local User Account</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-user">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the following example, we will create a local user named
   <code class="systemitem">hareport</code> from command line. The
   password can be anything that meets your security requirements.
   Alternatively, you can create the user account and set the password with
   YaST.
  </p><div class="procedure" id="id-1.3.7.5.7.3" data-id-title="Creating a Dedicated User Account for Running Cluster Reports"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.1: </span><span class="title-name">Creating a Dedicated User Account for Running Cluster Reports </span></span><a title="Permalink" class="permalink" href="#id-1.3.7.5.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and create a user
     <code class="systemitem">hareport</code> with a home
     directory <code class="filename">/home/hareport </code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">useradd</code> -m -d /home/hareport -c "HA Report" hareport</pre></div></li><li class="step"><p>
     Set a password for the user:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>passwd hareport</pre></div></li><li class="step"><p>
     When prompted, enter and re-enter a password for the user.
    </p></li></ol></div></div><div id="id-1.3.7.5.7.4" data-id-title="Same User Is Required On Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same User Is Required On Each Cluster Node</div><p>
    To create the same user account on all nodes, repeat the steps above on
    each cluster node.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-ssh" data-id-title="Configuring a Passwordless SSH Account"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.2 </span><span class="title-name">Configuring a Passwordless SSH Account</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-ssh">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.3.7.5.8.2" data-id-title="Configuring the SSH Daemon for a Non-Standard Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.2: </span><span class="title-name">Configuring the SSH Daemon for a Non-Standard Port </span></span><a title="Permalink" class="permalink" href="#id-1.3.7.5.8.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    By default, the SSH daemon and the SSH client talk and listen on port
    <code class="literal">22</code>. If your network security guidelines require the
    default SSH port to be changed to an alternate high numbered port, you
    need to modify the daemon's configuration file
    <code class="filename">/etc/ssh/sshd_config</code>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     To modify the default port, search the file for the
     <code class="literal">Port</code> line, uncomment it and edit it according to
     your wishes. For example, set it to:
    </p><div class="verbatim-wrap"><pre class="screen">Port 5022</pre></div></li><li class="step"><p>
     If your organization does not permit the <code class="systemitem">root</code> user to access
     other servers, search the file for the
     <code class="literal">PermitRootLogin</code> entry, uncomment it and set it to
     <code class="literal">no</code>:
    </p><div class="verbatim-wrap"><pre class="screen">PermitRootLogin no</pre></div></li><li class="step"><p>
     Alternatively, add the respective lines to the end of the file by
     executing the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>echo “PermitRootLogin no” &gt;&gt; /etc/ssh/sshd_config
<code class="prompt root"># </code>echo “Port 5022” &gt;&gt; /etc/ssh/sshd_config</pre></div></li><li class="step"><p>
     After modifying <code class="filename">/etc/ssh/sshd_config</code>, restart the
     SSH daemon to make the new settings take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart sshd</pre></div></li></ol></div></div><div id="id-1.3.7.5.8.3" data-id-title="Same Settings Are Required On Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same Settings Are Required On Each Cluster Node</div><p>
    Repeat the SSH daemon configuration above on each cluster node.
   </p></div><div class="procedure" id="id-1.3.7.5.8.4" data-id-title="Configuring the SSH Client for a Non-Standard Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.3: </span><span class="title-name">Configuring the SSH Client for a Non-Standard Port </span></span><a title="Permalink" class="permalink" href="#id-1.3.7.5.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    If the SSH port change is going to be made on all nodes in the cluster,
    it is useful to modify the SSH configuration file,
    <code class="filename">/etc/ssh/sshd_config</code>.
    </p><ol class="procedure" type="1"><li class="step"><p>
     To modify the default port, search the file for the
     <code class="literal">Port</code> line, uncomment it and edit it according to
     your wishes. For example, set it to:
    </p><div class="verbatim-wrap"><pre class="screen">Port 5022</pre></div></li><li class="step"><p>
     Alternatively, add the respective line to the end of the file by
     executing the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>echo “Port 5022” &gt;&gt; /etc/ssh/ssh_config</pre></div></li></ol></div></div><div id="id-1.3.7.5.8.5" data-id-title="Settings Only Required on One Node" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings Only Required on One Node</div><p>
    The SSH client configuration above is only needed on the node on which
    you want to run the cluster report.</p><p>Alternatively, you can use the <code class="option">-X</code> option to run the
    <code class="command">crm report</code> with a custom SSH port or even make
    <code class="command">crm report</code> use your custom SSH port by default. For
    details, see <a class="xref" href="#pro-crmreport-custom-ssh" title="Generating a Cluster Report Using a Custom SSH Port">Procedure D.5, “Generating a Cluster Report Using a Custom SSH Port”</a>.</p></div><div class="procedure" id="id-1.3.7.5.8.6" data-id-title="Configuring Shared SSH Keys"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.4: </span><span class="title-name">Configuring Shared SSH Keys </span></span><a title="Permalink" class="permalink" href="#id-1.3.7.5.8.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    You can access other servers using SSH and not be asked for a
    password. While this may appear insecure at first sight, it is actually
    a very secure access method since the users can only access servers that
    their public key has been shared with. The shared key must be created as
    the user that will use the key.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to one of the nodes with the user account that you have created
     for running cluster reports (in our example above, the user account was
     <code class="systemitem">hareport</code>).
    </p></li><li class="step"><p>
     Generate a new key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh-keygen –t rsa</pre></div><p>
     This command will generate a 2048 bit key by default. The default
     location for the key is <code class="filename">~/.ssh/</code>. You are asked to
     set a passphrase on the key. However, do not enter a passphrase because
     for passwordless login there must not be a passphrase on the key.
     </p></li><li class="step"><p>
     After the keys have been generated, copy the public key to
     <span class="emphasis"><em>each</em></span> of the other nodes
     (<span class="emphasis"><em>including</em></span> the node where you created the key):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh-copy-id -i ~/.ssh/id_rsa.pub <em class="replaceable">HOSTNAME_OR_IP</em></pre></div><p>
     In the command, you can either use the DNS name for each server, an
     alias, or the IP address. During the copy process you will be asked to
     accept the host key for each node, and you will need to provide the
     password for the <code class="systemitem">hareport</code>
     user account (this will be the only time you need to enter it).
    </p></li><li class="step"><p>
     After the key is shared to all cluster nodes, test if you can log in as
     user <code class="systemitem">hareport</code> to the other
     nodes by using passwordless SSH:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh <em class="replaceable">HOSTNAME_OR_IP</em></pre></div><p>
     You should be automatically connected to the remote server without
     being asked to accept a certificate or enter a password.
    </p></li></ol></div></div><div id="id-1.3.7.5.8.7" data-id-title="Settings Only Required on One Node" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings Only Required on One Node</div><p>
    If you intend to run the cluster report from the same node each time, it
    is sufficient to execute the procedure above on this node only.
    Otherwise repeat the procedure on each node.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-sudo" data-id-title="Configuring sudo"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.3 </span><span class="title-name">Configuring <code class="command">sudo</code></span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-sudo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">sudo</code> command allows a regular user to quickly
   become <code class="systemitem">root</code> and issue a command, with or without providing a
   password. Sudo access can be given to all root-level commands or to
   specific commands only. Sudo typically uses aliases to define the entire
   command string.
  </p><p>
   To configure sudo either use <code class="command">visudo</code>
   (<span class="emphasis"><em>not</em></span> vi) or YaST.
  </p><div id="id-1.3.7.5.9.4" data-id-title="Do Not Use vi" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Do Not Use vi</div><p>
    For sudo configuration from command line, you must edit the sudoers file
    as <code class="systemitem">root</code> with <code class="command">visudo</code>. Using any other editor may
    result in syntax or file permission errors that prevent sudo from
    running.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     To open the <code class="filename">/etc/sudoers</code> file, enter
     <code class="command">visudo</code>.
    </p></li><li class="step"><p> Look for the following categories: <code class="literal">Host alias
      specification</code>,<code class="literal">User alias specification</code>,
      <code class="literal">Cmnd alias specification</code>, and <code class="literal">Runas alias
      specification</code>. </p></li><li class="step"><p>
     Add the following entries to the respective categories in
     <code class="filename">/etc/sudoers</code>:
    </p><div class="verbatim-wrap"><pre class="screen">Host_Alias	CLUSTER = alice,bob,charlie <span class="callout" id="ha-sudoers-host-alias">1</span>
User_Alias HA = hareport <span class="callout" id="ha-sudoers-user-alias">2</span>
Cmnd_Alias HA_ALLOWED = /bin/su, /usr/sbin/crm report *<span class="callout" id="ha-sudoers-cmd-alias">3</span>
Runas_Alias R = root <span class="callout" id="ha-sudoers-runas-alias">4</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-host-alias"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The host alias defines on which server (or range of servers) the sudo
       user has rights to issue commands. In the host alias you can use DNS
       names, or IP addresses, or specify an entire network range (for
       example, <code class="literal">172.17.12.0/24</code>). To limit the scope of
       access you should specify the host names for the cluster nodes only.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-user-alias"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The user alias allows you to add multiple local user accounts to a
       single alias. However, in this case you could avoid creating an alias since
       only one account is being used. In the example above, we added the
       <code class="systemitem">hareport</code> user which we have
       created for running cluster reports.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-cmd-alias"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The command alias defines which commands can be executed by the user.
       This is useful if you want to limit what the non-root user can access
       when using <code class="command">sudo</code>. In this case the
       <code class="systemitem">hareport</code>
       user account will need access to the commands <code class="command">crm report</code>
       and <code class="command">su</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-runas-alias"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The runas alias specifies the account that the command will be run
       as. In this case <code class="systemitem">root</code>.
      </p></td></tr></table></div></li><li class="step"><p>Search for the following two lines:</p><div class="verbatim-wrap"><pre class="screen">Defaults targetpw
ALL     ALL=(ALL) ALL</pre></div><p>As they would conflict with the setup we want to create, disable them:</p><div class="verbatim-wrap"><pre class="screen">#Defaults targetpw
#ALL     ALL=(ALL) ALL</pre></div></li><li class="step"><p>Look for the <code class="literal">User privilege specification</code> category.
     After having defined the aliases above, you can now add the following
     rule there:</p><div class="verbatim-wrap"><pre class="screen">HA	CLUSTER = (R) NOPASSWD:HA_ALLOWED</pre></div><p>The <code class="literal">NOPASSWORD</code> option ensures that the user
      <code class="systemitem">hareport</code> can execute the cluster
     report without providing a password.</p></li><li class="step"><p><span class="step-optional">(Optional)</span> 
     To allow the user <code class="systemitem">hareport</code> to run cluster reports using your local SSH keys, add the following line to the <code class="literal">Defaults specification</code> category. This preserves the <code class="literal">SSH_AUTH_SOCK</code> environment
     variable, which is required for SSH agent forwarding.
    </p><div class="verbatim-wrap"><pre class="screen">Defaults!HA_ALLOWED env_keep+=SSH_AUTH_SOCK</pre></div><p>
     When you log into a node as the user <code class="systemitem">hareport</code> via <code class="command">ssh -A</code>, and use <code class="command">sudo</code>
     to run <code class="command">crm report</code>, your local SSH keys are passed to the node for
     authentication.
    </p></li></ol></div></div><div id="id-1.3.7.5.9.6" data-id-title="Same sudo Configuration Is Required on Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same sudo Configuration Is Required on Each Cluster Node</div><p>
    This sudo configuration must be made on all nodes in the cluster. No
    other changes are needed for sudo and no services need to be restarted.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-execute" data-id-title="Generating a Cluster Report"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.4 </span><span class="title-name">Generating a Cluster Report</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-execute">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>To run cluster reports with the settings you have configured above, you need to be logged
   in to one of the nodes as user <code class="systemitem">hareport</code>.
   To start a cluster report, use the <code class="command">crm report</code> command.
   For example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code><code class="command">sudo crm report -u hareport -f 0:00 -n "alice bob charlie"</code></pre></div><p>This command will extract all information since <code class="literal">0 am</code> on the named nodes
   and create a <code class="literal">*.tar.bz2</code> archive named
     <code class="filename">pcmk-<em class="replaceable">DATE</em>.tar.bz2</code> in
     the current directory.</p><div class="procedure" id="pro-crmreport-custom-ssh" data-id-title="Generating a Cluster Report Using a Custom SSH Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.5: </span><span class="title-name">Generating a Cluster Report Using a Custom SSH Port </span></span><a title="Permalink" class="permalink" href="#pro-crmreport-custom-ssh">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>When using a custom SSH port, use the <code class="option">-X</code> with
     <code class="command">crm report</code> to modify the client's SSH port. For example,
     if your custom SSH port is <code class="literal">5022</code>, use the following
     command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm report -X "-p 5022" [...]</pre></div></li><li class="step"><p>To set your custom SSH port permanently for
     <code class="command">crm report</code>, start the interactive crm shell:</p><div class="verbatim-wrap"><pre class="screen">crm options</pre></div></li><li class="step"><p>
     Enter the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)options# </code> set core.report_tool_options "-X -oPort=5022"</pre></div></li></ol></div></div></section></section><section class="appendix" id="app-ha-docupdates" data-id-title="Documentation Updates"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">E </span><span class="title-name">Documentation Updates</span></span> <a title="Permalink" class="permalink" href="#app-ha-docupdates">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This chapter lists content changes for this document.
 </p><p>
  This manual was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp4-maint-2" title="E.1. 2020 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)">Section E.1, “2020 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp4-maint-1" title="E.2. 2019 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)">Section E.2, “2019 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp4" title="E.3. December 2018 (Initial Release of SUSE Linux Enterprise High Availability 12 SP4)">Section E.3, “December 2018 (Initial Release of SUSE Linux Enterprise High Availability 12 SP4)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp3-maint-2" title="E.4. October 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)">Section E.4, “October 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp3-maint-1" title="E.5. July 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)">Section E.5, “July 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp3" title="E.6. September 2017 (Initial Release of SUSE Linux Enterprise High Availability 12 SP3)">Section E.6, “September 2017 (Initial Release of SUSE Linux Enterprise High Availability 12 SP3)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp2-maint-3" title="E.7. April 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)">Section E.7, “April 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp2-maint-2" title="E.8. March 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)">Section E.8, “March 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp2-maint-1" title="E.9. November 2016 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)">Section E.9, “November 2016 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp2" title="E.10. November 2016 (Initial Release of SUSE Linux Enterprise High Availability 12 SP2)">Section E.10, “November 2016 (Initial Release of SUSE Linux Enterprise High Availability 12 SP2)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp1-maint-1" title="E.11. December 2015 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP1)">Section E.11, “December 2015 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP1)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-sp1" title="E.12. December 2015 (Initial Release of SUSE Linux Enterprise High Availability 12 SP1)">Section E.12, “December 2015 (Initial Release of SUSE Linux Enterprise High Availability 12 SP1)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-ha-docupdates-sle12-ga" title="E.13. October 2014 (Initial Release of SUSE Linux Enterprise High Availability 12)">Section E.13, “October 2014 (Initial Release of SUSE Linux Enterprise High Availability 12)”</a>
   </p></li></ul></div><section class="sect1" id="sec-ha-docupdates-sle12-sp4-maint-2" data-id-title="2020 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.1 </span><span class="title-name">2020 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp4-maint-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.6.2.1"><span class="term">Bugfixes</span></dt><dd><p>
      Add <a class="xref" href="#pro-ha-migration-rolling-upgrade-newsp-freshinstall" title="Performing a Cluster-wide Fresh Installation of a New Service Pack">Procedure 24.3, “Performing a Cluster-wide Fresh Installation of a New Service Pack”</a>
      for customers who prefer a fresh installation even for moving to the
      next service pack (Fate #325878).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp4-maint-1" data-id-title="2019 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.2 </span><span class="title-name">2019 (Documentation Maintenance Update for SUSE Linux Enterprise High Availability 12 SP4)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp4-maint-1">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.7.2.1"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="10.5. Calculation of Timeouts">Section 10.5, “Calculation of Timeouts”</a>, corrected the
       formula for timeout calculation in <a class="xref" href="#ex-ha-storage-protect-sbd-timings" title="Formula for Timeout Calculation">Example 10.1</a>
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1131440" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1131440</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>, mentioned that the RRP mode
       <code class="literal">active</code> is deprecated (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1121869" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1121869</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-clvm-scenario-drbd" title="19.2.4. Scenario: cLVM With DRBD">Section 19.2.4, “Scenario: cLVM With DRBD”</a>, removed the option
       <code class="literal">become-primary-on</code> from the example DRBD
       configuration (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1131776" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1131776</a>).
      </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp4" data-id-title="December 2018 (Initial Release of SUSE Linux Enterprise High Availability 12 SP4)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.3 </span><span class="title-name">December 2018 (Initial Release of SUSE Linux Enterprise High Availability 12 SP4)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.8.2.1"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Turned content of <a class="xref" href="#ex-ha-fencing-kdump" title="Configuration of a Kdump Device">Example 9.3, “Configuration of a Kdump Device”</a> into a procedure
       and added a step to rebuild <code class="filename">initrd</code>
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1118238" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1118238</a>).
      </p></li><li class="listitem"><p>In <a class="xref" href="#sec-ha-manual-config-crm" title="7.1. crmsh—Overview">Section 7.1, “crmsh—Overview”</a>, mentioned that we
        omit the host name in the interactive crm prompts in the documentation,
        unless you need to run the command on a specific node in the cluster
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1103828" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1103828</a>).
       </p></li><li class="listitem"><p>Fixed errors and line breaks in commands (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=1103846" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=1103846</a>,
        <a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=1103843" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=1103843</a>).
       </p></li><li class="listitem"><p>
        In <a class="xref" href="#sec-ha-maint-shutdown-node-maint-mode" title="23.9. Rebooting a Cluster Node While In Maintenance Mode">Section 23.9, “Rebooting a Cluster Node While In Maintenance Mode”</a>,
        specified how to recognize DLM resources (doc comment #38758).
       </p></li><li class="listitem"><p>
        In <a class="xref" href="#sec-conf-hawk2-overview" title="6.3. Hawk2 Overview: Main Elements">Section 6.3, “Hawk2 Overview: Main Elements”</a>, mentioned that Hawk2
        now displays <code class="literal">guest nodes</code>, too (Fate #324581).
       </p></li><li class="listitem"><p>Fixed several typos in links, names, packages, and terms
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1098429" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1098429</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108586" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108586</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108604" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108604</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108624" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108624</a>).
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp3-maint-2" data-id-title="October 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.4 </span><span class="title-name">October 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp3-maint-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.9.2.1"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        In <a class="xref" href="#cha-ha-drbd" title="Chapter 18. DRBD">Chapter 18, <em>DRBD</em></a>, added <a class="xref" href="#sec-ha-drbd-fencing" title="18.6. Using Resource-Level Fencing">Section 18.6, “Using Resource-Level Fencing”</a>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1095374" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1095374</a>).
       </p></li><li class="listitem"><p>
         Fixed several typos (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107588" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107588</a>, <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108279" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108279</a>, <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108287" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108287</a>,
         <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108626" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108626</a>, <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108313" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108313</a>).
        </p></li><li class="listitem"><p>
         Corrected or removed outdated links (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107878" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107878</a>, <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108603" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108603</a>, <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108644" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108644</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1108641" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1108641</a>).
        </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp3-maint-1" data-id-title="July 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.5 </span><span class="title-name">July 2018 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP3)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp3-maint-1">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.10.2.1"><span class="term">General Changes to this Guide</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Completely revised <a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a> and moved it
         to a different place in the book.
        </p></li><li class="listitem"><p>
         Added <a class="xref" href="#cha-ha-maintenance" title="Chapter 23. Executing Maintenance Tasks">Chapter 23, <em>Executing Maintenance Tasks</em></a>. Moved respective
         sections from <a class="xref" href="#cha-ha-config-basics" title="Chapter 5. Configuration and Administration Basics">Chapter 5, <em>Configuration and Administration Basics</em></a>,
         <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>, and <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>
         there. The new chapter gives an overview of different options the cluster stack
         provides for executing maintenance tasks.
        </p></li></ul></div></dd><dt id="id-1.3.7.6.10.2.2"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        In <a class="xref" href="#sec-ha-clvm-config" title="19.2. Configuration of cLVM">Section 19.2, “Configuration of cLVM”</a>, added that <code class="literal">use_lvmetad</code>
        must be set to zero when using cLVM
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1057178" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1057178</a>).
       </p></li><li class="listitem"><p>
         In <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a>, clarified the
         difference between setting a resource to <code class="literal">is-managed=false</code>
         and setting it to maintenance mode (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1070040" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1070040</a>).
       </p></li><li class="listitem"><p>
        In <a class="xref" href="#sec-ha-ocfs2-features" title="16.1. Features and Benefits">Section 16.1, “Features and Benefits”</a>, added a note to clarify
        support status of OCFS2
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1074694" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1074694</a>).
       </p></li><li class="listitem"><p>
        In <a class="xref" href="#sec-ha-storage-protect-sgpersist" title="10.10.1. Configuring an sg_persist Resource">Section 10.10.1, “Configuring an sg_persist Resource”</a>, clarified
        SCSI Disk Compatibility of <code class="systemitem">sg_persist</code>, removed
        the OCFS2 primitive and updated the procedure (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1063875" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1063875</a>).
        </p></li><li class="listitem"><p>
        In <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Procedure 24.1, “Upgrading from Product Version 11 to 12: Cluster Offline Upgrade”</a>, corrected a command and
        added a missing step
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1046594" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1046594</a>).
       </p></li><li class="listitem"><p>In <a class="xref" href="#sec-ha-clvm-config-cmirrord" title="19.2.2. Scenario: Configuring Cmirrord">Section 19.2.2, “Scenario: Configuring Cmirrord”</a>, corrected
         timings and intervals for <code class="literal">clvmd</code> primitive
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=985590" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=985590</a>).
        </p></li><li class="listitem"><p>
         In <a class="xref" href="#pro-ha-storage-protect-sw-watchdog" title="Loading the Softdog Kernel Module">Procedure 10.2, “Loading the Softdog Kernel Module”</a>, corrected
         command about loading the softdog module to be consistent with
         the Installation and Setup Quick Start
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1090169" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1090169</a>).
        </p></li><li class="listitem"><p>
         In <a class="xref" href="#pro-ha-installation-setup-channel1-udpu" title="Defining the First Communication Channel (Unicast)">Procedure 4.2, “Defining the First Communication Channel (Unicast)”</a>, removed
         one step and replaced a screenshot that shows the unicast configuration
         with YaST cluster
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1094218" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1094218</a>).
        </p></li><li class="listitem"><p>
         Moved the maintenance topic to <a class="xref" href="#cha-ha-maintenance" title="Chapter 23. Executing Maintenance Tasks">Chapter 23, <em>Executing Maintenance Tasks</em></a>. This new chapter also mentions the
         implications of taking down a cluster node and gives more detailed
         information about the variety of options that Pacemaker offers for
         doing maintenance work (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1080869" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1080869</a>).
        </p></li><li class="listitem"><p>
         In <a class="xref" href="#cha-ha-samba" title="Chapter 21. Samba Clustering">Chapter 21, <em>Samba Clustering</em></a>, renamed resource groups, clones,
         colocations and order constraints to match the <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions"><em>Naming Conventions</em></a> for the
         documentation (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1016758" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1016758</a>).
        </p></li><li class="listitem"><p>
         Added <a class="xref" href="#cha-ha-storage-dlm" title="Chapter 15. Distributed Lock Manager (DLM)">Chapter 15, <em>Distributed Lock Manager (DLM)</em></a>. Moved some content there that
         was formerly distributed across the <em class="citetitle">Administration Guide</em>.
         Added information about the protocols that DLM can use (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1089339" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1089339</a>).
        </p></li><li class="listitem"><p>
         In <a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>, improved the description of
         resource level fencing (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067076" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067076</a>).
        </p></li><li class="listitem"><p>In <a class="xref" href="#cha-ha-config-basics" title="Chapter 5. Configuration and Administration Basics">Chapter 5, <em>Configuration and Administration Basics</em></a>, added use case
         scenarios and quorum determination (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1088466" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1088466</a>).
         </p></li><li class="listitem"><p>In <a class="xref" href="#sec-ha-storage-protect-diskless-sbd" title="10.8. Setting Up Diskless SBD">Section 10.8, “Setting Up Diskless SBD”</a>, added a
          sentence to clarify that diskless SBD does not require a STONITH SBD
          resource (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1102199" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1102199</a>).
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp3" data-id-title="September 2017 (Initial Release of SUSE Linux Enterprise High Availability 12 SP3)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.6 </span><span class="title-name">September 2017 (Initial Release of SUSE Linux Enterprise High Availability 12 SP3)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.11.2.1"><span class="term">General Changes to this Guide</span></dt><dd><p>
      Configuration changes for <code class="literal">corosync</code> (that cannot
      gracefully be reloaded with <code class="command">corosync-cfgtool -R</code>), and
      any configuration changes to <code class="literal">sbd</code> require restart of
      the <code class="systemitem">corosync</code> service for the changes to take
      effect. Restarting the <code class="systemitem">pacemaker</code>
      service is not enough in those cases.
     </p><p>
      However, for any changes to <code class="filename">/etc/sysconfig/pacemaker</code>,
      restarting
      the <code class="systemitem">pacemaker</code> service is
      sufficient
      (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050371" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050371</a>,
      <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052088" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052088</a>).
     </p></dd><dt id="id-1.3.7.6.11.2.2"><span class="term"><a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a></span></dt><dd><p>
      Updated the chapter (including all screenshots) to reflect the software
      changes in Hawk2. The main changes have taken place in the
      <span class="guimenu">Dashboard</span> view, described in
      <a class="xref" href="#sec-conf-hawk2-monitor" title="6.8. Monitoring Clusters">Section 6.8, “Monitoring Clusters”</a>.
     </p></dd><dt id="id-1.3.7.6.11.2.3"><span class="term"><a class="xref" href="#cha-ha-acl" title="Chapter 11. Access Control Lists">Chapter 11, <em>Access Control Lists</em></a></span></dt><dd><p>
      Described how to enable the use of ACLs with Hawk2 in <a class="xref" href="#sec-ha-acl-enable" title="11.2. Enabling Use of ACLs in Your Cluster">Section 11.2, “Enabling Use of ACLs in Your Cluster”</a>. Added <a class="xref" href="#sec-ha-acl-config-hawk2" title="11.4. Configuring ACLs with Hawk2">Section 11.4, “Configuring ACLs with Hawk2”</a>.
     </p></dd><dt id="id-1.3.7.6.11.2.4"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       In <a class="xref" href="#cha-ha-install" title="Chapter 3. Installing SUSE Linux Enterprise High Availability">Chapter 3, <em>Installing SUSE Linux Enterprise High Availability</em></a> and <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>,
       added missing reference to the Installation and Setup Quick Start (doc
       comments #33439 and #33430).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>, removed
       misleading information (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1026391" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1026391</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-config-basics-utilization" title="5.5.6. Placing Resources Based on Their Load Impact">Section 5.5.6, “Placing Resources Based on Their Load Impact”</a>, mentioned different
       resource agents for Xen and <code class="systemitem">libvirt</code> (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=932825" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=932825</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-config-basics-remote-pace-remote" title="5.6.2. Managing Services on Remote Nodes with pacemaker_remote">Section 5.6.2, “Managing Services on Remote Nodes with <code class="literal">pacemaker_remote</code>”</a>, corrected
       node limit for Corosync membership
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1022226" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1022226</a>).
      </p></li><li class="listitem"><p>In <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a>, mentioned
       corner-cases for resources that depend on Pacemaker and Corosync services
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1039922" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1039922</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-lb-haproxy" title="13.3. Configuring Load Balancing with HAProxy">Section 13.3, “Configuring Load Balancing with HAProxy”</a>, clarified configuration of
       HAProxy and procedure
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1027630" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1027630</a>).
      </p></li><li class="listitem"><p>
       Updated <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="16.6. Configuring OCFS2 Resources With Hawk2">Section 16.6, “Configuring OCFS2 Resources With Hawk2”</a> to match the latest
       Hawk2 version (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040504" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040504</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#pro-gfs2-mount-cluster" title="Mounting a GFS2 Volume with the Cluster Manager">Procedure 17.4, “Mounting a GFS2 Volume with the Cluster Manager”</a>, added a missing backslash to
       a command (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1055941" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1055941</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-drbd-configure-init" title="18.3.3. Initializing and Formatting DRBD Resource">Section 18.3.3, “Initializing and Formatting DRBD Resource”</a>, clarified on which
       nodes to execute the commands (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040722" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040722</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-drbd-test" title="18.7. Testing the DRBD Service">Section 18.7, “Testing the DRBD Service”</a>, added missing DRBD resources
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=971420" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=971420</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-clvm-config-cmirrord" title="19.2.2. Scenario: Configuring Cmirrord">Section 19.2.2, “Scenario: Configuring Cmirrord”</a>, fixed a typo and
       enhanced a procedure
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1031261" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1031261</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#cha-ha-cluster-md" title="Chapter 20. Cluster Multi-device (Cluster MD)">Chapter 20, <em>Cluster Multi-device (Cluster MD)</em></a>, updated commands to create
       clustered MD devices by specifying the full path to the device (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048082" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048082</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-cluster-md-overview" title="20.1. Conceptual Overview">Section 20.1, “Conceptual Overview”</a>, mentioned that each
       disk needs to be accessible by Cluster MD on each node (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938502" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938502</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-cluster-md-create" title="20.2. Creating a Clustered MD RAID Device">Section 20.2, “Creating a Clustered MD RAID Device”</a>, mentioned that Cluster MD
       only supports version 1.2 metadata
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1031833" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1031833</a>).
       Clarified how to check if the DLM resource node is up and running (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040297" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040297</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-storage-protect-fencing-number" title="10.4. Number of SBD Devices">Section 10.4, “Number of SBD Devices”</a>, extended
       description about diskless SBD.
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 10.3, “Initializing the SBD Devices”</a>, specified the
       relationship between timeout values (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1042204" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1042204</a>).
      </p></li><li class="listitem"><p>
       In <a class="xref" href="#sec-ha-rear-config" title="22.2. Setting Up ReaR and Your Backup Solution">Section 22.2, “Setting Up ReaR and Your Backup Solution”</a> and <a class="xref" href="#sec-ha-rear-concept-limit" title="22.1.6. Limitations with Btrfs">Section 22.1.6, “Limitations with Btrfs”</a>, described different ReaR
       configuration files (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040371" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040371</a>).
      </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp2-maint-3" data-id-title="April 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.7 </span><span class="title-name">April 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp2-maint-3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.12.2.1"><span class="term">General Changes</span></dt><dd><p>Added a new guide, <em class="citetitle"> <em class="citetitle">Pacemaker Remote</em></em>, available
      at <a class="link" href="https://www.suse.com/documentation/sle-ha-12" target="_blank">https://www.suse.com/documentation/sle-ha-12</a>.
      It explains the setup of a cluster with virtual guest
      nodes or remote nodes, managed by <code class="systemitem">pacemaker_remote</code>.
     <span class="emphasis"><em>Remote</em></span> in this context does not mean physical
     distance, but <span class="quote">“<span class="quote">non-membership</span>”</span> of a cluster.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp2-maint-2" data-id-title="March 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.8 </span><span class="title-name">March 2017 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp2-maint-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.13.2.1"><span class="term">General Changes to this Guide</span></dt><dd><p>Prefer <code class="systemitem">ocf:heartbeat:clvm</code> over
     <code class="systemitem">ocf:lvm2:clvmd</code>/<code class="systemitem">ocf:lvm2:cmirrord</code>
      (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=1030841" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=1030841</a>).</p></dd><dt id="id-1.3.7.6.13.2.2"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added note about software watchdog limitation to <a class="xref" href="#pro-ha-storage-protect-sw-watchdog" title="Loading the Softdog Kernel Module">Procedure 10.2, “Loading the Softdog Kernel Module”</a> (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1029857" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1029857</a>).
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp2-maint-1" data-id-title="November 2016 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.9 </span><span class="title-name">November 2016 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP2)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp2-maint-1">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.14.2.1"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Updated home page entry for OCFS2 (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981498" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981498</a>).
       </p></li><li class="listitem"><p>Added timeouts for monitoring operation in resource configuration
       for OCFS2, GFS2 and cLVM (default values as set by the <code class="systemitem">ha-cluster-bootstrap</code> scripts,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981518" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981518</a>).
       </p></li><li class="listitem"><p>Updated screenshots and slightly adjusted text according to minor
        user interface changes
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981495" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981495</a>).
       </p></li><li class="listitem"><p>Fixed wrong device name in command (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=992757" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=992757</a>).
       </p></li><li class="listitem"><p>In <a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>, corrected and amended the
       configuration example for Kdump and enhanced the description of what
       Kdump does (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1007422" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1007422</a>).
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp2" data-id-title="November 2016 (Initial Release of SUSE Linux Enterprise High Availability 12 SP2)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.10 </span><span class="title-name">November 2016 (Initial Release of SUSE Linux Enterprise High Availability 12 SP2)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.7.6.15.2.1"><span class="term">General Changes to this Guide</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Numerous small fixes and additions to the documentation, based on
        technical feedback.</p></li><li class="listitem"><p>The former chapter
        <em class="citetitle">Installation and Basic Setup</em> has been
        restructured:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> The chapter has been renamed <a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module"><em>Using the YaST Cluster Module</em></a>.</p></li><li class="listitem"><p>How to manually install SUSE Linux Enterprise High Availability and how to automatically
          set up a cluster with the bootstrap scripts has moved into the new
          Installation and Setup Quick Start (Fate#320823). </p></li><li class="listitem"><p> The section <em class="citetitle">Mass Deployment with AutoYaST</em>
          has been moved to the new chapter <a class="xref" href="#cha-ha-install" title="Chapter 3. Installing SUSE Linux Enterprise High Availability">Chapter 3, <em>Installing SUSE Linux Enterprise High Availability</em></a>.
         </p></li></ul></div></li><li class="listitem"><p>The chapter <em class="citetitle">Configuring and Managing Cluster Resources
       (Web Interface)</em> (Hawk version 1) has been removed from the
        documentation. Hawk2 is documented in <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>.
       </p></li><li class="listitem"><p>
        The parameter <em class="parameter">no-quorum-policy=ignore</em> has been
        replaced with <em class="parameter">no-quorum-policy=stop</em> as the first one
        is no longer recommended (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981650" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981650</a>).
       </p></li><li class="listitem"><p>Updated STONITH resource configurations that contained a monitoring
        operation with <code class="literal">start-delay</code>. To prevent double fencing
        in 2-node clusters, use the parameter <code class="literal">pcmk_delay_max</code>
        instead (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=990213" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=990213</a>).
       </p></li><li class="listitem"><p> The e-mail address for documentation feedback has changed to
        <code class="literal">doc-team@suse.com</code>. </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.2"><span class="term"><a class="xref" href="#cha-ha-concepts" title="Chapter 1. Product Overview">Chapter 1, <em>Product Overview</em></a></span></dt><dd><p>
      Mentioned that 32-node limit can be extended by using
      <code class="literal">pacemaker_remote</code> (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981590" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981590</a>).
     </p></dd><dt id="id-1.3.7.6.15.2.3"><span class="term"><a class="xref" href="#cha-ha-requirements" title="Chapter 2. System Requirements and Recommendations">Chapter 2, <em>System Requirements and Recommendations</em></a></span></dt><dd><p>
      Clarified node count recommendation (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981646" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981646</a>).
     </p></dd><dt id="id-1.3.7.6.15.2.4"><span class="term"><a class="xref" href="#cha-ha-install" title="Chapter 3. Installing SUSE Linux Enterprise High Availability">Chapter 3, <em>Installing SUSE Linux Enterprise High Availability</em></a></span></dt><dd><p>
      New chapter. See also <em class="citetitle">General Changes to this Guide</em>.
     </p></dd><dt id="id-1.3.7.6.15.2.5"><span class="term"><a class="xref" href="#cha-ha-setup" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Completely revised. See also <em class="citetitle">General Changes to this Guide</em>.
     </p></li><li class="listitem"><p>All screenshots have been updated.</p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.6"><span class="term"><a class="xref" href="#cha-ha-migration" title="Chapter 24. Upgrading Your Cluster and Updating Software Packages">Chapter 24, <em>Upgrading Your Cluster and Updating Software Packages</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> Added entries for upgrading to SUSE Linux Enterprise High Availability 12 SP2. </p></li><li class="listitem"><p>Mentioned that Hawk2 shows a warning if the upgrade is incomplete
        (Fate#320759).</p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.7"><span class="term"><a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-manual-config-load" title="7.4.1. Loading Cluster Resources from a File">Section 7.4.1, “Loading Cluster Resources from a File”</a> (Fate#320389).
       </p></li><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-manual-config-show" title="7.5.1. Showing Cluster Resources">Section 7.5.1, “Showing Cluster Resources”</a> (Fate#320401).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.8"><span class="term"><a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Mentioned that Hawk is replaced with Hawk2 after
        upgrading the system to SUSE Linux Enterprise High Availability 12 SP2 (Fate#320560).
       </p></li><li class="listitem"><p>Lots of minor adjustments because of updates in the Web
        interface.</p></li><li class="listitem"><p>
        Added <em class="citetitle">Using Maintenance Mode</em>.
       </p></li><li class="listitem"><p>
        Added <a class="xref" href="#sec-conf-hawk2-batch" title="6.9. Using the Batch Mode">Section 6.9, “Using the Batch Mode”</a> (Fate#317834,
        Fate#318298, Fate#318314, Fate#317834).
       </p></li><li class="listitem"><p>
        Added <a class="xref" href="#sec-conf-hawk2-history" title="6.10. Viewing the Cluster History">Section 6.10, “Viewing the Cluster History”</a>.
       </p></li><li class="listitem"><p>
        Added <a class="xref" href="#sec-config-hawk2-utilization" title="6.6.8. Configuring Placement of Resources Based on Load Impact">Section 6.6.8, “Configuring Placement of Resources Based on Load Impact”</a> (Fate#318317,
        Fate#320420).
       </p></li><li class="listitem"><p>Added <a class="xref" href="#sec-conf-hawk2-rsc-show" title="6.5.1. Showing the Current Cluster Configuration (CIB)">Section 6.5.1, “Showing the Current Cluster Configuration (CIB)”</a> (Fate#319665).
       </p></li><li class="listitem"><p>Added <a class="xref" href="#sec-conf-hawk2-manage-edit" title="6.7.1. Editing Resources and Groups">Section 6.7.1, “Editing Resources and Groups”</a> (Fate#318316,
        Fate#318318).</p></li><li class="listitem"><p>Made <a class="xref" href="#sec-conf-hawk2-monitor" title="6.8. Monitoring Clusters">Section 6.8, “Monitoring Clusters”</a> more prominent by
        moving it up one level.</p></li><li class="listitem"><p>Updated all screenshots.</p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.9"><span class="term"><a class="xref" href="#cha-ha-drbd" title="Chapter 18. DRBD">Chapter 18, <em>DRBD</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-drbd-resource-stacking" title="18.5. Creating a Stacked DRBD Device">Section 18.5, “Creating a Stacked DRBD Device”</a>
        (Fate#321013).
       </p></li><li class="listitem"><p>
        Documented supported migration to DRBD 9 in <a class="xref" href="#sec-ha-drbd-migrate" title="18.4. Migrating from DRBD 8 to DRBD 9">Section 18.4, “Migrating from DRBD 8 to DRBD 9”</a> (Fate#320399).
       </p></li><li class="listitem"><p>
        Adapted documentation to new configuration format in DRBD 9
       (Fate#980842)
       </p></li><li class="listitem"><p>
        Fixed several bugs (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=975587" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=975587</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980822" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980822</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980825" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980825</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980835" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980835</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980842" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980842</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980861" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980861</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=980863" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=980863</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981765" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981765</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981766" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981766</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981768" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981768</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981776" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981776</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=990585" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=990585</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.10"><span class="term"><a class="xref" href="#cha-ha-cluster-md" title="Chapter 20. Cluster Multi-device (Cluster MD)">Chapter 20, <em>Cluster Multi-device (Cluster MD)</em></a></span></dt><dd><p>New chapter.
     </p></dd><dt id="id-1.3.7.6.15.2.11"><span class="term"><a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Reworked <a class="xref" href="#pro-ha-storage-protect-watchdog" title="Loading the Correct Kernel Module">Procedure 10.1, “Loading the Correct Kernel Module”</a>
      (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=967795" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=967795</a>).
     </p></li><li class="listitem"><p>
        Removed the <code class="option">-S</code> option (not mandatory) in variable
        <em class="parameter">SBD_OPTS</em> of <code class="filename">/etc/sysconfig/sbd</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=971040" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=971040</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.12"><span class="term"><a class="xref" href="#cha-ha-samba" title="Chapter 21. Samba Clustering">Chapter 21, <em>Samba Clustering</em></a></span></dt><dd><p>
      Fixed typo in package name <span class="package">tdb-tools</span> (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981512" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981512</a>),
      directory and resource names (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981889" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981889</a>).
     </p></dd><dt id="id-1.3.7.6.15.2.13"><span class="term"><a class="xref" href="#cha-ha-rear" title="Chapter 22. Disaster Recovery with Relax-and-Recover (ReaR)">Chapter 22, <em>Disaster Recovery with Relax-and-Recover (ReaR)</em></a></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added note about known incompatibilities.
       </p></li><li class="listitem"><p>
        Added information on support for UEFI systems
        (Fate#319332).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.14"><span class="term"><a class="xref" href="#app-ha-troubleshooting" title="Appendix A. Troubleshooting">Appendix A, <em>Troubleshooting</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-troubleshooting-hawk2" title="A.6. Hawk2">Section A.6, “Hawk2”</a>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=979095" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=979095</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.15"><span class="term"><a class="xref" href="#app-crmreport-nonroot" title="Appendix D. Running Cluster Reports Without root Access">Appendix D, <em>Running Cluster Reports Without <code class="systemitem">root</code> Access</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added feedback to chapter (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981749" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981749</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981751" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981751</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981753" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981753</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=982172" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=982172</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.15.2.16"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Fixed typos in commands or names (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981494" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981494</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981749" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981749</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981751" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981751</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981753" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981753</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=981759" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=981759</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=982504" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=982504</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=983220" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=983220</a>.
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp1-maint-1" data-id-title="December 2015 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP1)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.11 </span><span class="title-name">December 2015 (Maintenance Update of SUSE Linux Enterprise High Availability 12 SP1)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp1-maint-1">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-docupdates-sle12-sp1-maint-1"><span class="term">General</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> Added a chapter for Hawk version 2: <a class="xref" href="#cha-conf-hawk2" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>. </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-sp1" data-id-title="December 2015 (Initial Release of SUSE Linux Enterprise High Availability 12 SP1)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.12 </span><span class="title-name">December 2015 (Initial Release of SUSE Linux Enterprise High Availability 12 SP1)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-sp1">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-docupdates-sle12-sp1"><span class="term">General</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Updated some links to external documentation sources (doc comment
        #26851).
       </p></li><li class="listitem"><p>
        Updated the screenshots.
       </p></li><li class="listitem"><p>
        Add-ons provided by SUSE have been renamed to modules and
        extensions. The manuals have been updated to reflect this change.
       </p></li><li class="listitem"><p>
        <code class="command">crm_report</code> and
        <code class="command">hb_report</code> have been
        replaced with <code class="command">crm report</code> (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=950483" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=950483</a>).
        Adjusted the manuals accordingly (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938427" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938427</a>.
       </p></li><li class="listitem"><p>
        Numerous small fixes and additions to the manual based on technical
        feedback.
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.2"><span class="term"><a class="xref" href="#cha-ha-requirements" title="Chapter 2. System Requirements and Recommendations">Chapter 2, <em>System Requirements and Recommendations</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Rephrased sentence about STONITH devices to include not only
        physical devices but also STONITH mechanism like watchdog (doc
        comment #27070).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.3"><span class="term"><em class="citetitle">Installation and Basic Setup</em>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        The example setup files for multicast and unicast setup have moved
        to <code class="filename">/usr/share/doc/packages/corosync</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938305" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938305</a>).
       </p></li><li class="listitem"><p>
        The procedure <em class="citetitle">Automatically Setting Up the First
        Node</em> mentions
        that <code class="command">ha-cluster-init</code> lets you also configure a
        virtual IP address for Hawk (Fate#318549).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.4"><span class="term"><a class="xref" href="#cha-ha-migration" title="Chapter 24. Upgrading Your Cluster and Updating Software Packages">Chapter 24, <em>Upgrading Your Cluster and Updating Software Packages</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Moved chapter from appendix to <a class="xref" href="#part-install" title="Part I. Installation and Setup">Part I, “Installation and Setup”</a> for
        better visibility.
       </p></li><li class="listitem"><p>
        Completely revised <a class="xref" href="#sec-ha-migration-upgrade" title="24.2. Upgrading your Cluster to the Latest Product Version">Section 24.2, “Upgrading your Cluster to the Latest Product Version”</a>.
       </p></li><li class="listitem"><p>
        Added terms <code class="literal">rolling upgrade</code> and <code class="literal">offline
        migration</code> to
        <a class="xref" href="#sec-ha-migration-terminology" title="24.1. Terminology">Section 24.1, “Terminology”</a>.
       </p></li><li class="listitem"><p>
        Inserted overview of
        <a class="xref" href="#sec-ha-migration-upgrade-oview" title="24.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Supported Upgrade Paths for SLE HA and SLE HA Geo</a>.
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.5"><span class="term"><a class="xref" href="#cha-ha-config-basics" title="Chapter 5. Configuration and Administration Basics">Chapter 5, <em>Configuration and Administration Basics</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added note about restrictions for constraints and certain types of
        resources
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=927423" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=927423</a>).
       </p></li><li class="listitem"><p>
        Modified paragraph about calculating probing timeouts to make it
        more clear and moved it to section
        <a class="xref" href="#sec-ha-config-basics-monitoring" title="5.4. Resource Monitoring">Section 5.4, “Resource Monitoring”</a>.
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.6"><span class="term"><a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Added new <a class="xref" href="#sec-ha-manual-config-clusterscripts" title="7.1.5. Using crmsh's Cluster Scripts">Section 7.1.5, “Using crmsh's Cluster Scripts”</a>
         (Fate#318211).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.7"><span class="term"><a class="xref" href="#cha-ha-fencing" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Removed cloning of STONITH resources from examples as this is no
        longer needed
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=870963" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=870963</a>).
       </p></li><li class="listitem"><p>
        Removed some STONITH devices that are for testing purposes only
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921019" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921019</a>).
       </p></li><li class="listitem"><p>
        Removed <code class="literal">external/kdumpcheck</code> resource agent and
        added example configuration for the <code class="literal">fence_kdump</code>
        resource agent instead (Fate#317135).
       </p></li><li class="listitem"><p>
        Mentioned available packages for STONITH resource agents:
        <code class="systemitem">cluster-glue</code> and
        <code class="systemitem"> fence-agents</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=920980" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=920980</a>).
       </p></li><li class="listitem"><p>
        Added <code class="command">crm ra list stonith</code> as another command to
        list supported STONITH devices.
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.8"><span class="term"><a class="xref" href="#cha-ha-acl" title="Chapter 11. Access Control Lists">Chapter 11, <em>Access Control Lists</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        
        Added <em class="citetitle">Configuring ACLs with Hawk2</em>.
       </p></li><li class="listitem"><p>
        Updated chapter according to the new ACL features that become
        available after upgrading the CIB validation version. For details,
        see <a class="xref" href="#note-ha-cib-upgrade" title="Note: Upgrading the CIB Syntax Version">Note: Upgrading the CIB Syntax Version</a>.
       </p><p>
        If you have upgraded from a former SUSE Linux Enterprise High Availability version and kept
        your former CIB version, refer to the <em class="citetitle">Access Control
        List</em> chapter in the Administration Guide for SUSE Linux Enterprise High Availability 11 SP3.
        It is available from
        <a class="link" href="http://www.suse.com/documentation/" target="_blank">http://www.suse.com/documentation/</a>.
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.9"><span class="term"><a class="xref" href="#cha-ha-lb" title="Chapter 13. Load Balancing">Chapter 13, <em>Load Balancing</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Added reference to load balancing strategies (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938345" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938345</a>).</p></li><li class="listitem"><p>Added additional paragraph about IPVS and KTCPVS. Added link
           to LVS Knowledge Base
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938330" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938330</a>).</p></li><li class="listitem"><p>Removed deprecated parameter <em class="parameter">contimeout</em>
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938357" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938357</a>).
         </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.10"><span class="term"><a class="xref" href="#cha-ha-ocfs2" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Fixed logical error between <em class="citetitle">Configuring a DLM Resource</em>
        and <a class="xref" href="#pro-ocfs2-mount-cluster" title="Mounting an OCFS2 Volume with the Cluster Resource Manager">Procedure 16.4, “Mounting an OCFS2 Volume with the Cluster Resource Manager”</a>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938543" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938543</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.11"><span class="term"><a class="xref" href="#cha-ha-drbd" title="Chapter 18. DRBD">Chapter 18, <em>DRBD</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Revised procedure in <a class="xref" href="#sec-ha-drbd-configure-init" title="18.3.3. Initializing and Formatting DRBD Resource">Section 18.3.3, “Initializing and Formatting DRBD Resource”</a>
         to add information about how to speed up the initial synchronization
         (Fate#317940).
       </p></li><li class="listitem"><p>Clarified sentence about automatic LVM filter adjustment in
           <a class="xref" href="#pro-drbd-configure-yast" title="Using YaST to Configure DRBD">Procedure 18.2, “Using YaST to Configure DRBD”</a> (Fate#317957).</p></li><li class="listitem"><p>Mentioned how to remove stale cache entries with YaST's
           DRBD module (Fate#318555).</p></li><li class="listitem"><p>Mentioned the Firewall port in YaST's DRBD module
           (Fate#318391).</p></li><li class="listitem"><p>Reflected changes of DRBD packages (Fate#938292).</p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.12"><span class="term"><a class="xref" href="#cha-ha-clvm" title="Chapter 19. Cluster Logical Volume Manager (cLVM)">Chapter 19, <em>Cluster Logical Volume Manager (cLVM)</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Fixed typo in configuration file name:
        <code class="filename">/etc/iscsi/iscsid.conf</code> instead of
        <code class="filename">/etc/iscsi/iscsi.conf</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938669" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938669</a>).
       </p></li><li class="listitem"><p>
        Adjusted title of a YaST element in
        <a class="xref" href="#pro-ha-clvm-scenario-iscsi-targets" title="Configuring iSCSI Targets (SAN)">Procedure 19.4, “Configuring iSCSI Targets (SAN)”</a>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938679" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938679</a>).
       </p></li><li class="listitem"><p>
        Corrected step in
        <a class="xref" href="#pro-ha-clvm-scenarios-iscsi-initiator" title="Configuring iSCSI Initiators">Procedure 19.5, “Configuring iSCSI Initiators”</a>:
        Only establish a connection to the targets of the necessary portals,
        fixed typos in node names
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938681" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938681</a>).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.13"><span class="term"><a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Restructured chapter and added new sections:
           <em class="citetitle">Conceptual Overview</em>  and
           <a class="xref" href="#sec-ha-storage-protect-req" title="10.3. Requirements">Section 10.3, “Requirements”</a>.
         </p></li><li class="listitem"><p>
        Added note about STONITH configuration for 2-node clusters.
       </p></li><li class="listitem"><p>
        Mentioned minimum allowed value for the emulated watchdog in
        <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 10.3, “Initializing the SBD Devices”</a> and
        <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="10.5. Calculation of Timeouts">Section 10.5, “Calculation of Timeouts”</a>.
       </p></li><li class="listitem"><p>Added new procedure about how to determine the right watchdog
           module in <a class="xref" href="#pro-ha-storage-protect-watchdog" title="Loading the Correct Kernel Module">Procedure 10.1, “Loading the Correct Kernel Module”</a>.
           </p></li><li class="listitem"><p>Added note about how to ensure SBD has started on all nodes
         (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938491" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938491</a>).
         </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.14"><span class="term"><a class="xref" href="#cha-ha-samba" title="Chapter 21. Samba Clustering">Chapter 21, <em>Samba Clustering</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Added reference to shared storage (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921265" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921265</a>).</p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.15"><span class="term"><a class="xref" href="#cha-ha-rear" title="Chapter 22. Disaster Recovery with Relax-and-Recover (ReaR)">Chapter 22, <em>Disaster Recovery with Relax-and-Recover (ReaR)</em></a>
    </span></dt><dd><p>
      Completely revised and updated the chapter.
     </p></dd><dt id="id-1.3.7.6.17.2.16"><span class="term"><a class="xref" href="#app-ha-troubleshooting" title="Appendix A. Troubleshooting">Appendix A, <em>Troubleshooting</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Added new <a class="xref" href="#sec-ha-troubleshooting-history" title="A.5. History">Section A.5, “History”</a>
         (Fate#318500).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.17"><span class="term"><a class="xref" href="#app-crmreport-nonroot" title="Appendix D. Running Cluster Reports Without root Access">Appendix D, <em>Running Cluster Reports Without <code class="systemitem">root</code> Access</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        New appendix (Fate#314907).
       </p></li></ul></div></dd><dt id="id-1.3.7.6.17.2.18"><span class="term">Bugfixes</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Fixed typos in commands
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=907118" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=907118</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921229" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921229</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921234" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921234</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921237" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921237</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938340" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938340</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938334" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938334</a>,
        <a class="link" href=" https://bugzilla.suse.com/show_bug.cgi?id=938486" target="_blank"> https://bugzilla.suse.com/show_bug.cgi?id=938486</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938487" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938487</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938490" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938490</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938515" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938515</a>).
       </p></li><li class="listitem"><p>
        Fixed three identically described hyperlinks in
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=868143" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=868143</a>).
       </p></li><li class="listitem"><p>
        Fixed name for logical volume
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=920990" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=920990</a>).
       </p></li><li class="listitem"><p>
        Fixed name of resource agent
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921004" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921004</a>).
       </p></li><li class="listitem"><p>
        Mentioned YaST option to open a port in the firewall
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921016" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921016</a>).
       </p></li><li class="listitem"><p>
        Adjusted the device names
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921022" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921022</a>
        and
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=921022" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=921022</a>).
       </p></li><li class="listitem"><p>
        Updated how to file service requests
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=882059" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=882059</a>).
       </p></li><li class="listitem"><p>
        Fixed example for Xen resource
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=932828" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=932828</a>).
       </p></li><li class="listitem"><p>
        Fixed indentation in screen
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=932294" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=932294</a>).
       </p></li><li class="listitem"><p>
        Fixed wording
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938291" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938291</a>).
       </p></li><li class="listitem"><p>
        Removed duplicated entries
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938313" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938313</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938332" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938332</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938536" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938536</a>).
       </p></li><li class="listitem"><p>
        Mentioned default value for keyword <code class="literal">locking_type</code>
        in <code class="filename">/etc/lvm/lvm.conf</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938526" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938526</a>).
       </p></li><li class="listitem"><p>Fixed errors in <code class="command">crm script</code> commands
           (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=938358" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=938358</a>).
         </p></li><li class="listitem"><p>Fixed path name for STONITH plug-ins
       (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=952815" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=952815</a>).
       </p></li></ul></div></dd></dl></div></section><section class="sect1" id="sec-ha-docupdates-sle12-ga" data-id-title="October 2014 (Initial Release of SUSE Linux Enterprise High Availability 12)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">E.13 </span><span class="title-name">October 2014 (Initial Release of SUSE Linux Enterprise High Availability 12)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-docupdates-sle12-ga">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_docupdates.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-docupdates-sle12-ga-general"><span class="term">General</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Adjusted section <em class="citetitle">Automatic Cluster Setup
        (ha-cluster-bootstrap)</em>
        because of the renaming of the bootstrap scripts
        <code class="filename">sleha-init</code>, <code class="filename">sleha-join</code>,
        and <code class="filename">sleha-remove</code> to
        <code class="filename">ha-cluster-init</code>,
        <code class="filename">ha-cluster-join</code>, and
        <code class="filename">ha-cluster-remove</code>, respectively. Also the
        location of the log file has changed from
        <code class="filename">/var/log/sleha-bootstrap.log</code> to
        <code class="filename">/var/log/ha-cluster-bootstrap.log</code>
        (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=853772" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=853772</a>).
       </p></li><li class="listitem"><p>
        Replaced any occurrence of <code class="command">chkconfig</code> and
        <code class="command">rc*</code> commands with systemd equivalents
        (<a class="link" href="http://bugzilla.suse.com/show_bug.cgi?id=853533" target="_blank">http://bugzilla.suse.com/show_bug.cgi?id=853533</a>).
       </p></li><li class="listitem"><p>
        Introduced a consistent naming scheme for cluster names, node names,
        cluster resources, and constraints and applied it to the
        documentation. See <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>. (Fate#314938).
       </p></li><li class="listitem"><p>
        Removed Pacemaker GUI and any references pointing to it (Fate#312948,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853508" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853508</a>).
       </p></li><li class="listitem"><p>
        Replaced OpenAIS with Corosync
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853507" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853507</a>,
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853556" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853556</a>).
       </p></li><li class="listitem"><p>
        Removed all occurrences of the following parameters as they are
        obsolete: <code class="varname">default-resource-stickiness</code>,
        <code class="varname">is-managed-default</code>, and
        <code class="varname">default-action-timeout</code>.
       </p></li><li class="listitem"><p>
        Removed any occurrences of <code class="literal">o2cb</code> resources as the
        respective resource agent is now obsolete.
       </p></li><li class="listitem"><p>
        Improved the consistency of crm shell examples.
       </p></li><li class="listitem"><p>
        Adjusted changed package names throughout the whole manual
        (Fate#314807).
       </p></li><li class="listitem"><p>
        Removed list of resource agents from the appendix.
       </p></li><li class="listitem"><p>
        Moved documentation for Geo Clustering for SUSE Linux Enterprise High Availability into a separate document
        (Fate#316121). See the new <em class="citetitle">Geo Clustering for SUSE Linux Enterprise High Availability
        Geo Clustering Quick Start</em>, available from
        <a class="link" href="http://www.suse.com/documentation/" target="_blank">http://www.suse.com/documentation/</a>.
       </p></li><li class="listitem"><p>
        Removed <em class="citetitle">Example of Setting Up a Simple Testing
        Resource</em> from the appendix as this can be done now by
        the Hawk wizard.
       </p></li><li class="listitem"><p>
        Changed terminology for <code class="literal">master-slave</code> resources,
        which are now called <code class="literal">multi-state</code> resources in the
        upstream documentation.
       </p></li><li class="listitem"><p>
        Updated the screenshots.
       </p></li><li class="listitem"><p>
        Mentioned both <code class="command">hb_report</code> and
        <code class="command">crm_report</code> as command line tools for creating
        detailed cluster reports.
       </p></li><li class="listitem"><p>
        Numerous small fixes and additions to the manual based on technical
        feedback.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-concepts"><span class="term"><a class="xref" href="#cha-ha-concepts" title="Chapter 1. Product Overview">Chapter 1, <em>Product Overview</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Changed terminology from multi-site clusters to geographically dispersed (or
        Geo) clusters for consistency reasons.
       </p></li><li class="listitem"><p>
        Added section about availability of SUSE Linux Enterprise High Availability and Geo Clustering for SUSE Linux Enterprise High Availability as
        extensions: <a class="xref" href="#sec-ha-availability" title="1.1. Availability as a Module or Extension">Section 1.1, “Availability as a Module or Extension”</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-require"><span class="term"><a class="xref" href="#cha-ha-requirements" title="Chapter 2. System Requirements and Recommendations">Chapter 2, <em>System Requirements and Recommendations</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Restructured contents.
       </p></li><li class="listitem"><p>
        Mentioned how to create a cluster report when using a non-standard
        SSH port (Fate#314906). See <a class="xref" href="#vle-ha-req-ssh">SSH</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-install"><span class="term"><em class="citetitle">Installation and Basic Setup</em>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Updated section <em class="citetitle">Manual Cluster Setup (YaST)</em> to
        reflect the software changes to the YaST cluster module.
       </p></li><li class="listitem"><p>
        Csync2 also accepts a combination of host name and IP address for
        each cluster node. See
        <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the Configuration to All Nodes">Section 4.7, “Transferring the Configuration to All Nodes”</a> (Fate#314956).
       </p></li><li class="listitem"><p>
        <a class="xref" href="#sec-ha-installation-autoyast" title="3.2. Mass Installation and Deployment with AutoYaST">Section 3.2, “Mass Installation and Deployment with AutoYaST”</a> mentions the option
        to clone DRBD settings via AutoYaST (Fate#315128).
       </p></li><li class="listitem"><p>The procedure <em class="citetitle">Automatically Setting Up the First
        Node</em> mentions
        the <code class="option">-t</code> option for <code class="command">ha-cluster-init</code> to
        perform additional cluster configurations based on templates
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=821123" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=821123</a>).
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-migrate"><span class="term"><a class="xref" href="#cha-ha-migration" title="Chapter 24. Upgrading Your Cluster and Updating Software Packages">Chapter 24, <em>Upgrading Your Cluster and Updating Software Packages</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added definition of terms:
        <a class="xref" href="#sec-ha-migration-terminology" title="24.1. Terminology">Section 24.1, “Terminology”</a>.
       </p></li><li class="listitem"><p>
        Added new subsection to <a class="xref" href="#sec-ha-migration-upgrade" title="24.2. Upgrading your Cluster to the Latest Product Version">Section 24.2, “Upgrading your Cluster to the Latest Product Version”</a>.
       </p></li><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-migration-update" title="24.3. Updating Software Packages on Cluster Nodes">Section 24.3, “Updating Software Packages on Cluster Nodes”</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-basics"><span class="term"><a class="xref" href="#cha-ha-config-basics" title="Chapter 5. Configuration and Administration Basics">Chapter 5, <em>Configuration and Administration Basics</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <a class="xref" href="#sec-ha-config-basics-global-stonith" title="5.2.3. Global Option stonith-enabled">Section 5.2.3, “Global Option <code class="literal">stonith-enabled</code>”</a> mentions
        policy change in DLM services when the global cluster option
        <code class="literal">stonith-enabled</code> is set to
        <code class="literal">false</code> (Fate#315195).
       </p></li><li class="listitem"><p>
        <a class="xref" href="#sec-ha-config-basics-tags" title="5.5.7. Grouping Resources by Using Tags">Section 5.5.7, “Grouping Resources by Using Tags”</a> describes a new option
        to group conceptually related resources, without creating any
        colocation or ordering relationship between them (Fate#315101).
       </p></li><li class="listitem"><p>
        <a class="xref" href="#sec-ha-config-basics-constraints-rscset" title="5.5.1.1. Resource Sets">Section 5.5.1.1, “Resource Sets”</a> explains
        the concept of resource sets as an alternative format for defining
        constraints. As of SUSE Linux Enterprise High Availability 12, resource sets can now also be
        used within location constraints (whereas they could formerly only
        be used within colocation and ordering constraints).
       </p></li><li class="listitem"><p>
        Restructured <a class="xref" href="#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a> to
        also cover the option of setting a whole cluster to maintenance mode
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=829864" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=829864</a>).
       </p></li><li class="listitem"><p>
        Added attributes for <code class="literal">pacemaker_remote</code> service to
        <a class="xref" href="#sec-ha-config-basics-meta-attr" title="5.3.6. Resource Options (Meta Attributes)">Section 5.3.6, “Resource Options (Meta Attributes)”</a> and added new section:
        <a class="xref" href="#sec-ha-config-basics-remote" title="5.6. Managing Services on Remote Hosts">Section 5.6, “Managing Services on Remote Hosts”</a>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853535" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853535</a>).
       </p></li><li class="listitem"><p>
        Added new resource agent classes to
        <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>:
        <code class="literal">systemd</code>, <code class="literal">service</code>, and
        <code class="literal">nagios</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853520" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853520</a>).
       </p></li><li class="listitem"><p>
        Updated <a class="xref" href="#sec-ha-config-basics-remote-nagios" title="5.6.1. Monitoring Services on Remote Hosts with Monitoring Plug-ins">Section 5.6.1, “Monitoring Services on Remote Hosts with Monitoring Plug-ins”</a> to
        reflect the package name changes from
        <code class="systemitem">nagios-plugins</code> and
        <code class="systemitem">nagios-plugins-metadata</code> to
        <code class="systemitem">monitoring-plugins</code> and
        <code class="systemitem">monitoring-plugins-metadata</code>, respectively
        (Fate#317780).
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-manualconfig"><span class="term"><a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-manual-config-constraints-weak-bond" title="7.4.5.3. Collocating Sets for Resources Without Dependency">Section 7.4.5.3, “Collocating Sets for Resources Without Dependency”</a>
        (Fate#314917).
       </p></li><li class="listitem"><p>
        Added a section about the health status (Fate#316464):
        <a class="xref" href="#sec-ha-manual-config-cli-health" title="7.5.7. Getting Health Status">Section 7.5.7, “Getting Health Status”</a>.
       </p></li><li class="listitem"><p>
        Added a section about tagging resources (Fate#315101):
        <a class="xref" href="#sec-ha-manual-config-tag" title="7.5.6. Grouping/Tagging Resources">Section 7.5.6, “Grouping/Tagging Resources”</a>.
       </p></li><li class="listitem"><p>
        Updated overview section about tab completion and getting help
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853643" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853643</a>).
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-acl"><span class="term"><a class="xref" href="#cha-ha-acl" title="Chapter 11. Access Control Lists">Chapter 11, <em>Access Control Lists</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Restructured chapter.
       </p></li><li class="listitem"><p>
       Added <em class="citetitle">Configuring ACLs with Hawk2</em>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-loadbal"><span class="term"><a class="xref" href="#cha-ha-lb" title="Chapter 13. Load Balancing">Chapter 13, <em>Load Balancing</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Restructured chapter and added
        <a class="xref" href="#sec-ha-lb-haproxy" title="13.3. Configuring Load Balancing with HAProxy">Section 13.3, “Configuring Load Balancing with HAProxy”</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-ocfs2"><span class="term"><a class="xref" href="#cha-ha-ocfs2" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="16.6. Configuring OCFS2 Resources With Hawk2">Section 16.6, “Configuring OCFS2 Resources With Hawk2”</a> (Fate#316322).
       </p></li><li class="listitem"><p>
        <a class="xref" href="#pro-ocfs2-volume" title="Creating and Formatting an OCFS2 Volume">Procedure 16.2, “Creating and Formatting an OCFS2 Volume”</a>: Fixed
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=853631" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=853631</a>.
       </p></li><li class="listitem"><p>
        <a class="xref" href="#tab-ha-ofcs2-mkfs-ocfs2-params" title="Important OCFS2 Parameters">Table 16.2, “Important OCFS2 Parameters”</a>: Fixed
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=883550" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=883550</a>.
       </p></li><li class="listitem"><p>
        Re-sorted some procedures and sections
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=894158" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=894158</a>).
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-gfs2"><span class="term"><a class="xref" href="#cha-ha-gfs2" title="Chapter 17. GFS2">Chapter 17, <em>GFS2</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        New chapter.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-drbd"><span class="term"><a class="xref" href="#cha-ha-drbd" title="Chapter 18. DRBD">Chapter 18, <em>DRBD</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Removed table <em class="citetitle">DRBD RPM packages</em> from
        <a class="xref" href="#sec-ha-drbd-install" title="18.2. Installing DRBD Services">Section 18.2, “Installing DRBD Services”</a>, as all former
        packages are now contained in the
        <code class="systemitem">drbd</code> package.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-clvm"><span class="term"><a class="xref" href="#cha-ha-clvm" title="Chapter 19. Cluster Logical Volume Manager (cLVM)">Chapter 19, <em>Cluster Logical Volume Manager (cLVM)</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Described option <code class="option">mirrored</code> in a clustered
        environment (Fate#314367):
        <a class="xref" href="#sec-ha-clvm-config-cmirrord" title="19.2.2. Scenario: Configuring Cmirrord">Section 19.2.2, “Scenario: Configuring Cmirrord”</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-storeprotect"><span class="term"><a class="xref" href="#cha-ha-storage-protect" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Added <a class="xref" href="#sec-ha-storage-protect-sgpersist" title="10.10.1. Configuring an sg_persist Resource">Section 10.10.1, “Configuring an sg_persist Resource”</a>
        (Fate#312345).
       </p></li><li class="listitem"><p>
        <em class="citetitle">Setting Up the Watchdog</em>: Added note about
        watchdog and other software that accesses the watchdog timer
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=891340" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=891340</a>).
       </p></li><li class="listitem"><p>
        <em class="citetitle">Setting Up the Watchdog</em>: Added how to load
        the watchdog driver at boot time
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=892344" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=892344</a>).
       </p></li><li class="listitem"><p>
        <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the Cluster to Use SBD">Procedure 10.7, “Configuring the Cluster to Use SBD”</a>: Added advice about
        length of <code class="literal">stonith-timeout</code> in relation to
        <code class="literal">msgwait</code> timeout
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=891346" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=891346</a>).
       </p></li><li class="listitem"><p>
        Adjusted <em class="citetitle">Starting the SBD Daemon</em>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=891499" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=891499</a>).
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-samba"><span class="term"><a class="xref" href="#cha-ha-samba" title="Chapter 21. Samba Clustering">Chapter 21, <em>Samba Clustering</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Documented separate Samba and Winbind resources (Fate#316336):
        <a class="xref" href="#pro-ha-samba-basicconf" title="Setting Up a Basic Clustered Samba Server">Procedure 21.1, “Setting Up a Basic Clustered Samba Server”</a>.
       </p></li><li class="listitem"><p>
        Adjusted several sections and procedures according to
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=886095" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=886095</a>.
       </p></li><li class="listitem"><p>
        <a class="xref" href="#pro-ha-samba-basicconf" title="Setting Up a Basic Clustered Samba Server">Procedure 21.1, “Setting Up a Basic Clustered Samba Server”</a>: Fixed
        <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=886082" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=886082</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-rear"><span class="term"><a class="xref" href="#cha-ha-rear" title="Chapter 22. Disaster Recovery with Relax-and-Recover (ReaR)">Chapter 22, <em>Disaster Recovery with Relax-and-Recover (ReaR)</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Adjusted chapter to reflect update to latest stable ReaR version
        (Fate#316508).
       </p></li><li class="listitem"><p>
        Updated example screen of <code class="filename">/etc/rear/local.conf</code>
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=885881" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=885881</a>).
       </p></li><li class="listitem"><p>
        Fixed minor problems
        (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=878054" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=878054</a>).
       </p></li><li class="listitem"><p>
        Removed any occurrence of <code class="command">rear-SUSE</code> and
        configuration file <code class="filename">/etc/rear/site.conf</code>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-trouble"><span class="term"><a class="xref" href="#app-ha-troubleshooting" title="Appendix A. Troubleshooting">Appendix A, <em>Troubleshooting</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Mentioned how to create a cluster report when using a non-standard
        SSH port (Fate#314906). See <a class="xref" href="#vle-ha-crmreport">How can I create a report with an analysis of all my cluster nodes?</a>.
       </p></li></ul></div></dd><dt id="vle-ha-docupdates-sle12-ga-ocf-ra"><span class="term"><em class="citetitle">HA OCF Agents</em>
    </span></dt><dd><p>
      This chapter has been removed. The latest information about the OCF
      resource agents can be viewed in the installed system as described in
      <a class="xref" href="#sec-ha-manual-config-ocf" title="7.1.3. Displaying Information about OCF Resource Agents">Section 7.1.3, “Displaying Information about OCF Resource Agents”</a>.
     </p></dd><dt id="vle-ha-docupdates-sle12-ga-naming"><span class="term"><a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>
    </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        New appendix explaining naming scheme.
       </p></li></ul></div></dd></dl></div></section></section></div><section class="glossary"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Glossary</span></span> <a title="Permalink" class="permalink" href="#gl-heartb">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/ha_glossary.xml" title="Edit source document"> </a></div></div></div></div></div><div class="line"/><dl><dt id="id-1.3.8.3"><span><span class="glossterm">active/active, active/passive</span> <a title="Permalink" class="permalink" href="#id-1.3.8.3">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time. For
    example, it has <span class="emphasis"><em>some</em></span> services running, but can take
    over other services from the other node. Compare with primary/secondary
    and dual-primary in DRBD speak.
   </p></dd><dt id="id-1.3.8.4"><span><span class="glossterm">arbitrator</span> <a title="Permalink" class="permalink" href="#id-1.3.8.4">#</a></span></dt><dd class="glossdef"><p>
    Additional instance in a Geo cluster that helps to reach consensus
    about decisions such as failover of resources across sites. Arbitrators
    are single machines that run one or more booth instances in a special
    mode.
   </p></dd><dt id="id-1.3.8.5"><span><span class="glossterm">AutoYaST</span> <a title="Permalink" class="permalink" href="#id-1.3.8.5">#</a></span></dt><dd class="glossdef"><p>
    AutoYaST is a system for installing one or more SUSE Linux Enterprise systems automatically
    and without user intervention. 
   </p></dd><dt id="id-1.3.8.6"><span><span class="glossterm">bindnetaddr (bind network address)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.6">#</a></span></dt><dd class="glossdef"><p>
    The network address the Corosync executive should bind to. 
   </p></dd><dt id="id-1.3.8.7"><span><span class="glossterm">booth</span> <a title="Permalink" class="permalink" href="#id-1.3.8.7">#</a></span></dt><dd class="glossdef"><p>
    The instance that manages the failover process between the sites of a
    Geo cluster. It aims to get multi-site resources active on one and
    only one site. This is achieved by using so-called tickets that are
    treated as failover domain between cluster sites, in case a site should
    be down.
   </p></dd><dt id="id-1.3.8.8"><span><span class="glossterm">boothd (booth
  daemon)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.8">#</a></span></dt><dd class="glossdef"><p>
    Each of the participating clusters and arbitrators in a Geo cluster
    runs a service, the <code class="systemitem">boothd</code>. It
    connects to the booth daemons running at the other sites and exchanges
    connectivity details.
   </p></dd><dt id="id-1.3.8.9"><span><span class="glossterm">CCM (consensus cluster membership)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.9">#</a></span></dt><dd class="glossdef"><p>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </p></dd><dt id="id-1.3.8.10"><span><span class="glossterm">CIB (cluster information base)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.10">#</a></span></dt><dd class="glossdef"><p>
    A representation of the whole cluster configuration and status (cluster
    options, nodes, resources, constraints and the relationship to each
    other). It is written in XML and resides in memory. A master CIB is kept
    and maintained on the
    <a class="xref" href="#glos-dc" title="DC (designated coordinator)">DC (designated coordinator)</a> and replicated to
    the other nodes. Normal read and write operations on the CIB are
    serialized through the master CIB.
   </p></dd><dt id="id-1.3.8.11"><span><span class="glossterm">cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.8.11">#</a></span></dt><dd class="glossdef"><p>
    A <span class="emphasis"><em>high-performance</em></span> cluster is a group of computers
    (real or virtual) sharing the application load to achieve faster
    results. A <span class="emphasis"><em>high-availability</em></span> cluster is designed
    primarily to secure the highest possible availability of services.
   </p></dd><dt id="id-1.3.8.12"><span><span class="glossterm">cluster partition</span> <a title="Permalink" class="permalink" href="#id-1.3.8.12">#</a></span></dt><dd class="glossdef"><p>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    split brain scenario develops (see also
    <a class="xref" href="#glos-splitbrain" title="split brain">split brain</a>).
   </p></dd><dt id="id-1.3.8.13"><span><span class="glossterm">concurrency violation</span> <a title="Permalink" class="permalink" href="#id-1.3.8.13">#</a></span></dt><dd class="glossdef"><p>
    A resource that should be running on only one node in the cluster is
    running on several nodes.
   </p></dd><dt id="id-1.3.8.14"><span><span class="glossterm">conntrack tools</span> <a title="Permalink" class="permalink" href="#id-1.3.8.14">#</a></span></dt><dd class="glossdef"><p>
    Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes.
   </p></dd><dt id="id-1.3.8.15"><span><span class="glossterm">CRM (cluster resource manager)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.15">#</a></span></dt><dd class="glossdef"><p>
    The main management entity responsible for coordinating all non-local
    interactions. SUSE Linux Enterprise High Availability uses Pacemaker as CRM. Each node of the
    cluster has its own CRM instance, but the one running on the DC is the
    one elected to relay decisions to the other non-local CRMs and process
    their input. A CRM interacts with several components: local resource
    managers, both on its own node and on the other nodes, non-local CRMs,
    administrative commands, the fencing functionality, the membership
    layer, and booth.
   </p></dd><dt id="id-1.3.8.16"><span><span class="glossterm">crmd (cluster
  resource manager daemon)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.16">#</a></span></dt><dd class="glossdef"><p>
    The CRM is implemented as daemon, crmd. It has an instance on each
    cluster node. All cluster decision-making is centralized by electing one
    of the crmd instances to act as a master. If the elected crmd process
    fails (or the node it ran on), a new one is established.
   </p></dd><dt id="id-1.3.8.17"><span><span class="glossterm">crmsh</span> <a title="Permalink" class="permalink" href="#id-1.3.8.17">#</a></span></dt><dd class="glossdef"><p>
    The command line utility crmsh manages your cluster, nodes, and
    resources.
   </p><p>
    See <a class="xref" href="#cha-ha-manual-config" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a> for more information.
   </p></dd><dt id="id-1.3.8.18"><span><span class="glossterm">Csync2</span> <a title="Permalink" class="permalink" href="#id-1.3.8.18">#</a></span></dt><dd class="glossdef"><p>
    A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters.
   </p></dd><dt id="glos-dc"><span><span class="glossterm">DC (designated coordinator)</span> <a title="Permalink" class="permalink" href="#glos-dc">#</a></span></dt><dd class="glossdef"><p>
    One CRM in the cluster is elected as the Designated Coordinator (DC).
    The DC is the only entity in the cluster that can decide that a
    cluster-wide change needs to be performed, such as fencing a node or
    moving resources around. The DC is also the node where the master copy
    of the CIB is kept. All other nodes get their configuration and resource
    allocation information from the current DC. The DC is elected from all
    nodes in the cluster after a membership change.
   </p></dd><dt id="id-1.3.8.20"><span><span class="glossterm">Disaster</span> <a title="Permalink" class="permalink" href="#id-1.3.8.20">#</a></span></dt><dd class="glossdef"><p>
    Unexpected interruption of critical infrastructure induced by nature,
    humans, hardware failure, or software bugs.
   </p></dd><dt id="id-1.3.8.22"><span><span class="glossterm">Disaster Recover Plan</span> <a title="Permalink" class="permalink" href="#id-1.3.8.22">#</a></span></dt><dd class="glossdef"><p>
    A strategy to recover from a disaster with minimum impact on IT
    infrastructure.
   </p></dd><dt id="id-1.3.8.21"><span><span class="glossterm">Disaster Recovery</span> <a title="Permalink" class="permalink" href="#id-1.3.8.21">#</a></span></dt><dd class="glossdef"><p>
    Disaster recovery is the process by which a business function is
    restored to the normal, steady state after a disaster.
   </p></dd><dt id="id-1.3.8.23"><span><span class="glossterm">DLM (distributed lock manager)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.23">#</a></span></dt><dd class="glossdef"><p>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </p></dd><dt id="id-1.3.8.24"><span><span class="glossterm">DRBD</span> <a title="Permalink" class="permalink" href="#id-1.3.8.24">#</a></span></dt><dd class="glossdef"><p>
    <span class="trademark">DRBD</span>® is a block device
    designed for building high availability clusters. The whole block device
    is mirrored via a dedicated network and is seen as a network RAID-1.
   </p></dd><dt id="id-1.3.8.25"><span><span class="glossterm">existing cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.8.25">#</a></span></dt><dd class="glossdef"><p>
      The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
   </p></dd><dt id="glo-failover"><span><span class="glossterm">failover</span> <a title="Permalink" class="permalink" href="#glo-failover">#</a></span></dt><dd class="glossdef"><p>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </p></dd><dt id="id-1.3.8.27"><span><span class="glossterm">failover domain</span> <a title="Permalink" class="permalink" href="#id-1.3.8.27">#</a></span></dt><dd class="glossdef"><p>
    A named subset of cluster nodes that are eligible to run a cluster
    service if a node fails.
   </p></dd><dt id="id-1.3.8.28"><span><span class="glossterm">fencing</span> <a title="Permalink" class="permalink" href="#id-1.3.8.28">#</a></span></dt><dd class="glossdef"><p>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. Should a cluster node fail, it will
    be shut down or reset to prevent it from causing trouble. This way,
    resources are locked out of a node whose status is uncertain.

    
   </p></dd><dt id="glos-geo"><span><span class="glossterm">Geo cluster</span> <a title="Permalink" class="permalink" href="#glos-geo">#</a></span></dt><dd class="glossdef"><p>
    Consists of multiple, geographically dispersed sites with a local cluster
    each. The sites communicate via IP. Failover across the sites is
    coordinated by a higher-level entity, the booth. Geo clusters need
    to cope with limited network bandwidth and high latency. Storage is
    replicated asynchronously.
   </p></dd><dt id="id-1.3.8.29"><span><span class="glossterm">geo cluster (geographically dispersed cluster)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.29">#</a></span></dt><dd class="glossdef"><p>
    See <a class="xref" href="#glos-geo" title="Geo cluster">Geo cluster</a>.
    
   </p></dd><dt id="id-1.3.8.30"><span><span class="glossterm">heartbeat</span> <a title="Permalink" class="permalink" href="#id-1.3.8.30">#</a></span></dt><dd class="glossdef"><p>
    A CCM, in version 3 an alternative to Corosync. Supports more than
    two communication paths, but not cluster file systems.
   </p></dd><dt id="glos-lb"><span><span class="glossterm">load balancing</span> <a title="Permalink" class="permalink" href="#glos-lb">#</a></span></dt><dd class="glossdef"><p>
    The ability to make several servers participate in the same service and
    do the same work.
   </p></dd><dt id="id-1.3.8.32"><span><span class="glossterm">local cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.8.32">#</a></span></dt><dd class="glossdef"><p>
    A single cluster in one location (for example, all nodes are located in
    one data center). Network latency can be neglected. Storage is typically
    accessed synchronously by all nodes.
   </p></dd><dt id="id-1.3.8.33"><span><span class="glossterm">LRM (local resource manager)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.33">#</a></span></dt><dd class="glossdef"><p>
    Responsible for performing operations on resources. It uses the resource
    agent scripts to carry out these operations. The LRM is
    <span class="quote">“<span class="quote">dumb</span>”</span> in that it does not know of any policy. It needs the
    DC to tell it what to do.
   </p></dd><dt id="id-1.3.8.34"><span><span class="glossterm">mcastaddr (multicast address)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.34">#</a></span></dt><dd class="glossdef"><p>
      IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6. 
   </p></dd><dt id="id-1.3.8.35"><span><span class="glossterm">mcastport (multicast port)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.35">#</a></span></dt><dd class="glossdef"><p>
    
      The port to use for cluster communication.
   </p></dd><dt id="id-1.3.8.36"><span><span class="glossterm">metro cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.8.36">#</a></span></dt><dd class="glossdef"><p>
    A single cluster that can stretch over multiple buildings or data
    centers, with all sites connected by fibre channel. Network latency is
    usually low (&lt;5 ms for distances of approximately
    20 miles). Storage is frequently replicated (mirroring or
    synchronous replication).
   </p></dd><dt id="id-1.3.8.37"><span><span class="glossterm">multicast</span> <a title="Permalink" class="permalink" href="#id-1.3.8.37">#</a></span></dt><dd class="glossdef"><p>
      A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
   </p></dd><dt id="id-1.3.8.39"><span><span class="glossterm">node</span> <a title="Permalink" class="permalink" href="#id-1.3.8.39">#</a></span></dt><dd class="glossdef"><p>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </p></dd><dt id="id-1.3.8.40"><span><span class="glossterm">PE (policy engine)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.40">#</a></span></dt><dd class="glossdef"><p>
    The policy engine computes the actions that need to be taken to
    implement policy changes in the CIB. The PE also produces a transition
    graph containing a list of (resource) actions and dependencies to
    achieve the next cluster state. The PE always runs on the DC.
   </p></dd><dt id="gloss-quorum"><span><span class="glossterm">quorum</span> <a title="Permalink" class="permalink" href="#gloss-quorum">#</a></span></dt><dd class="glossdef"><p>
    In a cluster, a cluster partition is defined to have quorum (can
    <span class="quote">“<span class="quote">quorate</span>”</span>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </p></dd><dt id="id-1.3.8.42"><span><span class="glossterm">RA (resource agent)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.42">#</a></span></dt><dd class="glossdef"><p>
    A script acting as a proxy to manage a resource (for example, to start,
    stop, or monitor a resource). SUSE Linux Enterprise High Availability supports different
    kinds of resource agents. For details, see
    <a class="xref" href="#sec-ha-config-basics-raclasses" title="5.3.2. Supported Resource Agent Classes">Section 5.3.2, “Supported Resource Agent Classes”</a>.
   </p></dd><dt id="id-1.3.8.43"><span><span class="glossterm">ReaR (Relax and Recover)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.43">#</a></span></dt><dd class="glossdef"><p>
    An administrator tool set for creating disaster recovery images.
   </p></dd><dt id="id-1.3.8.44"><span><span class="glossterm">resource</span> <a title="Permalink" class="permalink" href="#id-1.3.8.44">#</a></span></dt><dd class="glossdef"><p>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </p><p>
    The term <span class="quote">“<span class="quote">resource</span>”</span> is also used for DRBD, where it names a
    set of block devices that are using a common connection for replication.
   </p></dd><dt id="id-1.3.8.45"><span><span class="glossterm">RRP (redundant ring protocol)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.45">#</a></span></dt><dd class="glossdef"><p>
     Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol.
   </p></dd><dt id="id-1.3.8.46"><span><span class="glossterm">SBD (STONITH Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.46">#</a></span></dt><dd class="glossdef"><p>
    Provides a node fencing mechanism through the exchange of messages via shared
    block storage (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode.
    Needs a hardware or software watchdog on each node to ensure that misbehaving
    nodes are really stopped.
   </p></dd><dt id="id-1.3.8.47"><span><span class="glossterm">SFEX (shared disk file exclusiveness)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.47">#</a></span></dt><dd class="glossdef"><p>
    SFEX provides storage protection over SAN.
   </p></dd><dt id="glos-splitbrain"><span><span class="glossterm">split brain</span> <a title="Permalink" class="permalink" href="#glos-splitbrain">#</a></span></dt><dd class="glossdef"><p>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). STONITH prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <span class="quote">“<span class="quote">partitioned
    cluster</span>”</span> scenario.
   </p><p>
    The term split brain is also used in DRBD but means that the two nodes
    contain different data.
   </p></dd><dt id="id-1.3.8.49"><span><span class="glossterm">SPOF (single point of failure)</span> <a title="Permalink" class="permalink" href="#id-1.3.8.49">#</a></span></dt><dd class="glossdef"><p>
    Any component of a cluster that, should it fail, triggers the failure of
    the entire cluster.
   </p></dd><dt id="glo-stonith"><span><span class="glossterm">STONITH</span> <a title="Permalink" class="permalink" href="#glo-stonith">#</a></span></dt><dd class="glossdef"><p>
    The acronym for <span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>. It
    refers to the fencing mechanism that shuts down a misbehaving node to
    prevent it from causing trouble in a cluster.
   </p></dd><dt id="id-1.3.8.51"><span><span class="glossterm">switchover</span> <a title="Permalink" class="permalink" href="#id-1.3.8.51">#</a></span></dt><dd class="glossdef"><p>
    Planned, on-demand moving of services to other nodes in a cluster. See
    <a class="xref" href="#glo-failover" title="failover">failover</a>.
   </p></dd><dt id="id-1.3.8.52"><span><span class="glossterm">ticket</span> <a title="Permalink" class="permalink" href="#id-1.3.8.52">#</a></span></dt><dd class="glossdef"><p>
    A component used in Geo clusters. A ticket grants the right to run
    certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by
    dependencies. Only if the defined ticket is available at a site, the
    respective resources are started. Vice versa, if the ticket is removed,
    the resources depending on that ticket are automatically stopped.
   </p></dd><dt id="id-1.3.8.53"><span><span class="glossterm">unicast</span> <a title="Permalink" class="permalink" href="#id-1.3.8.53">#</a></span></dt><dd class="glossdef"><p>
    A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync,
    unicast is implemented as UDP-unicast (UDPU).
   </p></dd></dl></section><div class="legal-section"><section class="appendix" id="id-1.3.9" data-id-title="GNU licenses"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">F </span><span class="title-name">GNU licenses</span></span> <a title="Permalink" class="permalink" href="#id-1.3.9">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_legal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This appendix contains the GNU Free Documentation License version 1.2.
 </p><section class="sect1" id="id-1.3.9.4" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title legal"><span class="title-number-name"><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.3.9.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP4/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="http://www.gnu.org/copyleft/" target="_blank">http://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.3.9.4.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.3.9.4.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2023</span></div></div></footer></body></html>