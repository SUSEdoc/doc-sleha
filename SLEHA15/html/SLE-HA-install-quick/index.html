<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SLEHA 15 | Installation and Setup Quick Start</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Installation and Setup Quick Start | SLEHA 15"/>
<meta name="description" content="This document guides you through the setup of a very basic two-node cluster, using the bootstrap scripts provided by the ha-cluster-bootstrap package…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability Extension"/>
<meta name="product-number" content="15"/>
<meta name="book-title" content="Installation and Setup Quick Start"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 15"/>
<meta property="og:title" content="Installation and Setup Quick Start | SLEHA 15"/>
<meta property="og:description" content="This document guides you through the setup of a very basic two-node cluster, using the bootstrap scripts provided by the ha-cluster-bootstrap package…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Installation and Setup Quick Start | SLEHA 15"/>
<meta name="twitter:description" content="This document guides you through the setup of a very basic two-node cluster, using the bootstrap scripts provided by the ha-cluster-bootstrap package…"/>
<link rel="prev" href="book-sleha-quickstarts.html" title="Quick Start Guides"/><link rel="next" href="bk01ar01apa.html" title="A. GNU Licenses"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Installation and Setup Quick Start</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title"><em class="citetitle">Installation and Setup Quick Start</em></div> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="article" id="art-sleha-install-quick" data-id-title="Installation and Setup Quick Start"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Availability Extension</span> <span class="productnumber">15</span></div><div><div class="title-container"><h1 class="title"><em class="citetitle">Installation and Setup Quick Start</em> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div><div class="abstract"><p>
     This document guides you through the setup of a very basic two-node cluster,
    using the bootstrap scripts provided by the
    <code class="systemitem">ha-cluster-bootstrap</code> package.
    This includes the configuration of a virtual IP address as a cluster
    resource and the use of SBD on shared storage as a node fencing mechanism.
   </p></div><div class="date"><span class="imprint-label">Publication Date: </span>January 05, 2023
</div></div></div><div><div xml:lang="en" class="legalnotice" id="id-1.3.2.2.4"><p>
  Copyright © 2006–2023

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="sect1" id="sec-ha-inst-quick-usage-scenario" data-id-title="Usage Scenario"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Usage Scenario</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-usage-scenario">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The procedures in this document will lead to a minimal setup of a two-node
    cluster with the following properties:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Two nodes: <code class="systemitem">alice</code> (IP: <code class="systemitem">192.168.1.1</code>)
      and <code class="systemitem">bob</code> (IP: <code class="systemitem">192.168.1.2</code>),
      connected to each other via network.
     </p></li><li class="listitem"><p>
      A floating, virtual IP address (<code class="systemitem">192.168.2.1</code>)
      that allows clients to connect to the service no matter which node it is running on.
      This IP address is used to connect to the graphical management tool Hawk2.
     </p></li><li class="listitem"><p>A shared storage device, used as SBD fencing mechanism.
      This avoids split brain scenarios.
     </p></li><li class="listitem"><p>
      Failover of resources from one node to the other if the active host breaks
      down (<span class="emphasis"><em>active/passive</em></span> setup).
     </p></li></ul></div><p>
    You can use the two-node cluster for testing purposes or as a minimal
    cluster configuration that you can extend later on. Before using the
    cluster in a production environment, modify it according to your
    requirements.
   </p></section><section class="sect1" id="sec-ha-inst-quick-req" data-id-title="System Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">System Requirements</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-req">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section informs you about the key system requirements for the
    scenario described in <a class="xref" href="art-sleha-install-quick.html#sec-ha-inst-quick-usage-scenario" title="1. Usage Scenario">Section 1</a>.
    To adjust the cluster for use in a production environment,
    refer to the full list in <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 2 “System Requirements and Recommendations”</span>.
   </p><section class="sect2" id="vl-ha-inst-quick-req-hw" data-id-title="Hardware Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Hardware Requirements</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#vl-ha-inst-quick-req-hw">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.4.3.2.1"><span class="term">Servers</span></dt><dd><p>
       Two servers with software as specified in <a class="xref" href="art-sleha-install-quick.html#il-ha-inst-quick-req-sw" title="2.2. Software Requirements">Section 2.2, “Software Requirements”</a>.
      </p><p>
      The servers can be bare metal or virtual machines. They do not require
      identical hardware (memory, disk space, etc.), but they must have the
      same architecture. Cross-platform clusters are not supported.
     </p></dd><dt id="id-1.3.2.4.3.2.2"><span class="term">Communication Channels</span></dt><dd><p>
       At least two TCP/IP communication media per cluster node.
       The network equipment must support the communication means you want to use
       for cluster communication: multicast or unicast. The communication
       media should support a data rate of 100 Mbit/s or higher.
       For a supported cluster setup two or more redundant communication paths
       are required. This can be done via:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Network Device Bonding (preferred).
         </p></li><li class="listitem"><p>
          A second communication channel in Corosync.
         </p></li></ul></div></dd><dt id="id-1.3.2.4.3.2.3"><span class="term">Node fencing/STONITH</span></dt><dd><p>
       A node fencing (STONITH) device to avoid split-brain scenarios. This can
       be either a physical device (a power switch) or a mechanism like SBD
       (STONITH by disk) in combination with a watchdog.
      </p><p>This document describes using SBD for node fencing. To use SBD, the
       following requirements must be met:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         A shared storage device. For information on setting up shared storage, see the
       <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/book-storage.html" target="_blank">
       <em class="citetitle">Storage Administration Guide</em> for SUSE Linux Enterprise Server</a>.
        </p></li><li class="listitem"><p>The path to the shared storage device must be persistent and
         consistent across all nodes in the cluster. Use stable device names
         such as <code class="filename">/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</code>.
        </p></li><li class="listitem"><p> The SBD device <span class="emphasis"><em>must not</em></span>
         use host-based RAID, LVM2, nor reside on a DRBD* instance.
        </p></li></ul></div><p>
       For more information on STONITH, see <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 11 “Fencing and STONITH”</span>.
       For more information on SBD, see <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 12 “Storage Protection and SBD”</span>.
      </p></dd></dl></div></section><section class="sect2" id="il-ha-inst-quick-req-sw" data-id-title="Software Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Software Requirements</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#il-ha-inst-quick-req-sw">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Base System Module 15</p></li><li class="listitem"><p>Server Applications Module 15</p></li><li class="listitem"><p>SUSE Linux Enterprise High Availability Extension 15</p></li></ul></div></section><section class="sect2" id="vl-ha-inst-quick-req-other" data-id-title="Other Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">Other Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#vl-ha-inst-quick-req-other">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.4.5.2.1"><span class="term">Time Synchronization</span></dt><dd><p>
     Cluster nodes must synchronize to an NTP server outside the cluster.
     Since SUSE Linux Enterprise High Availability Extension 15, chrony is the default implementation of NTP.
     For more information, see the
     <a class="link" href="https://documentation.suse.com/sles/15-GA/html/SLES-all/cha-ntp.html" target="_blank">
     <em class="citetitle">Administration Guide</em> for SUSE Linux Enterprise Server 15</a>.
    </p><p>
     If nodes are not synchronized, the cluster may not work properly.
     In addition, log files and cluster reports are very hard to analyze
     without synchronization.
     If you use the bootstrap scripts, you will be
     warned if NTP is not configured yet.
    </p></dd><dt id="id-1.3.2.4.5.2.2"><span class="term">Host Name and IP Address</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> Use static IP addresses. </p></li><li class="listitem"><p>
         Only the primary IP address is supported.
        </p></li><li class="listitem"><p>
     List all cluster nodes in the <code class="filename">/etc/hosts</code> file
     with their fully qualified host name and short host name. It is essential that
     members of the cluster can find each other by name. If the names are not
     available, internal cluster communication will fail.
   </p></li></ul></div></dd><dt id="id-1.3.2.4.5.2.3"><span class="term">SSH</span></dt><dd><p>
    All cluster nodes must be able to access each other via SSH. Tools
    like <code class="command">crm report</code> (for troubleshooting) and
    Hawk2's <span class="guimenu">History Explorer</span> require passwordless
    SSH access between the nodes,
    otherwise they can only collect data from the current node.
  </p><p> If you use the bootstrap scripts for setting up the
       cluster, the SSH keys will automatically be created and copied. </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-inst-quick-bootstrap" data-id-title="Overview of the Bootstrap Scripts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Overview of the Bootstrap Scripts</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-bootstrap">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All commands from the <span class="package">ha-cluster-bootstrap</span> package
   execute bootstrap scripts that require only a minimum of time and manual
   intervention.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     With <code class="command">ha-cluster-init</code>, define the basic parameters needed
     for cluster communication. This leaves you with a running one-node cluster.
    </p></li><li class="listitem"><p>
     With <code class="command">ha-cluster-join</code>, add more nodes to your cluster.
    </p></li><li class="listitem"><p>
     With <code class="command">ha-cluster-remove</code>, remove nodes from your cluster.
    </p></li></ul></div><p>
   All bootstrap scripts log to <code class="filename">/var/log/ha-cluster-bootstrap.log</code>.
   Check this file for any details of the bootstrap process. Any options set
   during the bootstrap process can be modified later with the
   YaST cluster module. See <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 4 “Using the YaST Cluster Module”</span>
   for details.
  </p><p>Each script comes with a man page covering the range of functions, the
   script's options, and an overview of the files the script can create and modify.
  </p><p>
   The bootstrap script <code class="command">ha-cluster-init</code> checks and
   configures the following components:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.5.7.1"><span class="term">NTP</span></dt><dd><p>
      Checks if NTP is configured to start at boot time. If not, a message appears.
     </p></dd><dt id="id-1.3.2.5.7.2"><span class="term">SSH</span></dt><dd><p>Creates SSH keys for passwordless login between cluster nodes.
     </p></dd><dt id="id-1.3.2.5.7.3"><span class="term">Csync2</span></dt><dd><p>
      Configures Csync2 to replicate configuration files across all nodes
      in a cluster.
     </p></dd><dt id="id-1.3.2.5.7.4"><span class="term">Corosync</span></dt><dd><p>Configures the cluster communication system.</p></dd><dt id="id-1.3.2.5.7.5"><span class="term">SBD/Watchdog</span></dt><dd><p>Checks if a watchdog exists and asks you whether to configure SBD
      as node fencing mechanism.</p></dd><dt id="id-1.3.2.5.7.6"><span class="term">Virtual Floating IP</span></dt><dd><p>Asks you whether to configure a virtual IP address for cluster
      administration with Hawk2.</p></dd><dt id="id-1.3.2.5.7.7"><span class="term">Firewall</span></dt><dd><p>Opens the ports in the firewall that are needed for cluster communication.</p></dd><dt id="id-1.3.2.5.7.8"><span class="term">Cluster Name</span></dt><dd><p>Defines a name for the cluster, by default
        <code class="systemitem">hacluster</code>. This
      is optional and mostly useful for Geo clusters. Usually, the cluster
      name reflects the location and makes it easier to distinguish a site
      inside a Geo cluster.</p></dd><dt id="id-1.3.2.5.7.9"><span class="term">QDevice/QNetd</span></dt><dd><p>
      Asks you whether to configure QDevice/QNetd to participate in
      quorum decisions. We recommend using QDevice and QNetd for clusters
      with an even number of nodes, and especially for two-node clusters.
     </p><p>
      This configuration is not covered here, but you can set it up later as
      described in <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 13 “QDevice and QNetd”</span>.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-inst-quick-installation" data-id-title="Installing the High Availability Packages"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Installing the High Availability Packages</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The packages for configuring and managing a cluster
      are included in the <code class="literal">High Availability</code> installation pattern.
      This pattern is only available after the SUSE Linux Enterprise High Availability Extension is installed.
    </p><p>
     You can register to the SUSE Customer Center and install the High Availability Extension while installing SUSE Linux Enterprise Server,
     or after installation. For more information, see the
     <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/cha-register-sle.html" target="_blank">
     <em class="citetitle">Deployment Guide</em></a> for SUSE Linux Enterprise Server.
    </p><div class="procedure" id="pro-ha-inst-quick-pattern" data-id-title="Installing the High Availability Pattern"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 1: </span><span class="title-name">Installing the High Availability Pattern </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-pattern">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Install the High Availability pattern from the command line:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper install -t pattern ha_sles</code></pre></div></li><li class="step"><p>
          Install the High Availability pattern on <span class="emphasis"><em>all</em></span> machines that
          will be part of your cluster.
       </p><div id="id-1.3.2.6.4.3.2" data-id-title="Installing software packages on all nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Installing software packages on all nodes</div><p>
         For an automated installation of SUSE Linux Enterprise Server 15 and the High Availability Extension,
         use AutoYaST to clone existing nodes. For more
         information, see <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 3 “Installing the High Availability Extension”, Section 3.2 “Mass Installation and Deployment with AutoYaST”</span>.
        </p></div></li></ol></div></div></section><section class="sect1" id="sec-ha-inst-quick-sbd" data-id-title="Using SBD for Node Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Using SBD for Node Fencing</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-sbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before you can configure SBD with the bootstrap script, you must enable a watchdog
    on each node. SUSE Linux Enterprise Server ships with several kernel modules that provide hardware-specific
    watchdog drivers. The High Availability Extension uses the SBD daemon as the software component
    that <span class="quote">“<span class="quote">feeds</span>”</span> the watchdog.
   </p><p>
    The following procedure uses the <code class="systemitem">softdog</code>
    watchdog.
   </p><div id="id-1.3.2.7.4" data-id-title="Softdog Limitations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Softdog Limitations</div><p>
     The softdog driver assumes that at least one CPU is still running. If
     all CPUs are stuck, the code in the softdog driver that should reboot the
     system will never be executed. In contrast, hardware watchdogs keep
     working even if all CPUs are stuck.
    </p><p>Before using the cluster in a production environment, we highly
     recommend replacing the <code class="systemitem">softdog</code> module with the
     hardware module that best fits your hardware.
    </p><p>However, if no watchdog matches your hardware,
     <code class="systemitem">softdog</code> can be used as kernel
     watchdog module.</p></div><div class="procedure" id="pro-ha-inst-quick-sbd-setup" data-id-title="Enabling the softdog watchdog for SBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 2: </span><span class="title-name">Enabling the softdog watchdog for SBD </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-sbd-setup">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On each node, enable the softdog watchdog:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo softdog &gt; /etc/modules-load.d/watchdog.conf</code>
<code class="prompt root"># </code><code class="command">systemctl restart systemd-modules-load</code></pre></div></li><li class="step"><p>Test if the softdog module is loaded correctly:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod | grep dog</code>
softdog           16384  1</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-inst-quick-setup-1st-node" data-id-title="Setting Up the First Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Setting Up the First Node</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-setup-1st-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Set up the first node with the <code class="command">ha-cluster-init</code> script.
   This requires only a minimum of time and manual intervention.
  </p><div class="procedure" id="pro-ha-inst-quick-setup-ha-cluster-init" data-id-title="Setting Up the First Node (alice) with ha-cluster-init"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3: </span><span class="title-name">Setting Up the First Node (<code class="systemitem">alice</code>) with
    <code class="command">ha-cluster-init</code> </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-setup-ha-cluster-init">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code> to the physical or virtual machine to
     use as cluster node.
    </p></li><li class="step"><p>
     Start the bootstrap script:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ha-cluster-init --name <em class="replaceable">CLUSTERNAME</em></code></pre></div><p>Replace the <em class="replaceable">CLUSTERNAME</em>
     placeholder with a meaningful name, like the geographical location of your
     cluster (for example, <code class="literal">amsterdam</code>).
     This is especially helpful to create a Geo cluster later on,
     as it simplifies the identification of a site.
    </p><p>
     If you need unicast instead of multicast (the default) for your cluster
     communication, use the option <code class="option">-u</code>. After installation,
     find the value <code class="literal">udpu</code> in the file
     <code class="filename">/etc/corosync/corosync.conf</code>.
     If <code class="command">ha-cluster-init</code> detects a node running on
     Amazon Web Services (AWS), the script will use unicast automatically as
     default for cluster communication.
    </p><p>
     The script checks for NTP configuration and a hardware watchdog service.
     It generates the public and private SSH keys used for SSH access and
     Csync2 synchronization and starts the respective services.
    </p></li><li class="step"><p>
     Configure the cluster communication layer (Corosync):
    </p><ol type="a" class="substeps"><li class="step"><p>
       Enter a network address to bind to. By default, the script will
       propose the network address of <code class="systemitem">eth0</code>.
       Alternatively, enter a different network address, for example the
       address of <code class="literal">bond0</code>.
      </p></li><li class="step"><p>
       Enter a multicast address. The script proposes a random address that
       you can use as default. Of course, your particular network needs to
       support this multicast address.
      </p></li><li class="step"><p>
       Enter a multicast port. The script proposes <code class="literal">5405</code>
       as default.
      </p></li></ol></li><li class="step"><p>
    Set up SBD as the node fencing mechanism:</p><ol type="a" class="substeps"><li class="step"><p>Confirm with <code class="literal">y</code> that you want to use SBD.</p></li><li class="step"><p>Enter a persistent path to the partition of your block device that
       you want to use for SBD.
       The path must be consistent across all nodes in the cluster.</p><p>The script creates a small partition on the device to be used for SBD.</p></li></ol></li><li class="step" id="st-ha-cluster-init-ip"><p>Configure a virtual IP address for cluster administration with Hawk2:</p><ol type="a" class="substeps"><li class="step"><p>Confirm with <code class="literal">y</code> that you want to configure a
      virtual IP address.</p></li><li class="step"><p>Enter an unused IP address that you want to use as administration IP
       for Hawk2: <code class="literal">192.168.2.1</code>
      </p><p>Instead of logging in to an individual cluster node with Hawk2,
       you can connect to the virtual IP address.</p></li></ol></li><li class="step"><p>
     Choose whether to configure QDevice and QNetd. For the minimal setup
     described in this document, decline with <code class="literal">n</code> for now.
     You can set up QDevice and QNetd later, as described in
     <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 13 “QDevice and QNetd”</span>.
    </p></li></ol></div></div><p>
   Finally, the script will start the Pacemaker service to bring the
   cluster online and enable Hawk2. The URL to use for Hawk2 is
   displayed on the screen.
  </p><p>
   You now have a running one-node cluster. To view its status, proceed as follows:
  </p><div class="procedure" id="pro-ha-inst-quick-hawk2-login" data-id-title="Logging In to the Hawk2 Web Interface"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4: </span><span class="title-name">Logging In to the Hawk2 Web Interface </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-hawk2-login">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled. </p></li><li class="step"><p>As URL, enter the virtual IP address that you configured with the bootstrap script:</p><div class="verbatim-wrap"><pre class="screen">https://192.168.2.1:7630/</pre></div><div id="id-1.3.2.8.6.3.3" data-id-title="Certificate Warning" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Certificate Warning</div><p> If a certificate warning appears when you try to access the URL for
      the first time, a self-signed certificate is in use. Self-signed
      certificates are not considered trustworthy by default. </p><p> Ask your cluster operator for the certificate details to verify the
      certificate. </p><p> To proceed anyway, you can add an exception in the browser to bypass
      the warning. </p></div></li><li class="step"><p> On the Hawk2 login screen, enter the
      <span class="guimenu">Username</span> and <span class="guimenu">Password</span> of the
       user that was created by the bootstrap script (user <code class="systemitem">hacluster</code>, password
      <code class="literal">linux</code>).</p><div id="id-1.3.2.8.6.4.2" data-id-title="Secure Password" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Secure Password</div><p>Replace the default password with a secure one as soon as possible:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">passwd hacluster</code></pre></div></div></li><li class="step"><p>
     Click <span class="guimenu">Log In</span>. The Hawk2 Web interface
     shows the Status screen by default:
    </p><div class="figure" id="fig-ha-inst-quick-one-node-status"><div class="figure-contents"><div class="mediaobject"><a href="images/installquick-one-nodecluster.png"><img src="images/installquick-one-nodecluster.png" width="80%" alt="Status of the One-Node Cluster in Hawk2" title="Status of the One-Node Cluster in Hawk2"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1: </span><span class="title-name">Status of the One-Node Cluster in Hawk2 </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#fig-ha-inst-quick-one-node-status">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></li></ol></div></div></section><section class="sect1" id="sec-ha-inst-quick-setup-2nd-node" data-id-title="Adding the Second Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Adding the Second Node</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-setup-2nd-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Add a second node to the cluster with the <code class="command">ha-cluster-join</code>
    bootstrap script.
    The script only needs access to an existing cluster node and
    will complete the basic setup on the current machine automatically.
  </p><p>
   For more information, see the <code class="command">ha-cluster-join</code> man page.
  </p><div class="procedure" id="pro-ha-inst-quick-setup-ha-cluster-join" data-id-title="Adding the Second Node (bob) with ha-cluster-join"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5: </span><span class="title-name">Adding the Second Node (<code class="systemitem">bob</code>) with
    <code class="command">ha-cluster-join</code> </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-setup-ha-cluster-join">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code> to the physical or virtual machine you want to add to the cluster.
    </p></li><li class="step"><p>
     Start the bootstrap script:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ha-cluster-join</code></pre></div><p>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device.
     You are warned if none is present.
    </p></li><li class="step"><p>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address of the first node
     (<code class="systemitem">alice</code>, <code class="systemitem">192.168.1.1</code>).
    </p></li><li class="step"><p>
     If you have not already configured passwordless SSH access between
     both machines, you will be prompted for the <code class="systemitem">root</code> password
     of the existing node.
    </p><p>
     After logging in to the specified node, the script will copy the
     Corosync configuration, configure SSH and Csync2,
     bring the current machine online as new cluster node, and
     start the service needed for Hawk2. 
    </p></li></ol></div></div><p>
   Check the cluster status in Hawk2. Under <span class="guimenu">Status</span> › <span class="guimenu">Nodes</span> you should see two nodes with a green status:
  </p><div class="figure" id="fig-ha-inst-quick-two-node-cluster"><div class="figure-contents"><div class="mediaobject"><a href="images/installquick-two-nodecluster-status.png"><img src="images/installquick-two-nodecluster-status.png" width="80%" alt="Status of the Two-Node Cluster" title="Status of the Two-Node Cluster"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 2: </span><span class="title-name">Status of the Two-Node Cluster </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#fig-ha-inst-quick-two-node-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-inst-quick-test" data-id-title="Testing the Cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Testing the Cluster</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-test">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following tests can help you identify issues with the cluster setup.
    However, a realistic test involves specific use cases and scenarios.
    Before using the cluster in a production environment, test it thoroughly
    according to your use cases.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The command <code class="command">sbd -d <em class="replaceable">DEVICE_NAME</em> list</code>
      lists all of the nodes that are visible to SBD. For the setup described in
      this document, the output should show both <code class="systemitem">alice</code>
      and <code class="systemitem">bob</code>.
     </p></li><li class="listitem"><p>
      <a class="xref" href="art-sleha-install-quick.html#sec-ha-inst-quick-test-resource-failover" title="8.1. Testing Resource Failover">Section 8.1, “Testing Resource Failover”</a> is a simple test
      to check if the cluster moves the virtual IP address to the other node if
      the node that currently runs the resource is set to <code class="literal">standby</code>.
     </p></li><li class="listitem"><p>
      <a class="xref" href="art-sleha-install-quick.html#sec-ha-inst-quick-test-with-cluster-script" title="8.2. Testing with the crm cluster crash_test command">Section 8.2, “Testing with the <code class="command">crm cluster crash_test</code> command”</a> simulates cluster
      failures and reports the results.
     </p></li></ul></div><section class="sect2" id="sec-ha-inst-quick-test-resource-failover" data-id-title="Testing Resource Failover"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Testing Resource Failover</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-test-resource-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     As a quick test, the following procedure checks on resource failovers:
    </p><div class="procedure" id="pro-ha-inst-quick-test" data-id-title="Testing Resource Failover"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6: </span><span class="title-name">Testing Resource Failover </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#pro-ha-inst-quick-test">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal and ping <code class="systemitem">192.168.2.1</code>,
      your virtual IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ping 192.168.2.1</code></pre></div></li><li class="step"><p>
      Log in to Hawk2.
     </p></li><li class="step"><p>
      Under <span class="guimenu">Status</span> › <span class="guimenu">Resources</span>,
      check which node the virtual IP address (resource
      <code class="systemitem">admin_addr</code>) is running on.
      This procedure assumes the resource is running on <code class="systemitem">alice</code>.
      </p></li><li class="step"><p>
      Put <code class="systemitem">alice</code> into
      <span class="guimenu">Standby</span> mode:
     </p><div class="figure" id="fig-ha-inst-quick-standby"><div class="figure-contents"><div class="mediaobject"><a href="images/installquick-standby-node.png"><img src="images/installquick-standby-node.png" width="60%" alt="Node alice in Standby Mode" title="Node alice in Standby Mode"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 3: </span><span class="title-name">Node <code class="systemitem">alice</code> in Standby Mode </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#fig-ha-inst-quick-standby">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
      Click <span class="guimenu">Status</span> › <span class="guimenu">Resources</span>. The resource <code class="systemitem">admin_addr</code>
      has been migrated to <code class="systemitem">bob</code>.
     </p></li></ol></div></div><p>
    During the migration, you should see an uninterrupted flow of pings to
    the virtual IP address. This shows that the cluster setup and the floating
    IP work correctly. Cancel the <code class="command">ping</code> command with
    <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>.
   </p></section><section class="sect2" id="sec-ha-inst-quick-test-with-cluster-script" data-id-title="Testing with the crm cluster crash_test command"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Testing with the <code class="command">crm cluster crash_test</code> command</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-test-with-cluster-script">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The command <code class="command">crm cluster crash_test</code> triggers cluster failures to find
     problems. Before you use your cluster in production, it is
     recommended to use this command to make sure everything works as expected.
    </p><p>
     The command supports the following checks:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.10.5.4.1"><span class="term"><code class="option">--split-brain-iptables</code></span></dt><dd><p>
        Simulates a split brain scenario by blocking the Corosync port.
        Checks whether one node can be fenced as expected.
       </p></dd><dt id="id-1.3.2.10.5.4.2"><span class="term"><code class="option">--kill-sbd</code>/<code class="option">--kill-corosync</code>/
       <code class="option">--kill-pacemakerd</code></span></dt><dd><p>
        Kills the daemons for SBD, Corosync, and Pacemaker.
        After running one of these tests, you can find a report in the
        directory <code class="filename">/var/lib/crmsh/crash_test/</code>.
        The report includes a test case description, action logging,
        and an explanation of possible results.
       </p></dd><dt id="id-1.3.2.10.5.4.3"><span class="term"><code class="option">--fence-node <em class="replaceable">NODE</em></code></span></dt><dd><p>
        Fences a specific node passed from the command line.
       </p></dd></dl></div><p>
     For more information, see <code class="command">crm cluster crash_test --help</code>.
    </p><div class="example" id="ex-test-with-cluster-script" data-id-title="Testing the cluster: node fencing"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 1: </span><span class="title-name">Testing the cluster: node fencing </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#ex-test-with-cluster-script">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_mon -1</code>
Stack: corosync
Current DC: alice (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on alice

2 nodes configured
1 resource configured

Online: [ alice bob ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started alice

<code class="prompt root"># </code><code class="command">crm cluster crash_test</code><code class="command"> --fence-node bob</code>

==============================================
Testcase:          Fence node bob
Fence action:      reboot
Fence timeout:     60

!!! WARNING WARNING WARNING !!!
THIS CASE MAY LEAD TO NODE BE FENCED.
TYPE Yes TO CONTINUE, OTHER INPUTS WILL CANCEL THIS CASE [Yes/No](No): <code class="command">Yes</code>
INFO: Trying to fence node "bob"
INFO: Waiting 60s for node "bob" reboot...
INFO: Node "bob" will be fenced by "alice"!
INFO: Node "bob" was successfully fenced by "alice"</pre></div></div></div><p>
      To watch <code class="systemitem">bob</code> change status during
      the test, log in to Hawk2 and navigate to <span class="guimenu">Status</span> › <span class="guimenu">Nodes</span>.
     </p></section></section><section class="sect1" id="sec-ha-inst-quick-next" data-id-title="Next Steps"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Next Steps</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-next">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The bootstrap scripts provide a quick way to set up a basic High Availability cluster that
    can be used for testing purposes. However, to expand this cluster into a functioning
    High Availability cluster that can be used in production environments, more steps are recommended.
   </p><div class="variablelist" id="vl-ha-inst-quick-next-rec" data-id-title="Recommended steps to complete the High Availability cluster setup"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Recommended steps to complete the High Availability cluster setup </span></span><a title="Permalink" class="permalink" href="art-sleha-install-quick.html#vl-ha-inst-quick-next-rec">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.2.11.3.2"><span class="term">Adding more nodes</span></dt><dd><p>
       Add more nodes to the cluster using one of the following methods:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         For individual nodes, use the <code class="command">crm cluster join</code> script
         as described in <a class="xref" href="art-sleha-install-quick.html#sec-ha-inst-quick-setup-2nd-node" title="7. Adding the Second Node">Section 7, “Adding the Second Node”</a>.
        </p></li><li class="listitem"><p>
         For mass installation of multiple nodes, use AutoYaST as described in
         <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 3 “Installing the High Availability Extension”, Section 3.2 “Mass Installation and Deployment with AutoYaST”</span>.
        </p></li></ul></div><p>
       A regular cluster can contain up to 32 nodes. With the <code class="systemitem">pacemaker_remote</code> service,
       High Availability clusters can be extended to include additional nodes beyond this limit.
       See <span class="intraxref">Article “<em class="citetitle">Pacemaker Remote Quick Start</em>”</span> for more details.
      </p></dd><dt id="id-1.3.2.11.3.3"><span class="term">Configuring QDevice</span></dt><dd><p>
       If the cluster has an even number of nodes, configure QDevice and QNetd to
       participate in quorum decisions. QDevice provides a configurable number of votes,
       allowing a cluster to sustain more node failures than the standard quorum rules allow.
       For details, see <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 13 “QDevice and QNetd”</span>.
      </p></dd><dt id="id-1.3.2.11.3.4"><span class="term">Enabling a hardware watchdog</span></dt><dd><p>
       Before using the cluster in a production environment, replace the
       <code class="literal">softdog</code> module with the hardware module that best fits your
       hardware. For details, see <span class="intraxref">Book “<em class="citetitle">Administration Guide</em>”, Chapter 12 “Storage Protection and SBD”, Section 12.6 “Setting Up the Watchdog”</span>.
      </p></dd></dl></div></section><section class="sect1" id="sec-ha-inst-quick-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="art-sleha-install-quick.html#sec-ha-inst-quick-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15/xml/art_sle_ha_install_quick.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     More documentation for this product is available at
     <a class="link" href="https://documentation.suse.com/sle-ha/15-GA" target="_blank">https://documentation.suse.com/sle-ha/15-GA</a>.
     For further configuration and administration tasks, see the comprehensive
     <a class="link" href="https://documentation.suse.com/sle-ha/15-GA/html/SLE-HA-all/book-sleha-guide.html" target="_blank">
      <em class="citetitle">Administration Guide</em></a>.
    </p></section></section><nav class="bottom-pagination"><div> </div><div><a class="pagination-link next" href="bk01ar01apa.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Appendix A </span>GNU Licenses</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-usage-scenario"><span class="title-number">1 </span><span class="title-name">Usage Scenario</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-req"><span class="title-number">2 </span><span class="title-name">System Requirements</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-bootstrap"><span class="title-number">3 </span><span class="title-name">Overview of the Bootstrap Scripts</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-installation"><span class="title-number">4 </span><span class="title-name">Installing the High Availability Packages</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-sbd"><span class="title-number">5 </span><span class="title-name">Using SBD for Node Fencing</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-setup-1st-node"><span class="title-number">6 </span><span class="title-name">Setting Up the First Node</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-setup-2nd-node"><span class="title-number">7 </span><span class="title-name">Adding the Second Node</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-test"><span class="title-number">8 </span><span class="title-name">Testing the Cluster</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-next"><span class="title-number">9 </span><span class="title-name">Next Steps</span></a></span></li><li><span class="sect1"><a href="art-sleha-install-quick.html#sec-ha-inst-quick-moreinfo"><span class="title-number">10 </span><span class="title-name">For More Information</span></a></span></li><li><span class="appendix"><a href="bk01ar01apa.html"><span class="title-number">A </span><span class="title-name">GNU Licenses</span></a></span><ul><li><span class="sect1"><a href="bk01ar01apa.html#id-1.3.2.13.4"><span class="title-number">A.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2023</span></div></div></footer></body></html>