<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 12 SP5 | Geo Clustering Guide | Conceptual Overview</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Conceptual Overview | SLE HA 12 SP5"/>
<meta name="description" content="Geo clusters based on SUSE® Linux Enterprise High Avai…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="12 SP5"/>
<meta name="book-title" content="Geo Clustering Guide"/>
<meta name="chapter-title" content="Chapter 2. Conceptual Overview"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 12 SP5"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Conceptual Overview | SLE HA 12 SP5"/>
<meta property="og:description" content="Geo clusters based on SUSE® Linux Enterprise High Availability can be considered overlay clusters where each cluster site co…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Conceptual Overview | SLE HA 12 SP5"/>
<meta name="twitter:description" content="Geo clusters based on SUSE® Linux Enterprise High Availability can be considered overlay clusters where each cluster site co…"/>
<link rel="prev" href="cha-ha-geo-challenges.html" title="Chapter 1. Challenges for Geo Clusters"/><link rel="next" href="cha-ha-geo-req.html" title="Chapter 3. Requirements"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/geo_concept_i.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Geo Clustering Guide</a><span> / </span><a class="crumb" href="cha-ha-geo-concept.html">Conceptual Overview</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Geo Clustering Guide</div><ol><li><a href="cha-ha-geo-challenges.html" class=" "><span class="title-number">1 </span><span class="title-name">Challenges for Geo Clusters</span></a></li><li><a href="cha-ha-geo-concept.html" class=" you-are-here"><span class="title-number">2 </span><span class="title-name">Conceptual Overview</span></a></li><li><a href="cha-ha-geo-req.html" class=" "><span class="title-number">3 </span><span class="title-name">Requirements</span></a></li><li><a href="cha-ha-geo-booth.html" class=" "><span class="title-number">4 </span><span class="title-name">Setting Up the Booth Services</span></a></li><li><a href="cha-ha-geo-sync.html" class=" "><span class="title-number">5 </span><span class="title-name">Synchronizing Configuration Files Across All Sites and Arbitrators</span></a></li><li><a href="cha-ha-geo-rsc.html" class=" "><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources and Constraints</span></a></li><li><a href="cha-ha-geo-ip-relocation.html" class=" "><span class="title-number">7 </span><span class="title-name">Setting Up IP Relocation via DNS Update</span></a></li><li><a href="cha-ha-geo-manage.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Geo Clusters</span></a></li><li><a href="cha-ha-geo-trouble.html" class=" "><span class="title-number">9 </span><span class="title-name">Troubleshooting</span></a></li><li><a href="cha-ha-geo-upgrade.html" class=" "><span class="title-number">10 </span><span class="title-name">Upgrading to the Latest Product Version</span></a></li><li><a href="sec-ha-geo-more.html" class=" "><span class="title-number">11 </span><span class="title-name">For More Information</span></a></li><li><a href="bk02apa.html" class=" "><span class="title-number">A </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ha-geo-concept" data-id-title="Conceptual Overview"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">12 SP5</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="cha-ha-geo-concept.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/geo_concept_i.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Geo clusters based on SUSE® Linux Enterprise High Availability can be considered
    <span class="quote">“<span class="quote">overlay</span>”</span> clusters where each cluster site corresponds to a
    cluster node in a traditional cluster. The overlay cluster is managed by
    the booth cluster ticket manager (in the following called booth).
   </p></div></div></div></div><p>
  Each of the parties involved in a Geo cluster runs a service, the <code class="systemitem">boothd</code>.
  It connects to the booth daemons running at the other sites and exchanges
  connectivity details. For making cluster resources highly available across
  sites, booth relies on cluster objects called tickets. A ticket grants the
  right to run certain resources on a specific cluster site. Booth guarantees
  that every ticket is granted to no more than one site at a time.
 </p><p>
  If the communication between two booth instances breaks down, it might be
  because of a network breakdown between the cluster sites
  <span class="emphasis"><em>or</em></span> because of
  an outage of one cluster site. In this case, you need an additional instance
  (a third cluster site or an
  <code class="literal">arbitrator</code>) to
  reach consensus about decisions (such as failover of resources across sites).
  Arbitrators are single machines (outside of the clusters) that run a booth
  instance in a special mode. Each Geo cluster can have one or multiple
  arbitrators.
 </p><p>
  The most common scenario probably is a Geo cluster with two sites and a
  single arbitrator on a third site. This requires three booth instances, see
  <a class="xref" href="cha-ha-geo-concept.html#fig-ha-geo-example-setup" title="Two-Site Cluster (2x2 Nodes + Arbitrator)">Figure 2.1, “Two-Site Cluster (2x2 Nodes + Arbitrator)”</a>.
 </p><div class="figure" id="fig-ha-geo-example-setup"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_geocluster.png"><img src="images/ha_geocluster.png" width="85%" alt="Two-Site Cluster (2x2 Nodes + Arbitrator)" title="Two-Site Cluster (2x2 Nodes + Arbitrator)"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 2.1: </span><span class="title-name">Two-Site Cluster (2x2 Nodes + Arbitrator) </span></span><a title="Permalink" class="permalink" href="cha-ha-geo-concept.html#fig-ha-geo-example-setup">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/geo_concept_i.xml" title="Edit source document"> </a></div></div></div><p>
  The following list explains the components and mechanisms for Geo clusters
  in more detail.
 </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-geo-components-arbitrator"><span class="term">Arbitrator</span></dt><dd><p>
     Each site runs one booth instance that is responsible for communicating
     with the other sites. If you have a setup with an even number of sites,
     you need an additional instance to reach consensus about decisions such as
     failover of resources across sites. In this case, add one or more
     arbitrators running at additional sites. Arbitrators are single machines
     that run a booth instance in a special mode. As all booth instances
     communicate with each other, arbitrators help to make more reliable
     decisions about granting or revoking tickets. Arbitrators cannot hold any
     tickets.
    </p><p>
     An arbitrator is especially important for a two-site scenario: For
     example, if site <code class="literal">A</code> can no longer communicate with site
     <code class="literal">B</code>, there are two possible causes for that:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A network failure between <code class="literal">A</code> and <code class="literal">B</code>.
      </p></li><li class="listitem"><p>
       Site <code class="literal">B</code> is down.
      </p></li></ul></div><p>
     However, if site <code class="literal">C</code> (the arbitrator) can still
     communicate with site <code class="literal">B</code>, site <code class="literal">B</code> must
     still be up and running.
    </p></dd><dt id="vle-ha-geo-components-booth"><span class="term">Booth Cluster Ticket Manager</span></dt><dd><p>
     Booth is the instance managing the ticket distribution, and thus, the
     failover process between the sites of a Geo cluster. Each of the
     participating clusters and arbitrators runs a service, the <code class="systemitem">boothd</code>. It
     connects to the booth daemons running at the other sites and exchanges
     connectivity details. After a ticket has been granted to a site, the booth
     mechanism can manage the ticket automatically: If the site that holds the
     ticket is out of service, the booth daemons will vote which of the other
     sites will get the ticket. To protect against brief connection failures,
     sites that lose the vote (either explicitly or implicitly by being
     disconnected from the voting body) need to relinquish the ticket after a
     time-out. Thus, it is made sure that a ticket will only be redistributed
     after it has been relinquished by the previous site. See also
     <a class="xref" href="cha-ha-geo-concept.html#vle-ha-geo-components-deadman">Dead Man Dependency (<code class="literal">loss-policy="fence"</code>)</a>.
    </p><p>
     For a Geo cluster with two sites and arbitrator, you need 3 booth
     instances: one instance per site plus the instance running on the
     arbitrator.
    </p><div id="id-1.4.4.8.2.2.3" data-id-title="Limited Number of Booth Instances" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Limited Number of Booth Instances</div><p>
      The upper limit is (currently) 16 booth instances.
     </p></div></dd><dt id="vle-ha-geo-components-deadman"><span class="term">Dead Man Dependency (<code class="literal">loss-policy="fence"</code>)</span></dt><dd><p>
     After a ticket is revoked, it can take a long time until all resources
     depending on that ticket are stopped, especially in case of cascaded
     resources. To cut that process short, the cluster administrator can
     configure a <code class="literal">loss-policy</code> (together with the ticket
     dependencies) for the case that a ticket gets revoked from a site. If the
     loss-policy is set to <code class="literal">fence</code>, the nodes that are hosting
     dependent resources are fenced.
    </p><div id="id-1.4.4.8.3.2.2" data-id-title="Potential Loss of Data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Potential Loss of Data</div><p>
      On the one hand, <code class="literal">loss-policy="fence"</code> considerably
      speeds up the recovery process of the cluster and makes sure that
      resources can be migrated more quickly.
     </p><p>
      On the other hand, it can lead to loss of all unwritten data, such as:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Data lying on shared storage (for example, DRBD).
       </p></li><li class="listitem"><p>
        Data in a replicating database (for example, MariaDB or PostgreSQL)
        that has not yet reached the other site, because of a slow network
        link.
       </p></li></ul></div></div></dd><dt id="vle-ha-geo-components-ticket"><span class="term">Ticket</span></dt><dd><p>
     A ticket grants the right to run certain resources on a specific cluster
     site. A ticket can only be owned by one site at a time. Initially, none of
     the sites has a ticket—each ticket must be granted once by the
     cluster administrator. After that, tickets are managed by the booth for
     automatic failover of resources. But administrators may also intervene and
     grant or revoke tickets manually.
    </p><p>
     After a ticket is administratively revoked, it is not managed by booth
     anymore. For booth to start managing the ticket again, the ticket must be
     again granted to a site.
    </p><p>
     Resources can be bound to a certain ticket by dependencies. Only if the
     defined ticket is available at a site, the respective resources are
     started. Vice versa, if the ticket is removed, the resources depending on
     that ticket are automatically stopped.
    </p><p>
     The presence or absence of tickets for a site is stored in the CIB as a
     cluster status. With regard to a certain ticket, there are only two states
     for a site: <code class="literal">true</code> (the site has the ticket) or
     <code class="literal">false</code> (the site does not have the ticket). The absence
     of a certain ticket (during the initial state of the Geo cluster) is not
     treated differently from the situation after the ticket has been revoked.
     Both are reflected by the value <code class="literal">false</code>.
    </p><p>
     A ticket within an overlay cluster is similar to a resource in a
     traditional cluster. But in contrast to traditional clusters, tickets are
     the only type of resource in an overlay cluster. They are primitive
     resources that do not need to be configured or cloned.
    </p></dd><dt id="id-1.4.4.8.5"><span class="term">Ticket Failover</span></dt><dd><p>
     If the ticket gets lost, which means other booth instances do not hear
     from the ticket owner in a sufficiently long time, one of the remaining
     sites will acquire the ticket. This is what is called ticket failover. If
     the remaining members cannot form a majority, then the ticket cannot fail
     over.
    </p></dd></dl></div></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ha-geo-challenges.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 1 </span>Challenges for Geo Clusters</span></a> </div><div><a class="pagination-link next" href="cha-ha-geo-req.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 3 </span>Requirements</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>