<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 12 SP5 | Administration Guide | Cluster Logical Volume Manager (cLVM)</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Cluster Logical Volume Manager (cLVM) | SLE HA 12 SP5"/>
<meta name="description" content="When managing shared storage on a cluster, every node …"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="12 SP5"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 19. Cluster Logical Volume Manager (cLVM)"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 12 SP5"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Cluster Logical Volume Manager (cLVM) | SLE HA 12 SP5"/>
<meta property="og:description" content="When managing shared storage on a cluster, every node must be informed about changes that are done to the storage subsystem.…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Cluster Logical Volume Manager (cLVM) | SLE HA 12 SP5"/>
<meta name="twitter:description" content="When managing shared storage on a cluster, every node must be informed about changes that are done to the storage subsystem.…"/>
<link rel="prev" href="cha-ha-drbd.html" title="Chapter 18. DRBD"/><link rel="next" href="cha-ha-cluster-md.html" title="Chapter 20. Cluster Multi-device (Cluster MD)"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-storage.html">Storage and Data Replication</a><span> / </span><a class="crumb" href="cha-ha-clvm.html">Cluster Logical Volume Manager (cLVM)</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="pre-ha.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-install.html" class="has-children "><span class="title-number">I </span><span class="title-name">Installation and Setup</span></a><ol><li><a href="cha-ha-concepts.html" class=" "><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></li><li><a href="cha-ha-requirements.html" class=" "><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></li><li><a href="cha-ha-install.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></li><li><a href="cha-ha-setup.html" class=" "><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></li></ol></li><li><a href="part-config.html" class="has-children "><span class="title-number">II </span><span class="title-name">Configuration and Administration</span></a><ol><li><a href="cha-ha-config-basics.html" class=" "><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></li><li><a href="cha-conf-hawk2.html" class=" "><span class="title-number">6 </span><span class="title-name">Configuring and Managing Cluster Resources with Hawk2</span></a></li><li><a href="cha-ha-manual-config.html" class=" "><span class="title-number">7 </span><span class="title-name">Configuring and Managing Cluster Resources (Command Line)</span></a></li><li><a href="cha-ha-agents.html" class=" "><span class="title-number">8 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></li><li><a href="cha-ha-fencing.html" class=" "><span class="title-number">9 </span><span class="title-name">Fencing and STONITH</span></a></li><li><a href="cha-ha-storage-protect.html" class=" "><span class="title-number">10 </span><span class="title-name">Storage Protection and SBD</span></a></li><li><a href="cha-ha-acl.html" class=" "><span class="title-number">11 </span><span class="title-name">Access Control Lists</span></a></li><li><a href="cha-ha-netbonding.html" class=" "><span class="title-number">12 </span><span class="title-name">Network Device Bonding</span></a></li><li><a href="cha-ha-lb.html" class=" "><span class="title-number">13 </span><span class="title-name">Load Balancing</span></a></li><li><a href="cha-ha-geo.html" class=" "><span class="title-number">14 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></li></ol></li><li class="active"><a href="part-storage.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Storage and Data Replication</span></a><ol><li><a href="cha-ha-storage-dlm.html" class=" "><span class="title-number">15 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></li><li><a href="cha-ha-ocfs2.html" class=" "><span class="title-number">16 </span><span class="title-name">OCFS2</span></a></li><li><a href="cha-ha-gfs2.html" class=" "><span class="title-number">17 </span><span class="title-name">GFS2</span></a></li><li><a href="cha-ha-drbd.html" class=" "><span class="title-number">18 </span><span class="title-name">DRBD</span></a></li><li><a href="cha-ha-clvm.html" class=" you-are-here"><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></a></li><li><a href="cha-ha-cluster-md.html" class=" "><span class="title-number">20 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></li><li><a href="cha-ha-samba.html" class=" "><span class="title-number">21 </span><span class="title-name">Samba Clustering</span></a></li><li><a href="cha-ha-rear.html" class=" "><span class="title-number">22 </span><span class="title-name">Disaster Recovery with Relax-and-Recover (ReaR)</span></a></li></ol></li><li><a href="part-maintenance.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Maintenance and Upgrade</span></a><ol><li><a href="cha-ha-maintenance.html" class=" "><span class="title-number">23 </span><span class="title-name">Executing Maintenance Tasks</span></a></li><li><a href="cha-ha-migration.html" class=" "><span class="title-number">24 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></li></ol></li><li><a href="part-appendix.html" class="has-children "><span class="title-number">V </span><span class="title-name">Appendix</span></a><ol><li><a href="app-ha-troubleshooting.html" class=" "><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></li><li><a href="app-naming.html" class=" "><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></li><li><a href="app-ha-management.html" class=" "><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></li><li><a href="app-crmreport-nonroot.html" class=" "><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></li></ol></li><li><a href="gl-heartb.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="bk01ape.html" class=" "><span class="title-number">E </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ha-clvm" data-id-title="Cluster Logical Volume Manager (cLVM)"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">12 SP5</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    When managing shared storage on a cluster, every node must be informed
    about changes that are done to the storage subsystem. The Logical Volume
    Manager 2 (LVM2), which is widely used to manage local storage,
    has been extended to support transparent management of volume groups
    across the whole cluster. Clustered volume groups can be managed using
    the same commands as local storage.
   </p></div></div></div></div><section class="sect1" id="sec-ha-clvm-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Clustered LVM2 is coordinated with different tools:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.3.3.1"><span class="term">Distributed Lock Manager (DLM)</span></dt><dd><p> Coordinates disk access for cLVM and mediates metadata access
      through locking.</p></dd><dt id="id-1.3.5.7.3.3.2"><span class="term">Logical Volume Manager2 (LVM2)</span></dt><dd><p>
      Enables flexible distribution of one file system over several disks.
      LVM2 provides a virtual pool of disk space.
     </p></dd><dt id="id-1.3.5.7.3.3.3"><span class="term">Clustered Logical Volume Manager (cLVM)</span></dt><dd><p>
      Coordinates access to the LVM2 metadata so every node knows about
      changes. cLVM does not coordinate access to the shared data itself; to
      enable cLVM to do so, you must configure OCFS2 or other cluster-aware
      applications on top of the cLVM-managed storage.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-clvm-config" data-id-title="Configuration of cLVM"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.2 </span><span class="title-name">Configuration of cLVM</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Depending on your scenario it is possible to create a RAID 1
   device with cLVM with the following layers:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">LVM2. </span>
      This is a very flexible solution if you want to increase or decrease
      your file system size, add more physical storage, or create snapshots
      of your file systems. This method is described in
      <a class="xref" href="cha-ha-clvm.html#sec-ha-clvm-scenario-iscsi" title="19.2.3. Scenario: cLVM with iSCSI on SANs">Section 19.2.3, “Scenario: cLVM with iSCSI on SANs”</a>.
     </p></li><li class="listitem"><p><span class="formalpara-title">DRBD. </span>
      This solution only provides RAID 0 (striping) and
      RAID 1 (mirroring). The last method is described in
      <a class="xref" href="cha-ha-clvm.html#sec-ha-clvm-scenario-drbd" title="19.2.4. Scenario: cLVM With DRBD">Section 19.2.4, “Scenario: cLVM With DRBD”</a>.
     </p></li></ul></div><p>
   Make sure you have fulfilled the following prerequisites:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A shared storage device is available, such as provided by a Fibre
     Channel, FCoE, SCSI, iSCSI SAN, or DRBD*.
    </p></li><li class="listitem"><p>
     In case of DRBD, both nodes must be primary (as described in the
     following procedure).
    </p></li><li class="listitem"><p>
     Check if the locking type of LVM2 is cluster-aware. The keyword
     <code class="literal">locking_type</code> in
     <code class="filename">/etc/lvm/lvm.conf</code> must contain the value
     <code class="literal">3</code> (the default is <code class="literal">1</code>). Copy the configuration to all nodes, if necessary.
    </p></li><li class="listitem"><p>
     Check if the <code class="systemitem">lvmetad</code> daemon is
     disabled, because it cannot work with cLVM. In <code class="filename">/etc/lvm/lvm.conf</code>,
     the keyword <code class="literal">use_lvmetad</code> must be set to <code class="literal">0</code>
     (the default is <code class="literal">1</code>).
     Copy the configuration to all nodes, if necessary.
    </p></li></ul></div><section class="sect2" id="sec-ha-clvm-config-resources" data-id-title="Creating the Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.1 </span><span class="title-name">Creating the Cluster Resources</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-config-resources">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Preparing the cluster for use of cLVM includes the following basic
    steps:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-dlmresource" title="Creating a DLM Resource">Creating a DLM Resource</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-config-cmirrord" title="Configuring DLM, CLVM, and STONITH">Configuring DLM, CLVM, and STONITH</a>
     </p></li></ul></div><div class="procedure" id="pro-ha-clvm-dlmresource" data-id-title="Creating a DLM Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.1: </span><span class="title-name">Creating a DLM Resource </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-dlmresource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Check the current configuration of the cluster resources:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure show</pre></div></li><li class="step"><p>
      If you have already configured a DLM resource (and a corresponding
      base group and base clone), continue with
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-config-cmirrord" title="Configuring DLM, CLVM, and STONITH">Procedure 19.2, “Configuring DLM, CLVM, and STONITH”</a>.
     </p><p>
      Otherwise, configure a DLM resource and a corresponding base group and
      base clone as described in <a class="xref" href="cha-ha-storage-dlm.html#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 15.1, “Configuring a Base Group for DLM”</a>.
     </p></li><li class="step"><p>
      Leave the crm live configuration with <code class="command">exit</code>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-clvm-config-cmirrord" data-id-title="Scenario: Configuring Cmirrord"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.2 </span><span class="title-name">Scenario: Configuring Cmirrord</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-config-cmirrord">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To track mirror log information in a cluster, the
    <code class="systemitem">cmirrord</code> daemon is used. Cluster
    mirrors are not possible without this daemon running.
   </p><p> We assume that <code class="filename">/dev/sda</code> and
     <code class="filename">/dev/sdb</code> are the shared storage devices as with
    DRBD, iSCSI, and others. Replace these with your own device name(s), if
    necessary. Proceed as follows: </p><div class="procedure" id="pro-ha-clvm-config-cmirrord" data-id-title="Configuring DLM, CLVM, and STONITH"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.2: </span><span class="title-name">Configuring DLM, CLVM, and STONITH </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-config-cmirrord">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Create a cluster with at least two nodes as described in
       Installation and Setup Quick Start.</p></li><li class="step"><p>
      Configure your cluster to run <code class="command">dlm</code>,
      <code class="command">clvmd</code>, and STONITH:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> clvmd ocf:heartbeat:clvm \
        params with_cmirrord=1 \
        op stop interval=0 timeout=100 \
	       op start interval=0 timeout=90 \
	       op monitor interval=20 timeout=20
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dlm ocf:pacemaker:controld \
        op start timeout="90" \
        op stop timeout="100" \
        op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
        params pcmk_delay_max=30
<code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-storage dlm clvmd
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-storage g-storage \
        meta interleave="true" ordered=true</pre></div></li><li class="step"><p>
      Leave crmsh with <code class="command">exit</code> and commit your changes.
     </p></li></ol></div></div><p>Continue configuring your disks with <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-config-cmirrord-disks" title="Configuring the Disks for cLVM">Procedure 19.3</a>.
   </p><div class="procedure" id="pro-ha-clvm-config-cmirrord-disks" data-id-title="Configuring the Disks for cLVM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.3: </span><span class="title-name">Configuring the Disks for cLVM </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-config-cmirrord-disks">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a clustered volume group (VG):

     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sda /dev/sdb
<code class="prompt root"># </code><code class="command">vgcreate</code> -cy vg1 /dev/sda /dev/sdb</pre></div></li><li class="step"><p>
      Create a mirrored-log logical volume (LV) in your cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -n lv1 -m1 -l10%VG vg1 --mirrorlog mirrored</pre></div></li><li class="step"><p>
      Use <code class="command">lvs</code> to show the progress. If the percentage
      number has reached 100%, the mirrored disk is successfully
      synchronized.
     </p></li><li class="step" id="st-ha-clvm-config-cmirrord-test"><p>
      To test the clustered volume <code class="filename">/dev/vg1/lv1</code>, use the
      following steps:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Read or write to <code class="filename">/dev/vg1/lv1</code>.
       </p></li><li class="step"><p>
        Deactivate your LV with <code class="command">lvchange</code>
        <code class="option">-an</code>.
       </p></li><li class="step"><p>
        Activate your LV with <code class="command">lvchange</code>
        <code class="option">-ay</code>.
       </p></li><li class="step"><p>
        Use <code class="command">lvconvert</code> to convert a mirrored log to a disk
        log.
       </p></li></ol></li><li class="step"><p>
      Create a mirrored-log LV in another cluster VG. This is a different
      volume group from the previous one.
     </p></li></ol></div></div><p>
    The current cLVM can only handle one physical volume (PV) per mirror
    side. If one mirror is actually made up of several PVs that need to be
    concatenated or striped, <code class="command">lvcreate</code> does not understand
    this. For this reason, <code class="command">lvcreate</code> and
    <code class="systemitem">cmirrord</code> metadata needs to understand
    <span class="quote">“<span class="quote">grouping</span>”</span> of PVs into one side, effectively supporting
    RAID10.
   </p><p>
    To support RAID10 for <code class="systemitem">cmirrord</code>,
    use the following procedure (assuming that <code class="filename">/dev/sda</code>,
    <code class="filename">/dev/sdb</code>, <code class="filename">/dev/sdc</code>, and
    <code class="filename">/dev/sdd</code> are the shared storage devices):
   </p><div class="procedure" id="pro-ha-clvm-config-raid10"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a volume group (VG):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/sda /dev/sdb /dev/sdc /dev/sdd
  Physical volume "/dev/sda" successfully created
  Physical volume "/dev/sdb" successfully created
  Physical volume "/dev/sdc" successfully created
  Physical volume "/dev/sdd" successfully created
<code class="prompt root"># </code><code class="command">vgcreate</code> vgtest /dev/sda /dev/sdb /dev/sdc /dev/sdd
  Clustered volume group "vgtest" successfully created</pre></div></li><li class="step"><p>
      Open the file <code class="filename">/etc/lvm/lvm.conf</code> and go to the
      section <code class="literal">allocation</code>. Set the following line and save
      the file:
     </p><div class="verbatim-wrap"><pre class="screen">mirror_logs_require_separate_pvs = 1</pre></div></li><li class="step"><p>
      Add your tags to your PVs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvchange</code> --addtag @a /dev/sda /dev/sdb
<code class="prompt root"># </code><code class="command">pvchange</code> --addtag @b /dev/sdc /dev/sdd</pre></div><p>
      A tag is an unordered keyword or term assigned to the metadata of a
      storage object. Tagging allows you to classify collections of LVM2
      storage objects in ways that you find useful by attaching an unordered
      list of tags to their metadata.
     </p></li><li class="step"><p>
      List your tags:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvs</code> -o pv_name,vg_name,pv_tags /dev/sd{a,b,c,d}</pre></div><p>
      You should receive this output:
     </p><div class="verbatim-wrap"><pre class="screen">PV        VG   PV Tags
/dev/sda  vgtest   a
/dev/sdb  vgtest   a
/dev/sdc  vgtest   b
/dev/sdd  vgtest   b</pre></div></li></ol></div></div><p>
    If you need further information regarding LVM2, refer to the SUSE Linux Enterprise Server
    12 SP5 Storage Administration Guide: <a class="link" href="https://documentation.suse.com/sles-12/html/SLES-all/cha-lvm.html" target="_blank">https://documentation.suse.com/sles-12/html/SLES-all/cha-lvm.html</a>.
   </p></section><section class="sect2" id="sec-ha-clvm-scenario-iscsi" data-id-title="Scenario: cLVM with iSCSI on SANs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.3 </span><span class="title-name">Scenario: cLVM with iSCSI on SANs</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-scenario-iscsi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenario uses two SAN boxes which export their iSCSI
    targets to several clients. The general idea is displayed in
    <a class="xref" href="cha-ha-clvm.html#fig-ha-clvm-scenario-iscsi" title="Setup of iSCSI with cLVM">Figure 19.1, “Setup of iSCSI with cLVM”</a>.
   </p><div class="figure" id="fig-ha-clvm-scenario-iscsi"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_clvm.png"><img src="images/ha_clvm.png" width="45%" alt="Setup of iSCSI with cLVM" title="Setup of iSCSI with cLVM"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 19.1: </span><span class="title-name">Setup of iSCSI with cLVM </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#fig-ha-clvm-scenario-iscsi">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div><div id="id-1.3.5.7.4.9.4" data-id-title="Data Loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Data Loss</div><p>
     The following procedures will destroy any data on your disks!
    </p></div><p>
    Configure only one SAN box first. Each SAN box needs to export its own
    iSCSI target. Proceed as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenario-iscsi-targets" data-id-title="Configuring iSCSI Targets (SAN)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.4: </span><span class="title-name">Configuring iSCSI Targets (SAN) </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-scenario-iscsi-targets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI LIO Target</span> to
      start the iSCSI Server module.
     </p></li><li class="step"><p>
      If you want to start the iSCSI target whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise choose
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      If you have a firewall running, enable <span class="guimenu">Open Port in
      Firewall</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Global</span> tab. If you need
      authentication enable incoming or outgoing authentication or both. In
      this example, we select <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Add a new iSCSI target:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Switch to the <span class="guimenu">Targets</span> tab.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step" id="st-ha-clvm-iscsi-iqn"><p>
        Enter a target name. The name needs to be formatted like this:
       </p><div class="verbatim-wrap"><pre class="screen">iqn.<em class="replaceable">DATE</em>.<em class="replaceable">DOMAIN</em></pre></div><p>
        For more information about the format, refer to <em class="citetitle">Section
        3.2.6.3.1. Type "iqn." (iSCSI Qualified Name) </em> at
        <a class="link" href="http://www.ietf.org/rfc/rfc3720.txt" target="_blank">http://www.ietf.org/rfc/rfc3720.txt</a>.
       </p></li><li class="step"><p>
        If you want a more descriptive name, you can change it as long as
        your identifier is unique for your different targets.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step"><p>
        Enter the device name in <span class="guimenu">Path</span> and use a
        <span class="guimenu">Scsiid</span>.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Next</span> twice.
       </p></li></ol></li><li class="step"><p>
      Confirm the warning box with <span class="guimenu">Yes</span>.
     </p></li><li class="step"><p>
      Open the configuration file <code class="filename">/etc/iscsi/iscsid.conf</code>
      and change the parameter <code class="literal">node.startup</code> to
      <code class="literal">automatic</code>.
     </p></li></ol></div></div><p>
    Now set up your iSCSI initiators as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-initiator" data-id-title="Configuring iSCSI Initiators"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.5: </span><span class="title-name">Configuring iSCSI Initiators </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-scenarios-iscsi-initiator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI Initiator</span>.
     </p></li><li class="step"><p>
      If you want to start the iSCSI initiator whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise set
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      Change to the <span class="guimenu">Discovery</span> tab and click the
      <span class="guimenu">Discovery</span> button.
     </p></li><li class="step"><p>
      Add your IP address and your port of your iSCSI target (see
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-scenario-iscsi-targets" title="Configuring iSCSI Targets (SAN)">Procedure 19.4, “Configuring iSCSI Targets (SAN)”</a>). Normally, you
      can leave the port as it is and use the default value.
     </p></li><li class="step"><p>
      If you use authentication, insert the incoming and outgoing user name
      and password, otherwise activate <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Select <span class="guimenu">Next</span>. The found connections are displayed in
      the list.
     </p></li><li class="step"><p>
      Proceed with <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>
      Open a shell, log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Test if the iSCSI initiator has been started successfully:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m discovery -t st -p 192.168.3.100
192.168.3.100:3260,1 iqn.2010-03.de.jupiter:san1</pre></div></li><li class="step"><p>
      Establish a session:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m node -l -p 192.168.3.100 -T iqn.2010-03.de.jupiter:san1
Logging in to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]
Login to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]: successful</pre></div><p>
      See the device names with <code class="command">lsscsi</code>:
     </p><div class="verbatim-wrap"><pre class="screen">...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</pre></div><p>
      Look for entries with <code class="literal">IET</code> in their third column. In
      this case, the devices are <code class="filename">/dev/sdd</code> and
      <code class="filename">/dev/sde</code>.
     </p></li></ol></div></div><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-lvm" data-id-title="Creating the LVM2 Volume Groups"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.6: </span><span class="title-name">Creating the LVM2 Volume Groups </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-scenarios-iscsi-lvm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a <code class="systemitem">root</code> shell on one of the nodes you have run the iSCSI
      initiator from
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-scenarios-iscsi-initiator" title="Configuring iSCSI Initiators">Procedure 19.5, “Configuring iSCSI Initiators”</a>.
     </p></li><li class="step"><p>
      Prepare the physical volume for LVM2 with the command
      <code class="command">pvcreate</code> on the disks <code class="filename">/dev/sdd</code>
      and <code class="filename">/dev/sde</code>, using their stable device names (for example, in
      <code class="filename">/dev/disk/by-id/</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em>
<code class="prompt root"># </code><code class="command">pvcreate</code> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em></pre></div></li><li class="step"><p>
      Create the cluster-aware volume group on both disks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate</code> --clustered y clustervg /dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em></pre></div></li><li class="step"><p>
      Create logical volumes as needed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -m1 --name clusterlv --size 500M clustervg</pre></div></li><li class="step"><p>
      Check the physical volume with <code class="command">pvdisplay</code>:
     </p><div class="verbatim-wrap"><pre class="screen">  --- Physical volume ---
      PV Name               /dev/sdd
      VG Name               clustervg
      PV Size               509,88 MB / not usable 1,88 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               52okH4-nv3z-2AUL-GhAN-8DAZ-GMtU-Xrn9Kh

      --- Physical volume ---
      PV Name               /dev/sde
      VG Name               clustervg
      PV Size               509,84 MB / not usable 1,84 MB
      Allocatable           yes
      PE Size (KByte)       4096
      Total PE              127
      Free PE               127
      Allocated PE          0
      PV UUID               Ouj3Xm-AI58-lxB1-mWm2-xn51-agM2-0UuHFC</pre></div></li><li class="step"><p>
      Check the volume group with <code class="command">vgdisplay</code>:
     </p><div class="verbatim-wrap"><pre class="screen">  --- Volume group ---
      VG Name               clustervg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      Clustered             yes
      Shared                no
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</pre></div></li></ol></div></div><p>
    After you have created the volumes and started your resources you should
    have a new device named
    <code class="filename">/dev/dm-<em class="replaceable">*</em></code>.

    It is recommended to use a clustered file system on top of your LVM2
    resource, for example OCFS. For more information, see
    <a class="xref" href="cha-ha-ocfs2.html" title="Chapter 16. OCFS2">Chapter 16, <em>OCFS2</em></a>.
   </p></section><section class="sect2" id="sec-ha-clvm-scenario-drbd" data-id-title="Scenario: cLVM With DRBD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">19.2.4 </span><span class="title-name">Scenario: cLVM With DRBD</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-scenario-drbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenarios can be used if you have data centers located in
    different parts of your city, country, or continent.
   </p><div class="procedure" id="pro-ha-clvm-withdrbd" data-id-title="Creating a Cluster-Aware Volume Group With DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.7: </span><span class="title-name">Creating a Cluster-Aware Volume Group With DRBD </span></span><a title="Permalink" class="permalink" href="cha-ha-clvm.html#pro-ha-clvm-withdrbd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primary/primary DRBD resource:
     </p><ol type="a" class="substeps"><li class="step"><p>
        First, set up a DRBD device as primary/secondary as described in
        <a class="xref" href="cha-ha-drbd.html#pro-drbd-configure" title="Manually Configuring DRBD">Procedure 18.1, “Manually Configuring DRBD”</a>. Make sure the disk state is
        <code class="literal">up-to-date</code> on both nodes. Check this with
        <code class="command">drbdadm status</code>.
       </p></li><li class="step"><p>
        Add the following options to your configuration file (usually
        something like <code class="filename">/etc/drbd.d/r0.res</code>):
       </p><div class="verbatim-wrap"><pre class="screen">resource r0 {
  net {
     allow-two-primaries;
  }
  ...
}</pre></div></li><li class="step"><p>
        Copy the changed configuration file to the other node, for example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.d/r0.res venus:/etc/drbd.d/</pre></div></li><li class="step"><p>
        Run the following commands on <span class="emphasis"><em>both</em></span> nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> disconnect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> connect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
        Check the status of your nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li></ol></li><li class="step"><p>
      Include the clvmd resource as a clone in the pacemaker configuration,
      and make it depend on the DLM clone resource. See
      <a class="xref" href="cha-ha-clvm.html#pro-ha-clvm-dlmresource" title="Creating a DLM Resource">Procedure 19.1, “Creating a DLM Resource”</a> for detailed instructions.
      Before proceeding, confirm that these resources have started
      successfully on your cluster. You may use <code class="command">crm status</code>
      or the Web interface to check the running services.
     </p></li><li class="step"><p>
      Prepare the physical volume for LVM2 with the command
      <code class="command">pvcreate</code>. For example, on the device
      <code class="filename">/dev/drbd_r0</code> the command would look like this:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create a cluster-aware volume group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate</code> --clustered y myclusterfs /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create logical volumes as needed. You may probably want to change the
      size of the logical volume. For example, create a 4 GB logical
      volume with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> -m1 --name testlv -L 4G myclusterfs</pre></div></li><li class="step"><p>
      
      The logical volumes within the VG are now available as file system
      mounts or raw usage. Ensure that services using them have proper
      dependencies to collocate them with and order them after the VG has
      been activated.
     </p></li></ol></div></div><p>
    After finishing these configuration steps, the LVM2 configuration can be
    done like on any stand-alone workstation.
   </p></section></section><section class="sect1" id="sec-ha-clvm-drbd" data-id-title="Configuring Eligible LVM2 Devices Explicitly"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.3 </span><span class="title-name">Configuring Eligible LVM2 Devices Explicitly</span></span> <a title="Permalink" class="permalink" href="cha-ha-clvm.html#sec-ha-clvm-drbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_clvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or DRBD), it is recommended to
   explicitly configure the devices which LVM2 scans for PVs.
  </p><p>
   For example, if the command <code class="command">vgcreate</code> uses the physical
   device instead of using the mirrored block device, DRBD will be confused
   which may result in a split brain condition for DRBD.
  </p><p>
   To deactivate a single device for LVM2, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit the file <code class="filename">/etc/lvm/lvm.conf</code> and search for the
     line starting with <code class="literal">filter</code>.
    </p></li><li class="step"><p>
     The patterns there are handled as regular expressions. A leading
     <span class="quote">“<span class="quote">a</span>”</span> means to accept a device pattern to the scan, a
     leading <span class="quote">“<span class="quote">r</span>”</span> rejects the devices that follow the device
     pattern.
    </p></li><li class="step"><p>
     To remove a device named <code class="filename">/dev/sdb1</code>, add the
     following expression to the filter rule:
    </p><div class="verbatim-wrap"><pre class="screen">"r|^/dev/sdb1$|"</pre></div><p>
     The complete filter line will look like the following:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|", "r|/dev/.*/by-id/.*|", "a/.*/" ]</pre></div><p>
     A filter line, that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/" ]</pre></div></li><li class="step"><p>
     Write the configuration file and copy it to all cluster nodes.
    </p></li></ol></div></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ha-drbd.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 18 </span>DRBD</span></a> </div><div><a class="pagination-link next" href="cha-ha-cluster-md.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 20 </span>Cluster Multi-device (Cluster MD)</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ha-clvm.html#sec-ha-clvm-overview"><span class="title-number">19.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="cha-ha-clvm.html#sec-ha-clvm-config"><span class="title-number">19.2 </span><span class="title-name">Configuration of cLVM</span></a></span></li><li><span class="sect1"><a href="cha-ha-clvm.html#sec-ha-clvm-drbd"><span class="title-number">19.3 </span><span class="title-name">Configuring Eligible LVM2 Devices Explicitly</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>