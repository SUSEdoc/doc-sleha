<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SLE HA 12 SP5 | Administration Guide | Configuration and Administration Basics</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Configuration and Administration Basics | SLE HA 12 SP5"/>
<meta name="description" content="The main purpose of an HA cluster is to manage user se…"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="12 SP5"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 5. Configuration and Administration Basics"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 12 SP5"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Configuration and Administration Basics | SLE HA 12 SP5"/>
<meta property="og:description" content="The main purpose of an HA cluster is to manage user services. Typical examples of user services are an Apache Web server or …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Configuration and Administration Basics | SLE HA 12 SP5"/>
<meta name="twitter:description" content="The main purpose of an HA cluster is to manage user services. Typical examples of user services are an Apache Web server or …"/>
<link rel="prev" href="part-config.html" title="Part II. Configuration and Administration"/><link rel="next" href="cha-conf-hawk2.html" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-config.html">Configuration and Administration</a><span> / </span><a class="crumb" href="cha-ha-config-basics.html">Configuration and Administration Basics</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="pre-ha.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-install.html" class="has-children "><span class="title-number">I </span><span class="title-name">Installation and Setup</span></a><ol><li><a href="cha-ha-concepts.html" class=" "><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></li><li><a href="cha-ha-requirements.html" class=" "><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></li><li><a href="cha-ha-install.html" class=" "><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></li><li><a href="cha-ha-setup.html" class=" "><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></li></ol></li><li class="active"><a href="part-config.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">Configuration and Administration</span></a><ol><li><a href="cha-ha-config-basics.html" class=" you-are-here"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></li><li><a href="cha-conf-hawk2.html" class=" "><span class="title-number">6 </span><span class="title-name">Configuring and Managing Cluster Resources with Hawk2</span></a></li><li><a href="cha-ha-manual-config.html" class=" "><span class="title-number">7 </span><span class="title-name">Configuring and Managing Cluster Resources (Command Line)</span></a></li><li><a href="cha-ha-agents.html" class=" "><span class="title-number">8 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></li><li><a href="cha-ha-fencing.html" class=" "><span class="title-number">9 </span><span class="title-name">Fencing and STONITH</span></a></li><li><a href="cha-ha-storage-protect.html" class=" "><span class="title-number">10 </span><span class="title-name">Storage Protection and SBD</span></a></li><li><a href="cha-ha-acl.html" class=" "><span class="title-number">11 </span><span class="title-name">Access Control Lists</span></a></li><li><a href="cha-ha-netbonding.html" class=" "><span class="title-number">12 </span><span class="title-name">Network Device Bonding</span></a></li><li><a href="cha-ha-lb.html" class=" "><span class="title-number">13 </span><span class="title-name">Load Balancing</span></a></li><li><a href="cha-ha-geo.html" class=" "><span class="title-number">14 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></li></ol></li><li><a href="part-storage.html" class="has-children "><span class="title-number">III </span><span class="title-name">Storage and Data Replication</span></a><ol><li><a href="cha-ha-storage-dlm.html" class=" "><span class="title-number">15 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></li><li><a href="cha-ha-ocfs2.html" class=" "><span class="title-number">16 </span><span class="title-name">OCFS2</span></a></li><li><a href="cha-ha-gfs2.html" class=" "><span class="title-number">17 </span><span class="title-name">GFS2</span></a></li><li><a href="cha-ha-drbd.html" class=" "><span class="title-number">18 </span><span class="title-name">DRBD</span></a></li><li><a href="cha-ha-clvm.html" class=" "><span class="title-number">19 </span><span class="title-name">Cluster Logical Volume Manager (cLVM)</span></a></li><li><a href="cha-ha-cluster-md.html" class=" "><span class="title-number">20 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></li><li><a href="cha-ha-samba.html" class=" "><span class="title-number">21 </span><span class="title-name">Samba Clustering</span></a></li><li><a href="cha-ha-rear.html" class=" "><span class="title-number">22 </span><span class="title-name">Disaster Recovery with Relax-and-Recover (ReaR)</span></a></li></ol></li><li><a href="part-maintenance.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Maintenance and Upgrade</span></a><ol><li><a href="cha-ha-maintenance.html" class=" "><span class="title-number">23 </span><span class="title-name">Executing Maintenance Tasks</span></a></li><li><a href="cha-ha-migration.html" class=" "><span class="title-number">24 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></li></ol></li><li><a href="part-appendix.html" class="has-children "><span class="title-number">V </span><span class="title-name">Appendix</span></a><ol><li><a href="app-ha-troubleshooting.html" class=" "><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></li><li><a href="app-naming.html" class=" "><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></li><li><a href="app-ha-management.html" class=" "><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></li><li><a href="app-crmreport-nonroot.html" class=" "><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></li></ol></li><li><a href="gl-heartb.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="bk01ape.html" class=" "><span class="title-number">E </span><span class="title-name">GNU licenses</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ha-config-basics" data-id-title="Configuration and Administration Basics"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">12 SP5</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped—the nature of the service is
    irrelevant to the cluster.
   </p><p>
    In this chapter, we will introduce some basic concepts you need to know
    when configuring resources and administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools SUSE Linux Enterprise High Availability
    provides.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-scenarios" data-id-title="Use Case Scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-scenarios">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>In general, clusters fall into one of two categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Two-node clusters</p></li><li class="listitem"><p>Clusters with more than two nodes. This usually means an odd number of nodes.</p></li></ul></div><p>
    Adding also different topologies, different use cases can be derived.
    The following use cases are the most common:
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name"> </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.3.4.3.3.5.2"><span class="term">Two-node cluster in one location</span></dt><dd><p><span class="formalpara-title">Configuration:</span>FC SAN or similar shared storage, layer 2 network.</p><p><span class="formalpara-title">Usage scenario:</span>Embedded clusters that focus on service high
       availability and not data redundancy for data replication.
       Such a setup is used for radio stations or assembly line controllers,
       for example.
       </p></dd><dt id="vl-2x2node-2locs"><span class="term">Two-node clusters in two locations (most widely used)</span></dt><dd><p><span class="formalpara-title">Configuration:</span>Symmetrical stretched cluster, FC SAN, and layer 2 network
        all across two locations.</p><p><span class="formalpara-title">Usage scenario:</span>Classic stretched clusters, focus on high availability of services
        and local data redundancy. For databases and enterprise
        resource planning. One of the most popular setups.
       </p></dd><dt id="vl-n-nodes-3locs"><span class="term">Odd number of nodes in three locations</span></dt><dd><p><span class="formalpara-title">Configuration:</span>2×N+1 nodes, FC SAN across two main locations. Auxiliary
        third site with no FC SAN, but acts as a majority maker.
        Layer 2 network at least across two main locations.
       </p><p><span class="formalpara-title">Usage scenario:</span>Classic stretched cluster, focus on high availability of services
        and data redundancy. For example, databases, enterprise resource planning.
       </p></dd></dl></div></section><section class="sect1" id="sec-ha-config-basics-global" data-id-title="Quorum Determination"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-global">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Whenever communication fails between one or more nodes and the rest of the
   cluster, a cluster partition occurs. The nodes can only communicate with
   other nodes in the same partition and are unaware of the separated nodes.
   A cluster partition is defined as having quorum (can <span class="quote">“<span class="quote">quorate</span>”</span>)
   if it has the majority of nodes (or votes).
   How this is achieved is done by <span class="emphasis"><em>quorum calculation</em></span>.
   Quorum is a requirement for fencing.
   </p><p>
   Quorum calculation has changed between SUSE Linux Enterprise High Availability 11 and
   SUSE Linux Enterprise High Availability 12. For SUSE Linux Enterprise High Availability 11, quorum was calculated by
   Pacemaker.
   Starting with SUSE Linux Enterprise High Availability 12, Corosync can handle quorum for
   two-node clusters directly without changing the Pacemaker configuration.
  </p><p>How quorum is calculated is influenced by the following factors:</p><div class="variablelist"><dl class="variablelist"><dt id="vl-ha-config-basics-global-number-of-cluster-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>To keep services running, a cluster with more than two nodes
       relies on quorum (majority vote) to resolve cluster partitions.
       Based on the following formula, you can calculate the minimum
       number of operational nodes required for the cluster to function:</p><div class="verbatim-wrap"><pre class="screen">N ≥ C/2 + 1

N = minimum number of operational nodes
C = number of cluster nodes</pre></div><p>For example, a five-node cluster needs a minimum of three operational
       nodes (or two nodes which can fail). </p><p>
       We strongly recommend to use either a two-node cluster or an odd number
       of cluster nodes.
       Two-node clusters make sense for stretched setups across two sites.
       Clusters with an odd number of nodes can be built on either one single
       site or might being spread across three sites.
      </p></dd><dt id="id-1.3.4.3.4.5.2"><span class="term">Corosync Configuration</span></dt><dd><p>Corosync is a messaging and membership layer, see
      <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-corosync-2-node" title="5.2.4. Corosync Configuration for Two-Node Clusters">Section 5.2.4, “Corosync Configuration for Two-Node Clusters”</a> and
       <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-corosync-n-node" title="5.2.5. Corosync Configuration for N-Node Clusters">Section 5.2.5, “Corosync Configuration for N-Node Clusters”</a>.
      </p></dd></dl></div><section class="sect2" id="sec-ha-config-basics-global-options" data-id-title="Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-global-options">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> Global cluster options control how the cluster behaves when
    confronted with certain situations. They are grouped into sets and can be
    viewed and modified with the cluster management tools like Hawk2 and
    the <code class="command">crm</code> shell. </p><p> The predefined values can usually be kept. However, to make key
    functions of your cluster work correctly, you need to adjust the
    following parameters after basic cluster setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-global-quorum" title="5.2.2. Global Option no-quorum-policy">Global Option <code class="literal">no-quorum-policy</code></a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-global-stonith" title="5.2.3. Global Option stonith-enabled">Global Option <code class="literal">stonith-enabled</code></a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-global-quorum" data-id-title="Global Option no-quorum-policy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">Global Option <code class="literal">no-quorum-policy</code></span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-global-quorum">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This global option defines what to do when a cluster partition does not
    have quorum (no majority of nodes is part of the partition).
   </p><p>
    Allowed values are:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.4.7.4.1"><span class="term"><code class="literal">ignore</code>
     </span></dt><dd><p>
       Setting <code class="literal">no-quorum-policy</code> to <code class="literal">ignore</code> makes
       the cluster behave like it has quorum. Resource management is
       continued.
      </p><p>
       On SLES 11 this was the recommended setting for a two-node cluster.
       Starting with SLES 12, this option is obsolete.
       Based on configuration and conditions, Corosync gives cluster nodes
       or a single node <span class="quote">“<span class="quote">quorum</span>”</span>—or not.
      </p><p>
      For two-node clusters the only meaningful behavior is to always
      react in case of quorum loss. The first step should always be
      to try to fence the lost node.
      </p></dd><dt id="id-1.3.4.3.4.7.4.2"><span class="term"><code class="literal">freeze</code>
     </span></dt><dd><p>
       If quorum is lost, the cluster partition freezes. Resource management
       is continued: running resources are not stopped (but possibly
       restarted in response to monitor events), but no further resources
       are started within the affected partition.
      </p><p>
       This setting is recommended for clusters where certain resources
       depend on communication with other nodes (for example, OCFS2 mounts).
       In this case, the default setting
       <code class="literal">no-quorum-policy=stop</code> is not useful, as it would
       lead to the following scenario: Stopping those resources would not be
       possible while the peer nodes are unreachable. Instead, an attempt to
       stop them would eventually time out and cause a <code class="literal">stop
       failure</code>, triggering escalated recovery and fencing.
      </p></dd><dt id="id-1.3.4.3.4.7.4.3"><span class="term"><code class="literal">stop</code> (default value)</span></dt><dd><p>
       If quorum is lost, all resources in the affected cluster partition
       are stopped in an orderly fashion.
      </p></dd><dt id="id-1.3.4.3.4.7.4.4"><span class="term"><code class="literal">suicide</code>
     </span></dt><dd><p>
       If quorum is lost, all nodes in the affected cluster partition are
       fenced. This option works only in combination with SBD, see
       <a class="xref" href="cha-ha-storage-protect.html" title="Chapter 10. Storage Protection and SBD">Chapter 10, <em>Storage Protection and SBD</em></a>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-global-stonith" data-id-title="Global Option stonith-enabled"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.3 </span><span class="title-name">Global Option <code class="literal">stonith-enabled</code></span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-global-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This global option defines whether to apply fencing, allowing STONITH
    devices to shoot failed nodes and nodes with resources that cannot be
    stopped. By default, this global option is set to
    <code class="literal">true</code>, because for normal cluster operation it is
    necessary to use STONITH devices. According to the default value,
    the cluster will refuse to start any resources if no STONITH
    resources have been defined.
   </p><p>
    If you need to disable fencing for any reasons, set
    <code class="literal">stonith-enabled</code> to <code class="literal">false</code>, but be
    aware that this has impact on the support status for your product.
    Furthermore, with <code class="literal">stonith-enabled="false"</code>, resources
    like the Distributed Lock Manager (DLM) and all services depending on
    DLM (such as cLVM, GFS2, and OCFS2) will fail to start.
   </p><div id="id-1.3.4.3.4.8.4" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><p>
     A cluster without STONITH is not supported.
    </p></div></section><section class="sect2" id="sec-ha-config-basics-corosync-2-node" data-id-title="Corosync Configuration for Two-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.4 </span><span class="title-name">Corosync Configuration for Two-Node Clusters</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-corosync-2-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When using the bootstrap scripts, the Corosync configuration contains
    a <code class="literal">quorum</code> section with the following options:
   </p><div class="example" id="ex-ha-config-basics-corosync-quorum" data-id-title="Excerpt of Corosync Configuration for a Two-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.1: </span><span class="title-name">Excerpt of Corosync Configuration for a Two-Node Cluster </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-ha-config-basics-corosync-quorum">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   # Enable and configure quorum subsystem (default: off)
   # see also corosync.conf.5 and votequorum.5
   provider: corosync_votequorum
   expected_votes: 2
   two_node: 1
}</pre></div></div></div><p>
    As opposed to SUSE Linux Enterprise 11, the votequorum subsystem in SUSE Linux Enterprise 12 and later
    is powered by Corosync version 2.x. This means that the
    <code class="literal">no-quorum-policy=ignore</code> option must not be used.
   </p><p>
    By default, when <code class="literal">two_node: 1</code> is set, the
    <code class="literal">wait_for_all</code> option is automatically enabled.
    If <code class="literal">wait_for_all</code> is not enabled, the cluster should be
    started on both nodes in parallel. Otherwise the first node will perform
    a startup-fencing on the missing second node.
   </p></section><section class="sect2" id="sec-ha-config-basics-corosync-n-node" data-id-title="Corosync Configuration for N-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.5 </span><span class="title-name">Corosync Configuration for N-Node Clusters</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-corosync-n-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> When not using a two-node cluster, we strongly recommend an odd
    number of nodes for your N-node cluster. With regard to quorum
    configuration, you have the following options: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Adding additional nodes with the <code class="command">ha-cluster-join</code>
     command, or</p></li><li class="listitem"><p>Adapting the Corosync configuration manually.</p></li></ul></div><p>
    If you adjust <code class="filename">/etc/corosync/corosync.conf</code> manually,
    use the following settings:
   </p><div class="example" id="id-1.3.4.3.4.10.5" data-id-title="Excerpt of Corosync Configuration for an N-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.2: </span><span class="title-name">Excerpt of Corosync Configuration for an N-Node Cluster </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.4.10.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   provider: corosync_votequorum <span class="callout" id="co-corosync-quorum-n-node-corosync-votequorum">1</span>
   expected_votes: <em class="replaceable">N</em> <span class="callout" id="co-corosync-quorum-n-node-expected-votes">2</span>
   wait_for_all: 1 <span class="callout" id="co-corosync-quorum-n-node-wait-for-all">3</span>
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-corosync-votequorum"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Use the quorum service from Corosync</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-expected-votes"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The number of votes to expect. This parameter can either be
       provided inside the <code class="literal">quorum</code> section, or is
       automatically calculated when the <code class="literal">nodelist</code>
       section is available.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-wait-for-all"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Enables the wait for all (WFA) feature.
       When WFA is enabled, the cluster will be quorate for the first time
       only after all nodes have become visible.
       To avoid some start-up race conditions, setting <code class="option">wait_for_all</code>
       to <code class="literal">1</code> may help.
       For example, in a five-node cluster every node has one vote and thus,
       <code class="option">expected_votes</code> is set to <code class="literal">5</code>.
       When three or more nodes are visible to each other, the cluster
       partition becomes quorate and can start operating.
      </p></td></tr></table></div></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources" data-id-title="Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Cluster Resources</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </p><section class="sect2" id="sec-ha-config-basics-resources-management" data-id-title="Resource Management"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Resource Management</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-management">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before you can use a resource in the cluster, it must be set up. For
    example, to use an Apache server as a cluster resource, set
    up the Apache server first and complete the Apache configuration before
    starting the respective resource in your cluster.
   </p><p>
    If a resource has specific environment requirements, make sure they are
    present and identical on all cluster nodes. This kind of configuration
    is not managed by the High Availability software. You must do this yourself.
   </p><div id="id-1.3.4.3.5.3.4" data-id-title="Do Not Touch Services Managed by the Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Do Not Touch Services Managed by the Cluster</div><p>
     When managing a resource with the SUSE Linux Enterprise High Availability, the same resource must not
     be started or stopped otherwise (outside of the cluster, for example
     manually or on boot or reboot). The High Availability software is responsible
     for all service start or stop actions.
    </p><p>
     If you need to execute testing or maintenance tasks after the services
     are already running under cluster control, make sure to put the
     resources, nodes, or the whole cluster into maintenance mode before you
     touch any of them manually. For details, see
     <a class="xref" href="cha-ha-maintenance.html#sec-ha-maint-overview" title="23.2. Different Options for Maintenance Tasks">Section 23.2, “Different Options for Maintenance Tasks”</a>.
    </p></div><p>
    After having configured the resources in the cluster, use the cluster
    management tools to start, stop, clean up, remove or migrate any
    resources manually. For details how to do so with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="cha-conf-hawk2.html" title="Chapter 6. Configuring and Managing Cluster Resources with Hawk2">Chapter 6, <em>Configuring and Managing Cluster Resources with Hawk2</em></a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html" title="Chapter 7. Configuring and Managing Cluster Resources (Command Line)">Chapter 7, <em>Configuring and Managing Cluster Resources (Command Line)</em></a>
     </p></li></ul></div><div id="id-1.3.4.3.5.3.7" data-id-title="Resource IDs and Node Names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Resource IDs and Node Names</div><p>Cluster resources and cluster nodes should be named differently.
     Otherwise Hawk2 will fail.</p></div></section><section class="sect2" id="sec-ha-config-basics-raclasses" data-id-title="Supported Resource Agent Classes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Supported Resource Agent Classes</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-raclasses">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For each cluster resource you add, you need to define the standard that
    the resource agent conforms to. Resource agents abstract the services
    they provide and present an accurate status to the cluster, which allows
    the cluster to be non-committal about the resources it manages. The
    cluster relies on the resource agent to react appropriately when given a
    start, stop or monitor command.
   </p><p>
    Typically, resource agents come in the form of shell scripts. SUSE Linux Enterprise High Availability
    supports the following classes of resource agents:
   </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-resources-ocf-ra"><span class="term">Open Cluster Framework (OCF) Resource Agents</span></dt><dd><p>
       OCF RA agents are best suited for use with High Availability, especially when
       you need multi-state resources or special monitoring abilities. The
       agents are generally located in
       <code class="filename">/usr/lib/ocf/resource.d/<em class="replaceable">provider</em>/</code>.
       Their functionality is similar to that of LSB scripts. However, the
       configuration is always done with environmental variables which allow
       them to accept and process parameters easily. The OCF specification
       (as it relates to resource agents) can be found at
       <a class="link" href="https://github.com/ClusterLabs/OCF-spec/blob/master/ra/1.0/resource-agent-api.md" target="_blank">https://github.com/ClusterLabs/OCF-spec/blob/master/ra/1.0/resource-agent-api.md</a>.
       OCF specifications have strict definitions of which exit codes must
       be returned by actions, see <a class="xref" href="cha-ha-agents.html#sec-ha-errorcodes" title="8.3. OCF Return Codes and Failure Recovery">Section 8.3, “OCF Return Codes and Failure Recovery”</a>. The
       cluster follows these specifications exactly.
      </p><p>
       All OCF Resource Agents are required to have at least the actions
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">status</code>, <code class="literal">monitor</code>, and
       <code class="literal">meta-data</code>. The <code class="literal">meta-data</code> action
       retrieves information about how to configure the agent. For example,
       if you want to know more about the <code class="literal">IPaddr</code> agent by
       the provider <code class="literal">heartbeat</code>, use the following command:
      </p><div class="verbatim-wrap"><pre class="screen">OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/IPaddr meta-data</pre></div><p>
       The output is information in XML format, including several sections
       (general description, available parameters, available actions for the
       agent).
      </p><p>
       Alternatively, use the crmsh to view information on OCF resource
       agents. For details, see <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-ocf" title="7.1.3. Displaying Information about OCF Resource Agents">Section 7.1.3, “Displaying Information about OCF Resource Agents”</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.2"><span class="term">Linux Standards Base (LSB) Scripts</span></dt><dd><p>
       LSB resource agents are generally provided by the operating
       system/distribution and are found in
       <code class="filename">/etc/init.d</code>. To be used with the cluster, they
       must conform to the LSB init script specification. For example, they
       must have several actions implemented, which are, at minimum,
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">restart</code>, <code class="literal">reload</code>,
       <code class="literal">force-reload</code>, and <code class="literal">status</code>. For
       more information, see
       <a class="link" href="http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html" target="_blank">http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html</a>.
      </p><p>
       The configuration of those services is not standardized. If you
       intend to use an LSB script with High Availability, make sure that you
       understand how the relevant script is configured. Often you can find
       information about this in the documentation of the relevant package
       in
       <code class="filename">/usr/share/doc/packages/<em class="replaceable">PACKAGENAME</em></code>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.3"><span class="term">Systemd</span></dt><dd><p>
       Starting with SUSE Linux Enterprise 12, systemd is a replacement for the popular
       System V init daemon. Pacemaker can manage systemd services if they
       are present. Instead of init scripts, systemd has unit files.
       Generally the services (or unit files) are provided by the operating
       system. In case you want to convert existing init scripts, find more
       information at
       <a class="link" href="http://0pointer.de/blog/projects/systemd-for-admins-3.html" target="_blank">http://0pointer.de/blog/projects/systemd-for-admins-3.html</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.4"><span class="term">Service</span></dt><dd><p>
       There are currently many <span class="quote">“<span class="quote">common</span>”</span> types of system
       services that exist in parallel: <code class="literal">LSB</code> (belonging to
       System V init), <code class="literal">systemd</code>, and (in some
       distributions) <code class="literal">upstart</code>. Therefore, Pacemaker
       supports a special alias which intelligently figures out which one
       applies to a given cluster node. This is particularly useful when the
       cluster contains a mix of systemd, upstart, and LSB services.
       Pacemaker will try to find the named service in the following order:
       as an LSB (SYS-V) init script, a systemd unit file, or an Upstart
       job.
      </p></dd><dt id="id-1.3.4.3.5.4.4.5"><span class="term">Nagios</span></dt><dd><p>
       Monitoring plug-ins (formerly called Nagios plug-ins) allow to
       monitor services on remote hosts. Pacemaker can do remote monitoring
       with the monitoring plug-ins if they are present. For detailed
       information, see
       <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-remote-nagios" title="5.6.1. Monitoring Services on Remote Hosts with Monitoring Plug-ins">Section 5.6.1, “Monitoring Services on Remote Hosts with Monitoring Plug-ins”</a>.
      </p></dd><dt id="id-1.3.4.3.5.4.4.6"><span class="term">STONITH (Fencing) Resource Agents</span></dt><dd><p>
       This class is used exclusively for fencing related resources. For
       more information, see <a class="xref" href="cha-ha-fencing.html" title="Chapter 9. Fencing and STONITH">Chapter 9, <em>Fencing and STONITH</em></a>.
      </p></dd></dl></div><p>
    The agents supplied with SUSE Linux Enterprise High Availability are written to OCF
    specifications.
   </p></section><section class="sect2" id="sec-ha-config-basics-resources-types" data-id-title="Types of Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.3 </span><span class="title-name">Types of Resources</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-types">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following types of resources can be created:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.5.3.1"><span class="term">Primitives</span></dt><dd><p>
       A primitive resource, the most basic type of resource.
      </p><p>
       Learn how to create primitive resources with your preferred cluster
       management tool:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>
        </p></li><li class="listitem"><p>
         crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-create" title="7.4.2. Creating Cluster Resources">Section 7.4.2, “Creating Cluster Resources”</a>
        </p></li></ul></div></dd><dt id="id-1.3.4.3.5.5.3.2"><span class="term">Groups</span></dt><dd><p>
       Groups contain a set of resources that need to be located together,
       started sequentially and stopped in the reverse order. For more
       information, refer to
       <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-groups" title="5.3.5.1. Groups">Section 5.3.5.1, “Groups”</a>.
      </p></dd><dt id="id-1.3.4.3.5.5.3.3"><span class="term">Clones</span></dt><dd><p>
       Clones are resources that can be active on multiple hosts. Any
       resource can be cloned, provided the respective resource agent
       supports it. For more information, refer to
       <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-clones" title="5.3.5.2. Clones">Section 5.3.5.2, “Clones”</a>.
      </p></dd><dt id="id-1.3.4.3.5.5.3.4"><span class="term">Multi-state Resources (formerly known as Master/Slave Resources)</span></dt><dd><p>
       Multi-state resources are a special type of clone resources that can
       have multiple modes. For more information, refer to
       <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-masters" title="5.3.5.3. Multi-state Resources">Section 5.3.5.3, “Multi-state Resources”</a>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-resources-templates" data-id-title="Resource Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.4 </span><span class="title-name">Resource Templates</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-templates">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to create lots of resources with similar configurations,
    defining a resource template is the easiest way. After having been
    defined, it can be referenced in primitives—or in certain types
    of constraints, as described in
    <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-templates" title="5.5.3. Resource Templates and Constraints">Section 5.5.3, “Resource Templates and Constraints”</a>.
   </p><p>
    If a template is referenced in a primitive, the primitive will inherit
    all operations, instance attributes (parameters), meta attributes, and
    utilization attributes defined in the template. Additionally, you can
    define specific operations or attributes for your primitive. If any of
    these are defined in both the template and the primitive, the values
    defined in the primitive will take precedence over the ones defined in
    the template.
   </p><p>
    Learn how to define resource templates with your preferred cluster
    configuration tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-template-add" title="Adding a Resource Template">Procedure 6.6, “Adding a Resource Template”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-rsc-template" title="7.4.3. Creating Resource Templates">Section 7.4.3, “Creating Resource Templates”</a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-resources-advanced" data-id-title="Advanced Resource Types"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.5 </span><span class="title-name">Advanced Resource Types</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Whereas primitives are the simplest kind of resources and therefore easy
    to configure, you will probably also need more advanced resource types
    for cluster configuration, such as groups, clones or multi-state
    resources.
   </p><section class="sect3" id="sec-ha-config-basics-resources-advanced-groups" data-id-title="Groups"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.1 </span><span class="title-name">Groups</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-groups">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Some cluster resources depend on other components or resources. They
     require that each component or resource starts in a specific order and
     runs together on the same server with resources it depends on. To
     simplify this configuration, you can use cluster resource groups.
    </p><div class="complex-example"><div class="example" id="ex-ha-config-resource-group" data-id-title="Resource Group for a Web Server"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.3: </span><span class="title-name">Resource Group for a Web Server </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-ha-config-resource-group">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
      An example of a resource group would be a Web server that requires an
      IP address and a file system. In this case, each component is a
      separate resource that is combined into a cluster resource group. The
      resource group would run on one or more servers. In case of a software
      or hardware malfunction, the group would fail over to another server
      in the cluster, similar to an individual cluster resource.
     </p></div></div></div><div class="figure" id="id-1.3.4.3.5.7.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/webserver_groupresource_a.png"><img src="images/webserver_groupresource_a.png" width="63%" alt="Group Resource" title="Group Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.1: </span><span class="title-name">Group Resource </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.5.7.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div><p>
     Groups have the following properties:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.7.3.6.1"><span class="term">Starting and Stopping</span></dt><dd><p>
        Resources are started in the order they appear in and stopped in the
        reverse order.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.2"><span class="term">Dependency</span></dt><dd><p>
        If a resource in the group cannot run anywhere, then none of the
        resources located after that resource in the group is allowed to
        run.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.3"><span class="term">Contents</span></dt><dd><p>
        Groups may only contain a collection of primitive cluster resources.
        Groups must contain at least one resource, otherwise the
        configuration is not valid. To refer to the child of a group
        resource, use the child’s ID instead of the group’s ID.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.4"><span class="term">Constraints</span></dt><dd><p>
        Although it is possible to reference the group’s children in
        constraints, it is usually preferable to use the group’s name
        instead.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.5"><span class="term">Stickiness</span></dt><dd><p>
        Stickiness is additive in groups. Every <span class="emphasis"><em>active</em></span>
        member of the group will contribute its stickiness value to the
        group’s total. So if the default
        <code class="literal">resource-stickiness</code> is <code class="literal">100</code> and
        a group has seven members (ﬁve of which are active), the group as
        a whole will prefer its current location with a score of
        <code class="literal">500</code>.
       </p></dd><dt id="id-1.3.4.3.5.7.3.6.6"><span class="term">Resource Monitoring</span></dt><dd><p>
        To enable resource monitoring for a group, you must configure
        monitoring separately for each resource in the group that you want
        monitored.
       </p></dd></dl></div><p>
     Learn how to create groups with your preferred cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-group" title="Adding a Resource Group">Procedure 6.9, “Adding a Resource Group”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-group" title="7.4.10. Configuring a Cluster Resource Group">Section 7.4.10, “Configuring a Cluster Resource Group”</a>
      </p></li></ul></div></section><section class="sect3" id="sec-ha-config-basics-resources-advanced-clones" data-id-title="Clones"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.2 </span><span class="title-name">Clones</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-clones">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     You may want certain resources to run simultaneously on multiple nodes
     in your cluster. To do this you must configure a resource as a clone.
     Examples of resources that might be configured as clones include
     cluster file systems like OCFS2. You can clone any
     resource provided. This is supported by the resource’s Resource
     Agent. Clone resources may even be configured differently depending on
     which nodes they are hosted.
    </p><p>
     There are three types of resource clones:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.7.4.4.1"><span class="term">Anonymous Clones</span></dt><dd><p>
        These are the simplest type of clones. They behave identically
        anywhere they are running. Because of this, there can only be one
        instance of an anonymous clone active per machine.
       </p></dd><dt id="id-1.3.4.3.5.7.4.4.2"><span class="term">Globally Unique Clones</span></dt><dd><p>
        These resources are distinct entities. An instance of the clone
        running on one node is not equivalent to another instance on another
        node; nor would any two instances on the same node be equivalent.
       </p></dd><dt id="id-1.3.4.3.5.7.4.4.3"><span class="term">Stateful Clones (Multi-state Resources)</span></dt><dd><p>
        Active instances of these resources are divided into two states,
        active and passive. These are also sometimes called primary and
        secondary, or master and slave. Stateful clones can be either
        anonymous or globally unique. See also
        <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-masters" title="5.3.5.3. Multi-state Resources">Section 5.3.5.3, “Multi-state Resources”</a>.
       </p></dd></dl></div><p>
     Clones must contain exactly one group or one regular resource.
    </p><p>
     When configuring resource monitoring or constraints, clones have
     different requirements than simple resources. For details, see
      <em class="citetitle">Pacemaker Explained</em>, available from <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to section
     <em class="citetitle">Clones - Resources That Get Active on Multiple
     Hosts</em>.
    </p><p>
     Learn how to create clones with your preferred cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-clone" title="Adding a Clone Resource">Procedure 6.10, “Adding a Clone Resource”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-clone" title="7.4.11. Configuring a Clone Resource">Section 7.4.11, “Configuring a Clone Resource”</a>.
      </p></li></ul></div></section><section class="sect3" id="sec-ha-config-basics-resources-advanced-masters" data-id-title="Multi-state Resources"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.3.5.3 </span><span class="title-name">Multi-state Resources</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-advanced-masters">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Multi-state resources are a specialization of clones. They allow the
     instances to be in one of two operating modes (called
     <code class="literal">master</code> or <code class="literal">slave</code>, but can mean
     whatever you want them to mean). Multi-state resources must contain
     exactly one group or one regular resource.
    </p><p>
     When configuring resource monitoring or constraints, multi-state
     resources have different requirements than simple resources. For
     details, see  <em class="citetitle">Pacemaker Explained</em>, available from <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>. Refer to
     section <em class="citetitle">Multi-state - Resources That Have Multiple
     Modes</em>.
    </p></section></section><section class="sect2" id="sec-ha-config-basics-meta-attr" data-id-title="Resource Options (Meta Attributes)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.6 </span><span class="title-name">Resource Options (Meta Attributes)</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-meta-attr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For each resource you add, you can define options. Options are used by
    the cluster to decide how your resource should behave—they tell
    the CRM how to treat a specific resource. Resource options can be set
    with the <code class="command">crm_resource --meta</code> command or with
    Hawk2 as described in
    <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>.
   </p><p>
    The following resource options are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.8.4.1"><span class="term"><code class="literal">priority</code></span></dt><dd><p>
       If not all resources can be active, the cluster stops lower
       priority resources to keep higher priority resources active.
      </p><p>
       The default value is <code class="literal">0</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.2"><span class="term"><code class="literal">target-role</code></span></dt><dd><p>
       In what state should the cluster attempt to keep this resource?
       Allowed values: <code class="literal">Stopped</code>, <code class="literal">Started</code>,
       <code class="literal">Unpromoted</code>, <code class="literal">Promoted</code>.
      </p><p>
       The default value is <code class="literal">Started</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.3"><span class="term"><code class="literal">is-managed</code></span></dt><dd><p>
       Is the cluster allowed to start and stop the resource? Allowed
       values: <code class="literal">true</code>, <code class="literal">false</code>. If the
       value is set to <code class="literal">false</code>, the status of the
       resource is still monitored and any failures are reported. This is
       different from setting a resource to
       <code class="literal">maintenance="true"</code>.
      </p><p>
       The default value is <code class="literal">true</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.4"><span class="term"><code class="literal">maintenance</code></span></dt><dd><p>
       Can the resources be touched manually? Allowed values:
       <code class="literal">true</code>, <code class="literal">false</code>. If set to
       <code class="literal">true</code>, all resources become unmanaged: the
       cluster stops monitoring them and does not know their
       status. You can stop or restart cluster resources without
       the cluster attempting to restart them.
      </p><p>
       The default value is <code class="literal">false</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.5"><span class="term"><code class="literal">resource-stickiness</code></span></dt><dd><p>
       How much does the resource prefer to stay where it is?
      </p><p>
       The default value is <code class="literal">1</code> for individual clone instances,
       and <code class="literal">0</code> for all other resources.
      </p></dd><dt id="id-1.3.4.3.5.8.4.6"><span class="term"><code class="literal">migration-threshold</code></span></dt><dd><p>
       How many failures should occur for this resource on a node before
       making the node ineligible to host this resource?
      </p><p>
       The default value is <code class="literal">INFINITY</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.7"><span class="term"><code class="literal">multiple-active</code></span></dt><dd><p>
       What should the cluster do if it ever finds the resource active on
       more than one node? Allowed values: <code class="literal">block</code> (mark
       the resource as unmanaged), <code class="literal">stop_only</code>,
       <code class="literal">stop_start</code>.
      </p><p>
       The default value is <code class="literal">stop_start</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.8"><span class="term"><code class="literal">failure-timeout</code></span></dt><dd><p>
       How many seconds to wait before acting as if the failure did not
       occur (and potentially allowing the resource back to the node on
       which it failed)?
      </p><p>
       The default value is <code class="literal">0</code> (disabled).
      </p></dd><dt id="id-1.3.4.3.5.8.4.9"><span class="term"><code class="literal">allow-migrate</code></span></dt><dd><p>
       Whether to allow live migration for resources that support
       <code class="literal">migrate_to</code> and <code class="literal">migrate_from</code>
       actions. If the value is set to <code class="literal">true</code>, the resource can
       be migrated without loss of state. If the value is set to <code class="literal">false</code>,
       the resource will be shut down on the first node and restarted on the second node.
      </p><p>
       The default value is <code class="literal">true</code> for
       <code class="literal">ocf:pacemaker:remote</code> resources, and
       <code class="literal">false</code> for all other resources.
      </p></dd><dt id="id-1.3.4.3.5.8.4.10"><span class="term"><code class="literal">remote-node</code></span></dt><dd><p>
       The name of the remote node this resource defines. This both
       enables the resource as a remote node and defines the unique name
       used to identify the remote node. If no other parameters are set,
       this value is also assumed as the host name to connect to at the
       <code class="varname">remote-port</code> port.
      </p><p>
       This option is disabled by default.
      </p><div id="id-1.3.4.3.5.8.4.10.2.3" data-id-title="Use unique IDs" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Use unique IDs</div><p>
        This value must not overlap with any existing resource or node IDs.
       </p></div></dd><dt id="id-1.3.4.3.5.8.4.11"><span class="term"><code class="literal">remote-port</code></span></dt><dd><p>
       Custom port for the guest connection to pacemaker_remote.
      </p><p>
       The default value is <code class="literal">3121</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.12"><span class="term"><code class="literal">remote-addr</code></span></dt><dd><p>
       The IP address or host name to connect to if the remote node's
       name is not the host name of the guest.
      </p><p>
       The default value is the value set by <code class="literal">remote-node</code>.
      </p></dd><dt id="id-1.3.4.3.5.8.4.13"><span class="term"><code class="literal">remote-connect-timeout</code></span></dt><dd><p>
       How long before a pending guest connection times out?
      </p><p>
       The default value is <code class="literal">60s</code>.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-inst-attr" data-id-title="Instance Attributes (Parameters)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.7 </span><span class="title-name">Instance Attributes (Parameters)</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-inst-attr">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The scripts of all resource classes can be given parameters which
    determine how they behave and which instance of a service they control.
    If your resource agent supports parameters, you can add them with the
    <code class="command">crm_resource</code> command or with

    Hawk2 as described in
    <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.5, “Adding a Primitive Resource”</a>. In the
    <code class="command">crm</code> command line utility and in Hawk2, instance
    attributes are called <code class="literal">params</code> or
    <code class="literal">Parameter</code>, respectively. The list of instance
    attributes supported by an OCF script can be found by executing the
    following command as <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info <em class="replaceable">[class:[provider:]]resource_agent</em></pre></div><p>
    or (without the optional parts):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info <em class="replaceable">resource_agent</em></pre></div><p>
    The output lists all the supported attributes, their purpose and default
    values.
   </p><p>
    For example, the command
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info IPaddr</pre></div><p>
    returns the following output:
   </p><div class="verbatim-wrap"><pre class="screen">Manages virtual IPv4 addresses (portable version) (ocf:heartbeat:IPaddr)

This script manages IP alias IP addresses
It can add an IP alias, or remove one.

Parameters (* denotes required, [] the default):

ip* (string): IPv4 address
The IPv4 address to be configured in dotted quad notation, for example
"192.168.1.1".

nic (string, [eth0]): Network interface
The base network interface on which the IP address will be brought
online.

If left empty, the script will try and determine this from the
routing table.

Do NOT specify an alias interface in the form eth0:1 or anything here;
rather, specify the base interface only.

cidr_netmask (string): Netmask
The netmask for the interface in CIDR format. (ie, 24), or in
dotted quad notation  255.255.255.0).

If unspecified, the script will also try to determine this from the
routing table.

broadcast (string): Broadcast address
Broadcast address associated with the IP. If left empty, the script will
determine this from the netmask.

iflabel (string): Interface label
You can specify an additional label for your IP address here.

lvs_support (boolean, [false]): Enable support for LVS DR
Enable support for LVS Direct Routing configurations. In case a IP
address is stopped, only move it to the loopback device to allow the
local node to continue to service requests, but no longer advertise it
on the network.

local_stop_script (string):
Script called when the IP is released

local_start_script (string):
Script called when the IP is added

ARP_INTERVAL_MS (integer, [500]): milliseconds between gratuitous ARPs
milliseconds between ARPs

ARP_REPEAT (integer, [10]): repeat count
How many gratuitous ARPs to send out when bringing up a new address

ARP_BACKGROUND (boolean, [yes]): run in background
run in background (no longer any reason to do this)

ARP_NETMASK (string, [ffffffffffff]): netmask for ARP
netmask for ARP - in nonstandard hexadecimal format.

Operations' defaults (advisory minimum):

start         timeout=90
stop          timeout=100
monitor_0     interval=5s timeout=20s</pre></div><div id="id-1.3.4.3.5.9.11" data-id-title="Instance Attributes for Groups, Clones or Multi-state Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Instance Attributes for Groups, Clones or Multi-state Resources</div><p>
     Note that groups, clones and multi-state resources do not have instance
     attributes. However, any instance attributes set will be inherited by
     the group's, clone's or multi-state resource's children.
    </p></div></section><section class="sect2" id="sec-ha-config-basics-operations" data-id-title="Resource Operations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.8 </span><span class="title-name">Resource Operations</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-operations">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    By default, the cluster will not ensure that your resources are still
    healthy. To instruct the cluster to do this, you need to add a monitor
    operation to the resource’s definition. Monitor operations can be
    added for all classes or resource agents. For more information, refer to
    <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-monitoring" title="5.4. Resource Monitoring">Section 5.4, “Resource Monitoring”</a>.
   </p><div class="table" id="id-1.3.4.3.5.10.3" data-id-title="Resource Operation Properties"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.1: </span><span class="title-name">Resource Operation Properties </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.5.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Operation
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         Description
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">id</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Your name for the action. Must be unique. (The ID is not shown).
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">name</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The action to perform. Common values: <code class="literal">monitor</code>,
         <code class="literal">start</code>, <code class="literal">stop</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">interval</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         How frequently to perform the operation. Unit: seconds
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">timeout</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         How long to wait before declaring the action has failed.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">requires</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         What conditions need to be satisfied before this action occurs.
         Allowed values: <code class="literal">nothing</code>,
         <code class="literal">quorum</code>, <code class="literal">fencing</code>. The default
         depends on whether fencing is enabled and if the resource’s class
         is <code class="literal">stonith</code>. For STONITH resources, the
         default is <code class="literal">nothing</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">on-fail</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         The action to take if this action ever fails. Allowed values:
        </p>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           <code class="literal">ignore</code>: Pretend the resource did not fail.
          </p></li><li class="listitem"><p>
           <code class="literal">block</code>: Do not perform any further operations
           on the resource.
          </p></li><li class="listitem"><p>
           <code class="literal">stop</code>: Stop the resource and do not start it
           elsewhere.
          </p></li><li class="listitem"><p>
           <code class="literal">restart</code>: Stop the resource and start it again
           (possibly on a different node).
          </p></li><li class="listitem"><p>
           <code class="literal">fence</code>: Bring down the node on which the
           resource failed (STONITH).
          </p></li><li class="listitem"><p>
           <code class="literal">standby</code>: Move <span class="emphasis"><em>all</em></span>
           resources away from the node on which the resource failed.
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">enabled</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         If <code class="literal">false</code>, the operation is treated as if it does
         not exist. Allowed values: <code class="literal">true</code>,
         <code class="literal">false</code>.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">role</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <p>
         Run the operation only if the resource has this role.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         <code class="literal">record-pending</code>
        </p>
       </td><td style="border-bottom: 1px solid ; ">

        <p>
         Can be set either globally or for individual resources. Makes the
         CIB reflect the state of <span class="quote">“<span class="quote">in-flight</span>”</span> operations on
         resources.
        </p>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>
         <code class="literal">description</code>
        </p>
       </td><td>
        <p>
         Description of the operation.
        </p>
       </td></tr></tbody></table></div></div></section><section class="sect2" id="sec-ha-config-basics-timeouts" data-id-title="Timeout Values"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.9 </span><span class="title-name">Timeout Values</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-timeouts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Timeouts values for resources can be influenced by the following
    parameters:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="varname">op_defaults</code> (global timeout for operations),
     </p></li><li class="listitem"><p>
      a specific timeout value defined in a resource template,
     </p></li><li class="listitem"><p>
      a specific timeout value defined for a resource.
     </p></li></ul></div><div id="id-1.3.4.3.5.11.4" data-id-title="Priority of Values" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Priority of Values</div><p>
     If a <span class="emphasis"><em>specific</em></span> value is defined for a resource, it
     takes precedence over the global default. A specific value for a
     resource also takes precedence over a value that is defined in a
     resource template.
    </p></div><p>
    Getting timeout values right is very important. Setting them too low
    will result in a lot of (unnecessary) fencing operations for the
    following reasons:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      If a resource runs into a timeout, it fails and the cluster will try
      to stop it.
     </p></li><li class="listitem"><p>
      If stopping the resource also fails (for example, because the timeout
      for stopping is set too low), the cluster will fence the node. It
      considers the node where this happens to be out of control.
     </p></li></ol></div><p>
    You can adjust the global default for operations and set any specific
    timeout values with both crmsh and Hawk2. The best practice for
    determining and setting timeout values is as follows:
   </p><div class="procedure" id="id-1.3.4.3.5.11.8" data-id-title="Determining Timeout Values"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Determining Timeout Values </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.5.11.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check how long it takes your resources to start and stop (under load).
     </p></li><li class="step"><p>
      If needed, add the <code class="varname">op_defaults</code> parameter and set
      the (default) timeout value accordingly:
     </p><ol type="a" class="substeps"><li class="step"><p>
        For example, set <code class="literal">op_defaults</code> to
        <code class="literal">60</code> seconds:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code> op_defaults timeout=60</pre></div></li><li class="step"><p>
        For resources that need longer periods of time, define individual
        timeout values.
       </p></li></ol></li><li class="step"><p>
      When configuring operations for a resource, add separate
      <code class="literal">start</code> and <code class="literal">stop</code> operations. When
      configuring operations with Hawk2, it will provide useful timeout
      proposals for those operations.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-monitoring" data-id-title="Resource Monitoring"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Resource Monitoring</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-monitoring">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to ensure that a resource is running, you must configure
   resource monitoring for it.
  </p><p>
   If the resource monitor detects a failure, the following takes place:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Log file messages are generated, according to the configuration
     specified in the <code class="literal">logging</code> section of
     <code class="filename">/etc/corosync/corosync.conf</code>.

    </p></li><li class="listitem"><p>
     The failure is reflected in the cluster management tools (Hawk2,
     <code class="command">crm status</code>), and in the CIB status section.
    </p></li><li class="listitem"><p>
     The cluster initiates noticeable recovery actions which may include
     stopping the resource to repair the failed state and restarting the
     resource locally or on another node. The resource also may not be
     restarted, depending on the configuration and state of the cluster.
    </p></li></ul></div><p>
   If you do not configure resource monitoring, resource failures after a
   successful start will not be communicated, and the cluster will always
   show the resource as healthy.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.6.6.1"><span class="term">Monitoring Stopped Resources</span></dt><dd><p>
      Usually, resources are only monitored by the cluster as long as they
      are running. However, to detect concurrency violations, also configure
      monitoring for resources which are stopped. For example:
     </p><div class="verbatim-wrap"><pre class="screen">primitive dummy1 Dummy \
    op monitor interval="300s" role="Stopped" timeout="10s" \
    op monitor interval="30s" timeout="10s"</pre></div><p>
      This configuration triggers a monitoring operation every
      <code class="literal">300</code> seconds for the resource
      <code class="literal">dummy1</code> when it is in
      <code class="literal">role="Stopped"</code>. When running, it will be monitored
      every <code class="literal">30</code> seconds.
     </p></dd><dt id="id-1.3.4.3.6.6.2"><span class="term">Probing</span></dt><dd><p>
      The CRM executes an initial monitoring for each resource on every
      node, the so-called <code class="literal">probe</code>. A probe is also executed
      after the cleanup of a resource. If multiple monitoring operations are
      defined for a resource, the CRM will select the one with the smallest
      interval and will use its timeout value as default timeout for
      probing. If no monitor operation is configured, the cluster-wide
      default applies. The default is <code class="literal">20</code> seconds (if not
      specified otherwise by configuring the <code class="varname">op_defaults</code>
      parameter). If you do not want to rely on the automatic calculation or
      the <code class="systemitem">op_defaults</code> value, define a specific
      monitoring operation for the <span class="emphasis"><em>probing</em></span> of this
      resource. Do so by adding a monitoring operation with the
      <code class="literal">interval</code> set to <code class="literal">0</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> rsc1 ocf:pacemaker:Dummy \
    op monitor interval="0" timeout="60"</pre></div><p>
      The probe of <code class="systemitem">rsc1</code> will time out in
      <code class="literal">60s</code>, independent of the global timeout defined in
      <code class="varname">op_defaults</code>, or any other operation timeouts
      configured. If you did not set <code class="literal">interval="0"</code> for
      specifying the probing of the respective resource, the CRM will
      automatically check for any other monitoring operations defined for
      that resource and will calculate the timeout value for probing as
      described above.
     </p></dd></dl></div><p>
   Learn how to add monitor operations to resources with your preferred
   cluster management tool:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-hawk2-operations" title="Adding and Modifying an Operation">Procedure 6.13, “Adding and Modifying an Operation”</a>
    </p></li><li class="listitem"><p>
     crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-monitor" title="7.4.9. Configuring Resource Monitoring">Section 7.4.9, “Configuring Resource Monitoring”</a>
    </p></li></ul></div></section><section class="sect1" id="sec-ha-config-basics-constraints" data-id-title="Resource Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.5 </span><span class="title-name">Resource Constraints</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Having all the resources configured is only part of the job. Even if the
   cluster knows all needed resources, it might still not be able to handle
   them correctly. Resource constraints let you specify which cluster nodes
   resources can run on, what order resources will load, and what other
   resources a specific resource is dependent on.
  </p><section class="sect2" id="sec-ha-config-basics-constraints-types" data-id-title="Types of Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.1 </span><span class="title-name">Types of Constraints</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-types">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three different kinds of constraints available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.3.3.1"><span class="term">Resource Location
    </span></dt><dd><p>
       Locational constraints that define on which nodes a resource may be
       run, may not be run or is preferred to be run.
      </p></dd><dt id="id-1.3.4.3.7.3.3.2"><span class="term">Resource Colocation</span></dt><dd><p>
       Colocational constraints that tell the cluster which resources may or
       may not run together on a node.
      </p></dd><dt id="id-1.3.4.3.7.3.3.3"><span class="term">Resource Order</span></dt><dd><p>
       Ordering constraints to define the sequence of actions.
      </p></dd></dl></div><div id="id-1.3.4.3.7.3.4" data-id-title="Restrictions for Constraints and Certain Types of Resources" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Restrictions for Constraints and Certain Types of Resources</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Do not create colocation constraints for <span class="emphasis"><em>members</em></span> of a resource
       group. Create a colocation constraint pointing to the resource group as a whole instead. All
       other types of constraints are safe to use for members of a resource group.</p></li><li class="listitem"><p>Do not use any constraints on a resource that has a clone resource or a multi-state
       resource applied to it. The constraints must apply to the clone or multi-state resource, not
       to the child resource.</p></li></ul></div></div><section class="sect3" id="sec-ha-config-basics-constraints-rscset" data-id-title="Resource Sets"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1 </span><span class="title-name">Resource Sets</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-rscset">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect4" id="sec-ha-config-basics-constraints-rscset-constraints" data-id-title="Using Resource Sets for Defining Constraints"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1.1 </span><span class="title-name">Using Resource Sets for Defining Constraints</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-rscset-constraints">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      As an alternative format for defining location, colocation or ordering
      constraints, you can use <code class="literal">resource sets</code>, where
      primitives are grouped together in one set. Previously this was
      possible either by defining a resource group (which could not always
      accurately express the design), or by defining each relationship as an
      individual constraint. The latter caused a constraint explosion as the
      number of resources and combinations grew. The configuration via
      resource sets is not necessarily less verbose, but is easier to
      understand and maintain, as the following examples show.
     </p><div class="complex-example"><div class="example" id="ex-config-basic-resourceset-loc" data-id-title="A Resource Set for Location Constraints"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.4: </span><span class="title-name">A Resource Set for Location Constraints </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-config-basic-resourceset-loc">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
       For example, you can use the following configuration of a resource
       set (<code class="varname">loc-alice</code>) in the crmsh to place
       two virtual IPs (<code class="varname">vip1</code> and <code class="varname">vip2</code>)
       on the same node, <code class="varname">alice</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip1 IPaddr2 params ip=192.168.1.5
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> vip2 IPaddr2 params ip=192.168.1.6
<code class="prompt custom">crm(live)configure# </code><code class="command">location</code> loc-alice { vip1 vip2 } inf: alice</pre></div></div></div></div><p>
      To use resource sets to replace a configuration of
      colocation constraints, consider the following two examples:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.5" data-id-title="A Chain of Colocated Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.5: </span><span class="title-name">A Chain of Colocated Resources </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.7.3.5.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_colocation id="coloc-1" rsc="B" with-rsc="A" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-2" rsc="C" with-rsc="B" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-3" rsc="D" with-rsc="C" score="INFINITY"/&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same configuration expressed by a resource set:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
    &lt;rsc_colocation id="coloc-1" score="INFINITY" &gt;
     &lt;resource_set id="colocated-set-example" sequential="true"&gt;
      &lt;resource_ref id="A"/&gt;
      &lt;resource_ref id="B"/&gt;
      &lt;resource_ref id="C"/&gt;
      &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
    &lt;/rsc_colocation&gt;
&lt;/constraints&gt;</pre></div><p>
      If you want to use resource sets to replace a configuration of
      ordering constraints, consider the following two examples:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.9" data-id-title="A Chain of Ordered Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.6: </span><span class="title-name">A Chain of Ordered Resources </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.7.3.5.3.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1" first="A" then="B" /&gt;
     &lt;rsc_order id="order-2" first="B" then="C" /&gt;
     &lt;rsc_order id="order-3" first="C" then="D" /&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same purpose can be achieved by using a resource set with ordered
      resources:
     </p><div class="example" id="id-1.3.4.3.7.3.5.3.11" data-id-title="A Chain of Ordered Resources Expressed as Resource Set"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.7: </span><span class="title-name">A Chain of Ordered Resources Expressed as Resource Set </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#id-1.3.4.3.7.3.5.3.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1"&gt;
     &lt;resource_set id="ordered-set-example" sequential="true"&gt;
     &lt;resource_ref id="A"/&gt;
     &lt;resource_ref id="B"/&gt;
     &lt;resource_ref id="C"/&gt;
     &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
     &lt;/rsc_order&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      Sets can be either ordered (<code class="literal">sequential=true</code>) or
      unordered (<code class="literal">sequential=false</code>). Furthermore, the
      <code class="literal">require-all</code> attribute can be used to switch between
      <code class="literal">AND</code> and <code class="literal">OR</code> logic.
     </p></section><section class="sect4" id="sec-ha-config-basics-constraints-rscset-constraints-dep" data-id-title="Resource Sets for Colocation Constraints Without Dependencies"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">5.5.1.1.2 </span><span class="title-name">Resource Sets for Colocation Constraints Without Dependencies</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-rscset-constraints-dep">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Sometimes it is useful to place a group of resources on the same node
      (defining a colocation constraint), but without having hard
      dependencies between the resources. For example, you want two
      resources to be placed on the same node, but you do
      <span class="emphasis"><em>not</em></span> want the cluster to restart the other one if
      one of them fails. This can be achieved on the crm shell by using
      the <code class="command">weak bond</code> command.
     </p><p>
      Learn how to set these <span class="quote">“<span class="quote">weak bonds</span>”</span> with your preferred
      cluster management tool:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        crmsh:
        <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-constraints-weak-bond" title="7.4.5.3. Collocating Sets for Resources Without Dependency">Section 7.4.5.3, “Collocating Sets for Resources Without Dependency”</a>
       </p></li></ul></div></section></section><section class="sect3" id="sec-ha-config-basics-constraints-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.1.2 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-more">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Learn how to add the various kinds of constraints with your preferred
     cluster management tool:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="cha-conf-hawk2.html#sec-conf-hawk2-cons" title="6.6. Configuring Constraints">Section 6.6, “Configuring Constraints”</a>
      </p></li><li class="listitem"><p>
       crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-constraints" title="7.4.5. Configuring Resource Constraints">Section 7.4.5, “Configuring Resource Constraints”</a>
      </p></li></ul></div><p>
     For more information on configuring constraints and detailed background
     information about the basic concepts of ordering and colocation, refer
     to the following documents. They are available at <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <em class="citetitle">Pacemaker Explained</em>, chapter <em class="citetitle">Resource Constraints</em>
      </p></li><li class="listitem"><p>
       <em class="citetitle">Colocation Explained</em>
      </p></li><li class="listitem"><p>
       <em class="citetitle">Ordering Explained</em>
      </p></li></ul></div></section></section><section class="sect2" id="sec-ha-config-basics-constraints-scores" data-id-title="Scores and Infinity"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.2 </span><span class="title-name">Scores and Infinity</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-scores">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When defining constraints, you also need to deal with scores. Scores of
    all kinds are integral to how the cluster works. Practically everything
    from migrating a resource to deciding which resource to stop in a
    degraded cluster is achieved by manipulating scores in some way. Scores
    are calculated on a per-resource basis and any node with a negative
    score for a resource cannot run that resource. After calculating the
    scores for a resource, the cluster then chooses the node with the
    highest score.
   </p><p>
    <code class="literal">INFINITY</code> is currently deﬁned as
    <code class="literal">1,000,000</code>. Additions or subtractions with it stick to
    the following three basic rules:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Any value + INFINITY = INFINITY
     </p></li><li class="listitem"><p>
      Any value - INFINITY = -INFINITY
     </p></li><li class="listitem"><p>
      INFINITY - INFINITY = -INFINITY
     </p></li></ul></div><p>
    When defining resource constraints, you specify a score for each
    constraint. The score indicates the value you are assigning to this
    resource constraint. Constraints with higher scores are applied before
    those with lower scores. By creating additional location constraints
    with different scores for a given resource, you can specify an order for
    the nodes that a resource will fail over to.
   </p></section><section class="sect2" id="sec-ha-config-basics-constraints-templates" data-id-title="Resource Templates and Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.3 </span><span class="title-name">Resource Templates and Constraints</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-constraints-templates">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you have defined a resource template (see
    <a class="xref" href="cha-ha-config-basics.html#sec-ha-config-basics-resources-templates" title="5.3.4. Resource Templates">Section 5.3.4, “Resource Templates”</a>), it can be
    referenced in the following types of constraints:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      order constraints,
     </p></li><li class="listitem"><p>
      colocation constraints,
     </p></li><li class="listitem"><p>
      rsc_ticket constraints (for Geo clusters).
     </p></li></ul></div><p>
    However, colocation constraints must not contain more than one reference
    to a template. Resource sets must not contain a reference to a template.
   </p><p>
    Resource templates referenced in constraints stand for all primitives
    which are derived from that template. This means, the constraint applies
    to all primitive resources referencing the resource template.
    Referencing resource templates in constraints is an alternative to
    resource sets and can simplify the cluster configuration considerably.
    For details about resource sets, refer to
    <a class="xref" href="cha-conf-hawk2.html#pro-hawk2-constraints-sets" title="Using a Resource Set for Constraints">Procedure 6.17, “Using a Resource Set for Constraints”</a>.
   </p></section><section class="sect2" id="sec-ha-config-basics-failover" data-id-title="Failover Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.4 </span><span class="title-name">Failover Nodes</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource will be automatically restarted if it fails. If that cannot
    be achieved on the current node, or it fails <code class="literal">N</code> times
    on the current node, it will try to fail over to another node. Each time
    the resource fails, its failcount is raised. You can define a number of
    failures for resources (a <code class="literal">migration-threshold</code>), after
    which they will migrate to a new node. If you have more than two nodes
    in your cluster, the node a particular resource fails over to is chosen
    by the High Availability software.
   </p><p>
    However, you can specify the node a resource will fail over to by
    configuring one or several location constraints and a
    <code class="literal">migration-threshold</code> for that resource.
   </p><p>
    Learn how to specify failover nodes with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="cha-conf-hawk2.html#sec-conf-hawk2-failover" title="6.6.6. Specifying Resource Failover Nodes">Section 6.6.6, “Specifying Resource Failover Nodes”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-failover" title="7.4.6. Specifying Resource Failover Nodes">Section 7.4.6, “Specifying Resource Failover Nodes”</a>
     </p></li></ul></div><div class="complex-example"><div class="example" id="ex-ha-config-basics-failover" data-id-title="Migration Threshold—Process Flow"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.8: </span><span class="title-name">Migration Threshold—Process Flow </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-ha-config-basics-failover">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     For example, let us assume you have configured a location constraint
     for resource <code class="literal">rsc1</code> to preferably run on
     <code class="literal">alice</code>. If it fails there,
     <code class="literal">migration-threshold</code> is checked and compared to the
     failcount. If failcount &gt;= migration-threshold then the resource is
     migrated to the node with the next best preference.
    </p><p>
     After the threshold has been reached, the node will no longer be
     allowed to run the failed resource until the resource's failcount is
     reset. This can be done manually by the cluster administrator or by
     setting a <code class="literal">failure-timeout</code> option for the resource.
    </p><p>
     For example, a setting of <code class="literal">migration-threshold=2</code> and
     <code class="literal">failure-timeout=60s</code> would cause the resource to
     migrate to a new node after two failures. It would be allowed to move
     back (depending on the stickiness and constraint scores) after one
     minute.
    </p></div></div></div><p>
    There are two exceptions to the migration threshold concept, occurring
    when a resource either fails to start or fails to stop:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Start failures set the failcount to <code class="literal">INFINITY</code> and
      thus always cause an immediate migration.
     </p></li><li class="listitem"><p>
      Stop failures cause fencing (when <code class="literal">stonith-enabled</code>
      is set to <code class="literal">true</code> which is the default).
     </p><p>
      In case there is no STONITH resource defined (or
      <code class="literal">stonith-enabled</code> is set to
      <code class="literal">false</code>), the resource will not migrate.
     </p></li></ul></div><p>
    For details on using migration thresholds and resetting failcounts with
    your preferred cluster management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="cha-conf-hawk2.html#sec-conf-hawk2-failover" title="6.6.6. Specifying Resource Failover Nodes">Section 6.6.6, “Specifying Resource Failover Nodes”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-failover" title="7.4.6. Specifying Resource Failover Nodes">Section 7.4.6, “Specifying Resource Failover Nodes”</a>
     </p></li></ul></div></section><section class="sect2" id="sec-ha-config-basics-failback" data-id-title="Failback Nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.5 </span><span class="title-name">Failback Nodes</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-failback">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource might fail back to its original node when that node is back
    online and in the cluster. To prevent a resource from
    failing back to the node that it was running on, or
    to specify a different node for the resource to fail back to,
    change its resource stickiness value. You can
    either specify resource stickiness when you are creating a resource or
    afterward.
   </p><p>
    Consider the following implications when specifying resource stickiness
    values:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.7.4.1"><span class="term">Value is <code class="literal">0</code>:</span></dt><dd><p>
       This is the default. The resource will be placed optimally in the
       system. This may mean that it is moved when a <span class="quote">“<span class="quote">better</span>”</span>
       or less loaded node becomes available. This option is almost
       equivalent to automatic failback, except that the resource may be
       moved to a node that is not the one it was previously active on.
      </p></dd><dt id="id-1.3.4.3.7.7.4.2"><span class="term">Value is greater than <code class="literal">0</code>:</span></dt><dd><p>
       The resource will prefer to remain in its current location, but may
       be moved if a more suitable node is available. Higher values indicate
       a stronger preference for a resource to stay where it is.
      </p></dd><dt id="id-1.3.4.3.7.7.4.3"><span class="term">Value is less than <code class="literal">0</code>:</span></dt><dd><p>
       The resource prefers to move away from its current location. Higher
       absolute values indicate a stronger preference for a resource to be
       moved.
      </p></dd><dt id="id-1.3.4.3.7.7.4.4"><span class="term">Value is <code class="literal">INFINITY</code>:</span></dt><dd><p>
       The resource will always remain in its current location unless forced
       off because the node is no longer eligible to run the resource (node
       shutdown, node standby, reaching the
       <code class="literal">migration-threshold</code>, or configuration change).
       This option is almost equivalent to completely disabling automatic
       failback.
      </p></dd><dt id="id-1.3.4.3.7.7.4.5"><span class="term">Value is <code class="literal">-INFINITY</code>:</span></dt><dd><p>
       The resource will always move away from its current location.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-utilization" data-id-title="Placing Resources Based on Their Load Impact"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.6 </span><span class="title-name">Placing Resources Based on Their Load Impact</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Not all resources are equal. Some, such as Xen guests, require that
    the node hosting them meets their capacity requirements. If resources
    are placed such that their combined need exceed the provided capacity,
    the resources diminish in performance (or even fail).
   </p><p>
    To take this into account, SUSE Linux Enterprise High Availability allows you to specify the
    following parameters:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      The capacity a certain node <span class="emphasis"><em>provides</em></span>.
     </p></li><li class="listitem"><p>
      The capacity a certain resource <span class="emphasis"><em>requires</em></span>.
     </p></li><li class="listitem"><p>
      An overall strategy for placement of resources.
     </p></li></ol></div><p>
    Learn how to configure these settings with your preferred cluster
    management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="cha-conf-hawk2.html#sec-config-hawk2-utilization" title="6.6.8. Configuring Placement of Resources Based on Load Impact">Section 6.6.8, “Configuring Placement of Resources Based on Load Impact”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-utilization" title="7.4.8. Configuring Placement of Resources Based on Load Impact">Section 7.4.8, “Configuring Placement of Resources Based on Load Impact”</a>
     </p></li></ul></div><p>
    A node is considered eligible for a resource if it has sufficient free
    capacity to satisfy the resource's requirements. The nature of the
    capacities is completely irrelevant for the High Availability software; it only makes
    sure that all capacity requirements of a resource are satisfied before
    moving a resource to a node.
   </p><p>
    To manually configure the resource's requirements and the capacity a
    node provides, use utilization attributes. You can name the utilization
    attributes according to your preferences and define as many name/value
    pairs as your configuration needs. However, the attribute's values must
    be integers.
   </p><p>
    If multiple resources with utilization attributes are grouped or have
    colocation constraints, SUSE Linux Enterprise High Availability takes that into account. If
    possible, the resources will be placed on a node that can fulfill
    <span class="emphasis"><em>all</em></span> capacity requirements.
   </p><div id="id-1.3.4.3.7.8.10" data-id-title="Utilization Attributes for Groups" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Utilization Attributes for Groups</div><p>
     It is impossible to set utilization attributes directly for a resource
     group. However, to simplify the configuration for a group, you can add
     a utilization attribute with the total capacity needed to any of the
     resources in the group.
    </p></div><p>
    SUSE Linux Enterprise High Availability also provides means to detect and configure both node
    capacity and resource requirements automatically:
   </p><p>
    The <code class="systemitem">NodeUtilization</code> resource agent checks the
    capacity of a node (regarding CPU and RAM).
    To configure automatic detection, create a clone resource of the
    following class, provider, and type:
    <code class="literal">ocf:pacemaker:NodeUtilization</code>. One instance of the
    clone should be running on each node. After the instance has started, a
    utilization section will be added to the node's configuration in CIB.
   </p><p>
    For automatic detection of a resource's minimal requirements (regarding
    RAM and CPU) the <code class="systemitem">Xen</code> resource agent has been
    improved. Upon start of a <code class="systemitem">Xen</code> resource, it will
    reflect the consumption of RAM and CPU. Utilization attributes will
    automatically be added to the resource configuration.
   </p><div id="id-1.3.4.3.7.8.14" data-id-title="Different Resource Agents for Xen and libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Different Resource Agents for Xen and libvirt</div><p>
     The <code class="systemitem">ocf:heartbeat:Xen</code> resource agent should not be
     used with <code class="literal">libvirt</code>, as <code class="literal">libvirt</code> expects
     to be able to modify the machine description file.
    </p><p>
     For <code class="literal">libvirt</code>, use the
     <code class="systemitem">ocf:heartbeat:VirtualDomain</code> resource agent.
    </p></div><p>
    Apart from detecting the minimal requirements, SUSE Linux Enterprise High Availability also allows
    to monitor the current utilization via the
    <code class="systemitem">VirtualDomain</code> resource agent. It detects CPU
    and RAM use of the virtual machine. To use this feature, configure a
    resource of the following class, provider and type:
    <code class="literal">ocf:heartbeat:VirtualDomain</code>. The following instance
    attributes are available: <code class="varname">autoset_utilization_cpu</code> and
    <code class="varname">autoset_utilization_hv_memory</code>. Both default to
    <code class="literal">true</code>. This updates the utilization values in the CIB
    during each monitoring cycle.
   </p><p>
    Independent of manually or automatically configuring capacity and
    requirements, the placement strategy must be specified with the
    <code class="literal">placement-strategy</code> property (in the global cluster
    options). The following values are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.7.8.17.1"><span class="term"><code class="literal">default</code> (default value)</span></dt><dd><p>
       Utilization values are not considered. Resources are allocated
       according to location scoring. If scores are equal, resources are
       evenly distributed across nodes.
      </p></dd><dt id="id-1.3.4.3.7.8.17.2"><span class="term"><code class="literal">utilization</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. However,
       load-balancing is still done based on the number of resources
       allocated to a node.
      </p></dd><dt id="id-1.3.4.3.7.8.17.3"><span class="term"><code class="literal">minimal</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to concentrate the resources on as few nodes as possible
       (to achieve power savings on the remaining nodes).
      </p></dd><dt id="id-1.3.4.3.7.8.17.4"><span class="term"><code class="literal">balanced</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to distribute the resources evenly, thus optimizing resource
       performance.
      </p></dd></dl></div><div id="id-1.3.4.3.7.8.18" data-id-title="Configuring Resource Priorities" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Configuring Resource Priorities</div><p>
     The available placement strategies are best-effort—they do not
     yet use complex heuristic solvers to always reach optimum allocation
     results. Ensure that resource priorities are properly set so that
     your most important resources are scheduled first.
    </p></div><div class="complex-example"><div class="example" id="ex-ha-config-basics-utilization" data-id-title="Example Configuration for Load-Balanced Placing"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.9: </span><span class="title-name">Example Configuration for Load-Balanced Placing </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-ha-config-basics-utilization">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The following example demonstrates a three-node cluster of equal nodes,
     with four virtual machines.
    </p><div class="verbatim-wrap"><pre class="screen">node alice utilization hv_memory="4000"
node bob utilization hv_memory="4000"
node charlie utilization hv_memory="4000"
primitive xenA Xen utilization hv_memory="3500" \
     params xmfile="/etc/xen/shared-vm/vm1"
     meta priority="10"
primitive xenB Xen utilization hv_memory="2000" \
     params xmfile="/etc/xen/shared-vm/vm2"
     meta priority="1"
primitive xenC Xen utilization hv_memory="2000" \
     params xmfile="/etc/xen/shared-vm/vm3"
     meta priority="1"
primitive xenD Xen utilization hv_memory="1000" \
     params xmfile="/etc/xen/shared-vm/vm4"
     meta priority="5"
property placement-strategy="minimal"</pre></div><p>
     With all three nodes up, resource <code class="literal">xenA</code> will be
     placed onto a node first, followed by <code class="literal">xenD</code>.
     <code class="literal">xenB</code> and <code class="literal">xenC</code> would either be
     allocated together or one of them with <code class="literal">xenD</code>.
    </p><p>
     If one node failed, too little total memory would be available to host
     them all. <code class="literal">xenA</code> would be ensured to be allocated, as
     would <code class="literal">xenD</code>. However, only one of the remaining
     resources <code class="literal">xenB</code> or <code class="literal">xenC</code> could
     still be placed. Since their priority is equal, the result would still
     be open. To resolve this ambiguity as well, you would need to set a
     higher priority for either one.
    </p></div></div></div></section><section class="sect2" id="sec-ha-config-basics-tags" data-id-title="Grouping Resources by Using Tags"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.7 </span><span class="title-name">Grouping Resources by Using Tags</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-tags">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Tags are a new feature that has been added to Pacemaker recently. Tags
    are a way to refer to multiple resources at once, without creating any
    colocation or ordering relationship between them. This can be useful for
    grouping conceptually related resources. For example, if you have
    several resources related to a database, create a tag called
    <code class="literal">databases</code> and add all resources related to the
    database to this tag. This allows you to stop or start them all with a
    single command.
   </p><p>
    Tags can also be used in constraints. For example, the following
    location constraint <code class="literal">loc-db-prefer</code> applies to the set
    of resources tagged with <code class="literal">databases</code>:
   </p><div class="verbatim-wrap"><pre class="screen">location loc-db-prefer databases 100: alice</pre></div><p>
    Learn how to create tags with your preferred cluster management tool:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Hawk2: <a class="xref" href="cha-conf-hawk2.html#pro-conf-hawk2-tag" title="Adding a Tag">Procedure 6.12, “Adding a Tag”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="cha-ha-manual-config.html#sec-ha-manual-config-tag" title="7.5.7. Grouping/Tagging Resources">Section 7.5.7, “Grouping/Tagging Resources”</a>
     </p></li></ul></div></section></section><section class="sect1" id="sec-ha-config-basics-remote" data-id-title="Managing Services on Remote Hosts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.6 </span><span class="title-name">Managing Services on Remote Hosts</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-remote">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The possibilities for monitoring and managing services on remote hosts
   has become increasingly important during the last few years.
   SUSE Linux Enterprise High Availability 11 SP3 offered fine-grained monitoring of services on
   remote hosts via monitoring plug-ins. The recent addition of the
   <code class="literal">pacemaker_remote</code> service now allows SUSE Linux Enterprise High Availability
   12 SP5 to fully manage and monitor resources on remote hosts
   just as if they were a real cluster node—without the need to
   install the cluster stack on the remote machines.
  </p><section class="sect2" id="sec-ha-config-basics-remote-nagios" data-id-title="Monitoring Services on Remote Hosts with Monitoring Plug-ins"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.6.1 </span><span class="title-name">Monitoring Services on Remote Hosts with Monitoring Plug-ins</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-remote-nagios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Monitoring of virtual machines can be done with the VM agent (which only
    checks if the guest shows up in the hypervisor), or by external scripts
    called from the VirtualDomain or Xen agent. Up to now, more fine-grained
    monitoring was only possible with a full setup of the High Availability stack
    within the virtual machines.
   </p><p>
    By providing support for monitoring plug-ins (formerly named Nagios
    plug-ins), SUSE Linux Enterprise High Availability now also allows you to monitor services on
    remote hosts. You can collect external statuses on the guests without
    modifying the guest image. For example, VM guests might run Web services
    or simple network resources that need to be accessible. With the Nagios
    resource agents, you can now monitor the Web service or the network
    resource on the guest. If these services are not reachable anymore,
    SUSE Linux Enterprise High Availability will trigger a restart or migration of the respective
    guest.
   </p><p>
    If your guests depend on a service (for example, an NFS server to be
    used by the guest), the service can either be an ordinary resource,
    managed by the cluster, or an external service that is monitored with
    Nagios resources instead.
   </p><p>
    To configure the Nagios resources, the following packages must be
    installed on the host:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="systemitem">monitoring-plugins</code>
     </p></li><li class="listitem"><p>
      <code class="systemitem">monitoring-plugins-metadata</code>
     </p></li></ul></div><p>
    YaST or Zypper will resolve any dependencies on further packages,
    if required.
   </p><p>
    A typical use case is to configure the monitoring plug-ins as resources
    belonging to a resource container, which usually is a VM. The container
    will be restarted if any of its resources has failed. Refer to
    <a class="xref" href="cha-ha-config-basics.html#ex-ha-nagios-config" title="Configuring Resources for Monitoring Plug-ins">Example 5.10, “Configuring Resources for Monitoring Plug-ins”</a> for a configuration example.
    Alternatively, Nagios resource agents can also be configured as ordinary
    resources if you want to use them for monitoring hosts or services via
    the network.
   </p><div class="complex-example"><div class="example" id="ex-ha-nagios-config" data-id-title="Configuring Resources for Monitoring Plug-ins"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.10: </span><span class="title-name">Configuring Resources for Monitoring Plug-ins </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#ex-ha-nagios-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">primitive vm1 VirtualDomain \
    params hypervisor="qemu:///system" config="/etc/libvirt/qemu/vm1.xml" \
    op start interval="0" timeout="90" \
    op stop interval="0" timeout="90" \
    op monitor interval="10" timeout="30"
primitive vm1-sshd nagios:check_tcp \
    params hostname="vm1" port="22" \ <span class="callout" id="co-nagios-hostname">1</span>
    op start interval="0" timeout="120" \ <span class="callout" id="co-nagios-startinterval">2</span>
    op monitor interval="10"
group g-vm1-and-services vm1 vm1-sshd \
    meta container="vm1" <span class="callout" id="co-nagios-container">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-hostname"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The supported parameters are the same as the long options of a
       monitoring plug-in. Monitoring plug-ins connect to services with the
       parameter <code class="literal">hostname</code>. Therefore the attribute's
       value must be a resolvable host name or an IP address.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-startinterval"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       As it takes some time to get the guest operating system up and its
       services running, the start timeout of the monitoring resource must
       be long enough.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-container"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A cluster resource container of type
       <code class="literal">ocf:heartbeat:Xen</code>,
       <code class="literal">ocf:heartbeat:VirtualDomain</code> or
       <code class="literal">ocf:heartbeat:lxc</code>. It can either be a VM or a
       Linux Container.
      </p></td></tr></table></div><p>
     The example above contains only one resource for the
     <code class="literal">check_tcp</code>plug-in, but multiple resources for
     different plug-in types can be configured (for example,
     <code class="literal">check_http</code> or <code class="literal">check_udp</code>).
    </p><p>
     If the host names of the services are the same, the
     <code class="literal">hostname</code> parameter can also be specified for the
     group, instead of adding it to the individual primitives. For example:
    </p><div class="verbatim-wrap"><pre class="screen">group g-vm1-and-services vm1 vm1-sshd vm1-httpd \
     meta container="vm1" \
     params hostname="vm1"</pre></div><p>
     If any of the services monitored by the monitoring plug-ins fail within
     the VM, the cluster will detect that and restart the container resource
     (the VM). Which action to take in this case can be configured by
     specifying the <code class="literal">on-fail</code> attribute for the service's
     monitoring operation. It defaults to
     <code class="literal">restart-container</code>.
    </p><p>
     Failure counts of services will be taken into account when considering
     the VM's migration-threshold.
    </p></div></div></div></section><section class="sect2" id="sec-ha-config-basics-remote-pace-remote" data-id-title="Managing Services on Remote Nodes with pacemaker_remote"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.6.2 </span><span class="title-name">Managing Services on Remote Nodes with <code class="literal">pacemaker_remote</code></span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-remote-pace-remote">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With the <code class="literal">pacemaker_remote</code> service, High Availability clusters
    can be extended to virtual nodes or remote bare-metal machines. They do
    not need to run the cluster stack to become members of the cluster.
   </p><p>
    SUSE Linux Enterprise High Availability can now launch virtual environments (KVM and LXC), plus
    the resources that live within those virtual environments without
    requiring the virtual environments to run Pacemaker or Corosync.
   </p><p>
    For the use case of managing both virtual machines as cluster resources
    plus the resources that live within the VMs, you can now use the
    following setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The <span class="quote">“<span class="quote">normal</span>”</span> (bare-metal) cluster nodes run SUSE Linux Enterprise High Availability.
     </p></li><li class="listitem"><p>
      The virtual machines run the <code class="literal">pacemaker_remote</code>
      service (almost no configuration required on the VM's side).
     </p></li><li class="listitem"><p>
      The cluster stack on the <span class="quote">“<span class="quote">normal</span>”</span> cluster nodes launches
      the VMs and connects to the <code class="literal">pacemaker_remote</code>
      service running on the VMs to integrate them as remote nodes into the
      cluster.
     </p></li></ul></div><p>
    As the remote nodes do not have the cluster stack installed, this has
    the following implications:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Remote nodes do not take part in quorum.
     </p></li><li class="listitem"><p>
      Remote nodes cannot become the DC.
     </p></li><li class="listitem"><p>
      Remote nodes are not bound by the scalability limits (Corosync
      has a member limit of 32 nodes).
     </p></li></ul></div><p>
    Find more information about the <code class="literal">remote_pacemaker</code>
    service, including multiple use cases with detailed setup instructions
    in <em class="citetitle">Pacemaker Remote—Extending High Availability into
    Virtual Nodes</em>, available at
    <a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>.
   </p></section></section><section class="sect1" id="sec-ha-config-basics-monitor-health" data-id-title="Monitoring System Health"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.7 </span><span class="title-name">Monitoring System Health</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-monitor-health">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To prevent a node from running out of disk space and thus being unable to
   manage any resources that have been assigned to it, SUSE Linux Enterprise High Availability
   provides a resource agent,
   <code class="systemitem">ocf:pacemaker:SysInfo</code>. Use it to monitor a
   node's health with regard to disk partitions.
   The SysInfo RA creates a node attribute named
   <code class="literal">#health_disk</code> which will be set to
   <code class="literal">red</code> if any of the monitored disks' free space is below
   a specified limit.
  </p><p>
   To define how the CRM should react in case a node's health reaches a
   critical state, use the global cluster option
   <code class="systemitem">node-health-strategy</code>.
  </p><div class="procedure" id="pro-ha-health-monitor" data-id-title="Configuring System Health Monitoring"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.2: </span><span class="title-name">Configuring System Health Monitoring </span></span><a title="Permalink" class="permalink" href="cha-ha-config-basics.html#pro-ha-health-monitor">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To automatically move resources away from a node in case the node runs
    out of disk space, proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Configure an <code class="systemitem">ocf:pacemaker:SysInfo</code> resource:
    </p><div class="verbatim-wrap"><pre class="screen">primitive sysinfo ocf:pacemaker:SysInfo \
     params disks="/tmp /var"<span class="callout" id="co-disks">1</span> min_disk_free="100M"<span class="callout" id="co-min-disk-free">2</span> disk_unit="M"<span class="callout" id="co-disk-unit">3</span> \
     op monitor interval="15s"</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disks"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Which disk partitions to monitor. For example,
       <code class="filename">/tmp</code>, <code class="filename">/usr</code>,
       <code class="filename">/var</code>, and <code class="filename">/dev</code>. To specify
       multiple partitions as attribute values, separate them with a blank.
      </p><div id="id-1.3.4.3.9.4.3.3.1.2" data-id-title="/ File System Always Monitored" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="filename">/</code> File System Always Monitored</div><p>
        You do not need to specify the root partition
        (<code class="filename">/</code>) in <code class="literal">disks</code>. It is always
        monitored by default.
       </p></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-min-disk-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minimum free disk space required for those partitions.
       Optionally, you can specify the unit to use for measurement (in the
       example above, <code class="literal">M</code> for megabytes is used). If not
       specified, <code class="systemitem">min_disk_free</code> defaults to the
       unit defined in the <code class="systemitem">disk_unit</code> parameter.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-unit"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The unit in which to report the disk space.
      </p></td></tr></table></div></li><li class="step"><p>
     To complete the resource configuration, create a clone of
     <code class="systemitem">ocf:pacemaker:SysInfo</code> and start it on each
     cluster node.
    </p></li><li class="step"><p>
     Set the <code class="systemitem">node-health-strategy</code> to
     <code class="literal">migrate-on-red</code>:
    </p><div class="verbatim-wrap"><pre class="screen">property node-health-strategy="migrate-on-red"</pre></div><p>
     In case of a <code class="systemitem">#health_disk</code> attribute set to
     <code class="literal">red</code>, the policy engine adds <code class="literal">-INF</code>
     to the resources' score for that node. This will cause any resources to
     move away from this node. The STONITH resource will be the last
     one to be stopped but even if the STONITH resource is not running
     anymore, the node can still be fenced. Fencing has direct access to the
     CIB and will continue to work.
    </p></li></ol></div></div><p>
   After a node's health status has turned to <code class="literal">red</code>, solve
   the issue that led to the problem. Then clear the <code class="literal">red</code>
   status to make the node eligible again for running resources. Log in to
   the cluster node and use one of the following methods:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Execute the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node status-attr <em class="replaceable">NODE</em> delete #health_disk</pre></div></li><li class="listitem"><p>
     Restart Pacemaker on that node.
    </p></li><li class="listitem"><p>
     Reboot the node.
    </p></li></ul></div><p>
   The node will be returned to service and can run resources again.
  </p></section><section class="sect1" id="sec-ha-config-basics-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="cha-ha-config-basics.html#sec-ha-config-basics-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA12SP5/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.10.2.1"><span class="term"><a class="link" href="http://crmsh.github.io/" target="_blank">http://crmsh.github.io/</a>
    </span></dt><dd><p>
      Home page of the crm shell (crmsh), the advanced command line
      interface for High Availability cluster management.
     </p></dd><dt id="id-1.3.4.3.10.2.2"><span class="term"><a class="link" href="http://crmsh.github.io/documentation" target="_blank">http://crmsh.github.io/documentation</a>
    </span></dt><dd><p>
      Holds several documents about the crm shell, including a
      <em class="citetitle">Getting Started</em> tutorial for basic cluster
      setup with crmsh and the comprehensive
      <em class="citetitle">Manual</em> for the crm shell. The latter is
      available at <a class="link" href="http://crmsh.github.io/man-2.0/" target="_blank">http://crmsh.github.io/man-2.0/</a>.
      Find the tutorial at
      <a class="link" href="http://crmsh.github.io/start-guide/" target="_blank">http://crmsh.github.io/start-guide/</a>.
     </p></dd><dt id="id-1.3.4.3.10.2.3"><span class="term"><a class="link" href="http://clusterlabs.org/" target="_blank">http://clusterlabs.org/</a>
    </span></dt><dd><p>
      Home page of Pacemaker, the cluster resource manager shipped with
      SUSE Linux Enterprise High Availability.
     </p></dd><dt id="id-1.3.4.3.10.2.4"><span class="term"><a class="link" href="http://www.clusterlabs.org/doc/" target="_blank">http://www.clusterlabs.org/doc/</a>
    </span></dt><dd><p>
      Holds several comprehensive manuals and some shorter documents
      explaining general concepts. For example:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <em class="citetitle">Pacemaker Explained</em>: Contains comprehensive and very detailed information
        for reference.
       </p></li><li class="listitem"><p>
        <em class="citetitle">Configuring Fencing with crmsh</em>: How to
        configure and use STONITH devices.
       </p></li><li class="listitem"><p>
        <em class="citetitle">Colocation Explained</em>
       </p></li><li class="listitem"><p>
        <em class="citetitle">Ordering Explained</em>
       </p></li></ul></div></dd><dt id="id-1.3.4.3.10.2.5"><span class="term"><a class="link" href="https://clusterlabs.org" target="_blank">https://clusterlabs.org</a>
    </span></dt><dd><p>
      Home page of the High Availability Linux Project.
     </p></dd></dl></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="part-config.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part II </span>Configuration and Administration</span></a> </div><div><a class="pagination-link next" href="cha-conf-hawk2.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 6 </span>Configuring and Managing Cluster Resources with Hawk2</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-scenarios"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-global"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-resources"><span class="title-number">5.3 </span><span class="title-name">Cluster Resources</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-monitoring"><span class="title-number">5.4 </span><span class="title-name">Resource Monitoring</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-constraints"><span class="title-number">5.5 </span><span class="title-name">Resource Constraints</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-remote"><span class="title-number">5.6 </span><span class="title-name">Managing Services on Remote Hosts</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-monitor-health"><span class="title-number">5.7 </span><span class="title-name">Monitoring System Health</span></a></span></li><li><span class="sect1"><a href="cha-ha-config-basics.html#sec-ha-config-basics-more"><span class="title-number">5.8 </span><span class="title-name">For More Information</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>