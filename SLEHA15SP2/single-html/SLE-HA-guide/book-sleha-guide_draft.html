<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><title>SLE HA 15 SP2 | Administration Guide</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Administration Guide | SLE HA 15 SP2"/>
<meta name="description" content="This guide is intended for administrators who need to …"/>
<meta name="product-name" content="SUSE Linux Enterprise High Availability"/>
<meta name="product-number" content="15 SP2"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Linux Enterprise High Availability Extension 15 SP2"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Administration Guide | SLE HA 15 SP2"/>
<meta property="og:description" content="This guide is intended for administrators who need to set up, configure, and maintain clusters with SUSE® Linux Enterprise H…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration Guide | SLE HA 15 SP2"/>
<meta name="twitter:description" content="This guide is intended for administrators who need to set up, configure, and maintain clusters with SUSE® Linux Enterprise H…"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml"/></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-sleha-guide">Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-sleha-guide" data-id-title="Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Linux Enterprise High Availability</span> <span class="productnumber">15 SP2</span></div><div class="title-container"><h1 class="title">Administration Guide</h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div><div class="abstract"><p> This guide is intended for administrators who need to set up, configure,
  and maintain clusters with SUSE® Linux Enterprise High Availability. For quick and efficient
  configuration and administration, the product includes both a graphical user
  interface and a command line interface (CLI). For performing key tasks,
  both approaches are covered in this guide. Thus, you can choose the appropriate
  tool that matches your needs.</p></div><div class="date"><span class="imprint-label">Publication Date: </span>
        March 04, 2024

      </div></div></div><div class="toc"><ul><li><span class="preface"><a href="#pre-ha"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.2.5"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.6"><span class="title-name">Improving the documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.7"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.8"><span class="title-name">Support</span></a></span></li></ul></li><li><span class="part"><a href="#part-install"><span class="title-number">I </span><span class="title-name">Installation and Setup</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-concepts"><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-availability"><span class="title-number">1.1 </span><span class="title-name">Availability as a Module or Extension</span></a></span></li><li><span class="sect1"><a href="#sec-ha-features"><span class="title-number">1.2 </span><span class="title-name">Key Features</span></a></span></li><li><span class="sect1"><a href="#sec-ha-benefits"><span class="title-number">1.3 </span><span class="title-name">Benefits</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clusterconfig"><span class="title-number">1.4 </span><span class="title-name">Cluster Configurations: Storage</span></a></span></li><li><span class="sect1"><a href="#sec-ha-architecture"><span class="title-number">1.5 </span><span class="title-name">Architecture</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-requirements"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-requirements-hw"><span class="title-number">2.1 </span><span class="title-name">Hardware Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-sw"><span class="title-number">2.2 </span><span class="title-name">Software Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-disk"><span class="title-number">2.3 </span><span class="title-name">Storage Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-requirements-other"><span class="title-number">2.4 </span><span class="title-name">Other Requirements and Recommendations</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-install"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-install-manual"><span class="title-number">3.1 </span><span class="title-name">Manual Installation</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-autoyast"><span class="title-number">3.2 </span><span class="title-name">Mass Installation and Deployment with AutoYaST</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-ycluster"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-installation-terms"><span class="title-number">4.1 </span><span class="title-name">Definition of Terms</span></a></span></li><li><span class="sect1"><a href="#sec-ha-setup-yast-overview"><span class="title-number">4.2 </span><span class="title-name">YaST Cluster Module</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-channels"><span class="title-number">4.3 </span><span class="title-name">Defining the Communication Channels</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-security"><span class="title-number">4.4 </span><span class="title-name">Defining Authentication Settings</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-conntrackd"><span class="title-number">4.5 </span><span class="title-name">Synchronizing Connection Status Between Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-services"><span class="title-number">4.6 </span><span class="title-name">Configuring Services</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-setup-csync2"><span class="title-number">4.7 </span><span class="title-name">Transferring the configuration to all nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-installation-start"><span class="title-number">4.8 </span><span class="title-name">Bringing the Cluster Online Node by Node</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-config"><span class="title-number">II </span><span class="title-name">Configuration and Administration</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-config-basics"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-config-basics-scenarios"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-global"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-global-options"><span class="title-number">5.3 </span><span class="title-name">Global Cluster Options</span></a></span></li><li><span class="sect1"><a href="#cha-conf-hawk2"><span class="title-number">5.4 </span><span class="title-name">Introduction to Hawk2</span></a></span></li><li><span class="sect1"><a href="#cha-ha-manual-config"><span class="title-number">5.5 </span><span class="title-name">Introduction to crmsh</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-more"><span class="title-number">5.6 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-ha-config-basics-resources"><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-config-basics-resources-types"><span class="title-number">6.1 </span><span class="title-name">Types of Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-raclasses"><span class="title-number">6.2 </span><span class="title-name">Supported Resource Agent Classes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-timeouts"><span class="title-number">6.3 </span><span class="title-name">Timeout Values</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources-management"><span class="title-number">6.4 </span><span class="title-name">Creating Primitive Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources-advanced-groups"><span class="title-number">6.5 </span><span class="title-name">Creating Resource Groups</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources-advanced-clones"><span class="title-number">6.6 </span><span class="title-name">Creating Clone Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources-advanced-masters"><span class="title-number">6.7 </span><span class="title-name">Creating Promotable Clones (Multi-state Resources)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-resources-templates"><span class="title-number">6.8 </span><span class="title-name">Creating Resource Templates</span></a></span></li><li><span class="sect1"><a href="#sec-ha-conf-stonith-rsc"><span class="title-number">6.9 </span><span class="title-name">Creating STONITH Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-monitoring"><span class="title-number">6.10 </span><span class="title-name">Configuring Resource Monitoring</span></a></span></li><li><span class="sect1"><a href="#sec-ha-manual-config-load"><span class="title-number">6.11 </span><span class="title-name">Loading Resources from a File</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-meta-attr"><span class="title-number">6.12 </span><span class="title-name">Resource Options (Meta Attributes)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-inst-attr"><span class="title-number">6.13 </span><span class="title-name">Instance Attributes (Parameters)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-operations"><span class="title-number">6.14 </span><span class="title-name">Resource Operations</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-ha-config-basics-constraints"><span class="title-number">7 </span><span class="title-name">Configuring Resource Constraints</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-config-basics-constraints-types"><span class="title-number">7.1 </span><span class="title-name">Types of Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-constraints-scores"><span class="title-number">7.2 </span><span class="title-name">Scores and Infinity</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-constraints-templates"><span class="title-number">7.3 </span><span class="title-name">Resource Templates and Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-conf-add-location-constraints"><span class="title-number">7.4 </span><span class="title-name">Adding Location Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-conf-add-colocation-constraints"><span class="title-number">7.5 </span><span class="title-name">Adding Colocation Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-conf-add-order-constraints"><span class="title-number">7.6 </span><span class="title-name">Adding Order Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-constraints-rscset"><span class="title-number">7.7 </span><span class="title-name">Using Resource Sets to Define Constraints</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-failover"><span class="title-number">7.8 </span><span class="title-name">Specifying Resource Failover Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-failback"><span class="title-number">7.9 </span><span class="title-name">Specifying Resource Failback Nodes (Resource Stickiness)</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-utilization"><span class="title-number">7.10 </span><span class="title-name">Placing Resources Based on Their Load Impact</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-constraints-more"><span class="title-number">7.11 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-manage-resources"><span class="title-number">8 </span><span class="title-name">Managing Cluster Resources</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-resource-show"><span class="title-number">8.1 </span><span class="title-name">Showing Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-edit"><span class="title-number">8.2 </span><span class="title-name">Editing Resources and Groups</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-start"><span class="title-number">8.3 </span><span class="title-name">Starting Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-stop"><span class="title-number">8.4 </span><span class="title-name">Stopping Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-clean"><span class="title-number">8.5 </span><span class="title-name">Cleaning Up Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-remove"><span class="title-number">8.6 </span><span class="title-name">Removing Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-resource-migrate"><span class="title-number">8.7 </span><span class="title-name">Migrating Cluster Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-tags"><span class="title-number">8.8 </span><span class="title-name">Grouping Resources by Using Tags</span></a></span></li></ul></li><li><span class="chapter"><a href="#sec-ha-config-basics-remote"><span class="title-number">9 </span><span class="title-name">Managing Services on Remote Hosts</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-config-basics-remote-nagios"><span class="title-number">9.1 </span><span class="title-name">Monitoring Services on Remote Hosts with Monitoring Plug-ins</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-remote-pace-remote"><span class="title-number">9.2 </span><span class="title-name">Managing Services on Remote Nodes with <code class="literal">pacemaker_remote</code></span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-agents"><span class="title-number">10 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-stonithagents"><span class="title-number">10.1 </span><span class="title-name">STONITH Agents</span></a></span></li><li><span class="sect1"><a href="#sec-ha-writingresourceagents"><span class="title-number">10.2 </span><span class="title-name">Writing OCF Resource Agents</span></a></span></li><li><span class="sect1"><a href="#sec-ha-errorcodes"><span class="title-number">10.3 </span><span class="title-name">OCF Return Codes and Failure Recovery</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-monitor-clusters"><span class="title-number">11 </span><span class="title-name">Monitoring Clusters</span></a></span><ul><li><span class="sect1"><a href="#sec-conf-hawk2-monitor"><span class="title-number">11.1 </span><span class="title-name">Monitoring Cluster Status</span></a></span></li><li><span class="sect1"><a href="#sec-conf-health"><span class="title-number">11.2 </span><span class="title-name">Verifying Cluster Health</span></a></span></li><li><span class="sect1"><a href="#sec-conf-hawk2-history"><span class="title-number">11.3 </span><span class="title-name">Viewing the Cluster History</span></a></span></li><li><span class="sect1"><a href="#sec-ha-config-basics-monitor-health"><span class="title-number">11.4 </span><span class="title-name">Monitoring System Health</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-fencing"><span class="title-number">12 </span><span class="title-name">Fencing and STONITH</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-fencing-classes"><span class="title-number">12.1 </span><span class="title-name">Classes of Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-nodes"><span class="title-number">12.2 </span><span class="title-name">Node Level Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-config"><span class="title-number">12.3 </span><span class="title-name">STONITH Resources and Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-monitor"><span class="title-number">12.4 </span><span class="title-name">Monitoring Fencing Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-special"><span class="title-number">12.5 </span><span class="title-name">Special Fencing Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-recommend"><span class="title-number">12.6 </span><span class="title-name">Basic Recommendations</span></a></span></li><li><span class="sect1"><a href="#sec-ha-fencing-more"><span class="title-number">12.7 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-storage-protect"><span class="title-number">13 </span><span class="title-name">Storage Protection and SBD</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-storage-protect-overview"><span class="title-number">13.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-steps"><span class="title-number">13.2 </span><span class="title-name">Overview of Manually Setting Up SBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-req"><span class="title-number">13.3 </span><span class="title-name">Requirements</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-fencing-number"><span class="title-number">13.4 </span><span class="title-name">Number of SBD Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-watchdog-timings"><span class="title-number">13.5 </span><span class="title-name">Calculation of Timeouts</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-watchdog"><span class="title-number">13.6 </span><span class="title-name">Setting Up the Watchdog</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-fencing-setup"><span class="title-number">13.7 </span><span class="title-name">Setting Up SBD with Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-diskless-sbd"><span class="title-number">13.8 </span><span class="title-name">Setting Up Diskless SBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-test"><span class="title-number">13.9 </span><span class="title-name">Testing SBD and Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-rsc-fencing"><span class="title-number">13.10 </span><span class="title-name">Additional Mechanisms for Storage Protection</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-protect-moreinfo"><span class="title-number">13.11 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-qdevice"><span class="title-number">14 </span><span class="title-name">QDevice and QNetd</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-qdevice-overview"><span class="title-number">14.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-require"><span class="title-number">14.2 </span><span class="title-name">Requirements and Prerequisites</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-setup-qnetd"><span class="title-number">14.3 </span><span class="title-name">Setting Up the QNetd Server</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-qdevice"><span class="title-number">14.4 </span><span class="title-name">Connecting QDevice Clients to the QNetd Server</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-heuristic"><span class="title-number">14.5 </span><span class="title-name">Setting Up a QDevice with Heuristics</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-status"><span class="title-number">14.6 </span><span class="title-name">Checking and Showing Quorum Status</span></a></span></li><li><span class="sect1"><a href="#sec-ha-qdevice-more"><span class="title-number">14.7 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-acl"><span class="title-number">15 </span><span class="title-name">Access Control Lists</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-acl-require"><span class="title-number">15.1 </span><span class="title-name">Requirements and Prerequisites</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-basics"><span class="title-number">15.2 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-enable"><span class="title-number">15.3 </span><span class="title-name">Enabling Use of ACLs in Your Cluster</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-create-ro-monitor-role"><span class="title-number">15.4 </span><span class="title-name">Creating a Read-Only Monitor Role</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-rm-user"><span class="title-number">15.5 </span><span class="title-name">Removing a User</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-rm-role"><span class="title-number">15.6 </span><span class="title-name">Removing an Existing Role</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-config-xpath"><span class="title-number">15.7 </span><span class="title-name">Setting ACL Rules via XPath Expressions</span></a></span></li><li><span class="sect1"><a href="#sec-ha-acl-config-tag"><span class="title-number">15.8 </span><span class="title-name">Setting ACL Rules via Abbreviations</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-netbonding"><span class="title-number">16 </span><span class="title-name">Network Device Bonding</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-netbond-yast"><span class="title-number">16.1 </span><span class="title-name">Configuring Bonding Devices with YaST</span></a></span></li><li><span class="sect1"><a href="#sec-ha-netbond-hotpug-yast"><span class="title-number">16.2 </span><span class="title-name">Hotplugging devices into a bond</span></a></span></li><li><span class="sect1"><a href="#sec-ha-netbonding-more"><span class="title-number">16.3 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-lb"><span class="title-number">17 </span><span class="title-name">Load Balancing</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-lb-overview"><span class="title-number">17.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-lvs"><span class="title-number">17.2 </span><span class="title-name">Configuring Load Balancing with Linux Virtual Server</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-haproxy"><span class="title-number">17.3 </span><span class="title-name">Configuring Load Balancing with HAProxy</span></a></span></li><li><span class="sect1"><a href="#sec-ha-lb-more"><span class="title-number">17.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-geo"><span class="title-number">18 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></span></li></ul></li><li><span class="part"><a href="#part-storage"><span class="title-number">III </span><span class="title-name">Storage and Data Replication</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-storage-dlm"><span class="title-number">19 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-storage-dlm-protocol"><span class="title-number">19.1 </span><span class="title-name">Protocols for DLM Communication</span></a></span></li><li><span class="sect1"><a href="#sec-ha-storage-generic-dlm-config"><span class="title-number">19.2 </span><span class="title-name">Configuring DLM Cluster Resources</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-ocfs2"><span class="title-number">20 </span><span class="title-name">OCFS2</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-ocfs2-features"><span class="title-number">20.1 </span><span class="title-name">Features and Benefits</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-utils"><span class="title-number">20.2 </span><span class="title-name">OCFS2 Packages and Management Utilities</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-create-service"><span class="title-number">20.3 </span><span class="title-name">Configuring OCFS2 Services and a STONITH Resource</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-create"><span class="title-number">20.4 </span><span class="title-name">Creating OCFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-mount"><span class="title-number">20.5 </span><span class="title-name">Mounting OCFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-rsc-hawk2"><span class="title-number">20.6 </span><span class="title-name">Configuring OCFS2 Resources With Hawk2</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-quota"><span class="title-number">20.7 </span><span class="title-name">Using Quotas on OCFS2 File Systems</span></a></span></li><li><span class="sect1"><a href="#sec-ha-ocfs2-more"><span class="title-number">20.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-gfs2"><span class="title-number">21 </span><span class="title-name">GFS2</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-gfs2-utils"><span class="title-number">21.1 </span><span class="title-name">GFS2 Packages and Management Utilities</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-create-service"><span class="title-number">21.2 </span><span class="title-name">Configuring GFS2 Services and a STONITH Resource</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-create"><span class="title-number">21.3 </span><span class="title-name">Creating GFS2 Volumes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-gfs2-mount"><span class="title-number">21.4 </span><span class="title-name">Mounting GFS2 Volumes</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-drbd"><span class="title-number">22 </span><span class="title-name">DRBD</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-drbd-overview"><span class="title-number">22.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-install"><span class="title-number">22.2 </span><span class="title-name">Installing DRBD Services</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-configure"><span class="title-number">22.3 </span><span class="title-name">Setting Up DRBD Service</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-migrate"><span class="title-number">22.4 </span><span class="title-name">Migrating from DRBD 8 to DRBD 9</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-resource-stacking"><span class="title-number">22.5 </span><span class="title-name">Creating a Stacked DRBD Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-fencing"><span class="title-number">22.6 </span><span class="title-name">Using Resource-Level Fencing with STONITH</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-test"><span class="title-number">22.7 </span><span class="title-name">Testing the DRBD Service</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-monitor"><span class="title-number">22.8 </span><span class="title-name">Monitoring DRBD Devices</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-tuning"><span class="title-number">22.9 </span><span class="title-name">Tuning DRBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-trouble"><span class="title-number">22.10 </span><span class="title-name">Troubleshooting DRBD</span></a></span></li><li><span class="sect1"><a href="#sec-ha-drbd-more"><span class="title-number">22.11 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-clvm"><span class="title-number">23 </span><span class="title-name">Cluster Logical Volume Manager (Cluster LVM)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-clvm-overview"><span class="title-number">23.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-config"><span class="title-number">23.2 </span><span class="title-name">Configuration of Cluster LVM</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-drbd"><span class="title-number">23.3 </span><span class="title-name">Configuring Eligible LVM Devices Explicitly</span></a></span></li><li><span class="sect1"><a href="#sec-ha-clvm-migrate"><span class="title-number">23.4 </span><span class="title-name">Online Migration from Mirror LV to Cluster MD</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-cluster-md"><span class="title-number">24 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-cluster-md-overview"><span class="title-number">24.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-create"><span class="title-number">24.2 </span><span class="title-name">Creating a Clustered MD RAID Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-ra"><span class="title-number">24.3 </span><span class="title-name">Configuring a Resource Agent</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-add"><span class="title-number">24.4 </span><span class="title-name">Adding a Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-readd"><span class="title-number">24.5 </span><span class="title-name">Re-adding a Temporarily Failed Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-dev-remove"><span class="title-number">24.6 </span><span class="title-name">Removing a Device</span></a></span></li><li><span class="sect1"><a href="#sec-ha-cluster-md-convert-raid"><span class="title-number">24.7 </span><span class="title-name">Assembling Cluster MD as normal RAID at the disaster recovery site</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-samba"><span class="title-number">25 </span><span class="title-name">Samba Clustering</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-samba-overview"><span class="title-number">25.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-basicconf"><span class="title-number">25.2 </span><span class="title-name">Basic Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-ad"><span class="title-number">25.3 </span><span class="title-name">Joining an Active Directory Domain</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-testing"><span class="title-number">25.4 </span><span class="title-name">Debugging and Testing Clustered Samba</span></a></span></li><li><span class="sect1"><a href="#sec-ha-samba-moreinfo"><span class="title-number">25.5 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-rear"><span class="title-number">26 </span><span class="title-name">Disaster Recovery with ReaR (Relax-and-Recover)</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-rear-concept"><span class="title-number">26.1 </span><span class="title-name">Conceptual Overview</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-config"><span class="title-number">26.2 </span><span class="title-name">Setting Up ReaR and Your Backup Solution</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-mkbackup"><span class="title-number">26.3 </span><span class="title-name">Creating the Recovery Installation System</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-testing"><span class="title-number">26.4 </span><span class="title-name">Testing the Recovery Process</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-recover"><span class="title-number">26.5 </span><span class="title-name">Recovering from Disaster</span></a></span></li><li><span class="sect1"><a href="#sec-ha-rear-more"><span class="title-number">26.6 </span><span class="title-name">For More Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-maintenance"><span class="title-number">IV </span><span class="title-name">Maintenance and Upgrade</span></a></span><ul><li><span class="chapter"><a href="#cha-ha-maintenance"><span class="title-number">27 </span><span class="title-name">Executing Maintenance Tasks</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-maint-outline"><span class="title-number">27.1 </span><span class="title-name">Preparing and Finishing Maintenance Work</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-overview"><span class="title-number">27.2 </span><span class="title-name">Different Options for Maintenance Tasks</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-cluster"><span class="title-number">27.3 </span><span class="title-name">Putting the Cluster into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-node"><span class="title-number">27.4 </span><span class="title-name">Putting a Node into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-node-standby"><span class="title-number">27.5 </span><span class="title-name">Putting a Node into Standby Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-shutdown-node"><span class="title-number">27.6 </span><span class="title-name">Stopping the Cluster Services on a Node</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-mode-rsc"><span class="title-number">27.7 </span><span class="title-name">Putting a Resource into Maintenance Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-rsc-unmanaged"><span class="title-number">27.8 </span><span class="title-name">Putting a Resource into Unmanaged Mode</span></a></span></li><li><span class="sect1"><a href="#sec-ha-maint-shutdown-node-maint-mode"><span class="title-number">27.9 </span><span class="title-name">Rebooting a Cluster Node While in Maintenance Mode</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ha-migration"><span class="title-number">28 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-migration-terminology"><span class="title-number">28.1 </span><span class="title-name">Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-upgrade"><span class="title-number">28.2 </span><span class="title-name">Upgrading your Cluster to the Latest Product Version</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-update"><span class="title-number">28.3 </span><span class="title-name">Updating Software Packages on Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-ha-migration-more"><span class="title-number">28.4 </span><span class="title-name">For More Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-appendix"><span class="title-number">V </span><span class="title-name">Appendix</span></a></span><ul><li><span class="appendix"><a href="#app-ha-troubleshooting"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></span><ul><li><span class="sect1"><a href="#sec-ha-troubleshooting-install"><span class="title-number">A.1 </span><span class="title-name">Installation and First Steps</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-log"><span class="title-number">A.2 </span><span class="title-name">Logging</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-resource"><span class="title-number">A.3 </span><span class="title-name">Resources</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-stonith"><span class="title-number">A.4 </span><span class="title-name">STONITH and Fencing</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-history"><span class="title-number">A.5 </span><span class="title-name">History</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-hawk2"><span class="title-number">A.6 </span><span class="title-name">Hawk2</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-misc"><span class="title-number">A.7 </span><span class="title-name">Miscellaneous</span></a></span></li><li><span class="sect1"><a href="#sec-ha-troubleshooting-moreinfo"><span class="title-number">A.8 </span><span class="title-name">For More Information</span></a></span></li></ul></li><li><span class="appendix"><a href="#app-naming"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></span></li><li><span class="appendix"><a href="#app-ha-management"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></span></li><li><span class="appendix"><a href="#app-crmreport-nonroot"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></span><ul><li><span class="sect1"><a href="#sec-crmreport-nonroot-user"><span class="title-number">D.1 </span><span class="title-name">Creating a Local User Account</span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-ssh"><span class="title-number">D.2 </span><span class="title-name">Configuring a Passwordless SSH Account</span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-sudo"><span class="title-number">D.3 </span><span class="title-name">Configuring <code class="command">sudo</code></span></a></span></li><li><span class="sect1"><a href="#sec-crmreport-nonroot-execute"><span class="title-number">D.4 </span><span class="title-name">Generating a Cluster Report</span></a></span></li></ul></li></ul></li><li><span class="glossary"><a href="#gl-heartb"><span class="title-name">Glossary</span></a></span></li><li><span class="appendix"><a href="#id-1.4.9"><span class="title-number">E </span><span class="title-name">GNU licenses</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.9.4"><span class="title-number">E.1 </span><span class="title-name">GNU Free Documentation License</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.4.3.3.5.11"><span class="number">1.1 </span><span class="name">Three-Server Cluster</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.5.14"><span class="number">1.2 </span><span class="name">Three-Server Cluster after One Server Fails</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.6.5"><span class="number">1.3 </span><span class="name">Typical Fibre Channel Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.6.7"><span class="number">1.4 </span><span class="name">Typical iSCSI Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.6.9"><span class="number">1.5 </span><span class="name">Typical Cluster Configuration Without Shared Storage</span></a></span></li><li><span class="figure"><a href="#fig-ha-architecture"><span class="number">1.6 </span><span class="name">Architecture</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.5.7"><span class="number">4.1 </span><span class="name">YaST Cluster—Multicast Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.5.10"><span class="number">4.2 </span><span class="name">YaST Cluster—Unicast Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.6.4"><span class="number">4.3 </span><span class="name">YaST Cluster—Security</span></a></span></li><li><span class="figure"><a href="#fig-ha-installation-setup-conntrackd"><span class="number">4.4 </span><span class="name">YaST Cluster—<code class="systemitem">conntrackd</code></span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.8.4"><span class="number">4.5 </span><span class="name">YaST Cluster—Services</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.9.8.3"><span class="number">4.6 </span><span class="name">YaST <span class="guimenu">Cluster</span>—Csync2</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.3.6.8.4.3.4"><span class="number">5.1 </span><span class="name">Hawk2—Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.3.6.10.4"><span class="number">5.2 </span><span class="name">Hawk2—Wizard for Apache Web Server</span></a></span></li><li><span class="figure"><a href="#fig-hawk2-batch"><span class="number">5.3 </span><span class="name">Hawk2 Batch Mode Activated</span></a></span></li><li><span class="figure"><a href="#fig-hawk2-batch-show"><span class="number">5.4 </span><span class="name">Hawk2 Batch Mode—Injected Invents and Configuration Changes</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.6.7.3.8.4"><span class="number">6.1 </span><span class="name">Hawk2—Primitive Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.7.5"><span class="number">6.2 </span><span class="name">Group Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.7.8.4"><span class="number">6.3 </span><span class="name">Hawk2—Resource Group</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.8.8.4"><span class="number">6.4 </span><span class="name">Hawk2—Clone Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.9.5.4"><span class="number">6.5 </span><span class="name">Hawk2—Multi-state Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.11.4.3.9.2"><span class="number">6.6 </span><span class="name">Hawk2—STONITH Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.4.12.8.4"><span class="number">6.7 </span><span class="name">Hawk2—Resource Details</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.5.6.4.3"><span class="number">7.1 </span><span class="name">Hawk2—Location Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.5.7.4.3"><span class="number">7.2 </span><span class="name">Hawk2—Colocation Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.5.8.4.3"><span class="number">7.3 </span><span class="name">Hawk2—Order Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.5.9.4.2.3.2.4.2"><span class="number">7.4 </span><span class="name">Hawk2—Two Resource Sets in a Colocation Constraint</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.4.3.3.4.3"><span class="number">8.1 </span><span class="name">Hawk2—Editing A Primitive Resource</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.6.10.6.3"><span class="number">8.2 </span><span class="name">Hawk2—Tag</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.9.3.4.4"><span class="number">11.1 </span><span class="name">Hawk2—Cluster Status</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.9.3.5.5.3.3"><span class="number">11.2 </span><span class="name">Hawk2 Dashboard with One Cluster Site (<code class="literal">amsterdam</code>)</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.9.5.8.5"><span class="number">11.3 </span><span class="name">Hawk2—History Explorer Main View</span></a></span></li><li><span class="figure"><a href="#fig-ha-lvs-yast-global"><span class="number">17.1 </span><span class="name">YaST IP Load Balancing—Global Parameters</span></a></span></li><li><span class="figure"><a href="#fig-ha-lvs-yast-virtual"><span class="number">17.2 </span><span class="name">YaST IP Load Balancing—Virtual Services</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-concept"><span class="number">22.1 </span><span class="name">Position of DRBD within Linux</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-yast-resconfig"><span class="number">22.2 </span><span class="name">Resource Configuration</span></a></span></li><li><span class="figure"><a href="#fig-ha-drbd-resource-stacking"><span class="number">22.3 </span><span class="name">Resource Stacking</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.6.10.3"><span class="number">22.4 </span><span class="name">Showing a Good Connection by <code class="command">drbdmon</code></span></a></span></li><li><span class="figure"><a href="#id-1.4.5.6.10.5"><span class="number">22.5 </span><span class="name">Showing a Bad Connection by <code class="command">drbdmon</code></span></a></span></li><li><span class="figure"><a href="#fig-ha-clvm-scenario-iscsi"><span class="number">23.1 </span><span class="name">Setup of a Shared Disk with Cluster LVM</span></a></span></li><li><span class="figure"><a href="#fig-ha-samba-overview"><span class="number">25.1 </span><span class="name">Structure of a CTDB Cluster</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#id-1.4.3.4.4.5"><span class="number">2.1 </span><span class="name">System Roles and Installed Patterns</span></a></span></li><li><span class="table"><a href="#id-1.4.4.3.7.12.8.9"><span class="number">5.1 </span><span class="name">Common Parameters</span></a></span></li><li><span class="table"><a href="#id-1.4.4.4.16.3"><span class="number">6.1 </span><span class="name">Resource Operation Properties</span></a></span></li><li><span class="table"><a href="#id-1.4.4.8.5.4"><span class="number">10.1 </span><span class="name">Failure Recovery Types</span></a></span></li><li><span class="table"><a href="#id-1.4.4.8.5.7"><span class="number">10.2 </span><span class="name">OCF Return Codes</span></a></span></li><li><span class="table"><a href="#tab-fencing-classes"><span class="number">12.1 </span><span class="name">Classes of fencing</span></a></span></li><li><span class="table"><a href="#tab-ha-storage-protect-watchdog-drivers"><span class="number">13.1 </span><span class="name">Commonly used watchdog drivers</span></a></span></li><li><span class="table"><a href="#tab-ha-acl-operator"><span class="number">15.1 </span><span class="name">Operator Role—Access Types and XPath Expressions</span></a></span></li><li><span class="table"><a href="#id-1.4.5.4.4.4"><span class="number">20.1 </span><span class="name">OCFS2 Utilities</span></a></span></li><li><span class="table"><a href="#tab-ha-ofcs2-mkfs-ocfs2-params"><span class="number">20.2 </span><span class="name">Important OCFS2 Parameters</span></a></span></li><li><span class="table"><a href="#id-1.4.5.5.4.4"><span class="number">21.1 </span><span class="name">GFS2 Utilities</span></a></span></li><li><span class="table"><a href="#tab-ha-gfs2-mkfs-gfs2-params"><span class="number">21.2 </span><span class="name">Important GFS2 Parameters</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-ha-config-basics-corosync-quorum"><span class="number">5.1 </span><span class="name">Excerpt of Corosync Configuration for a Two-Node Cluster</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.4.7.5"><span class="number">5.2 </span><span class="name">Excerpt of Corosync Configuration for an N-Node Cluster</span></a></span></li><li><span class="example"><a href="#ex-ha-manual-config-crmshellscripts"><span class="number">5.3 </span><span class="name">A Simple crmsh Shell Script</span></a></span></li><li><span class="example"><a href="#ex-ha-config-resource-group"><span class="number">6.1 </span><span class="name">Resource Group for a Web Server</span></a></span></li><li><span class="example"><a href="#ex-config-basic-resourceset-loc"><span class="number">7.1 </span><span class="name">A Resource Set for Location Constraints</span></a></span></li><li><span class="example"><a href="#id-1.4.4.5.9.5.4"><span class="number">7.2 </span><span class="name">A Chain of Colocated Resources</span></a></span></li><li><span class="example"><a href="#id-1.4.4.5.9.5.8"><span class="number">7.3 </span><span class="name">A Chain of Ordered Resources</span></a></span></li><li><span class="example"><a href="#id-1.4.4.5.9.5.10"><span class="number">7.4 </span><span class="name">A Chain of Ordered Resources Expressed as Resource Set</span></a></span></li><li><span class="example"><a href="#ex-ha-config-basics-failover"><span class="number">7.5 </span><span class="name">Migration Threshold—Process Flow</span></a></span></li><li><span class="example"><a href="#ex-ha-nagios-config"><span class="number">9.1 </span><span class="name">Configuring Resources for Monitoring Plug-ins</span></a></span></li><li><span class="example"><a href="#id-1.4.4.10.5.11.5"><span class="number">12.1 </span><span class="name">Configuration of an IBM RSA Lights-out Device</span></a></span></li><li><span class="example"><a href="#id-1.4.4.10.5.11.6"><span class="number">12.2 </span><span class="name">Configuration of a UPS Fencing Device</span></a></span></li><li><span class="example"><a href="#ex-ha-fencing-kdump"><span class="number">12.3 </span><span class="name">Configuration of a Kdump Device</span></a></span></li><li><span class="example"><a href="#ex-ha-storage-protect-sbd-timings"><span class="number">13.1 </span><span class="name">Formula for Timeout Calculation</span></a></span></li><li><span class="example"><a href="#ex-ha-qdevice-crm-corosync-status-quorum"><span class="number">14.1 </span><span class="name">Status of QDevice</span></a></span></li><li><span class="example"><a href="#ex-ha-qdevice-crm-corosync-status-qnetd"><span class="number">14.2 </span><span class="name">Status of QNetd Server</span></a></span></li><li><span class="example"><a href="#ex-ha-acl-excerpt"><span class="number">15.1 </span><span class="name">Excerpt of a Cluster Configuration in XML</span></a></span></li><li><span class="example"><a href="#ex-ha-lvs-ldirectord"><span class="number">17.1 </span><span class="name">Simple ldirectord Configuration</span></a></span></li><li><span class="example"><a href="#exa-ha-drbd-stacked-drbd"><span class="number">22.1 </span><span class="name">Configuration of a Three-Node Stacked DRBD Resource</span></a></span></li><li><span class="example"><a href="#ex-ha-drbd-fencing"><span class="number">22.2 </span><span class="name">Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB)</span></a></span></li><li><span class="example"><a href="#ex-ha-rear-nfs-server-backup"><span class="number">26.1 </span><span class="name">Using an NFS Server to Store the File Backup</span></a></span></li><li><span class="example"><a href="#ex-ha-rear-config-EMC"><span class="number">26.2 </span><span class="name">Using Third-Party Backup Tools Like EMC NetWorker</span></a></span></li><li><span class="example"><a href="#id-1.4.7.2.4.2.4.2.4"><span class="number">A.1 </span><span class="name">Stopped Resources</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.4.1.6"><p>
  Copyright © 2006–2024

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Permission is granted to copy, distribute and/or modify this document under
  the terms of the GNU Free Documentation License, Version 1.2 or (at your
  option) version 1.3; with the Invariant Section being this copyright notice
  and license. A copy of the license version 1.2 is included in the section
  entitled <span class="quote">“<span class="quote">GNU Free Documentation License</span>”</span>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="pre-ha" data-id-title="About This Guide"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">About This Guide</span></span> <a title="Permalink" class="permalink" href="#pre-ha">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_intro.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p> This guide is intended for administrators who need to set up, configure,
  and maintain clusters with SUSE® Linux Enterprise High Availability. For quick and efficient
  configuration and administration, the product includes both a graphical user
  interface and a command line interface (CLI). For performing key tasks,
  both approaches are covered in this guide. Thus, you can choose the appropriate
  tool that matches your needs.</p></div></div></div></div><p>
  This guide is divided into the following parts:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.4.1"><span class="term"><a class="xref" href="#part-install" title="Part I. Installation and Setup">Installation and Setup</a>
   </span></dt><dd><p>
     Before starting to install and configure your cluster, make yourself
     familiar with cluster fundamentals and architecture, get an overview of
     the key features and benefits. Learn which hardware and software
     requirements must be met and what preparations to take before executing
     the next steps. Perform the installation and basic setup of your HA
     cluster using YaST. Learn how to upgrade your cluster to the most recent
     release version or how to update individual packages.
    </p></dd><dt id="id-1.4.2.4.2"><span class="term"><a class="xref" href="#part-config" title="Part II. Configuration and Administration">Configuration and Administration</a>
   </span></dt><dd><p>
     Add, configure and manage cluster resources with either the Web
     interface (Hawk2), or the command line interface (crmsh). To
     avoid unauthorized access to the cluster configuration, define roles
     and assign them to certain users for fine-grained control. Learn how to
     use load balancing and fencing. If you consider writing your own
     resource agents or modifying existing ones, get some background
     information on how to create different types of resource agents.
    </p></dd><dt id="id-1.4.2.4.3"><span class="term"><a class="xref" href="#part-storage" title="Part III. Storage and Data Replication">Storage and Data Replication</a>
   </span></dt><dd><p>
     SUSE Linux Enterprise High Availability ships with the cluster-aware file systems OCFS2 and
     GFS2, and the Cluster Logical Volume Manager (Cluster LVM). For
     replication of your data, use DRBD*. It lets you mirror the data
     of a High Availability service from the active node of a cluster to its standby
     node. Furthermore, a clustered Samba server also provides a High Availability
     solution for heterogeneous environments.
    </p></dd><dt id="id-1.4.2.4.4"><span class="term"><a class="xref" href="#part-appendix" title="Part V. Appendix">Appendix</a>
   </span></dt><dd><p>Contains an overview of common problems and their solution. Presents the
     naming conventions used in this documentation with regard to clusters,
     resources and constraints. 
     Contains a glossary with HA-specific terminology.</p></dd></dl></div><section xml:lang="en" class="sect1" id="id-1.4.2.5" data-id-title="Available documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Available documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.5">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_available_doc.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.5.3.1"><span class="term">Online documentation</span></dt><dd><p>
     Our documentation is available online at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
     Browse or download the documentation in various formats.
    </p><div id="id-1.4.2.5.3.1.2.2" data-id-title="Latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Latest updates</div><p>
      The latest updates are usually available in the English-language version of this documentation.
     </p></div></dd><dt id="id-1.4.2.5.3.2"><span class="term">SUSE Knowledgebase</span></dt><dd><p>
     If you have run into an issue, also check out the Technical Information
     Documents (TIDs) that are available online at <a class="link" href="https://www.suse.com/support/kb/" target="_blank">https://www.suse.com/support/kb/</a>.
     Search the SUSE Knowledgebase for known solutions driven by customer need.
     </p></dd><dt id="id-1.4.2.5.3.3"><span class="term">Release notes</span></dt><dd><p>
     For release notes, see
     <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
    </p></dd><dt id="id-1.4.2.5.3.4"><span class="term">In your system</span></dt><dd><p>
      For offline use, the release notes are also available under
      <code class="filename">/usr/share/doc/release-notes</code> on your system.
      The documentation for individual packages is available at
      <code class="filename">/usr/share/doc/packages</code>.
    </p><p>
      Many commands are also described in their <span class="emphasis"><em>manual
      pages</em></span>. To view them, run <code class="command">man</code>, followed
      by a specific command name. If the <code class="command">man</code> command is
      not installed on your system, install it with <code class="command">sudo zypper
      install man</code>.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.4.2.6" data-id-title="Improving the documentation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">Improving the documentation</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.6">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_feedback.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Your feedback and contributions to this documentation are welcome.
  The following channels for giving feedback are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.6.4.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.4.2.6.4.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
    </p><p>
     To simplify this process, click the <span class="guimenu">Report
     an issue</span> icon next to a headline in the HTML
     version of this document. This preselects the right product and
     category in Bugzilla and adds a link to the current section.
     You can start typing your bug report right away.
    </p><p>
     A Bugzilla account is required.
    </p></dd><dt id="id-1.4.2.6.4.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, click the <span class="guimenu">Edit source
     document</span> icon next to a headline in the HTML version of
     this document. This will take you to the source code on GitHub, where you
     can open a pull request.</p><p>
     A GitHub account is required.
    </p><div id="id-1.4.2.6.4.3.2.3" data-id-title="Edit source document only available for English" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <span class="guimenu">Edit source document</span> only available for English</div><p>
      The <span class="guimenu">Edit source document</span> icons are only available for the
      English version of each document. For all other languages, use the
      <span class="guimenu">Report an issue</span> icons instead.
     </p></div><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README.
    </p></dd><dt id="id-1.4.2.6.4.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.4.2.7" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.7">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_convention_ha.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. You can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as a non-privileged user:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands can be split into two or multiple lines by a backslash character
    (<code class="literal">\</code>) at the end of a line. The backslash informs the shell that
    the command invocation will continue after the line's end:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">echo</code> a b \
c d</pre></div></li><li class="listitem"><p>
     A code block that shows both the command (preceded by a prompt)
     and the respective output returned by the shell:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code>
output</pre></div></li><li class="listitem"><p>
    Commands executed in the interactive crm shell.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.4.2.7.4.16.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Warning notice</div><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.4.2.7.4.16.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Important notice</div><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.4.2.7.4.16.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Note notice</div><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.4.2.7.4.16.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Tip notice</div><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.4.2.7.4.17.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.4.2.7.4.17.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div><p>
  For an overview of naming conventions for cluster nodes and names,
  resources, and constraints, see <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>.
 </p></section><section xml:lang="en" class="sect1" id="id-1.4.2.8" data-id-title="Support"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Support</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.8">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Find the support statement for SUSE Linux Enterprise High Availability and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.4.2.8.5" data-id-title="Support statement for SUSE Linux Enterprise High Availability"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Linux Enterprise High Availability</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.8.5">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offers available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.8.5.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.4.2.8.5.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate a problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.4.2.8.5.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Linux Enterprise High Availability is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.4.2.8.6" data-id-title="Technology previews"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span></span> <a title="Permalink" class="permalink" href="#id-1.4.2.8.6">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_intro_support.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback.
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or otherwise
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a>.
  </p></section></section></section><div class="part" id="part-install" data-id-title="Installation and Setup"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part I </span><span class="title-name">Installation and Setup </span></span><a title="Permalink" class="permalink" href="#part-install">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-concepts"><span class="title-number">1 </span><span class="title-name">Product Overview</span></a></span></li><dd class="toc-abstract"><p>
    SUSE® Linux Enterprise High Availability is an integrated suite of open source clustering
    technologies. It enables you to implement highly available physical and
    virtual Linux clusters, and to eliminate single points of failure. It
    ensures the high availability and manageability of critical network
    resources including data, applications, and services. Thus, it helps you
    maintain business continuity, protect data integrity, and reduce
    unplanned downtime for your mission-critical Linux workloads.
   </p><p>
    It ships with essential monitoring, messaging, and cluster resource
    management functionality (supporting failover, failback, and migration
    (load balancing) of individually managed cluster resources).
   </p><p>
    This chapter introduces the main product features and benefits of SUSE Linux Enterprise High Availability.
    Inside you will find several example clusters and learn about
    the components making up a cluster. The last section provides an
    overview of the architecture, describing the individual architecture
    layers and processes within the cluster.
   </p><p>
    For explanations of some common terms used in the context of High Availability
    clusters, refer to <a class="xref" href="#gl-heartb" title="Glossary">Glossary</a>.
   </p></dd><li><span class="chapter"><a href="#cha-ha-requirements"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></a></span></li><dd class="toc-abstract"><p>
    The following section informs you about system requirements, and some
    prerequisites for SUSE® Linux Enterprise High Availability. It also includes recommendations
    for cluster setup.
   </p></dd><li><span class="chapter"><a href="#cha-ha-install"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></a></span></li><dd class="toc-abstract"><p>If you are setting up a High Availability cluster with SUSE® Linux Enterprise High Availability for the first time, the
    easiest way is to start with a basic two-node cluster. You can also use the
    two-node cluster to run some tests. Afterward, you can add more
    nodes by cloning existing cluster nodes with AutoYaST. The cloned nodes will
    have the same packages installed and the same system configuration as the
    original ones.
   </p><p>
    If you want to upgrade an existing cluster that runs an older version of
    SUSE Linux Enterprise High Availability, refer to <a class="xref" href="#cha-ha-migration" title="Chapter 28. Upgrading Your Cluster and Updating Software Packages">Chapter 28, <em>Upgrading Your Cluster and Updating Software Packages</em></a>.
   </p></dd><li><span class="chapter"><a href="#cha-ha-ycluster"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></a></span></li><dd class="toc-abstract"><p>The YaST cluster module allows you to set up a cluster manually
    (from scratch) or to modify options for an existing cluster.
   </p><p>
    However, if you prefer an automated approach for setting up a cluster,
    refer to <span class="intraxref">Article “Installation and Setup Quick Start”</span>. It describes how to install the
    needed packages and leads you to a basic two-node cluster, which is
    set up with the <code class="systemitem">ha-cluster-bootstrap</code> scripts.
   </p><p>
    You can also use a combination of both setup methods, for example: set up
    one node with YaST cluster and then use one of the bootstrap scripts
    to integrate more nodes (or vice versa).
   </p></dd></ul></div><section class="chapter" id="cha-ha-concepts" data-id-title="Product Overview"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">Product Overview</span></span> <a title="Permalink" class="permalink" href="#cha-ha-concepts">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    SUSE® Linux Enterprise High Availability is an integrated suite of open source clustering
    technologies. It enables you to implement highly available physical and
    virtual Linux clusters, and to eliminate single points of failure. It
    ensures the high availability and manageability of critical network
    resources including data, applications, and services. Thus, it helps you
    maintain business continuity, protect data integrity, and reduce
    unplanned downtime for your mission-critical Linux workloads.
   </p><p>
    It ships with essential monitoring, messaging, and cluster resource
    management functionality (supporting failover, failback, and migration
    (load balancing) of individually managed cluster resources).
   </p><p>
    This chapter introduces the main product features and benefits of SUSE Linux Enterprise High Availability.
    Inside you will find several example clusters and learn about
    the components making up a cluster. The last section provides an
    overview of the architecture, describing the individual architecture
    layers and processes within the cluster.
   </p><p>
    For explanations of some common terms used in the context of High Availability
    clusters, refer to <a class="xref" href="#gl-heartb" title="Glossary">Glossary</a>.
   </p></div></div></div></div><section class="sect1" id="sec-ha-availability" data-id-title="Availability as a Module or Extension"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">Availability as a Module or Extension</span></span> <a title="Permalink" class="permalink" href="#sec-ha-availability">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   High Availability is available as a module or extension for several products. For details, see
   <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/article-modules.html#art-modules-high-availability" target="_blank">https://documentation.suse.com/sles/html/SLES-all/article-modules.html#art-modules-high-availability</a>.
  </p></section><section class="sect1" id="sec-ha-features" data-id-title="Key Features"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">Key Features</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE® Linux Enterprise High Availability helps you ensure and manage the availability of your
   network resources. The following sections highlight some of the key
   features:
  </p><section class="sect2" id="sec-ha-features-scenarios" data-id-title="Wide Range of Clustering Scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.1 </span><span class="title-name">Wide Range of Clustering Scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability supports the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Active/active configurations
     </p></li><li class="listitem"><p>
      
      Active/passive configurations: N+1, N+M, N to 1, N to M
     </p></li><li class="listitem"><p>
      Hybrid physical and virtual clusters, allowing virtual servers to be
      clustered with physical servers. This improves service availability
      and resource usage.
     </p></li><li class="listitem"><p>
      Local clusters
     </p></li><li class="listitem"><p>
      Metro clusters (<span class="quote">“<span class="quote">stretched</span>”</span> local clusters)
     </p></li><li class="listitem"><p>
      Geo clusters (geographically dispersed clusters)
     </p></li></ul></div><div id="id-1.4.3.3.4.3.4" data-id-title="No support for mixed architectures" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No support for mixed architectures</div><p>
     All nodes belonging to a cluster should have the same processor platform:
     x86, IBM Z, or POWER. Clusters of mixed architectures are
     <span class="emphasis"><em>not</em></span> supported.
    </p></div><p>
    Your cluster can contain up to 32 Linux servers. Using
    pacemaker_remote, the cluster can be extended to include
    additional Linux servers beyond this limit.
    Any server in the cluster can restart resources (applications, services, IP
    addresses, and file systems) from a failed server in the cluster.
   </p></section><section class="sect2" id="sec-ha-features-flexibility" data-id-title="Flexibility"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.2 </span><span class="title-name">Flexibility</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-flexibility">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability ships with Corosync messaging and membership layer
    and Pacemaker Cluster Resource Manager. Using Pacemaker, administrators
    can continually monitor the health and status of their resources, and manage
    dependencies. They can automatically stop and start services based on highly
    configurable rules and policies. SUSE Linux Enterprise High Availability allows you to tailor a
    cluster to the specific applications and hardware infrastructure that
    fit your organization. Time-dependent configuration enables services to
    automatically migrate back to repaired nodes at specified times.
   </p></section><section class="sect2" id="sec-ha-features-storage" data-id-title="Storage and Data Replication"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.3 </span><span class="title-name">Storage and Data Replication</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-storage">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With SUSE Linux Enterprise High Availability you can dynamically assign and reassign server
    storage as needed. It supports Fibre Channel or iSCSI storage area
    networks (SANs). Shared disk systems are also supported, but they are
    not a requirement. SUSE Linux Enterprise High Availability also comes with a cluster-aware file
    system (OCFS2) and the cluster Logical Volume Manager (Cluster LVM).
    For replication of your data, use DRBD* to mirror the data of
    a High Availability service from the active node of a cluster to its standby node.
    Furthermore, SUSE Linux Enterprise High Availability also supports CTDB (Cluster Trivial Database),
    a technology for Samba clustering.
   </p></section><section class="sect2" id="sec-ha-features-virtualized" data-id-title="Support for Virtualized Environments"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.4 </span><span class="title-name">Support for Virtualized Environments</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-virtualized">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability supports the mixed clustering of both physical and
    virtual Linux servers. SUSE Linux Enterprise Server 15 SP2 ships with Xen,
    an open source virtualization hypervisor, and with KVM (Kernel-based
    Virtual Machine). KVM is a virtualization software for Linux which is based on
    hardware virtualization extensions. The cluster resource manager in SUSE Linux Enterprise High Availability
    can recognize, monitor, and manage services running within
    virtual servers and services running in physical servers. Guest
    systems can be managed as services by the cluster.
   </p><div id="id-1.4.3.3.4.6.3" data-id-title="Live migration in High Availability clusters" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Live migration in High Availability clusters</div><p>
     Use caution when performing live migration of nodes in an active cluster.
     The cluster stack might not tolerate an operating system freeze caused by the
     live migration process, which could lead to the node being fenced.
    </p><p>
     We recommend either of the following actions to help avoid node fencing during live migration:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Increase the Corosync token timeout and the SBD watchdog timeout, along with
       any other related settings. The appropriate values depend on your specific setup.
       For more information, see <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="13.5. Calculation of Timeouts">Section 13.5, “Calculation of Timeouts”</a>.
      </p></li><li class="listitem"><p>
       Before performing live migration, stop the cluster services on the node.
       For more information, see <a class="xref" href="#sec-ha-maint-overview" title="27.2. Different Options for Maintenance Tasks">Section 27.2, “Different Options for Maintenance Tasks”</a>.
      </p></li></ul></div><p>
     You <span class="bold"><strong>must</strong></span> thoroughly test this setup before attempting
     live migration in a production environment.
    </p></div></section><section class="sect2" id="sec-ha-features-geo" data-id-title="Support of Local, Metro, and Geo Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.5 </span><span class="title-name">Support of Local, Metro, and Geo Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-geo">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability supports different geographical scenarios,
    including geographically dispersed clusters (Geo clusters).
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.4.7.3.1"><span class="term">Local Clusters</span></dt><dd><p>
       A single cluster in one location (for example, all nodes are located
       in one data center). The cluster uses multicast or unicast for
       communication between the nodes and manages failover internally.
       Network latency can be neglected. Storage is typically accessed
       synchronously by all nodes.
      </p></dd><dt id="id-1.4.3.3.4.7.3.2"><span class="term">Metro Clusters</span></dt><dd><p>
       A single cluster that can stretch over multiple buildings or data
       centers, with all sites connected by fibre channel. The cluster uses
       multicast or unicast for communication between the nodes and manages
       failover internally. Network latency is usually low (&lt;5 ms for
       distances of approximately 20 miles). Storage is frequently
       replicated (mirroring or synchronous replication).
      </p></dd><dt id="id-1.4.3.3.4.7.3.3"><span class="term">Geo Clusters (Multi-Site Clusters)</span></dt><dd><p>
       Multiple, geographically dispersed sites with a local cluster each. The
       sites communicate via IP. Failover across the sites is coordinated by
       a higher-level entity. Geo clusters need to cope with limited
       network bandwidth and high latency. Storage is replicated
       asynchronously.
      </p><div id="id-1.4.3.3.4.7.3.3.2.2" data-id-title="Geo clustering and SAP workloads" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Geo clustering and SAP workloads</div><p>
        Currently Geo clusters do neither support SAP HANA system replication
        nor SAP S/4HANA and SAP NetWeaver enqueue replication setups.
       </p></div></dd></dl></div><p>
    The greater the geographical distance between individual cluster nodes,
    the more factors may potentially disturb the high availability of
    services the cluster provides. Network latency, limited bandwidth and
    access to storage are the main challenges for long-distance clusters.
   </p></section><section class="sect2" id="sec-ha-features-ra" data-id-title="Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.6 </span><span class="title-name">Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-ra">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability includes a huge number of resource agents to manage
    resources such as Apache, IPv4, IPv6 and many more. It also ships with
    resource agents for popular third party applications such as IBM
    WebSphere Application Server. For an overview of Open Cluster Framework
    (OCF) resource agents included with your product, use the <code class="command">crm
    ra</code> command as described in
    <a class="xref" href="#sec-ha-manual-config-ocf" title="5.5.3. Displaying Information about OCF Resource Agents">Section 5.5.3, “Displaying Information about OCF Resource Agents”</a>.
   </p></section><section class="sect2" id="sec-ha-features-tools" data-id-title="User-friendly Administration Tools"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.2.7 </span><span class="title-name">User-friendly Administration Tools</span></span> <a title="Permalink" class="permalink" href="#sec-ha-features-tools">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability ships with a set of powerful tools. Use them for basic installation
    and setup of your cluster and for effective configuration and
    administration:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.4.9.3.1"><span class="term">YaST </span></dt><dd><p>
       A graphical user interface for general system installation and
       administration. Use it to install SUSE Linux Enterprise High Availability on top of SUSE Linux Enterprise Server as
       described in the Installation and Setup Quick Start. YaST
       also provides the following modules in the High Availability category to help
       configure your cluster or individual components:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Cluster: Basic cluster setup. For details, refer to
         <a class="xref" href="#cha-ha-ycluster" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
        </p></li><li class="listitem"><p>
         DRBD: Configuration of a Distributed Replicated Block Device.
        </p></li><li class="listitem"><p>
         IP Load Balancing: Configuration of load balancing with Linux Virtual Server or
         HAProxy. For details, refer to <a class="xref" href="#cha-ha-lb" title="Chapter 17. Load Balancing">Chapter 17, <em>Load Balancing</em></a>.
        </p></li></ul></div></dd><dt id="id-1.4.3.3.4.9.3.2"><span class="term">Hawk2</span></dt><dd><p>
       A user-friendly Web-based interface with which you can monitor and
       administer your High Availability clusters from Linux or non-Linux machines alike.
       Hawk2 can be accessed from any machine inside or outside of the cluster
       by using a (graphical) Web browser. Therefore it is the ideal solution
       even if the system on which you are working only provides a minimal graphical
       user interface. For details, <a class="xref" href="#cha-conf-hawk2" title="5.4. Introduction to Hawk2">Section 5.4, “Introduction to Hawk2”</a>.
      </p></dd><dt id="id-1.4.3.3.4.9.3.3"><span class="term"><code class="command">crm</code> Shell
     </span></dt><dd><p>
       A powerful unified command line interface to configure resources and
       execute all monitoring or administration tasks. For details, refer to
       <a class="xref" href="#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a>.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-benefits" data-id-title="Benefits"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">Benefits</span></span> <a title="Permalink" class="permalink" href="#sec-ha-benefits">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE Linux Enterprise High Availability allows you to configure up to 32 Linux servers into a
   high-availability cluster (HA cluster). Resources can be
   dynamically switched or moved to any node in the cluster. Resources can
   be configured to automatically migrate if a node fails, or they can be
   moved manually to troubleshoot hardware or balance the workload.
  </p><p>
   SUSE Linux Enterprise High Availability provides high availability from commodity components. Lower
   costs are obtained through the consolidation of applications and
   operations onto a cluster. SUSE Linux Enterprise High Availability also allows you to centrally
   manage the complete cluster. You can adjust resources to meet changing
   workload requirements (thus, manually <span class="quote">“<span class="quote">load balance</span>”</span> the
   cluster). Allowing clusters of more than two nodes also provides savings
   by allowing several nodes to share a <span class="quote">“<span class="quote">hot spare</span>”</span>.
  </p><p>
   An equally important benefit is the potential reduction of unplanned
   service outages and planned outages for software and hardware
   maintenance and upgrades.
  </p><p>
   Reasons that you would want to implement a cluster include:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Increased availability
    </p></li><li class="listitem"><p>
     Improved performance
    </p></li><li class="listitem"><p>
     Low cost of operation
    </p></li><li class="listitem"><p>
     Scalability
    </p></li><li class="listitem"><p>
     Disaster recovery
    </p></li><li class="listitem"><p>
     Data protection
    </p></li><li class="listitem"><p>
     Server consolidation
    </p></li><li class="listitem"><p>
     Storage consolidation
    </p></li></ul></div><p>
   Shared disk fault tolerance can be obtained by implementing RAID on the
   shared disk subsystem.
  </p><p>
   The following scenario illustrates some benefits SUSE Linux Enterprise High Availability can
   provide.
  </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.3.3.5.9"><span class="name">Example Cluster Scenario</span><a title="Permalink" class="permalink" href="#id-1.4.3.3.5.9">#</a></h2></div><p>
   Suppose you have configured a three-node cluster, with a Web server
   installed on each of the three nodes in the cluster. Each of the
   nodes in the cluster hosts two Web sites. All the data, graphics, and
   Web page content for each Web site are stored on a shared disk subsystem
   connected to each of the nodes in the cluster. The following figure
   depicts how this setup might look.
  </p><div class="figure" id="id-1.4.3.3.5.11"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example1.png"><img src="images/ha_cluster_example1.png" width="85%" alt="Three-Server Cluster" title="Three-Server Cluster"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.1: </span><span class="title-name">Three-Server Cluster </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.5.11">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   During normal cluster operation, each node is in constant communication
   with the other nodes in the cluster and performs periodic polling of
   all registered resources to detect failure.
  </p><p>
   Suppose Web Server 1 experiences hardware or software problems and the
   users depending on Web Server 1 for Internet access, e-mail, and
   information lose their connections. The following figure shows how
   resources are moved when Web Server 1 fails.
  </p><div class="figure" id="id-1.4.3.3.5.14"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example2.png"><img src="images/ha_cluster_example2.png" width="75%" alt="Three-Server Cluster after One Server Fails" title="Three-Server Cluster after One Server Fails"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.2: </span><span class="title-name">Three-Server Cluster after One Server Fails </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.5.14">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Web Site A moves to Web Server 2 and Web Site B moves to Web Server 3. IP
   addresses and certificates also move to Web Server 2 and Web Server 3.
  </p><p>
   When you configured the cluster, you decided where the Web sites hosted
   on each Web server would go should a failure occur. In the previous
   example, you configured Web Site A to move to Web Server 2 and Web Site B
   to move to Web Server 3. This way, the workload formerly handled by Web
   Server 1 continues to be available and is evenly distributed between any
   surviving cluster members.
  </p><p>
   When Web Server 1 failed, the High Availability software did the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Detected a failure and verified with STONITH that Web Server 1 was
     really dead. STONITH is an acronym for <span class="quote">“<span class="quote">Shoot The Other Node
     In The Head</span>”</span>. It is a means of bringing down misbehaving nodes
     to prevent them from causing trouble in the cluster.
    </p></li><li class="listitem"><p>
     Remounted the shared data directories that were formerly mounted on Web
     server 1 on Web Server 2 and Web Server 3.
    </p></li><li class="listitem"><p>
     Restarted applications that were running on Web Server 1 on Web Server
     2 and Web Server 3.
    </p></li><li class="listitem"><p>
     Transferred IP addresses to Web Server 2 and Web Server 3.
    </p></li></ul></div><p>
   In this example, the failover process happened quickly and users regained
   access to Web site information within seconds, usually without needing to
   log in again.
  </p><p>
   Now suppose the problems with Web Server 1 are resolved, and Web Server 1
   is returned to a normal operating state. Web Site A and Web Site B can
   either automatically fail back (move back) to Web Server 1, or they can
   stay where they are. This depends on how you configured the resources for
   them. Migrating the services back to Web Server 1 will incur some
   down-time. Therefore SUSE Linux Enterprise High Availability also allows you to defer the migration until
   a period when it will cause little or no service interruption. There are
   advantages and disadvantages to both alternatives.
  </p><p>
   SUSE Linux Enterprise High Availability also provides resource migration capabilities. You can move
   applications, Web sites, etc. to other servers in your cluster as
   required for system management.
  </p><p>
   For example, you could have manually moved Web Site A or Web Site B from
   Web Server 1 to either of the other servers in the cluster. Use cases for
   this are upgrading or performing scheduled maintenance on Web Server 1,
   or increasing performance or accessibility of the Web sites.
  </p></section><section class="sect1" id="sec-ha-clusterconfig" data-id-title="Cluster Configurations: Storage"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">Cluster Configurations: Storage</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clusterconfig">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Cluster configurations with SUSE Linux Enterprise High Availability might or might not include a
   shared disk subsystem. The shared disk subsystem can be connected via
   high-speed Fibre Channel cards, cables, and switches, or it can be
   configured to use iSCSI. If a node fails, another designated node in
   the cluster automatically mounts the shared disk directories that were
   previously mounted on the failed node. This gives network users
   continuous access to the directories on the shared disk subsystem.
  </p><div id="id-1.4.3.3.6.3" data-id-title="Shared Disk Subsystem with LVM" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Shared Disk Subsystem with LVM</div><p>
    When using a shared disk subsystem with LVM, that subsystem must be
    connected to all servers in the cluster from which it needs to be
    accessed.
   </p></div><p>
   Typical resources might include data, applications, and services. The
   following figures show how a typical Fibre Channel cluster configuration
   might look.
   The green lines depict connections to an Ethernet power switch. Such
   a device can be controlled over a network and can reboot
   a node when a ping request fails.
  </p><div class="figure" id="id-1.4.3.3.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example3.png"><img src="images/ha_cluster_example3.png" width="85%" alt="Typical Fibre Channel Cluster Configuration" title="Typical Fibre Channel Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.3: </span><span class="title-name">Typical Fibre Channel Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Although Fibre Channel provides the best performance, you can also
   configure your cluster to use iSCSI. iSCSI is an alternative to Fibre
   Channel that can be used to create a low-cost Storage Area Network (SAN).
   The following figure shows how a typical iSCSI cluster configuration
   might look.
  </p><div class="figure" id="id-1.4.3.3.6.7"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example4.png"><img src="images/ha_cluster_example4.png" width="100%" alt="Typical iSCSI Cluster Configuration" title="Typical iSCSI Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.4: </span><span class="title-name">Typical iSCSI Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.6.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><p>
   Although most clusters include a shared disk subsystem, it is also
   possible to create a cluster without a shared disk subsystem. The
   following figure shows how a cluster without a shared disk subsystem
   might look.
  </p><div class="figure" id="id-1.4.3.3.6.9"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_cluster_example5.png"><img src="images/ha_cluster_example5.png" width="100%" alt="Typical Cluster Configuration Without Shared Storage" title="Typical Cluster Configuration Without Shared Storage"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.5: </span><span class="title-name">Typical Cluster Configuration Without Shared Storage </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.3.6.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-architecture" data-id-title="Architecture"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.5 </span><span class="title-name">Architecture</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This section provides a brief overview of SUSE Linux Enterprise High Availability architecture. It
   identifies and provides information on the architectural components, and
   describes how those components interoperate.
  </p><section class="sect2" id="sec-ha-architecture-layers" data-id-title="Architecture Layers"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.1 </span><span class="title-name">Architecture Layers</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability has a layered architecture.
    <a class="xref" href="#fig-ha-architecture" title="Architecture">Figure 1.6, “Architecture”</a> illustrates
    the different layers and their associated components.
   </p><div class="figure" id="fig-ha-architecture"><div class="figure-contents"><div class="mediaobject"><a href="images/cluster_stack_arch.png"><img src="images/cluster_stack_arch.png" width="100%" alt="Architecture" title="Architecture"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 1.6: </span><span class="title-name">Architecture </span></span><a title="Permalink" class="permalink" href="#fig-ha-architecture">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div><section class="sect3" id="sec-ha-architecture-layers-coro" data-id-title="Membership and Messaging Layer (Corosync)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.1 </span><span class="title-name">Membership and Messaging Layer (Corosync)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-coro">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     This component provides reliable messaging, membership, and quorum information
     about the cluster. This is handled by the Corosync cluster engine, a group
     communication system.
    </p></section><section class="sect3" id="sec-ha-architecture-layers-crm" data-id-title="Cluster Resource Manager (Pacemaker)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.2 </span><span class="title-name">Cluster Resource Manager (Pacemaker)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-crm">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Pacemaker as cluster resource manager is the <span class="quote">“<span class="quote">brain</span>”</span>
      which reacts to events occurring in the cluster. It is implemented as
      <code class="systemitem">pacemaker-controld</code>, the cluster
      controller, which coordinates all actions. Events can be nodes that join
      or leave the cluster, failure of resources, or scheduled activities such
      as maintenance, for example.
     </p><div class="variablelist"><dl class="variablelist"><dt id="vle-lrm"><span class="term">Local Resource Manager</span></dt><dd><p>
        The local resource manager is located between the Pacemaker layer and the
        resources layer on each node. It is implemented as <code class="systemitem">pacemaker-execd</code> daemon. Through this daemon,
        Pacemaker can start, stop, and monitor resources.
      </p></dd><dt id="vle-cib"><span class="term">Cluster Information Database (CIB)</span></dt><dd><p>
         On every node, Pacemaker maintains the cluster information database
         (CIB). It is an XML representation of the cluster configuration
         (including cluster options, nodes, resources, constraints and the
         relationship to each other). The CIB also reflects the current cluster
         status. Each cluster node contains a CIB replica, which is synchronized
         across the whole cluster. The <code class="systemitem">pacemaker-based</code>
         daemon takes care of reading and writing cluster configuration and
         status.</p></dd><dt id="vle-dc"><span class="term">Designated Coordinator (DC)</span></dt><dd><p>
        The DC is elected from all nodes in the cluster. This happens if there
        is no DC yet or if the current DC leaves the cluster for any reason.
        The DC is the only entity in the cluster that can decide that a
        cluster-wide change needs to be performed, such as fencing a node or
        moving resources around. All other nodes get their configuration and
        resource allocation information from the current DC.
       </p></dd><dt id="vle-pe"><span class="term">Policy Engine</span></dt><dd><p>
        The policy engine runs on every node, but the one on the DC is the active
        one. The engine is implemented as
        <code class="systemitem">pacemaker-schedulerd</code> daemon.
        When a cluster transition is needed, based on the current state and
        configuration, <code class="systemitem">pacemaker-schedulerd</code>
        calculates the expected next state of the cluster. It determines what
        actions need to be scheduled to achieve the next state.
       </p></dd></dl></div></section><section class="sect3" id="sec-ha-architecture-layers-rsc" data-id-title="Resources and Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.5.1.3 </span><span class="title-name">Resources and Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-layers-rsc">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In a High Availability cluster, the services that need to be highly available are
     called resources. Resource agents (RAs) are scripts that start, stop, and
     monitor cluster resources.
    </p></section></section><section class="sect2" id="sec-ha-architecture-processflow" data-id-title="Process Flow"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.5.2 </span><span class="title-name">Process Flow</span></span> <a title="Permalink" class="permalink" href="#sec-ha-architecture-processflow">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_concepts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="systemitem">pacemakerd</code> daemon launches and
    monitors all other related daemons. The daemon that coordinates all actions,
    <code class="systemitem">pacemaker-controld</code>, has an instance on
    each cluster node. Pacemaker centralizes all cluster decision-making by
    electing one of those instances as a primary. Should the elected <code class="systemitem">pacemaker-controld</code> daemon fail, a new primary is
    established.
   </p><p>
    Many actions performed in the cluster will cause a cluster-wide change.
    These actions can include things like adding or removing a cluster
    resource or changing resource constraints. It is important to understand
    what happens in the cluster when you perform such an action.
   </p><p>
    For example, suppose you want to add a cluster IP address resource. To
    do this, you can use the crm shell or the Web interface to modify the CIB.
    It is not required to perform the actions on the DC.
    You can use either tool on any node in the cluster and they will be
    relayed to the DC. The DC will then replicate the CIB change to all
    cluster nodes.
   </p><p>
    Based on the information in the CIB, the <code class="systemitem">pacemaker-schedulerd</code> then computes the ideal
    state of the cluster and how it should be achieved. It feeds a list of
    instructions to the DC. The DC sends commands via the messaging/infrastructure
    layer which are received by the <code class="systemitem">pacemaker-controld</code> peers on
    other nodes. Each of them uses its local resource agent executor (implemented
    as <code class="systemitem">pacemaker-execd</code>) to perform
    resource modifications. The <code class="systemitem">pacemaker-execd</code> is not cluster-aware and interacts
    directly with resource agents.
   </p><p>
    All peer nodes report the results of their operations back to the DC.
    After the DC concludes that all necessary operations are successfully
    performed in the cluster, the cluster will go back to the idle state and
    wait for further events. If any operation was not carried out as
    planned, the <code class="systemitem">pacemaker-schedulerd</code>
    is invoked again with the new information recorded in
    the CIB.
   </p><p>
    In some cases, it may be necessary to power off nodes to protect shared
    data or complete resource recovery. In a Pacemaker cluster, the implementation
    of node level fencing is STONITH. For this, Pacemaker comes with a
    fencing subsystem, <code class="systemitem">pacemaker-fenced</code>.
    STONITH devices have to be configured as cluster resources (that use
    specific fencing agents), because this allows to monitor the fencing devices.
    When clients detect a failure, they send a request to <code class="systemitem">pacemaker-fenced</code>,
    which then executes the fencing agent to bring down the node.
   </p></section></section></section><section class="chapter" id="cha-ha-requirements" data-id-title="System Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">2 </span><span class="title-name">System Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="#cha-ha-requirements">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The following section informs you about system requirements, and some
    prerequisites for SUSE® Linux Enterprise High Availability. It also includes recommendations
    for cluster setup.
   </p></div></div></div></div><section class="sect1" id="sec-ha-requirements-hw" data-id-title="Hardware Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.1 </span><span class="title-name">Hardware Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-hw">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following list specifies hardware requirements for a cluster based on
   SUSE® Linux Enterprise High Availability. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.4.3.3.1"><span class="term">Servers</span></dt><dd><p> 1 to 32 Linux servers with software as specified in <a class="xref" href="#sec-ha-requirements-sw" title="2.2. Software Requirements">Section 2.2, “Software Requirements”</a>. </p><p>
      The servers can be bare metal or virtual machines. They do not require
      identical hardware (memory, disk space, etc.), but they must have the
      same architecture. Cross-platform clusters are not supported.
     </p><p> Using <code class="literal">pacemaker_remote</code>, the cluster can be
      extended to include additional Linux servers beyond the 32-node limit.
     </p></dd><dt id="id-1.4.3.4.3.3.2"><span class="term">Communication Channels</span></dt><dd><p>
       At least two TCP/IP communication media per cluster node.
       The network equipment must support the communication means you want to use
       for cluster communication: multicast or unicast. The communication
       media should support a data rate of 100 Mbit/s or higher.
       For a supported cluster setup two or more redundant communication paths
       are required. This can be done via:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Network Device Bonding (preferred).
         </p></li><li class="listitem"><p>
          A second communication channel in Corosync.
         </p></li></ul></div><p>For details, refer to <a class="xref" href="#cha-ha-netbonding" title="Chapter 16. Network Device Bonding">Chapter 16, <em>Network Device Bonding</em></a> and <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>, respectively.
      
     </p></dd><dt id="id-1.4.3.4.3.3.3"><span class="term">Node Fencing/STONITH</span></dt><dd><p>
      To avoid a <span class="quote">“<span class="quote">split brain</span>”</span> scenario,
      clusters need a node fencing mechanism. In a split brain scenario, cluster
      nodes are divided into two or more groups that do not know about each other
      (because of a hardware or software failure or because of a cut network
      connection). A fencing mechanism isolates the node in question
      (usually by resetting or powering off the node). This is also called
      STONITH (<span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>). A node fencing
      mechanism can be either a physical device (a power switch) or a mechanism
      like SBD (STONITH by disk) in combination with a watchdog. Using SBD
      requires shared storage.
     </p><p>Unless SBD is used, each node in the High Availability cluster must have at least
      one STONITH device. We strongly recommend multiple
      STONITH devices per node.</p><div id="id-1.4.3.4.3.3.3.2.3" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div></dd></dl></div></section><section class="sect1" id="sec-ha-requirements-sw" data-id-title="Software Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.2 </span><span class="title-name">Software Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-sw">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Base System Module 15 SP2</p></li><li class="listitem"><p>Server Applications Module 15 SP2</p></li><li class="listitem"><p>SUSE Linux Enterprise High Availability 15 SP2</p></li></ul></div><p>
   Depending on the <code class="systemitem">system roles</code> you select during
   installation, the following software patterns are installed by default:
 </p><div class="table" id="id-1.4.3.4.4.5" data-id-title="System Roles and Installed Patterns"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 2.1: </span><span class="title-name">System Roles and Installed Patterns </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.4.4.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       System Role
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Software Pattern (YaST/Zypper)
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       HA Node
      </p>
     </td><td style="border-bottom: 1px solid ; ">
       <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          High Availability (sles_ha)
         </p></li><li class="listitem"><p>
         Enhanced Base System (enhanced_base)
         </p></li></ul></div>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       HA GEO Node
      </p>
     </td><td>
       <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Geo Clustering for High Availability (ha_geo)
         </p></li><li class="listitem"><p>
          Enhanced Base System (enhanced_base)
         </p></li></ul></div>
      </td></tr></tbody></table></div></div><div id="id-1.4.3.4.4.6" data-id-title="Minimal Installation" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Minimal Installation</div><p>
    An installation via those system roles results in a minimal installation only.
    You might need to add more packages manually, if required.</p><p>
    For machines that originally had another system role assigned, you need to
    manually install the <code class="systemitem">sles_ha</code> or
    <code class="systemitem">ha_geo</code> patterns and any further packages that you
    need.
   </p></div></section><section class="sect1" id="sec-ha-requirements-disk" data-id-title="Storage Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.3 </span><span class="title-name">Storage Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-disk">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Some services require shared storage. If using an external NFS share, it must
   be reliably accessible from all cluster nodes via redundant communication
   paths.</p><p>To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The shared disk system is properly set up and functional according to
     the manufacturer’s instructions.
    </p></li><li class="listitem"><p>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
    </p></li><li class="listitem"><p>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </p></li><li class="listitem"><p>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD—never the backing device. To leverage the
     redundancy it is possible to use the same NICs as the rest of the cluster.
    </p></li></ul></div><p>
   When using SBD as STONITH mechanism, additional requirements apply
   for the shared storage. For details, see <a class="xref" href="#sec-ha-storage-protect-req" title="13.3. Requirements">Section 13.3, “Requirements”</a>.
   </p></section><section class="sect1" id="sec-ha-requirements-other" data-id-title="Other Requirements and Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">2.4 </span><span class="title-name">Other Requirements and Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-requirements-other">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_requirements.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For a supported and useful High Availability setup, consider the following
   recommendations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-req-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>For clusters with more than two nodes, it is strongly recommended to use
       an odd number of cluster nodes to have quorum. For more information
       about quorum, see <a class="xref" href="#sec-ha-config-basics-global" title="5.2. Quorum Determination">Section 5.2, “Quorum Determination”</a>.
       A regular cluster may contain up to 32 nodes. With the <code class="systemitem">pacemaker_remote</code>
       service, High Availability clusters can be extended to include additional nodes
       beyond this limit. See Pacemaker Remote Quick Start for more details.
     </p></dd><dt id="id-1.4.3.4.6.3.2"><span class="term">Time Synchronization</span></dt><dd><p>
     Cluster nodes must synchronize to an NTP server outside the cluster.
     Since SUSE Linux Enterprise High Availability 15, chrony is the default implementation of NTP.
     For more information, see the
     <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html" target="_blank">
     Administration Guide for SUSE Linux Enterprise Server 15 SP2</a>.
    </p><p>
     The cluster might not work properly if the nodes are not synchronized,
     or even if they are synchronized but have different timezones configured.
     In addition, log files and cluster reports are very hard to analyze
     without synchronization.
     If you use the bootstrap scripts, you will be
     warned if NTP is not configured yet.
    </p></dd><dt id="id-1.4.3.4.6.3.3"><span class="term">Network Interface Card (NIC) Names</span></dt><dd><p>
      Must be identical on all nodes.
     </p></dd><dt id="id-1.4.3.4.6.3.4"><span class="term">Host Name and IP Address</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Use static IP addresses.
       </p></li><li class="listitem"><p>
        Only the primary IP address is supported.
       </p></li><li class="listitem"><p>
     List all cluster nodes in the <code class="filename">/etc/hosts</code> file
     with their fully qualified host name and short host name. It is essential that
     members of the cluster can find each other by name. If the names are not
     available, internal cluster communication will fail.
   </p><p>
        For details on how Pacemaker gets the node names, see also
        <a class="link" href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html" target="_blank">http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html</a>.
       </p></li></ul></div></dd><dt id="vle-ha-req-ssh"><span class="term">SSH</span></dt><dd><p>
    All cluster nodes must be able to access each other via SSH. Tools
    like <code class="command">crm report</code> (for troubleshooting) and
    Hawk2's <span class="guimenu">History Explorer</span> require passwordless
    SSH access between the nodes,
    otherwise they can only collect data from the current node.
  </p><div id="id-1.4.3.4.6.3.5.2.2" data-id-title="Regulatory Requirements" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Regulatory Requirements</div><p>
       If passwordless SSH access does not comply with regulatory
       requirements, you can use the work-around described in
       <a class="xref" href="#app-crmreport-nonroot" title="Appendix D. Running Cluster Reports Without root Access">Appendix D, <em>Running Cluster Reports Without <code class="systemitem">root</code> Access</em></a> for running
       <code class="command">crm report</code>.
      </p><p>
       For the <span class="guimenu">History Explorer</span> there is currently no
       alternative for passwordless login.
      </p></div></dd></dl></div></section></section><section class="chapter" id="cha-ha-install" data-id-title="Installing SUSE Linux Enterprise High Availability"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">3 </span><span class="title-name">Installing SUSE Linux Enterprise High Availability</span></span> <a title="Permalink" class="permalink" href="#cha-ha-install">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_install.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>If you are setting up a High Availability cluster with SUSE® Linux Enterprise High Availability for the first time, the
    easiest way is to start with a basic two-node cluster. You can also use the
    two-node cluster to run some tests. Afterward, you can add more
    nodes by cloning existing cluster nodes with AutoYaST. The cloned nodes will
    have the same packages installed and the same system configuration as the
    original ones.
   </p><p>
    If you want to upgrade an existing cluster that runs an older version of
    SUSE Linux Enterprise High Availability, refer to <a class="xref" href="#cha-ha-migration" title="Chapter 28. Upgrading Your Cluster and Updating Software Packages">Chapter 28, <em>Upgrading Your Cluster and Updating Software Packages</em></a>.
   </p></div></div></div></div><section class="sect1" id="sec-ha-install-manual" data-id-title="Manual Installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.1 </span><span class="title-name">Manual Installation</span></span> <a title="Permalink" class="permalink" href="#sec-ha-install-manual">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For the manual installation of the packages for High Availability refer to
   <span class="intraxref">Article “Installation and Setup Quick Start”</span>. It leads you through the setup of a
   basic two-node cluster.
  </p></section><section class="sect1" id="sec-ha-installation-autoyast" data-id-title="Mass Installation and Deployment with AutoYaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">3.2 </span><span class="title-name">Mass Installation and Deployment with AutoYaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-autoyast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_install.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have installed and set up a two-node cluster, you can extend the
   cluster by cloning existing nodes with AutoYaST and adding the clones to the cluster.
  </p><p>AutoYaST uses profiles that contains installation and configuration data.
   A profile tells AutoYaST what to install and how to configure the installed system to
   get a ready-to-use system in the end. This profile can then be used
   for mass deployment in different ways (for example, to clone existing
   cluster nodes).</p><p>
    For detailed instructions on how to use AutoYaST in various scenarios,
    see the <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/book-autoyast.html" target="_blank">
     AutoYaST Guide for SUSE Linux Enterprise Server 15 SP2</a>.
   </p><div id="id-1.4.3.5.4.5" data-id-title="Identical Hardware" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Identical Hardware</div><p>
     <a class="xref" href="#pro-ha-installation-clone-node" title="Cloning a Cluster Node with AutoYaST">Procedure 3.1, “Cloning a Cluster Node with AutoYaST”</a> assumes you are rolling
     out SUSE Linux Enterprise High Availability 15 SP2 to a set of machines with identical hardware
     configurations.
    </p><p>
     If you need to deploy cluster nodes on non-identical hardware, refer to the
     Deployment Guide for SUSE Linux Enterprise Server 15 SP2,
     chapter <em class="citetitle">Automated Installation</em>, section
     <em class="citetitle">Rule-Based Autoinstallation</em>.
    </p></div><div class="procedure" id="pro-ha-installation-clone-node" data-id-title="Cloning a Cluster Node with AutoYaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3.1: </span><span class="title-name">Cloning a Cluster Node with AutoYaST </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-clone-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_install.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the node you want to clone is correctly installed and
     configured. For details, see the Installation and Setup Quick Start or
     <a class="xref" href="#cha-ha-ycluster" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
    </p></li><li class="step"><p>
     Follow the description outlined in the SUSE Linux Enterprise
     15 SP2 Deployment Guide for simple mass
     installation. This includes the following basic steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Creating an AutoYaST profile. Use the AutoYaST GUI to create and modify
       a profile based on the existing system configuration. In AutoYaST,
       choose the <span class="guimenu">High Availability</span> module and click the
       <span class="guimenu">Clone</span> button. If needed, adjust the configuration
       in the other modules and save the resulting control file as XML.
      </p><p>
       If you have configured DRBD, you can select and clone this module in
       the AutoYaST GUI, too.
      </p></li><li class="step"><p>
       Determining the source of the AutoYaST profile and the parameter to
       pass to the installation routines for the other nodes.
      </p></li><li class="step"><p>
       Determining the source of the SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability
       installation data.
      </p></li><li class="step"><p>
       Determining and setting up the boot scenario for autoinstallation.
      </p></li><li class="step"><p>
       Passing the command line to the installation routines, either by
       adding the parameters manually or by creating an
       <code class="filename">info</code> file.
      </p></li><li class="step"><p>
       Starting and monitoring the autoinstallation process.
      </p></li></ol></li></ol></div></div><p>
   After the clone has been successfully installed, execute the following
   steps to make the cloned node join the cluster:
  </p><div class="procedure" id="pro-ha-installation-clone-start" data-id-title="Bringing the Cloned Node Online"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 3.2: </span><span class="title-name">Bringing the Cloned Node Online </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-clone-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_install.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Transfer the key configuration files from the already configured nodes
     to the cloned node with Csync2 as described in
     <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
    </p></li><li class="step"><p>
     To bring the node online, start the cluster services on the cloned
     node as described in <a class="xref" href="#sec-ha-installation-start" title="4.8. Bringing the Cluster Online Node by Node">Section 4.8, “Bringing the Cluster Online Node by Node”</a>.
    </p></li></ol></div></div><p>
   The cloned node will now join the cluster because the
   <code class="filename">/etc/corosync/corosync.conf</code> file has been applied to
   the cloned node via Csync2. The CIB is automatically synchronized
   among the cluster nodes.
  </p></section></section><section class="chapter" id="cha-ha-ycluster" data-id-title="Using the YaST Cluster Module"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">4 </span><span class="title-name">Using the YaST Cluster Module</span></span> <a title="Permalink" class="permalink" href="#cha-ha-ycluster">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>The YaST cluster module allows you to set up a cluster manually
    (from scratch) or to modify options for an existing cluster.
   </p><p>
    However, if you prefer an automated approach for setting up a cluster,
    refer to <span class="intraxref">Article “Installation and Setup Quick Start”</span>. It describes how to install the
    needed packages and leads you to a basic two-node cluster, which is
    set up with the <code class="systemitem">ha-cluster-bootstrap</code> scripts.
   </p><p>
    You can also use a combination of both setup methods, for example: set up
    one node with YaST cluster and then use one of the bootstrap scripts
    to integrate more nodes (or vice versa).
   </p></div></div></div></div><section class="sect1" id="sec-ha-installation-terms" data-id-title="Definition of Terms"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.1 </span><span class="title-name">Definition of Terms</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-terms">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Several key terms used in the YaST cluster module and in this chapter are
   defined below.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.6.3.3.1"><span class="term">Bind Network Address (<code class="systemitem">bindnetaddr</code>)
    </span></dt><dd><p>
      The network address the Corosync executive should bind to.  To simplify sharing configuration files across
      the cluster, Corosync uses network interface netmask to mask only
      the address bits that are used for routing the network. For example,
      if the local interface is <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.0</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.0</code>. If the local interface is
      <code class="literal">192.168.5.92</code> with netmask
      <code class="literal">255.255.255.192</code>, set
      <code class="systemitem">bindnetaddr</code> to
      <code class="literal">192.168.5.64</code>.
     </p><p> If <code class="systemitem">nodelist</code> with
       <code class="systemitem">ringX_addr</code> is explicitly configured in
       <code class="filename">/etc/corosync/corosync.conf</code>,
       <code class="systemitem">bindnetaddr</code> is not strictly required. </p><div id="id-1.4.3.6.3.3.1.2.3" data-id-title="Network Address for All Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Network Address for All Nodes</div><p>
       As the same Corosync configuration will be used on all nodes,
       make sure to use a network address as
       <code class="systemitem">bindnetaddr</code>, not the address of a specific
       network interface.
      </p></div></dd><dt id="id-1.4.3.6.3.3.2"><span class="term"><code class="systemitem">conntrack</code> Tools</span></dt><dd><p>
      Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes. For detailed information, refer to
      <a class="link" href="http://conntrack-tools.netfilter.org/" target="_blank">http://conntrack-tools.netfilter.org/</a>.
     </p></dd><dt id="id-1.4.3.6.3.3.3"><span class="term">Csync2</span></dt><dd><p>
      A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters. Csync2 can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which ﬁles
      should be synchronized in the synchronization group. The groups, the
      host names belonging to each group, and the include/exclude rules for
      each group are specified in the Csync2 configuration file,
      <code class="filename">/etc/csync2/csync2.cfg</code>.
     </p><p>
      For authentication, Csync2 uses the IP addresses and pre-shared
      keys within a synchronization group. You need to generate one key file
      for each synchronization group and copy it to all group members.
     </p><p>
      For more information about Csync2, refer to
      <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>
     </p></dd><dt id="id-1.4.3.6.3.3.4"><span class="term">Existing Cluster</span></dt><dd><p>
        The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
     </p></dd><dt id="id-1.4.3.6.3.3.5"><span class="term">Multicast</span></dt><dd><p>
        A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
     </p><div id="id-1.4.3.6.3.3.5.2.2" data-id-title="Switches and Multicast" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Switches and Multicast</div><p>
       To use multicast for cluster communication, make sure
       your switches support multicast.
      </p></div></dd><dt id="vle-ha-mcastaddr"><span class="term">Multicast Address (<code class="systemitem">mcastaddr</code>)
   </span></dt><dd><p>
        IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6.  If IPv6 networking is used, node IDs must be
      specified. You can use any multicast address in your private network.
     </p></dd><dt id="id-1.4.3.6.3.3.7"><span class="term">Multicast Port (<code class="systemitem">mcastport</code>)</span></dt><dd><p>
        The port to use for cluster communication. Corosync uses two ports: the specified
      <code class="literal">mcastport</code> for receiving multicast, and
      <code class="literal">mcastport -1</code> for sending multicast.
     </p></dd><dt id="vle-ha-rrp"><span class="term">Redundant Ring Protocol (RRP)</span></dt><dd><p>
       Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol. A logical token-passing ring is imposed on all
      participating nodes to deliver messages in a reliable and sorted
      manner. A node is allowed to broadcast a message only if it holds the
      token.
     </p><p>
      When having defined redundant communication channels in Corosync,
      use RRP to tell the cluster how to use these interfaces. RRP can have
      three modes (<code class="literal">rrp_mode</code>):
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If set to <code class="literal">active</code>, Corosync uses both
        interfaces actively. However, this mode is deprecated.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">passive</code>, Corosync sends messages
        alternatively over the available networks.
       </p></li><li class="listitem"><p>
        If set to <code class="literal">none</code>, RRP is disabled.
       </p></li></ul></div></dd><dt id="id-1.4.3.6.3.3.9"><span class="term">Unicast</span></dt><dd><p>
        A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync, unicast
    is implemented as UDP-unicast (UDPU).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-setup-yast-overview" data-id-title="YaST Cluster Module"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.2 </span><span class="title-name">YaST Cluster Module</span></span> <a title="Permalink" class="permalink" href="#sec-ha-setup-yast-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Start YaST and select <span class="guimenu">High Availability</span> › <span class="guimenu">Cluster</span>. Alternatively, start the
    module from command line:
   </p><div class="verbatim-wrap"><pre class="screen">sudo yast2 cluster</pre></div><p>
   The following list shows an overview of the available screens in the
   YaST cluster module. It also mentions whether the screen contains parameters that
   are <span class="emphasis"><em>required</em></span> for successful cluster setup or whether its
   parameters are <span class="emphasis"><em>optional</em></span>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.6.4.5.1"><span class="term">Communication Channels (required)</span></dt><dd><p> Allows you to define one or two communication channels for
      communication between the cluster nodes. As transport protocol,
      either use multicast (UDP) or unicast (UDPU). For details, see
      <a class="xref" href="#sec-ha-installation-setup-channels" title="4.3. Defining the Communication Channels">Section 4.3, “Defining the Communication Channels”</a>.</p><div id="id-1.4.3.6.4.5.1.2.2" data-id-title="Redundant Communication Paths" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant Communication Paths</div><p>For a supported cluster setup two or more redundant communication
       paths are required. The preferred way is to use network device bonding as
       described in <a class="xref" href="#cha-ha-netbonding" title="Chapter 16. Network Device Bonding">Chapter 16, <em>Network Device Bonding</em></a>.</p><p>If this is impossible, you need to define a second communication
       channel in Corosync.</p></div></dd><dt id="id-1.4.3.6.4.5.2"><span class="term">Security (optional but recommended)</span></dt><dd><p>Allows you to define the authentication settings for the cluster.
        HMAC/SHA1 authentication requires a shared secret used
        to protect and authenticate messages. For details, see
        <a class="xref" href="#sec-ha-installation-setup-security" title="4.4. Defining Authentication Settings">Section 4.4, “Defining Authentication Settings”</a>.
       </p></dd><dt id="id-1.4.3.6.4.5.3"><span class="term">Configure Csync2 (optional but recommended)</span></dt><dd><p>
      Csync2 helps you to keep track of configuration changes and to
      keep files synchronized across the cluster nodes. For details, see
      <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
     </p></dd><dt id="id-1.4.3.6.4.5.4"><span class="term">Configure conntrackd (optional)</span></dt><dd><p>
          Allows you to configure the user space
          <code class="systemitem">conntrackd</code>. Use the conntrack
          tools for <span class="emphasis"><em>stateful</em></span> packet inspection for iptables.
          For details, see <a class="xref" href="#sec-ha-installation-setup-conntrackd" title="4.5. Synchronizing Connection Status Between Cluster Nodes">Section 4.5, “Synchronizing Connection Status Between Cluster Nodes”</a>.
         </p></dd><dt id="id-1.4.3.6.4.5.5"><span class="term">Service (required)</span></dt><dd><p>
      Allows you to configure the service for bringing the cluster node online.
      Define whether to start the cluster services at boot time and whether to open the
      ports in the firewall that are needed for communication between the nodes.
      For details, see <a class="xref" href="#sec-ha-installation-setup-services" title="4.6. Configuring Services">Section 4.6, “Configuring Services”</a>.
     </p></dd></dl></div><p>
    If you start the cluster module for the first time, it appears as a
    wizard, guiding you through all the steps necessary for basic setup.
    Otherwise, click the categories on the left panel to access the
    configuration options for each step.
   </p><div id="id-1.4.3.6.4.7" data-id-title="Settings in the YaST Cluster Module" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings in the YaST Cluster Module</div><p>Some settings in the YaST cluster module apply only to the
      current node. Other settings may automatically be transferred to all nodes
      with Csync2. Find detailed information about this in the following
      sections.
    </p></div></section><section class="sect1" id="sec-ha-installation-setup-channels" data-id-title="Defining the Communication Channels"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.3 </span><span class="title-name">Defining the Communication Channels</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-channels">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For successful communication between the cluster nodes, define at least
    one communication channel. As transport protocol, either use multicast (UDP)
    or unicast (UDPU) as described in <a class="xref" href="#pro-ha-installation-setup-channel1-udp" title="Defining the First Communication Channel (Multicast)">Procedure 4.1</a>
    or <a class="xref" href="#pro-ha-installation-setup-channel1-udpu" title="Defining the First Communication Channel (Unicast)">Procedure 4.2</a>, respectively.
    If you want to define a second, redundant channel
    (<a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3</a>),
    both communication channels must use the <span class="emphasis"><em>same</em></span> protocol.
   </p><div id="id-1.4.3.6.5.3" data-id-title="Public Clouds: Use Unicast" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Public Clouds: Use Unicast</div><p>
     For deploying SUSE Linux Enterprise High Availability in public cloud platforms, use unicast as
     transport protocol. Multicast is generally not supported by the cloud
     platforms themselves.
    </p></div><p>All settings defined in the YaST
    <span class="guimenu">Communication Channels</span>
    screen are written to <code class="filename">/etc/corosync/corosync.conf</code>. Find example
    files for a multicast and a unicast setup in
    <code class="filename">/usr/share/doc/packages/corosync/</code>.
   </p><p>If you are using IPv4 addresses, node IDs are optional. If you are using
    IPv6 addresses, node IDs are required. Instead of specifying IDs manually
    for each node, the YaST cluster module contains an option to automatically
    generate a unique ID for every cluster node.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udp" data-id-title="Defining the First Communication Channel (Multicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.1: </span><span class="title-name">Defining the First Communication Channel (Multicast) </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel1-udp">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     When using multicast, the same <code class="systemitem">bindnetaddr</code>,
    <code class="systemitem">mcastaddr</code>, and <code class="systemitem">mcastport</code>
     will be used for all cluster nodes. All nodes in the cluster will know each
     other by using the same multicast address. For different clusters, use
     different multicast addresses.
    
    </p><ol class="procedure" type="1"><li class="step"><p>
       Start the YaST cluster module and switch to the <span class="guimenu">Communication
        Channels</span> category.
      </p></li><li class="step"><p>
      Set the <span class="guimenu">Transport</span> protocol to
      <code class="literal">Multicast</code>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Bind Network Address</span>. Set the value to
      the subnet you will use for cluster multicast.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span>.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Port</span>.
     </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.5.7"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_comm_mcast.png"><img src="images/yast_cluster_comm_mcast.png" width="75%" alt="YaST Cluster—Multicast Configuration" title="YaST Cluster—Multicast Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.1: </span><span class="title-name">YaST Cluster—Multicast Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.5.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>If you want to use unicast instead of multicast for cluster
   communication, proceed as follows.</p><div class="procedure" id="pro-ha-installation-setup-channel1-udpu" data-id-title="Defining the First Communication Channel (Unicast)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.2: </span><span class="title-name">Defining the First Communication Channel (Unicast) </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel1-udpu">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Communication
       Channels</span> category.
     </p></li><li class="step"><p>
        Set the <span class="guimenu">Transport</span> protocol to
        <code class="literal">Unicast</code>.
       </p></li><li class="step"><p>
        Define the <span class="guimenu">Port</span>.
       </p></li><li class="step"><p>
        For unicast communication, Corosync needs to know the IP
        addresses of all nodes in the cluster. For each node that will be
        part of the cluster, click <span class="guimenu">Add</span> and enter the
        following details:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <span class="guimenu">IP Address</span>
         </p></li><li class="listitem"><p>
          <span class="guimenu">Redundant IP Address</span> (only required if you use
          a second communication channel in Corosync)
         </p></li><li class="listitem"><p>
          <span class="guimenu">Node ID</span> (only required if the option
          <span class="guimenu">Auto Generate Node ID</span> is disabled)
         </p></li></ul></div><p>
        To modify or remove any addresses of cluster members, use the
        <span class="guimenu">Edit</span> or <span class="guimenu">Del</span> buttons.
       </p></li><li class="step"><p>
      To automatically generate a unique ID for every cluster node keep
      <span class="guimenu">Auto Generate Node ID</span> enabled.
     </p></li><li class="step"><p>
      Define a <span class="guimenu">Cluster Name</span>.
     </p></li><li class="step"><p>
      Enter the number of <span class="guimenu">Expected Votes</span>. This is
      important for Corosync to calculate
      <a class="xref" href="#gloss-quorum" title="quorum">quorum</a> in case of a partitioned cluster. By
      default, each node has <code class="literal">1</code> vote. The number of
      <span class="guimenu">Expected Votes</span> must match the number of nodes in
      your cluster.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li><li class="step"><p>
      If needed, define a redundant communication channel in Corosync as
      described in <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.5.10"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_comm_ucast.png"><img src="images/yast_cluster_comm_ucast.png" width="75%" alt="YaST Cluster—Unicast Configuration" title="YaST Cluster—Unicast Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.2: </span><span class="title-name">YaST Cluster—Unicast Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.5.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>
    If network device bonding cannot be used for any reason, the second
    best choice is to define a redundant communication channel (a second
    ring) in Corosync. That way, two physically separate networks can
    be used for communication. If one network fails, the cluster nodes
    can still communicate via the other network.
   </p><p>The additional communication channel in
    Corosync will form a second token-passing ring. In
    <code class="filename">/etc/corosync/corosync.conf</code>, the first channel you
    configured is the primary ring and gets the ring number
   <code class="literal">0</code>. The second ring (redundant channel) gets the ring number
    <code class="literal">1</code>.
   </p><p>When having defined redundant communication channels in Corosync,
    use RRP to tell the cluster how to use these interfaces. With RRP, two
    physically separate networks are used for communication. If one
    network fails, the cluster nodes can still communicate via the other
    network.</p><p>RRP can have three modes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If set to <code class="literal">active</code>, Corosync uses both
      interfaces actively. However, this mode is deprecated.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">passive</code>, Corosync sends messages
      alternatively over the available networks.
     </p></li><li class="listitem"><p>
      If set to <code class="literal">none</code>, RRP is disabled.
     </p></li></ul></div><div class="procedure" id="pro-ha-installation-setup-channel2" data-id-title="Defining a Redundant Communication Channel"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.3: </span><span class="title-name">Defining a Redundant Communication Channel </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-channel2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.4.3.6.5.16.2" data-id-title="Redundant Rings and /etc/hosts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Redundant Rings and <code class="filename">/etc/hosts</code></div><p> If multiple rings are configured in Corosync, each node can
     have multiple IP addresses. This needs to be reflected in the
      <code class="filename">/etc/hosts</code> file of all nodes. </p></div><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Communication Channels</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Redundant Channel</span>. The redundant channel
     must use the same protocol as the first communication channel you defined.
    </p></li><li class="step"><p> If you use multicast, enter the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, the <span class="guimenu">Multicast
      Address</span> and the <span class="guimenu">Port</span> for the
     redundant channel. </p><p> If you use unicast, define the following parameters: the
      <span class="guimenu">Bind Network Address</span> to use, and the <span class="guimenu">Port</span>.
     Enter the IP addresses of all nodes that will be part of
     the cluster. </p></li><li class="step"><p>To tell Corosync how and when to use the different channels,
     select the <span class="guimenu">rrp_mode</span> to use:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p> If only one communication channel is defined,
        <span class="guimenu">rrp_mode</span> is automatically disabled (value
        <code class="literal">none</code>).</p></li><li class="listitem"><p> If set to <code class="literal">active</code>, Corosync uses both
       interfaces actively. However, this mode is deprecated.</p></li><li class="listitem"><p> If set to <code class="literal">passive</code>, Corosync sends messages
       alternatively over the available networks. </p></li></ul></div><p>When RRP is used, SUSE Linux Enterprise High Availability monitors the status of the current
     rings and automatically re-enables redundant rings after faults.</p><p>Alternatively, check the ring status manually with
     <code class="command">corosync-cfgtool</code>. View the available options with
      <code class="option">-h</code>. </p></li><li class="step"><p> Confirm your changes. </p></li></ol></div></div></section><section class="sect1" id="sec-ha-installation-setup-security" data-id-title="Defining Authentication Settings"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.4 </span><span class="title-name">Defining Authentication Settings</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-security">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To define the authentication settings for the cluster, you can use HMAC/SHA1
    authentication. This requires a shared secret used
    to protect and authenticate messages. The authentication key (password)
    you specify will be used on all nodes in the cluster.
   </p><div class="procedure" id="pro-ha-installation-setup-security" data-id-title="Enabling Secure Authentication"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.4: </span><span class="title-name">Enabling Secure Authentication </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-security">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
      <span class="guimenu">Security</span> category. </p></li><li class="step"><p> Activate <span class="guimenu">Enable Security Auth</span>. </p></li><li class="step"><p> For a newly created cluster, click <span class="guimenu">Generate Auth Key
      File</span>. An authentication key is created and written to
      <code class="filename">/etc/corosync/authkey</code>. </p><p> If you want the current machine to join an existing cluster, do not
     generate a new key file. Instead, copy the
      <code class="filename">/etc/corosync/authkey</code> from one of the nodes to the
     current machine (either manually or with Csync2). </p></li><li class="step"><p> Confirm your changes. YaST writes the configuration to
      <code class="filename">/etc/corosync/corosync.conf</code>. </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_security.png"><img src="images/yast_cluster_security.png" width="75%" alt="YaST Cluster—Security" title="YaST Cluster—Security"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.3: </span><span class="title-name">YaST Cluster—Security </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.6.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-conntrackd" data-id-title="Synchronizing Connection Status Between Cluster Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.5 </span><span class="title-name">Synchronizing Connection Status Between Cluster Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-conntrackd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To enable <span class="emphasis"><em>stateful</em></span> packet inspection for iptables,
    configure and use the conntrack tools. This requires the following basic
    steps:
   </p><div class="procedure" id="pro-ha-installation-setup-conntrackd" data-id-title="Configuring the conntrackd with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.5: </span><span class="title-name">Configuring the <code class="systemitem">conntrackd</code> with YaST </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     Use the YaST cluster module to configure the user space
     <code class="systemitem">conntrackd</code> (see <a class="xref" href="#fig-ha-installation-setup-conntrackd" title="YaST Cluster—conntrackd">Figure 4.4, “YaST Cluster—<code class="systemitem">conntrackd</code>”</a>).  It needs a
     dedicated network interface that is not used for other communication
     channels. The daemon can be started via a resource agent afterward.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Start the YaST cluster module and switch to the <span class="guimenu">Configure
      conntrackd</span> category.
     </p></li><li class="step"><p>
      Define the <span class="guimenu">Multicast Address</span> to be used for
      synchronizing the connection status.
     </p></li><li class="step"><p>
      In <span class="guimenu">Group Number</span>, define a numeric ID for the group
      to synchronize the connection status to.
      
     </p></li><li class="step"><p>
      Click <span class="guimenu">Generate /etc/conntrackd/conntrackd.conf</span> to
      create the configuration file for
      <code class="systemitem">conntrackd</code>.
     </p></li><li class="step"><p>
      If you modified any options for an existing cluster, confirm your
      changes and close the cluster module.
     </p></li><li class="step"><p>
      For further cluster configuration, click <span class="guimenu">Next</span> and
      proceed with <a class="xref" href="#sec-ha-installation-setup-services" title="4.6. Configuring Services">Section 4.6, “Configuring Services”</a>.
     </p></li><li class="step"><p>
      Select a <span class="guimenu">Dedicated Interface</span> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in YaST. It must already be
      configured and it must support multicast.
      
     </p></li></ol></div></div><div class="figure" id="fig-ha-installation-setup-conntrackd"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_conntrackd.png"><img src="images/yast_cluster_conntrackd.png" width="75%" alt="YaST Cluster—conntrackd" title="YaST Cluster—conntrackd"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.4: </span><span class="title-name">YaST Cluster—<code class="systemitem">conntrackd</code> </span></span><a title="Permalink" class="permalink" href="#fig-ha-installation-setup-conntrackd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div><p>
    After having configured the conntrack tools, you can use them for Linux Virtual Server,
    see <a class="xref" href="#cha-ha-lb" title="Chapter 17. Load Balancing"><em>Load Balancing</em></a>.
   </p></section><section class="sect1" id="sec-ha-installation-setup-services" data-id-title="Configuring Services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.6 </span><span class="title-name">Configuring Services</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-services">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In the YaST cluster module define whether to start certain services
    on a node at boot time. You can also use the module to start and stop
    the services manually. To bring the cluster nodes online and start the
    cluster resource manager, Pacemaker must be running as a service.
   </p><div class="procedure" id="pro-ha-installation-setup-services" data-id-title="Enabling the cluster services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.6: </span><span class="title-name">Enabling the cluster services </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-services">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      In the YaST cluster module, switch to the
      <span class="guimenu">Service</span> category.
     </p></li><li class="step"><p>
      To start the cluster services each time this cluster node is booted, select the
      respective option in the <span class="guimenu">Booting</span> group. If you
      select <span class="guimenu">Off</span> in the <span class="guimenu">Booting</span> group,
      you must start the cluster services manually each time this node is booted. To
      start the cluster services manually, use the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      To start or stop the cluster services immediately, click the respective button.
     </p></li><li class="step"><p>
      To open the ports in the firewall that are needed for cluster
      communication on the current machine, activate <span class="guimenu">Open Port in
      Firewall</span>.
     </p></li><li class="step"><p>
      Confirm your changes. Note that the configuration only
      applies to the current machine, not to all cluster nodes.
     </p></li></ol></div></div><div class="figure" id="id-1.4.3.6.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_services.png"><img src="images/yast_cluster_services.png" width="75%" alt="YaST Cluster—Services" title="YaST Cluster—Services"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.5: </span><span class="title-name">YaST Cluster—Services </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-installation-setup-csync2" data-id-title="Transferring the configuration to all nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.7 </span><span class="title-name">Transferring the configuration to all nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-setup-csync2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Instead of copying the resulting configuration files to all nodes
    manually, use the <code class="command">csync2</code> tool for replication across
    all nodes in the cluster.
   </p><p>
    This requires the following basic steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Configuring Csync2 with YaST</a>.
     </p></li><li class="step"><p>
      <a class="xref" href="#pro-ha-installation-setup-csync2-start" title="Synchronizing the configuration files with Csync2">Synchronizing the configuration files with Csync2</a>.
     </p></li></ol></div></div><p>
    Csync2 helps you to keep track of configuration changes and to keep
    files synchronized across the cluster nodes:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can define a list of files that are important for operation.
     </p></li><li class="listitem"><p>
      You can show changes to these files (against the other cluster nodes).
     </p></li><li class="listitem"><p>
      You can synchronize the configured files with a single command.
     </p></li><li class="listitem"><p>
      With a simple shell script in <code class="filename">~/.bash_logout</code>, you
      can be reminded about unsynchronized changes before logging out of the
      system.
     </p></li></ul></div><p>
    Find detailed information about Csync2 at
    <a class="link" href="http://oss.linbit.com/csync2/" target="_blank">http://oss.linbit.com/csync2/</a> and
    <a class="link" href="http://oss.linbit.com/csync2/paper.pdf" target="_blank">http://oss.linbit.com/csync2/paper.pdf</a>.
   </p><section class="sect2" id="pro-ha-installation-setup-csync2-yast" data-id-title="Configuring Csync2 with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.1 </span><span class="title-name">Configuring Csync2 with YaST</span></span> <a title="Permalink" class="permalink" href="#pro-ha-installation-setup-csync2-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.3.6.9.8.2" data-id-title="Configuring Csync2 with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.7: </span><span class="title-name">Configuring Csync2 with YaST </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.9.8.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> Start the YaST cluster module and switch to the
       <span class="guimenu">Csync2</span> category. </p></li><li class="step"><p> To specify the synchronization group, click <span class="guimenu">Add</span>
      in the <span class="guimenu">Sync Host</span> group and enter the local host names
      of all nodes in your cluster. For each node, you must use exactly the
      strings that are returned by the <code class="command">hostname</code> command. </p><div id="id-1.4.3.6.9.8.2.3.2" data-id-title="Host name resolution" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Host name resolution</div><p> If host name resolution does not work properly in your
       network, you can also specify a combination of host name and IP address
       for each cluster node. To do so, use the string
        <em class="replaceable">HOSTNAME@IP</em> such as
        <code class="literal">alice@192.168.2.100</code>, for example. Csync2
       will then use the IP addresses when connecting. </p></div></li><li class="step" id="step-csync2-generate-key"><p> Click <span class="guimenu">Generate Pre-Shared-Keys</span> to create a key
      file for the synchronization group. The key file is written to
       <code class="filename">/etc/csync2/key_hagroup</code>. After it has been created,
      it must be copied manually to all members of the cluster. </p></li><li class="step"><p> To populate the <span class="guimenu">Sync File</span> list with the files
      that usually need to be synchronized among all nodes, click <span class="guimenu">Add
       Suggested Files</span>. </p></li><li class="step"><p> To <span class="guimenu">Edit</span>, <span class="guimenu">Add</span> or
       <span class="guimenu">Remove</span> files from the list of files to be synchronized
      use the respective buttons. You must enter the absolute path name for each
      file. </p></li><li class="step"><p> Activate Csync2 by clicking <span class="guimenu">Turn Csync2
       ON</span>. This will execute the following command to start
      Csync2 automatically at boot time: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable csync2.socket</pre></div></li><li class="step"><p>Click <span class="guimenu">Finish</span>. YaST writes the Csync2
      configuration to <code class="filename">/etc/csync2/csync2.cfg</code>.</p></li></ol></div></div><div class="figure" id="id-1.4.3.6.9.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_cluster_sync.png"><img src="images/yast_cluster_sync.png" width="75%" alt="YaST Cluster—Csync2" title="YaST Cluster—Csync2"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 4.6: </span><span class="title-name">YaST <span class="guimenu">Cluster</span>—Csync2 </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.9.8.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-setup-yast-csync2-sync" data-id-title="Synchronizing changes with Csync2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">4.7.2 </span><span class="title-name">Synchronizing changes with Csync2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-setup-yast-csync2-sync">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p> Before running Csync2 for the first time, you need to make the
    following preparations: </p><div class="procedure" id="id-1.4.3.6.9.9.3" data-id-title="Preparing for initial synchronization with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.8: </span><span class="title-name">Preparing for initial synchronization with Csync2 </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.9.9.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Copy the file <code class="filename">/etc/csync2/csync2.cfg</code>
      manually to all nodes after you have configured it as described in <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1, “Configuring Csync2 with YaST”</a>. </p></li><li class="step"><p> Copy the file <code class="filename">/etc/csync2/key_hagroup</code> that you
      have generated on one node in <a class="xref" href="#step-csync2-generate-key" title="Step 3">Step 3</a>
      of <a class="xref" href="#pro-ha-installation-setup-csync2-yast" title="4.7.1. Configuring Csync2 with YaST">Section 4.7.1</a>
      to <span class="emphasis"><em>all</em></span> nodes in the cluster. It is needed for
      authentication by Csync2. However, do <span class="emphasis"><em>not</em></span>
      regenerate the file on the other nodes—it needs to be the same
      file on all nodes. </p></li><li class="step"><p>Execute the following command on all nodes to start the service now: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start csync2.socket</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-installation-setup-csync2-start" data-id-title="Synchronizing the configuration files with Csync2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.9: </span><span class="title-name">Synchronizing the configuration files with Csync2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-installation-setup-csync2-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>To initially synchronize all files once, execute the following
      command on the machine that you want to copy the configuration
       <span class="emphasis"><em>from</em></span>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p> This will synchronize all the files once by pushing them to the
      other nodes. If all files are synchronized successfully, Csync2 will
      finish with no errors. </p><p> If one or several files that are to be synchronized have been
      modified on other nodes (not only on the current one), Csync2
      reports a conflict. You will get an output similar to the one below: </p><div class="verbatim-wrap"><pre class="screen">While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</pre></div></li><li class="step"><p> If you are sure that the file version on the current node is the
       <span class="quote">“<span class="quote">best</span>”</span> one, you can resolve the conflict by forcing this
      file and resynchronizing: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-f</code> /etc/corosync/corosync.conf
<code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-x</code></pre></div></li></ol></div></div><p> For more information on the Csync2 options, run</p><div class="verbatim-wrap"><pre class="screen">csync2 -help</pre></div><div id="id-1.4.3.6.9.9.7" data-id-title="Pushing synchronization after any changes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Pushing synchronization after any changes</div><p> Csync2 only pushes changes. It does <span class="emphasis"><em>not</em></span>
     continuously synchronize files between the machines. </p><p> Each time you update files that need to be synchronized, you need to
     push the changes to the other machines: Run <code class="command">csync2 </code>
     <code class="option">-xv</code> on the machine where you did the changes. If you run
     the command on any of the other machines with unchanged files, nothing will
     happen. </p></div></section></section><section class="sect1" id="sec-ha-installation-start" data-id-title="Bringing the Cluster Online Node by Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">4.8 </span><span class="title-name">Bringing the Cluster Online Node by Node</span></span> <a title="Permalink" class="permalink" href="#sec-ha-installation-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After the initial cluster configuration is done, start the cluster
    services on <span class="emphasis"><em>each</em></span> cluster node to bring the stack
    online:
   </p><div class="procedure" id="id-1.4.3.6.10.3" data-id-title="Starting Cluster Services and Checking the Status"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 4.10: </span><span class="title-name">Starting Cluster Services and Checking the Status </span></span><a title="Permalink" class="permalink" href="#id-1.4.3.6.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_yast_cluster.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to an existing node.
     </p></li><li class="step"><p>
      Check if the service is already running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster status</pre></div><p>
      If not, start the cluster services now:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      Repeat the steps above for each of the cluster nodes.
     </p></li><li class="step"><p>
      On one of the nodes, check the cluster status with the
      <code class="command">crm status</code> command. If all nodes are
      online, the output should be similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status
Cluster Summary:
  * Stack: corosync
  * Current DC: alice (version ...) - partition with quorum
  * Last updated: ...
  * Last change:  ... by hacluster via crmd on bob
  * 2 nodes configured
  * 1 resource instance configured

Node List:
  * Online: [ alice bob ]
...</pre></div><p>
      This output indicates that the cluster resource manager is started and
      is ready to manage resources.
     </p></li></ol></div></div><p>
    After the basic configuration is done and the nodes are online, you can
    start to configure cluster resources. Use one of the cluster management
    tools like the crm shell (crmsh) or Hawk2. For more
    information, see <a class="xref" href="#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a> or
    <a class="xref" href="#cha-conf-hawk2" title="5.4. Introduction to Hawk2">Section 5.4, “Introduction to Hawk2”</a>.
   </p></section></section></div><div class="part" id="part-config" data-id-title="Configuration and Administration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part II </span><span class="title-name">Configuration and Administration </span></span><a title="Permalink" class="permalink" href="#part-config">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-config-basics"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></a></span></li><dd class="toc-abstract"><p>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped—the nature of the service is
    irrelevant to the cluster.
   </p><p>
    This chapter introduces some basic concepts you need to know
    when administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools SUSE Linux Enterprise High Availability
    provides.
   </p></dd><li><span class="chapter"><a href="#sec-ha-config-basics-resources"><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources</span></a></span></li><dd class="toc-abstract"><p>
    As a cluster administrator, you need to create cluster resources for
    every resource or application you run on servers in your cluster. Cluster
    resources can include Web sites, e-mail servers, databases, file systems,
    virtual machines, and any other server-based applications or services you
    want to make available to users at all times.
   </p></dd><li><span class="chapter"><a href="#sec-ha-config-basics-constraints"><span class="title-number">7 </span><span class="title-name">Configuring Resource Constraints</span></a></span></li><dd class="toc-abstract"><p>
    Having all the resources configured is only part of the job. Even if the
    cluster knows all needed resources, it might still not be able to handle
    them correctly. Resource constraints let you specify which cluster nodes
    resources can run on, what order resources will load, and what other
    resources a specific resource is dependent on.
   </p></dd><li><span class="chapter"><a href="#cha-ha-manage-resources"><span class="title-number">8 </span><span class="title-name">Managing Cluster Resources</span></a></span></li><dd class="toc-abstract"><p>
    After configuring the resources in the cluster, use the cluster management tools
    to start, stop, clean up, remove or migrate the resources. This chapter describes
    how to use Hawk2 or crmsh for resource management tasks.
   </p></dd><li><span class="chapter"><a href="#sec-ha-config-basics-remote"><span class="title-number">9 </span><span class="title-name">Managing Services on Remote Hosts</span></a></span></li><dd class="toc-abstract"><p>
    The possibilities for monitoring and managing services on remote hosts
    has become increasingly important during the last few years.
    SUSE Linux Enterprise High Availability 11 SP3 offered fine-grained monitoring of services on
    remote hosts via monitoring plug-ins. The recent addition of the
    <code class="literal">pacemaker_remote</code> service now allows SUSE Linux Enterprise High Availability
    15 SP2 to fully manage and monitor resources on remote hosts
    just as if they were a real cluster node—without the need to
    install the cluster stack on the remote machines.
   </p></dd><li><span class="chapter"><a href="#cha-ha-agents"><span class="title-number">10 </span><span class="title-name">Adding or Modifying Resource Agents</span></a></span></li><dd class="toc-abstract"><p>
    All tasks that need to be managed by a cluster must be available as a
    resource. There are two major groups here to consider: resource agents
    and STONITH agents. For both categories, you can add your own
    agents, extending the abilities of the cluster to your own needs.
   </p></dd><li><span class="chapter"><a href="#cha-ha-monitor-clusters"><span class="title-number">11 </span><span class="title-name">Monitoring Clusters</span></a></span></li><dd class="toc-abstract"><p>
    This chapter describes how to monitor a cluster's health and view its history.
   </p></dd><li><span class="chapter"><a href="#cha-ha-fencing"><span class="title-number">12 </span><span class="title-name">Fencing and STONITH</span></a></span></li><dd class="toc-abstract"><p>
    Fencing is a very important concept in computer clusters for HA (High
    Availability). A cluster sometimes detects that one of the nodes is
    behaving strangely and needs to remove it. This is called
    <span class="emphasis"><em>fencing</em></span> and is commonly done with a STONITH
    resource. Fencing may be defined as a method to bring an HA cluster to a
    known state.
   </p><p>
    Every resource in a cluster has a state attached. For example:
    <span class="quote">“<span class="quote">resource r1 is started on alice</span>”</span>. In an HA cluster, such
    a state implies that <span class="quote">“<span class="quote">resource r1 is stopped on all nodes except
    alice</span>”</span>, because the cluster must make sure that every resource
    may be started on only one node. Every node must report every change
    that happens to a resource. The cluster state is thus a collection of
    resource states and node states.
   </p><p>
    When the state of a node or resource cannot be established with
    certainty, fencing comes in. Even when the cluster is not aware of what
    is happening on a given node, fencing can ensure that the node does not
    run any important resources.
   </p></dd><li><span class="chapter"><a href="#cha-ha-storage-protect"><span class="title-number">13 </span><span class="title-name">Storage Protection and SBD</span></a></span></li><dd class="toc-abstract"><p>
    SBD (STONITH Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </p><p>
    The <span class="package">ha-cluster-bootstrap</span> scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <span class="intraxref">Article “Installation and Setup Quick Start”</span>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </p><p>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split brain scenario.
   </p><p>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </p></dd><li><span class="chapter"><a href="#cha-ha-qdevice"><span class="title-number">14 </span><span class="title-name">QDevice and QNetd</span></a></span></li><dd class="toc-abstract"><p>
            QDevice and QNetd participate in quorum decisions. With
            assistance from the arbitrator <code class="systemitem">corosync-qnetd</code>,
            <code class="systemitem">corosync-qdevice</code> provides
            a configurable number of votes, allowing a cluster to sustain
            more node failures than the standard quorum rules allow. We
            recommend deploying <code class="systemitem">corosync-qnetd</code>
            and <code class="systemitem">corosync-qdevice</code> for
            clusters with an even number of nodes, and especially for two-node clusters.
         </p></dd><li><span class="chapter"><a href="#cha-ha-acl"><span class="title-number">15 </span><span class="title-name">Access Control Lists</span></a></span></li><dd class="toc-abstract"><p>
    The cluster administration tools like crm shell (crmsh) or
    Hawk2 can be used by <code class="systemitem">root</code> or any user in the group
    <code class="systemitem">haclient</code>. By default, these
    users have full read/write access. To limit access or assign more
    fine-grained access rights, you can use <span class="emphasis"><em>Access control
    lists</em></span> (ACLs).
   </p><p>
    Access control lists consist of an ordered set of access rules. Each
    rule allows read or write access or denies access to a part of the
    cluster configuration. Rules are typically combined to produce a
    specific role, then users may be assigned to a role that matches their
    tasks.
   </p></dd><li><span class="chapter"><a href="#cha-ha-netbonding"><span class="title-number">16 </span><span class="title-name">Network Device Bonding</span></a></span></li><dd class="toc-abstract"><p>
   For many systems, it is desirable to implement network connections that
   comply to more than the standard data security or availability
   requirements of a typical Ethernet device. In these cases, several
   Ethernet devices can be aggregated to a single bonding device.
   </p></dd><li><span class="chapter"><a href="#cha-ha-lb"><span class="title-number">17 </span><span class="title-name">Load Balancing</span></a></span></li><dd class="toc-abstract"><p>
  <span class="emphasis"><em>Load Balancing</em></span> makes a cluster of servers appear as
  one large, fast server to outside clients. This apparent single server is
  called a <span class="emphasis"><em>virtual server</em></span>. It consists of one or more
  load balancers dispatching incoming requests and several real servers
  running the actual services. With a load balancing setup of SUSE Linux Enterprise High Availability, you
  can build highly scalable and highly available network services, such as
  Web, cache, mail, FTP, media and VoIP services.
 </p></dd><li><span class="chapter"><a href="#cha-ha-geo"><span class="title-number">18 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></a></span></li><dd class="toc-abstract"><p>
    Apart from local clusters and metro area clusters, SUSE® Linux Enterprise High Availability
    15 SP2 also supports geographically dispersed clusters (Geo
    clusters, sometimes also called multi-site clusters). That means you can
    have multiple, geographically dispersed sites with a local cluster each.
    Failover between these clusters is coordinated by a higher level entity,
    the so-called <code class="literal">booth</code>. For details on how to
    use and set up Geo clusters, refer to <span class="intraxref">Article “Geo Clustering Quick Start”</span> and
   <span class="intraxref">Book “Geo Clustering Guide”</span>.
    </p></dd></ul></div><section class="chapter" id="cha-ha-config-basics" data-id-title="Configuration and Administration Basics"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">5 </span><span class="title-name">Configuration and Administration Basics</span></span> <a title="Permalink" class="permalink" href="#cha-ha-config-basics">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The main purpose of an HA cluster is to manage user services. Typical
    examples of user services are an Apache Web server or a database. From
    the user's point of view, the services do something specific when
    ordered to do so. To the cluster, however, they are only resources which
    may be started or stopped—the nature of the service is
    irrelevant to the cluster.
   </p><p>
    This chapter introduces some basic concepts you need to know
    when administering your cluster. The following
    chapters show you how to execute the main configuration and
    administration tasks with each of the management tools SUSE Linux Enterprise High Availability
    provides.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-scenarios" data-id-title="Use Case Scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.1 </span><span class="title-name">Use Case Scenarios</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-scenarios">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>In general, clusters fall into one of two categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Two-node clusters</p></li><li class="listitem"><p>Clusters with more than two nodes. This usually means an odd number of nodes.</p></li></ul></div><p>
    Adding also different topologies, different use cases can be derived.
    The following use cases are the most common:
   </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name"> </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.3.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.4.4.3.3.5.2"><span class="term">Two-node cluster in one location</span></dt><dd><p><span class="formalpara-title">Configuration:</span>FC SAN or similar shared storage, layer 2 network.</p><p><span class="formalpara-title">Usage scenario:</span>Embedded clusters that focus on service high
       availability and not data redundancy for data replication.
       Such a setup is used for radio stations or assembly line controllers,
       for example.
       </p></dd><dt id="vl-2x2node-2locs"><span class="term">Two-node clusters in two locations (most widely used)</span></dt><dd><p><span class="formalpara-title">Configuration:</span>Symmetrical stretched cluster, FC SAN, and layer 2 network
        all across two locations.</p><p><span class="formalpara-title">Usage scenario:</span>Classic stretched clusters, focus on high availability of services
        and local data redundancy. For databases and enterprise
        resource planning. One of the most popular setups.
       </p></dd><dt id="vl-n-nodes-3locs"><span class="term">Odd number of nodes in three locations</span></dt><dd><p><span class="formalpara-title">Configuration:</span>2×N+1 nodes, FC SAN across two main locations. Auxiliary
        third site with no FC SAN, but acts as a majority maker.
        Layer 2 network at least across two main locations.
       </p><p><span class="formalpara-title">Usage scenario:</span>Classic stretched cluster, focus on high availability of services
        and data redundancy. For example, databases, enterprise resource planning.
       </p></dd></dl></div></section><section class="sect1" id="sec-ha-config-basics-global" data-id-title="Quorum Determination"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.2 </span><span class="title-name">Quorum Determination</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Whenever communication fails between one or more nodes and the rest of the
   cluster, a cluster partition occurs. The nodes can only communicate with
   other nodes in the same partition and are unaware of the separated nodes.
   A cluster partition is defined as having quorum (being <span class="quote">“<span class="quote">quorate</span>”</span>)
   if it has the majority of nodes (or votes).
   How this is achieved is done by <span class="emphasis"><em>quorum calculation</em></span>.
   Quorum is a requirement for fencing.
   </p><p>
   Quorum is not calculated or determined by Pacemaker. Corosync can handle quorum for
   two-node clusters directly without changing the Pacemaker configuration.
  </p><p>How quorum is calculated is influenced by the following factors:</p><div class="variablelist"><dl class="variablelist"><dt id="vl-ha-config-basics-global-number-of-cluster-nodes"><span class="term">Number of Cluster Nodes</span></dt><dd><p>To keep services running, a cluster with more than two nodes
       relies on quorum (majority vote) to resolve cluster partitions.
       Based on the following formula, you can calculate the minimum
       number of operational nodes required for the cluster to function:</p><div class="verbatim-wrap"><pre class="screen">N ≥ C/2 + 1

N = minimum number of operational nodes
C = number of cluster nodes</pre></div><p>For example, a five-node cluster needs a minimum of three operational
       nodes (or two nodes which can fail). </p><p>
       We strongly recommend to use either a two-node cluster or an odd number
       of cluster nodes.
       Two-node clusters make sense for stretched setups across two sites.
       Clusters with an odd number of nodes can either be built on one single
       site or might be spread across three sites.
      </p></dd><dt id="id-1.4.4.3.4.5.2"><span class="term">Corosync Configuration</span></dt><dd><p>Corosync is a messaging and membership layer, see
      <a class="xref" href="#sec-ha-config-basics-corosync-2-node" title="5.2.1. Corosync Configuration for Two-Node Clusters">Section 5.2.1, “Corosync Configuration for Two-Node Clusters”</a> and
       <a class="xref" href="#sec-ha-config-basics-corosync-n-node" title="5.2.2. Corosync Configuration for N-Node Clusters">Section 5.2.2, “Corosync Configuration for N-Node Clusters”</a>.
      </p></dd></dl></div><section class="sect2" id="sec-ha-config-basics-corosync-2-node" data-id-title="Corosync Configuration for Two-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.1 </span><span class="title-name">Corosync Configuration for Two-Node Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-corosync-2-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When using the bootstrap scripts, the Corosync configuration contains
    a <code class="literal">quorum</code> section with the following options:
   </p><div class="example" id="ex-ha-config-basics-corosync-quorum" data-id-title="Excerpt of Corosync Configuration for a Two-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.1: </span><span class="title-name">Excerpt of Corosync Configuration for a Two-Node Cluster </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-basics-corosync-quorum">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   # Enable and configure quorum subsystem (default: off)
   # see also corosync.conf.5 and votequorum.5
   provider: corosync_votequorum
   expected_votes: 2
   two_node: 1
}</pre></div></div></div><p>
    By default, when <code class="literal">two_node: 1</code> is set, the
    <code class="literal">wait_for_all</code> option is automatically enabled.
    If <code class="literal">wait_for_all</code> is not enabled, the cluster should be
    started on both nodes in parallel. Otherwise the first node will perform
    a startup-fencing on the missing second node.
   </p></section><section class="sect2" id="sec-ha-config-basics-corosync-n-node" data-id-title="Corosync Configuration for N-Node Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.2.2 </span><span class="title-name">Corosync Configuration for N-Node Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-corosync-n-node">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> When not using a two-node cluster, we strongly recommend an odd
    number of nodes for your N-node cluster. With regards to quorum
    configuration, you have the following options: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Adding additional nodes with the <code class="command">ha-cluster-join</code>
     command, or</p></li><li class="listitem"><p>Adapting the Corosync configuration manually.</p></li></ul></div><p>
    If you adjust <code class="filename">/etc/corosync/corosync.conf</code> manually,
    use the following settings:
   </p><div class="example" id="id-1.4.4.3.4.7.5" data-id-title="Excerpt of Corosync Configuration for an N-Node Cluster"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.2: </span><span class="title-name">Excerpt of Corosync Configuration for an N-Node Cluster </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.3.4.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">quorum {
   provider: corosync_votequorum <span class="callout" id="co-corosync-quorum-n-node-corosync-votequorum">1</span>
   expected_votes: <em class="replaceable">N</em> <span class="callout" id="co-corosync-quorum-n-node-expected-votes">2</span>
   wait_for_all: 1 <span class="callout" id="co-corosync-quorum-n-node-wait-for-all">3</span>
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-corosync-votequorum"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>Use the quorum service from Corosync</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-expected-votes"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The number of votes to expect. This parameter can either be
       provided inside the <code class="literal">quorum</code> section, or is
       automatically calculated when the <code class="literal">nodelist</code>
       section is available.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-corosync-quorum-n-node-wait-for-all"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Enables the wait for all (WFA) feature.
       When WFA is enabled, the cluster will be quorate for the first time
       only after all nodes have become visible.
       To avoid some startup race conditions, setting <code class="option">wait_for_all</code>
       to <code class="literal">1</code> may help.
       For example, in a five-node cluster every node has one vote and thus,
       <code class="option">expected_votes</code> is set to <code class="literal">5</code>.
       As soon as three or more nodes are visible to each other, the cluster
       partition becomes quorate and can start operating.
      </p></td></tr></table></div></div></div></section></section><section class="sect1" id="sec-ha-config-basics-global-options" data-id-title="Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.3 </span><span class="title-name">Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-options">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p> Global cluster options control how the cluster behaves when
   confronted with certain situations. They are grouped into sets and can be
   viewed and modified with the cluster management tools like Hawk2 and
   the <code class="command">crm</code> shell. </p><p> The predefined values can usually be kept. However, to make key
   functions of your cluster work as expected, you might need to adjust the
   following parameters after basic cluster setup:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.3.1. Global Option no-quorum-policy">Global Option <code class="literal">no-quorum-policy</code></a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-stonith" title="5.3.2. Global Option stonith-enabled">Global Option <code class="literal">stonith-enabled</code></a>
    </p></li></ul></div><section class="sect2" id="sec-ha-config-basics-global-quorum" data-id-title="Global Option no-quorum-policy"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.1 </span><span class="title-name">Global Option <code class="literal">no-quorum-policy</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-quorum">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This global option defines what to do when a cluster partition does not
   have quorum (no majority of nodes is part of the partition).
  </p><p>
   The following values are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.5.5.4.1"><span class="term"><code class="literal">ignore</code>
    </span></dt><dd><p>
      Setting <code class="literal">no-quorum-policy</code> to <code class="literal">ignore</code> makes
      a cluster partition behave like it has quorum, even if it does not. The cluster
      partition is allowed to issue fencing and continue resource management.
     </p><p>
      On SLES 11 this was the recommended setting for a two-node cluster.
      Starting with SLES 12, the value <code class="literal">ignore</code> is obsolete
      and must not be used.
      Based on configuration and conditions, Corosync gives cluster nodes
      or a single node <span class="quote">“<span class="quote">quorum</span>”</span>—or not.
     </p><p>
     For two-node clusters the only meaningful behavior is to always
     react in case of node loss. The first step should always be
     to try to fence the lost node.
     </p></dd><dt id="id-1.4.4.3.5.5.4.2"><span class="term"><code class="literal">freeze</code>
    </span></dt><dd><p>
      If quorum is lost, the cluster partition freezes. Resource management
      is continued: running resources are not stopped (but possibly
      restarted in response to monitor events), but no further resources
      are started within the affected partition.
     </p><p>
      This setting is recommended for clusters where certain resources
      depend on communication with other nodes (for example, OCFS2 mounts).
      In this case, the default setting
      <code class="literal">no-quorum-policy=stop</code> is not useful, as it would
      lead to the following scenario: Stopping those resources would not be
      possible while the peer nodes are unreachable. Instead, an attempt to
      stop them would eventually time out and cause a <code class="literal">stop
      failure</code>, triggering escalated recovery and fencing.
     </p></dd><dt id="id-1.4.4.3.5.5.4.3"><span class="term"><code class="literal">stop</code> (default value)</span></dt><dd><p>
      If quorum is lost, all resources in the affected cluster partition
      are stopped in an orderly fashion.
     </p></dd><dt id="id-1.4.4.3.5.5.4.4"><span class="term"><code class="literal">suicide</code>
    </span></dt><dd><p>
      If quorum is lost, all nodes in the affected cluster partition are
      fenced. This option works only in combination with SBD, see
      <a class="xref" href="#cha-ha-storage-protect" title="Chapter 13. Storage Protection and SBD">Chapter 13, <em>Storage Protection and SBD</em></a>.
     </p></dd></dl></div></section><section class="sect2" id="sec-ha-config-basics-global-stonith" data-id-title="Global Option stonith-enabled"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.3.2 </span><span class="title-name">Global Option <code class="literal">stonith-enabled</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-global-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   This global option defines whether to apply fencing, allowing STONITH
   devices to shoot failed nodes and nodes with resources that cannot be
   stopped. By default, this global option is set to
   <code class="literal">true</code>, because for normal cluster operation it is
   necessary to use STONITH devices. According to the default value,
   the cluster will refuse to start any resources if no STONITH
   resources have been defined.
  </p><p>
   If you need to disable fencing for any reasons, set
   <code class="literal">stonith-enabled</code> to <code class="literal">false</code>, but be
   aware that this has impact on the support status for your product.
   Furthermore, with <code class="literal">stonith-enabled="false"</code>, resources
   like the Distributed Lock Manager (DLM) and all services depending on
   DLM (such as lvmlockd, GFS2, and OCFS2) will fail to start.
  </p><div id="id-1.4.4.3.5.6.4" data-id-title="No support without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No support without STONITH</div><p>
    A cluster without STONITH is not supported.
   </p></div></section></section><section class="sect1" id="cha-conf-hawk2" data-id-title="Introduction to Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.4 </span><span class="title-name">Introduction to Hawk2</span></span> <a title="Permalink" class="permalink" href="#cha-conf-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To configure and manage cluster resources, either use Hawk2, or
    the crm shell (crmsh) command line utility.
   </p><p>
    Hawk2's user-friendly Web interface allows you to monitor and administer
    your High Availability clusters from Linux or non-Linux machines alike. Hawk2 can be
    accessed from any machine inside or outside of the cluster by using a
    (graphical) Web browser.
   </p><section class="sect2" id="sec-conf-hawk2-req" data-id-title="Hawk2 Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.1 </span><span class="title-name">Hawk2 Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-req">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before users can log in to Hawk2, the following requirements need to be
   fulfilled:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.5.3.1"><span class="term"><span class="package">hawk2</span> Package</span></dt><dd><p>
      The <span class="package">hawk2</span> package must be installed on all
      cluster nodes you want to connect to with Hawk2.
     </p></dd><dt id="id-1.4.4.3.6.5.3.2"><span class="term">Web Browser</span></dt><dd><p>
      On the machine from which to access a cluster node using
      Hawk2, you need a (graphical) Web browser (with JavaScript and
      cookies enabled) to establish the connection.
     </p></dd><dt id="id-1.4.4.3.6.5.3.3"><span class="term">Hawk2 Service</span></dt><dd><p>
      To use Hawk2, the respective Web service must be started on
      the node that you want to connect to via the Web interface. See <a class="xref" href="#pro-ha-hawk2-service" title="Starting Hawk2 Services">Procedure 5.1, “Starting Hawk2 Services”</a>.
     </p><p>
      If you have set up your cluster with the scripts from the
       <code class="systemitem">ha-cluster-bootstrap</code> package,
      the Hawk2 service is already enabled.
     </p></dd><dt id="id-1.4.4.3.6.5.3.4"><span class="term">Username, Group and Password on Each Cluster Node</span></dt><dd><p>
      Hawk2 users must be members of the <code class="systemitem">haclient</code> group. The installation creates a
      Linux user named <code class="systemitem">hacluster</code>, who
      is added to the <code class="systemitem">haclient</code> group.
     </p><p>
      When using the <code class="command">ha-cluster-init</code> script for setup,
      a default password is set for the <code class="systemitem">hacluster</code> user. Before starting Hawk2, change it to a
      secure password. If you did not use the <code class="command">ha-cluster-init</code>
      script, either set a password for the <code class="systemitem">hacluster</code> first or create a new user which is a member of
      the <code class="systemitem">haclient</code> group. Do this on
      every node you will connect to with Hawk2.
     </p></dd><dt id="id-1.4.4.3.6.5.3.5"><span class="term">Wildcard certificate handling</span></dt><dd><p>
      A wildcard certificate is a public key certificate which is valid for
      multiple sub-domains. For example, a wildcard certificate for
      <code class="systemitem">*.example.com</code> secures the domains
      <code class="systemitem">www.example.com</code>,
      <code class="systemitem">login.example.com</code>,
      and possibly more.
     </p><p>
      Hawk2 supports wildcard certificates as well as conventional certificates.
      A self-signed default private key and certificate is generated by
      <code class="filename">/srv/www/hawk/bin/generate-ssl-cert</code>.
     </p><p>
      To use your own certificate (conventional or wildcard), replace the
      generated certificate at <code class="filename">/etc/ssl/certs/hawk.pem</code>
      with your own.
     </p></dd></dl></div><div class="procedure" id="pro-ha-hawk2-service" data-id-title="Starting Hawk2 Services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.1: </span><span class="title-name">Starting Hawk2 Services </span></span><a title="Permalink" class="permalink" href="#pro-ha-hawk2-service">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the node you want to connect to, open a shell and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check the status of the service by entering
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status hawk</pre></div></li><li class="step"><p>
     If the service is not running, start it with
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> start hawk</pre></div><p>
     If you want Hawk2 to start automatically at boot time, execute the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable hawk</pre></div></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-login" data-id-title="Logging In"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.2 </span><span class="title-name">Logging In</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-login">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Hawk2 Web interface uses the HTTPS protocol and port
   <code class="literal">7630</code>.
  </p><p>
   Instead of logging in to an individual cluster node with Hawk2, you can
   configure a floating, virtual IP address (<code class="literal">IPaddr</code> or
   <code class="literal">IPaddr2</code>) as a cluster resource. It does not need any
   special configuration. It allows clients to connect to the Hawk service no
   matter which physical node the service is running on.
  </p><p>
   When setting up the cluster with the
   <code class="systemitem">ha-cluster-bootstrap</code> scripts,
   you will be asked whether to configure a virtual IP for cluster
   administration.
  </p><div class="procedure" id="pro-ha-hawk2-login" data-id-title="Logging In to the Hawk2 Web Interface"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.2: </span><span class="title-name">Logging In to the Hawk2 Web Interface </span></span><a title="Permalink" class="permalink" href="#pro-ha-hawk2-login">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On any machine, start a Web browser and enter the following URL:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div><p>
     Replace <em class="replaceable">HAWKSERVER</em> with the IP address or host
     name of any cluster node running the Hawk Web service. If a virtual IP
     address has been configured for cluster administration with Hawk2,
     replace <em class="replaceable">HAWKSERVER</em> with the virtual IP address.
    </p><div id="id-1.4.4.3.6.6.5.2.4" data-id-title="Certificate Warning" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Certificate Warning</div><p>
      If a certificate warning appears when you try to access the URL for the
      first time, a self-signed certificate is in use. Self-signed certificates
      are not considered trustworthy by default.
     </p><p>
      To verify the certificate, ask your cluster operator for the certificate
      details.
     </p><p>
      To proceed anyway, you can add an exception in the browser to bypass the
      warning.
     </p><p>
      For information on how to replace the self-signed certificate with a
      certificate signed by an official Certificate Authority, refer to
      <a class="xref" href="#vle-trouble-hawk2-cert">Replacing the Self-Signed Certificate</a>.
     </p></div></li><li class="step"><p>
     On the Hawk2 login screen, enter the <span class="guimenu">Username</span> and
     <span class="guimenu">Password</span> of the
     <code class="systemitem">hacluster</code> user (or of any other
     user that is a member of the
     <code class="systemitem">haclient</code> group).
    </p></li><li class="step"><p>
     Click <span class="guimenu">Log In</span>.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-overview" data-id-title="Hawk2 Overview: Main Elements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.3 </span><span class="title-name">Hawk2 Overview: Main Elements</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After logging in to Hawk2, you will see a navigation bar on the left-hand
   side and a top-level row with several links on the right-hand side.
  </p><div id="id-1.4.4.3.6.7.3" data-id-title="Available Functions in Hawk2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Available Functions in Hawk2</div><p>
    By default, users logged in as <code class="systemitem">root</code> or
    <code class="systemitem">hacluster</code> have full
    read-write access to all cluster configuration tasks. However,
    <a class="xref" href="#cha-ha-acl" title="Chapter 15. Access Control Lists"><em>Access Control Lists</em></a> (ACLs) can be used to
    define fine-grained access permissions.
   </p><p>
    If ACLs are enabled in the CRM, the available functions in Hawk2 depend
    on the user role and their assigned access permissions. The
    <span class="guimenu">History Explorer</span> in Hawk2 can only be executed by the
    user <code class="systemitem">hacluster</code>.
   </p></div><section class="sect3" id="sec-conf-hawk2-overview-leftnav" data-id-title="Left Navigation Bar"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.4.3.1 </span><span class="title-name">Left Navigation Bar</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview-leftnav">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.7.4.2.1"><span class="term"><span class="guimenu">Monitoring</span>
     </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Status</span>: Displays the current cluster status at a
         glance (similar to <code class="command">crm status</code> on the crmsh). For
         details, see <a class="xref" href="#sec-conf-hawk2-manage-monitor-status" title="11.1.1. Monitoring a Single Cluster">Section 11.1.1, “Monitoring a Single Cluster”</a>. If
         your cluster includes <code class="literal">guest nodes</code> (nodes that run
         the <code class="literal">pacemaker_remote</code> daemon), they are displayed, too.
         
         The screen refreshes in near real-time: any status changes for nodes
         or resources are visible almost immediately.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Dashboard</span>: Allows you to monitor multiple clusters
         (also located on different sites, in case you have a Geo cluster
         setup). For details, see
         <a class="xref" href="#sec-conf-hawk2-manage-monitor-dash" title="11.1.2. Monitoring Multiple Clusters">Section 11.1.2, “Monitoring Multiple Clusters”</a>. If your cluster
         includes <code class="literal">guest nodes</code> (nodes that run
         the <code class="literal">pacemaker_remote</code> daemon), they are displayed, too.
         The screen
         refreshes in near real-time: any status changes for nodes or resources
         are visible almost immediately.
        </p></li></ul></div></dd><dt id="id-1.4.4.3.6.7.4.2.2"><span class="term"><span class="guimenu">Troubleshooting</span>
     </span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">History</span>: Opens the <span class="guimenu">History
         Explorer</span> from which you can generate cluster reports. For
         details, see <a class="xref" href="#sec-conf-hawk2-history" title="11.3. Viewing the Cluster History">Section 11.3, “Viewing the Cluster History”</a>.
        </p></li><li class="listitem"><p>
         <span class="guimenu">Command Log</span>: Lists the crmsh commands recently
         executed by Hawk2.
        </p></li></ul></div></dd><dt id="id-1.4.4.3.6.7.4.2.3"><span class="term">Configuration</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="guimenu">Add Resource</span>: Opens the resource configuration
         screen. For details, see <a class="xref" href="#sec-ha-config-basics-resources" title="Chapter 6. Configuring Cluster Resources">Chapter 6, <em>Configuring Cluster Resources</em></a>. </p></li><li class="listitem"><p>
         <span class="guimenu">Add Constraint</span>: Opens the constraint configuration
         screen. For details, see <a class="xref" href="#sec-ha-config-basics-constraints" title="Chapter 7. Configuring Resource Constraints">Chapter 7, <em>Configuring Resource Constraints</em></a>. </p></li><li class="listitem"><p>
         <span class="guimenu">Wizards</span>: Allows you to select from several wizards
         that guide you through the creation of resources for a certain
         workload, for example, a DRBD block device. For details, see <a class="xref" href="#sec-conf-hawk2-rsc-wizard" title="5.4.6. Adding Resources with the Wizard">Section 5.4.6, “Adding Resources with the Wizard”</a>. </p></li><li class="listitem"><p>
         <span class="guimenu">Edit Configuration</span>: Allows you to edit resources,
         constraints, node names and attributes, tags, <a class="link" href="http://crmsh.github.io/man/#cmdhelp_configure_alert" target="_blank">alerts</a>,
         and <a class="link" href="http://crmsh.github.io/man/#cmdhelp_configure_fencing_topology" target="_blank">fencing
         topologies</a>.
         </p></li><li class="listitem"><p>
         <span class="guimenu">Cluster Configuration</span>: Allows you to modify global
         cluster options and resource and operation defaults. For details, see
          <a class="xref" href="#sec-conf-hawk2-cluster-config" title="5.4.4. Configuring Global Cluster Options">Section 5.4.4, “Configuring Global Cluster Options”</a>. </p></li><li class="listitem"><p>
         <span class="guimenu">Access Control</span> › <span class="guimenu">Roles</span>: Opens a screen where you can create roles for access
         control lists (sets of rules describing access rights to the CIB). For
         details, see <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 15.2, “Adding a Monitor Role with Hawk2”</a>. </p></li><li class="listitem"><p>
         <span class="guimenu">Access Control</span> › <span class="guimenu">Targets</span>: Opens a screen where you can create targets (system
         users) for access control lists and assign roles to them. For details,
         see <a class="xref" href="#pro-ha-acl-hawk2-target" title="Assigning a Role to a Target with Hawk2">Procedure 15.3, “Assigning a Role to a Target with Hawk2”</a>. </p></li></ul></div></dd></dl></div></section><section class="sect3" id="sec-conf-hawk2-overview-toprow" data-id-title="Top-level Row"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.4.3.2 </span><span class="title-name">Top-level Row</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-overview-toprow">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Hawk2's top-level row shows the following entries:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="guimenu">Batch</span>: Click to switch to batch mode. This allows you
      to simulate and stage changes and to apply them as a single transaction.
      For details, see <a class="xref" href="#sec-conf-hawk2-batch" title="5.4.7. Using the Batch Mode">Section 5.4.7, “Using the Batch Mode”</a>.
     </p></li><li class="listitem"><p>
      <span class="guimenu"><em class="replaceable">USERNAME</em></span>: Allows you to set
      preferences for Hawk2 (for example, the language for the Web interface,
      or whether to display a warning if STONITH is disabled).
     </p></li><li class="listitem"><p>
      <span class="guimenu">Help</span>: Access the SUSE Linux Enterprise High Availability documentation, read the
      release notes or report a bug.
     </p></li><li class="listitem"><p>
      <span class="guimenu">Logout</span>: Click to log out.
     </p></li></ul></div></section></section><section class="sect2" id="sec-conf-hawk2-cluster-config" data-id-title="Configuring Global Cluster Options"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.4 </span><span class="title-name">Configuring Global Cluster Options</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cluster-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Global cluster options control how the cluster behaves when confronted with
   certain situations. They are grouped into sets and can be viewed and
   modified with cluster management tools like Hawk2 and crmsh. The
   predefined values can usually be kept. However, to ensure the key functions
   of your cluster work correctly, you need to adjust the following parameters
   after basic cluster setup:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.3.1. Global Option no-quorum-policy">Global Option <code class="literal">no-quorum-policy</code></a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-ha-config-basics-global-stonith" title="5.3.2. Global Option stonith-enabled">Global Option <code class="literal">stonith-enabled</code></a>
    </p></li></ul></div><div class="procedure" id="pro-conf-hawk2-global" data-id-title="Modifying Global Cluster Options"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.3: </span><span class="title-name">Modifying Global Cluster Options </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Cluster Configuration</span>.
    </p><p>
     The <span class="guimenu">Cluster Configuration</span> screen opens. It displays the
     global cluster options and their current values.
    </p><p>
     To display a short description of the parameter on the right-hand side of
     the screen, hover your mouse over a parameter.
     
    </p><div class="figure" id="id-1.4.4.3.6.8.4.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-cluster-config.png"><img src="images/hawk2-cluster-config.png" width="100%" alt="Hawk2—Cluster Configuration" title="Hawk2—Cluster Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.1: </span><span class="title-name">Hawk2—Cluster Configuration </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.8.4.3.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     Check the values for <span class="guimenu">no-quorum-policy</span> and
     <span class="guimenu">stonith-enabled</span> and adjust them, if necessary:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Set <span class="guimenu">no-quorum-policy</span> to the appropriate value. See
       <a class="xref" href="#sec-ha-config-basics-global-quorum" title="5.3.1. Global Option no-quorum-policy">Section 5.3.1, “Global Option <code class="literal">no-quorum-policy</code>”</a> for more details.
      </p></li><li class="step"><p>
       If you need to disable fencing for any reason, set
       <span class="guimenu">stonith-enabled</span> to <code class="literal">no</code>. By default,
       it is set to <code class="literal">true</code>, because using STONITH devices is
       necessary for normal cluster operation. According to the
       default value, the cluster will refuse to start any resources if no
       STONITH resources have been configured.
      </p><div id="id-1.4.4.3.6.8.4.4.2.2.2" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div></li><li class="step"><p>
       To remove a parameter from the cluster configuration, click the
       <span class="guimenu">Minus</span> icon next to the parameter. If a parameter is
       deleted, the cluster will behave as if that parameter had the default
       value.
      </p></li><li class="step"><p>
       To add a new parameter to the cluster configuration, choose one from the
       drop-down box.
      </p></li></ol></li><li class="step"><p>
     If you need to change <span class="guimenu">Resource Defaults</span> or
     <span class="guimenu">Operation Defaults</span>, proceed as follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       To adjust a value, either select a different value from the drop-down
       box or edit the value directly.
      </p></li><li class="step"><p>
       To add a new resource default or operation default, choose one from the
       empty drop-down box and enter a
       value. If there are default values, Hawk2 proposes them automatically.
      </p></li><li class="step"><p>
       To remove a parameter, click the <span class="guimenu">Minus</span> icon next to
       it. If no values are specified for <span class="guimenu">Resource Defaults</span>
       and <span class="guimenu">Operation Defaults</span>, the cluster uses the default
       values that are documented in
       <a class="xref" href="#sec-ha-config-basics-meta-attr" title="6.12. Resource Options (Meta Attributes)">Section 6.12, “Resource Options (Meta Attributes)”</a> and
       <a class="xref" href="#sec-ha-config-basics-operations" title="6.14. Resource Operations">Section 6.14, “Resource Operations”</a>.
      </p></li></ol></li><li class="step"><p>
     Confirm your changes.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-rsc-show" data-id-title="Showing the Current Cluster Configuration (CIB)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.5 </span><span class="title-name">Showing the Current Cluster Configuration (CIB)</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-show">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Sometimes a cluster administrator needs to know the cluster configuration.
   Hawk2 can show the current configuration in crm shell syntax, as XML and
   as a graph. To view the cluster configuration in crm shell syntax, from the
   left navigation bar select <span class="guimenu">Configuration</span> › <span class="guimenu">Edit Configuration</span> and click
   <span class="guimenu">Show</span>. To show the configuration in raw XML instead, click
   <span class="guimenu">XML</span>. Click <span class="guimenu">Graph</span> for a graphical
   representation of the nodes and resources configured in the CIB. It also
   shows the relationships between resources.
  </p></section><section class="sect2" id="sec-conf-hawk2-rsc-wizard" data-id-title="Adding Resources with the Wizard"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.6 </span><span class="title-name">Adding Resources with the Wizard</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-wizard">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The Hawk2 wizard is a convenient way of setting up simple resources like a
   virtual IP address or an SBD STONITH resource, for example. It is
   also useful for complex configurations that include multiple resources,
   like the resource configuration for a DRBD block device or an Apache Web
   server. The wizard guides you through the configuration steps and provides
   information about the parameters you need to enter.
  </p><div class="procedure" id="pro-conf-hawk2-wizard" data-id-title="Using the Resource Wizard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.4: </span><span class="title-name">Using the Resource Wizard </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-wizard">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Wizards</span>.
    </p></li><li class="step"><p>
     Expand the individual categories by clicking the arrow down icon next to
     them and select the desired wizard.
    </p></li><li class="step"><p>
     Follow the instructions on the screen. After the last configuration step,
     <span class="guimenu">Verify</span> the values you have entered.
    </p><p>
     Hawk2 shows which actions it is going to perform and what the
     configuration looks like. Depending on the configuration, you might be
     prompted for the <code class="systemitem">root</code> password before you can
     <span class="guimenu">Apply</span> the configuration.
    </p></li></ol></div></div><div class="figure" id="id-1.4.4.3.6.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-wizard-apache.png"><img src="images/hawk2-wizard-apache.png" width="100%" alt="Hawk2—Wizard for Apache Web Server" title="Hawk2—Wizard for Apache Web Server"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.2: </span><span class="title-name">Hawk2—Wizard for Apache Web Server </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.10.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_hawk2.xml" title="Edit source document"> </a></div></div></div><p>
   For more information, see <a class="xref" href="#sec-ha-config-basics-resources" title="Chapter 6. Configuring Cluster Resources">Chapter 6, <em>Configuring Cluster Resources</em></a>.
  </p></section><section class="sect2" id="sec-conf-hawk2-batch" data-id-title="Using the Batch Mode"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.4.7 </span><span class="title-name">Using the Batch Mode</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-batch">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides a <span class="guimenu">Batch Mode</span>, including a
  <span class="emphasis"><em>cluster simulator</em></span>. It can be used for the following:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Staging changes to the cluster and applying them as a single transaction,
    instead of having each change take effect immediately.
   </p></li><li class="listitem"><p>
    Simulating changes and cluster events, for example, to explore potential
    failure scenarios.
   </p></li></ul></div><p>
  For example, batch mode can be used when creating groups of resources that
  depend on each other. Using batch mode, you can avoid applying intermediate
  or incomplete configurations to the cluster.
 </p><p>
  While batch mode is enabled, you can add or edit resources and constraints or
  change the cluster configuration. It is also possible to simulate events in
  the cluster, including nodes going online or offline, resource operations and
  tickets being granted or revoked. See
  <a class="xref" href="#pro-hawk2-batch-inject" title="Injecting Node, Resource or Ticket Events">Procedure 5.6, “Injecting Node, Resource or Ticket Events”</a> for details.
 </p><p>
  The <span class="emphasis"><em>cluster simulator</em></span> runs automatically after every
  change and shows the expected outcome in the user interface. For example,
  this also means: If you stop a resource while in batch mode, the user
  interface shows the resource as stopped—while actually, the resource is
  still running.
 </p><div id="id-1.4.4.3.6.11.8" data-id-title="Wizards and Changes to the Live System" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Wizards and Changes to the Live System</div><p>
   Some wizards include actions beyond mere cluster configuration. When using
   those wizards in batch mode, any changes that go beyond cluster
   configuration would be applied to the live system immediately.
  </p><p>
   Therefore wizards that require <code class="systemitem">root</code> permission cannot be executed in
   batch mode.
  </p></div><div class="procedure" id="pro-conf-hawk2-manage-monitor-batch" data-id-title="Working with the Batch Mode"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.5: </span><span class="title-name">Working with the Batch Mode </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-manage-monitor-batch">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    To activate the batch mode, select <span class="guimenu">Batch</span> from the
    top-level row.
   </p><p>
    An additional bar appears below the top-level row. It indicates that batch
    mode is active and contains links to actions that you can execute in batch
    mode.
   </p><div class="figure" id="fig-hawk2-batch"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-batchmode-active.png"><img src="images/hawk2-batchmode-active.png" width="100%" alt="Hawk2 Batch Mode Activated" title="Hawk2 Batch Mode Activated"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.3: </span><span class="title-name">Hawk2 Batch Mode Activated </span></span><a title="Permalink" class="permalink" href="#fig-hawk2-batch">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
    While batch mode is active, perform any changes to your cluster, like
    adding or editing resources and constraints or editing the cluster
    configuration.
   </p><p>
    The changes will be simulated and shown in all screens.
   </p></li><li class="step"><p>
    To view details of the changes you have made, select
    <span class="guimenu">Show</span> from the batch mode bar. The <span class="guimenu">Batch
    Mode</span> window opens.
   </p><p>
    For any configuration changes it shows the difference between the live
    state and the simulated changes in crmsh syntax: Lines starting with a
    <code class="literal">-</code> character represent the current state whereas lines
    starting with <code class="literal">+</code> show the proposed state.
   </p></li><li class="step"><p>
    To inject events or view even more details, see
    <a class="xref" href="#pro-hawk2-batch-inject" title="Injecting Node, Resource or Ticket Events">Procedure 5.6</a>.
    Otherwise <span class="guimenu">Close</span> the window.
   </p></li><li class="step"><p>
    Choose to either <span class="guimenu">Discard</span> or <span class="guimenu">Apply</span> the
    simulated changes and confirm your choice. This also deactivates batch mode
    and takes you back to normal mode.
   </p></li></ol></div></div><p>
  When running in batch mode, Hawk2 also allows you to inject <span class="guimenu">Node
  Events</span> and <span class="guimenu">Resource Events</span>.
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.11.11.1"><span class="term"><span class="guimenu">Node Events</span>
   </span></dt><dd><p>
     Let you change the state of a node. Available states are
     <span class="guimenu">online</span>, <span class="guimenu">offline</span>, and
     <span class="guimenu">unclean</span>.
    </p></dd><dt id="id-1.4.4.3.6.11.11.2"><span class="term"><span class="guimenu">Resource Events</span>
   </span></dt><dd><p>
     Let you change some properties of a resource. For example, you can set an
     operation (like <code class="literal">start</code>, <code class="literal">stop</code>,
     <code class="literal">monitor</code>), the node it applies to, and the expected
     result to be simulated.
    </p></dd><dt id="id-1.4.4.3.6.11.11.3"><span class="term"><span class="guimenu">Ticket Events</span>
   </span></dt><dd><p>
     Let you test the impact of granting and revoking tickets (used for Geo
     clusters).
    </p></dd></dl></div><div class="procedure" id="pro-hawk2-batch-inject" data-id-title="Injecting Node, Resource or Ticket Events"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 5.6: </span><span class="title-name">Injecting Node, Resource or Ticket Events </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-batch-inject">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    If batch mode is not active yet, click <span class="guimenu">Batch</span> at the
    top-level row to switch to batch mode.
   </p></li><li class="step"><p>
    In the batch mode bar, click <span class="guimenu">Show</span> to open the
    <span class="guimenu">Batch Mode</span> window.
   </p></li><li class="step"><p>
    To simulate a status change of a node:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Node
      Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Node</span> you want to manipulate and select its
      target <span class="guimenu">State</span>.
     </p></li><li class="step"><p>
      Confirm your changes. Your event is added to the queue of events listed
      in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li></ol></li><li class="step"><p>
    To simulate a resource operation:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Resource
      Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Resource</span> you want to manipulate and select
      the <span class="guimenu">Operation</span> to simulate.
     </p></li><li class="step"><p>
      If necessary, define an <span class="guimenu">Interval</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Node</span> on which to run the operation and the
      targeted <span class="guimenu">Result</span>. Your event is added to the queue of
      events listed in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li></ol></li><li class="step"><p>
    To simulate a ticket action:
   </p><ol type="a" class="substeps"><li class="step"><p>
      Click <span class="guimenu">Inject</span> › <span class="guimenu">Ticket
       Event</span>.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Ticket</span> you want to manipulate and select
      the <span class="guimenu">Action</span> to simulate.
     </p></li><li class="step"><p>
      Confirm your changes. Your event is added to the queue of events listed
      in the <span class="guimenu">Batch Mode</span> dialog.
     </p></li></ol></li><li class="step"><p>
    The <span class="guimenu">Batch Mode</span> dialog
    (<a class="xref" href="#fig-hawk2-batch-show" title="Hawk2 Batch Mode—Injected Invents and Configuration Changes">Figure 5.4</a>)
    shows a new line per injected event. Any event listed here is simulated
    immediately and is reflected on the <span class="guimenu">Status</span> screen.
   </p><p>
    If you have made any configuration changes, too, the difference between the
    live state and the simulated changes is shown below the injected events.
   </p><div class="figure" id="fig-hawk2-batch-show"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-batchmode-show.png"><img src="images/hawk2-batchmode-show.png" width="100%" alt="Hawk2 Batch Mode—Injected Invents and Configuration Changes" title="Hawk2 Batch Mode—Injected Invents and Configuration Changes"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 5.4: </span><span class="title-name">Hawk2 Batch Mode—Injected Invents and Configuration Changes </span></span><a title="Permalink" class="permalink" href="#fig-hawk2-batch-show">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_batch_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
    To remove an injected event, click the <span class="guimenu">Remove</span> icon next
    to it. Hawk2 updates the <span class="guimenu">Status</span> screen accordingly.
   </p></li><li class="step"><p>
    To view more details about the simulation run, click
    <span class="guimenu">Simulator</span> and choose one of the following:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.11.12.10.2.1"><span class="term"><span class="guimenu">Summary</span>
     </span></dt><dd><p>
       Shows a detailed summary.
      </p></dd><dt id="id-1.4.4.3.6.11.12.10.2.2"><span class="term"><span class="guimenu">CIB (in)</span>/<span class="guimenu">CIB (out)</span>
     </span></dt><dd><p>
       <span class="guimenu">CIB (in)</span> shows the initial CIB state. <span class="guimenu">CIB
       (out)</span> shows what the CIB would look like after the transition.
      </p></dd><dt id="id-1.4.4.3.6.11.12.10.2.3"><span class="term"><span class="guimenu">Transition Graph</span>
     </span></dt><dd><p>
       Shows a graphical representation of the transition.
      </p></dd><dt id="id-1.4.4.3.6.11.12.10.2.4"><span class="term"><span class="guimenu">Transition</span>
     </span></dt><dd><p>
       Shows an XML representation of the transition.
      </p></dd></dl></div></li><li class="step"><p>
    If you have reviewed the simulated changes, close the <span class="guimenu">Batch
    Mode</span> window.
   </p></li><li class="step"><p>
    To leave the batch mode, either <span class="guimenu">Apply</span> or
    <span class="guimenu">Discard</span> the simulated changes.
   </p></li></ol></div></div></section></section><section class="sect1" id="cha-ha-manual-config" data-id-title="Introduction to crmsh"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.5 </span><span class="title-name">Introduction to crmsh</span></span> <a title="Permalink" class="permalink" href="#cha-ha-manual-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To configure and manage cluster resources, either use the crm shell
    (crmsh) command line utility or Hawk2, a Web-based
    user interface.
   </p><p>
    This section introduces the command line tool <code class="command">crm</code>.
    The <code class="command">crm</code> command has several subcommands which manage
   resources, CIBs, nodes, resource agents, and others. It offers a thorough
   help system with embedded examples. All examples follow a naming
   convention described in
   <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B</a>.
   </p><p>
   Events are logged to <code class="filename">/var/log/crmsh/crmsh.log</code>.
  </p><div id="id-1.4.4.3.7.6" data-id-title="User Privileges" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: User Privileges</div><p>
   Sufficient privileges are necessary to manage a cluster. The
   <code class="command">crm</code> command and its subcommands need to be run either
   as <code class="systemitem">root</code> user or as the CRM owner user (typically the user
   <code class="systemitem">hacluster</code>).
  </p><p>
   However, the <code class="option">user</code> option allows you to run
   <code class="command">crm</code> and its subcommands as a regular (unprivileged)
   user and to change its ID using <code class="command">sudo</code> whenever
   necessary. For example, with the following command <code class="command">crm</code>
   will use <code class="systemitem">hacluster</code> as the
   privileged user ID:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> options user hacluster</pre></div><p>
   Note that you need to set up <code class="filename">/etc/sudoers</code> so that
   <code class="command">sudo</code> does not ask for a password.
  </p></div><div id="id-1.4.4.3.7.7" data-id-title="Interactive crm Prompt" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Interactive crm Prompt</div><p>By using crm without arguments (or with only one sublevel as
    argument), the crm shell enters the interactive mode. This mode is
    indicated by the following prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live/HOSTNAME)</code></pre></div><p>
    For readability reasons, we omit the host name in the interactive crm
    prompts in our documentation. We only include the host name if you need
    to run the interactive shell on a specific node, like alice for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live/alice)</code></pre></div></div><section class="sect2" id="sec-ha-manual-config-crm-help" data-id-title="Getting Help"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.1 </span><span class="title-name">Getting Help</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-help">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Help can be accessed in several ways:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      To output the usage of <code class="command">crm</code> and its command line
      options:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> --help</pre></div></li><li class="listitem"><p>
      To give a list of all available commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help</pre></div></li><li class="listitem"><p>
      To access other help sections, not just the command reference:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help topics</pre></div></li><li class="listitem"><p>
      To view the extensive help text of the <code class="command">configure</code>
      subcommand:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure help</pre></div></li><li class="listitem"><p>
      To print the syntax, its usage, and examples of the <code class="command">group</code>
      subcommand of <code class="command">configure</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure help group</pre></div><p>
      This is the same:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> help configure group</pre></div></li></ul></div><p>
    Almost all output of the <code class="command">help</code> subcommand (do not mix
    it up with the <code class="option">--help</code> option) opens a text viewer. This
    text viewer allows you to scroll up or down and read the help text more
    comfortably. To leave the text viewer, press the <span class="keycap">Q</span> key.
   </p><div id="tip-crm-tabcompletion" data-id-title="Use Tab Completion in Bash and Interactive Shell" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Use Tab Completion in Bash and Interactive Shell</div><p>
     The crmsh supports full tab completion in Bash directly, not only
     for the interactive shell. For example, typing <code class="literal">crm help
     config</code><span class="keycap">→|</span> will complete the word
     like in the interactive shell.
    </p></div></section><section class="sect2" id="sec-ha-manual-config-crm-run" data-id-title="Executing crmshs Subcommands"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.2 </span><span class="title-name">Executing crmsh's Subcommands</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-run">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="command">crm</code> command itself can be used in the following
    ways:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Directly:</span>
       Concatenate all subcommands to <code class="command">crm</code>, press
       <span class="keycap">Enter</span> and you see the output immediately. For
       example, enter <code class="command">crm</code> <code class="option">help ra</code> to get
       information about the <code class="command">ra</code> subcommand (resource
       agents).
      </p><p>It is possible to abbreviate subcommands as long as they are
        unique. For example, you can shorten <code class="command">status</code> as
      <code class="command">st</code> and crmsh will know what you have meant.
      </p><p>Another feature is to shorten parameters. Usually, you add
        parameters through the <code class="command">params</code> keyword.
        You can leave out the <code class="literal">params</code> section if it is the first and only section.
        For example, this line:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> primitive ipaddr IPaddr2 params ip=192.168.0.55</pre></div><p>is equivalent to this line:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> primitive ipaddr IPaddr2 ip=192.168.0.55</pre></div></li><li class="listitem"><p><span class="formalpara-title">As crm Shell Script:</span>
       Crm shell scripts contain subcommands of <code class="command">crm</code>.
       For more information, see <a class="xref" href="#sec-ha-manual-config-crmshellscripts" title="5.5.4. Using crmsh's Shell Scripts">Section 5.5.4, “Using crmsh's Shell Scripts”</a>.
      </p></li><li class="listitem"><p><span class="formalpara-title">As crmsh Cluster Scripts:</span>These are a collection of metadata, references to RPM packages,
          configuration files, and crmsh subcommands bundled under a single,
          yet descriptive name. They are managed through the
          <code class="command">crm script</code> command.
        </p><p>Do not confuse them with crmsh shell scripts: although both share
        some common objectives, the crm shell scripts only contain subcommands
        whereas cluster scripts incorporate much more than a simple
        enumeration of commands. For more information, see <a class="xref" href="#sec-ha-manual-config-clusterscripts" title="5.5.5. Using crmsh's Cluster Scripts">Section 5.5.5, “Using crmsh's Cluster Scripts”</a>.
      </p></li><li class="listitem"><p><span class="formalpara-title">Interactive as Internal Shell:</span>
       Type <code class="command">crm</code> to enter the internal shell. The prompt
       changes to <code class="literal">crm(live)</code>. With
       <code class="command">help</code> you can get an overview of the available
       subcommands. As the internal shell has different levels of
       subcommands, you can <span class="quote">“<span class="quote">enter</span>”</span> one by typing this
       subcommand and press <span class="keycap">Enter</span>.
      </p><p>
      For example, if you type <code class="command">resource</code> you enter the
      resource management level. Your prompt changes to
      <code class="literal">crm(live)resource#</code>. To leave the
      internal shell, use the command <code class="command">quit</code>. If you need to go
      one level back, use <code class="command">back</code>, <code class="command">up</code>,
      <code class="command">end</code>, or <code class="command">cd</code>.
     </p><p>
      You can enter the level directly by typing <code class="command">crm</code> and
      the respective subcommand(s) without any options and press
      <span class="keycap">Enter</span>.
     </p><p>
      The internal shell supports also tab completion for subcommands and
      resources. Type the beginning of a command, press
      <span class="keycap">→|</span> and <code class="command">crm</code> completes the
      respective object.
     </p></li></ul></div><p>
    In addition to previously explained methods, crmsh also supports
    synchronous command execution. Use the <code class="option">-w</code> option to
    activate it. If you have started <code class="command">crm</code> without
    <code class="option">-w</code>, you can enable it later with the user preference's
    <code class="command">wait</code> set to <code class="literal">yes</code> (<code class="command">options
    wait yes</code>). If this option is enabled, <code class="command">crm</code>
    waits until the transition is finished. Whenever a transaction is
    started, dots are printed to indicate progress. Synchronous command
    execution is only applicable for commands like <code class="command">resource
    start</code>.
   </p><div id="id-1.4.4.3.7.9.5" data-id-title="Differentiate Between Management and Configuration Subcommands" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Differentiate Between Management and Configuration Subcommands</div><p>
     The <code class="command">crm</code> tool has management capability (the
     subcommands <code class="command">resource</code> and <code class="command">node</code>)
     and can be used for configuration (<code class="command">cib</code>,
     <code class="command">configure</code>).
    </p></div><p>
    The following subsections give you an overview of some important aspects
    of the <code class="command">crm</code> tool.
   </p></section><section class="sect2" id="sec-ha-manual-config-ocf" data-id-title="Displaying Information about OCF Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.3 </span><span class="title-name">Displaying Information about OCF Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-ocf">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    As you need to deal with resource agents in your cluster configuration
    all the time, the <code class="command">crm</code> tool contains the
    <code class="command">ra</code> command. Use it to show information about resource
    agents and to manage them (for additional information, see also
    <a class="xref" href="#sec-ha-config-basics-raclasses" title="6.2. Supported Resource Agent Classes">Section 6.2, “Supported Resource Agent Classes”</a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra
<code class="prompt custom">crm(live)ra# </code></pre></div><p>
    The command <code class="command">classes</code> lists all classes and providers:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">classes</code>
 lsb
 ocf / heartbeat linbit lvm2 ocfs2 pacemaker
 service
 stonith
 systemd</pre></div><p>
    To get an overview of all available resource agents for a class (and
    provider) use the <code class="command">list</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">list</code> ocf
AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          HealthCPU           HealthSMART         ICP
IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID
ManageVE            Pure-FTPd           Raid1               Route
SAPDatabase         SAPInstance         SendArp             ServeRAID
...</pre></div><p>
    An overview of a resource agent can be viewed with
    <code class="command">info</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)ra# </code><code class="command">info</code> ocf:linbit:drbd
This resource agent manages a DRBD* resource
as a master/slave resource. DRBD is a shared-nothing replicated storage
device. (ocf:linbit:drbd)

Master/Slave OCF Resource Agent for DRBD

Parameters (* denotes required, [] the default):

drbd_resource* (string): drbd resource name
    The name of the drbd resource from the drbd.conf file.

drbdconf (string, [/etc/drbd.conf]): Path to drbd.conf
    Full path to the drbd.conf file.

Operations' defaults (advisory minimum):

    start         timeout=240
    promote       timeout=90
    demote        timeout=90
    notify        timeout=90
    stop          timeout=100
    monitor_Slave_0 interval=20 timeout=20 start-delay=1m
    monitor_Master_0 interval=10 timeout=20 start-delay=1m</pre></div><p>
    Leave the viewer by pressing <span class="keycap">Q</span>.
   </p><div id="id-1.4.4.3.7.10.11" data-id-title="Use crm Directly" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Use <code class="command">crm</code> Directly</div><p>
     In the former example we used the internal shell of the
     <code class="command">crm</code> command. However, you do not necessarily need to
     use it. You get the same results if you add the respective subcommands
     to <code class="command">crm</code>. For example, you can list all the OCF
     resource agents by entering <code class="command">crm</code> <code class="option">ra list
     ocf</code> in your shell.
    </p></div></section><section class="sect2" id="sec-ha-manual-config-crmshellscripts" data-id-title="Using crmshs Shell Scripts"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.4 </span><span class="title-name">Using crmsh's Shell Scripts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crmshellscripts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The crmsh shell scripts provide a convenient way to enumerate crmsh
    subcommands into a file. This makes it easy to comment specific lines or
    to replay them later. Keep in mind that a crmsh shell script can contain
    <span class="emphasis"><em>only crmsh subcommands</em></span>. Any other commands are not
    allowed.
   </p><p>
    Before you can use a crmsh shell script, create a file with specific
    commands. For example, the following file prints the status of the cluster
    and gives a list of all nodes:
   </p><div class="example" id="ex-ha-manual-config-crmshellscripts" data-id-title="A Simple crmsh Shell Script"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 5.3: </span><span class="title-name">A Simple crmsh Shell Script </span></span><a title="Permalink" class="permalink" href="#ex-ha-manual-config-crmshellscripts">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># A small example file with some crm subcommands
<code class="command">status</code>
<code class="command">node</code> list</pre></div></div></div><p>
    Any line starting with the hash symbol (<code class="literal">#</code>) is a
    comment and is ignored. If a line is too long, insert a backslash
    (<code class="literal">\</code>) at the end and continue in the next line. It is
    recommended to indent lines that belong to a certain subcommand to improve
    readability.
   </p><p>To use this script, use one of the following methods:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> -f example.cli
<code class="prompt root"># </code><code class="command">crm</code> &lt; example.cli</pre></div></section><section class="sect2" id="sec-ha-manual-config-clusterscripts" data-id-title="Using crmshs Cluster Scripts"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.5 </span><span class="title-name">Using crmsh's Cluster Scripts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>Collecting information from all cluster nodes and deploying any
      changes is a key cluster administration task. Instead of performing
      the same procedures manually on different nodes (which is error-prone),
      you can use the crmsh cluster scripts.
   </p><p>
    Do not confuse them with the <span class="emphasis"><em>crmsh shell scripts</em></span>,
    which are explained in <a class="xref" href="#sec-ha-manual-config-crmshellscripts" title="5.5.4. Using crmsh's Shell Scripts">Section 5.5.4, “Using crmsh's Shell Scripts”</a>.
    </p><p>In contrast to crmsh shell scripts, cluster scripts performs
    additional tasks like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Installing software that is required for a specific task.</p></li><li class="listitem"><p>Creating or modifying any configuration files.</p></li><li class="listitem"><p>Collecting information and reporting potential problems with the
          cluster.</p></li><li class="listitem"><p>Deploying the changes to all nodes.</p></li></ul></div><p>crmsh cluster scripts do not replace other tools for managing
      clusters—they provide an integrated way to perform the above
      tasks across the cluster. Find detailed information at <a class="link" href="http://crmsh.github.io/scripts/" target="_blank">http://crmsh.github.io/scripts/</a>.
    </p><section class="sect3" id="sec-ha-manual-config-clusterscripts-usage" data-id-title="Usage"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.5.1 </span><span class="title-name">Usage</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts-usage">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>To get a list of all available cluster scripts, run:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script list</pre></div><p>To view the components of a script, use the
        <code class="command">show</code> command and the name of the cluster script,
        for example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script show mailto
mailto (Basic)
MailTo

 This is a resource agent for MailTo. It sends email to a sysadmin
whenever  a takeover occurs.

1. Notifies recipients by email in the event of resource takeover

  id (required)  (unique)
      Identifier for the cluster resource
  email (required)
      Email address
  subject
      Subject</pre></div><p>The output of <code class="command">show</code> contains a title, a
          short description, and a procedure. Each procedure is divided
          into a series of steps, performed in the given order. </p><p>Each step contains a list of required and optional parameters,
        along with a short description and its default value.</p><p>Each cluster script understands a set of common parameters.
        These parameters can be passed to any script:</p><div class="table" id="id-1.4.4.3.7.12.8.9" data-id-title="Common Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 5.1: </span><span class="title-name">Common Parameters </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.3.7.12.8.9">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Parameter</th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Argument</th><th style="border-bottom: 1px solid ; ">Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">action</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">INDEX</em></td><td style="border-bottom: 1px solid ; ">If set, only execute a single action (index, as
                returned by verify)</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">dry_run</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">BOOL</em></td><td style="border-bottom: 1px solid ; ">If set, simulate execution only (default: no) </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">nodes</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">LIST</em></td><td style="border-bottom: 1px solid ; ">List of nodes to execute the script for</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">port</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">NUMBER</em></td><td style="border-bottom: 1px solid ; ">Port to connect to</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">statefile</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">FILE</em></td><td style="border-bottom: 1px solid ; ">When single-stepping, the state is saved in the given
                file </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">sudo</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">BOOL</em></td><td style="border-bottom: 1px solid ; ">If set, crm will prompt for a sudo password and use sudo
                where appropriate (default: no) </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="parameter">timeout</em></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; "><em class="replaceable">NUMBER</em></td><td style="border-bottom: 1px solid ; ">Execution timeout in seconds (default: 600) </td></tr><tr><td style="border-right: 1px solid ; "><em class="parameter">user</em></td><td style="border-right: 1px solid ; "><em class="replaceable">USER</em></td><td>Run script as the given user </td></tr></tbody></table></div></div></section><section class="sect3" id="sec-ha-manual-config-clusterscripts-verify-run" data-id-title="Verifying and Running a Cluster Script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">5.5.5.2 </span><span class="title-name">Verifying and Running a Cluster Script</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clusterscripts-verify-run">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>Before running a cluster script, review the actions that it will
          perform and verify its parameters to avoid problems. A cluster script
          can potentially perform a series of actions and may fail for
          various reasons. Thus, verifying your parameters before
          running it helps to avoid problems.</p><p>For example, the <code class="systemitem">mailto</code> resource agent
        requires a unique identifier and an e-mail address. To verify these
        parameters, run:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script verify mailto id=sysadmin email=tux@example.org
1. Ensure mail package is installed

        mailx

2. Configure cluster resources

        primitive sysadmin MailTo
                email="tux@example.org"
                op start timeout="10"
                op stop timeout="10"
                op monitor interval="10" timeout="10"

        clone c-sysadmin sysadmin</pre></div><p>The <code class="command">verify</code> prints the steps and replaces
        any placeholders with your given parameters. If <code class="command">verify</code>
        finds any problems, it will report it.
        If everything is OK, replace the <code class="command">verify</code>
        command with <code class="command">run</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> script run mailto id=sysadmin email=tux@example.org
INFO: MailTo
INFO: Nodes: alice, bob
OK: Ensure mail package is installed
OK: Configure cluster resources</pre></div><p>Check whether your resource is integrated into your cluster
          with <code class="command">crm status</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> status
[...]
 Clone Set: c-sysadmin [sysadmin]
     Started: [ alice bob ]</pre></div></section></section><section class="sect2" id="sec-ha-manual-config-template" data-id-title="Using Configuration Templates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.6 </span><span class="title-name">Using Configuration Templates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.3.7.13.3" data-id-title="Deprecation Notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Deprecation Notice</div><p>The use of configuration templates is deprecated and will
        be removed in the future. Configuration templates will be replaced
        by cluster scripts, see <a class="xref" href="#sec-ha-manual-config-clusterscripts" title="5.5.5. Using crmsh's Cluster Scripts">Section 5.5.5, “Using crmsh's Cluster Scripts”</a>.
      </p></div><p>
    Configuration templates are ready-made cluster configurations for
    crmsh. Do not confuse them with the <span class="emphasis"><em>resource
    templates</em></span> (as described in
    <a class="xref" href="#sec-ha-manual-config-rsc-template" title="6.8.2. Creating Resource Templates with crmsh">Section 6.8.2, “Creating Resource Templates with crmsh”</a>). Those are
    templates for the <span class="emphasis"><em>cluster</em></span> and not for the crm
    shell.
   </p><p>
    Configuration templates require minimum effort to be tailored to the
    particular user's needs. Whenever a template creates a configuration,
    warning messages give hints which can be edited later for further
    customization.
   </p><p>
    The following procedure shows how to create a simple yet functional
    Apache configuration:
   </p><div class="procedure" id="pro-ha-manual-config-template"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Create a new configuration from a configuration template:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Switch to the <code class="command">template</code> subcommand:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">template</code></pre></div></li><li class="step"><p>
        List the available configuration templates:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">list</code> templates
gfs2-base   filesystem  virtual-ip  apache   clvm     ocfs2    gfs2</pre></div></li><li class="step"><p>
        Decide which configuration template you need. As we need an Apache
        configuration, we select the <code class="literal">apache</code> template and
        name it <code class="literal">g-intranet</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">new</code> g-intranet apache
INFO: pulling in template apache
INFO: pulling in template virtual-ip</pre></div></li></ol></li><li class="step"><p>
      Define your parameters:
     </p><ol type="a" class="substeps"><li class="step"><p>
        List the configuration you have created:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">list</code>
g-intranet</pre></div></li><li class="step" id="st-config-cli-show"><p>
        Display the minimum required changes that need to be filled out by
        you:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">show</code>
ERROR: 23: required parameter ip not set
ERROR: 61: required parameter id not set
ERROR: 65: required parameter configfile not set</pre></div></li><li class="step" id="st-config-cli-edit"><p>
        Invoke your preferred text editor and fill out all lines that have
        been displayed as errors in <a class="xref" href="#st-config-cli-show" title="Step 3.b">Step 3.b</a>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">edit</code></pre></div></li></ol></li><li class="step"><p>
      Show the configuration and check whether it is valid (bold text
      depends on the configuration you have entered in
      <a class="xref" href="#st-config-cli-edit" title="Step 3.c">Step 3.c</a>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">show</code>
primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip=<span class="strong"><strong>"192.168.1.101"</strong></span>
primitive apache apache \
    params configfile=<span class="strong"><strong>"/etc/apache2/httpd.conf"</strong></span>
    monitor apache 120s:60s
group <span class="strong"><strong>g-intranet</strong></span> \
    apache virtual-ip</pre></div></li><li class="step"><p>
      Apply the configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">apply</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">cd ..</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
      Submit your changes to the CIB:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    It is possible to simplify the commands even more, if you know the
    details. The above procedure can be summarized with the following
    command on the shell:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure template \
   new g-intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</pre></div><p>
    If you are inside your internal <code class="command">crm</code> shell, use the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure template# </code><code class="command">new</code> intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</pre></div><p>
    However, the previous command only creates its configuration from the
    configuration template. It does not apply nor commit it to the CIB.
   </p></section><section class="sect2" id="sec-ha-manual-config-shadowconfig" data-id-title="Testing with Shadow Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.7 </span><span class="title-name">Testing with Shadow Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-shadowconfig">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A shadow configuration is used to test different configuration
    scenarios. If you have created several shadow configurations, you can
    test them one by one to see the effects of your changes.
   </p><p>
    The usual process looks like this:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>
      Create a new shadow configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">cib</code> new myNewConfig
INFO: myNewConfig shadow CIB created</pre></div><p>
      If you omit the name of the shadow CIB, a temporary name
      <code class="literal">@tmp@</code> is created.
     </p></li><li class="step"><p>
      To copy the current live configuration into your shadow
      configuration, use the following command, otherwise skip this step:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)# <code class="command">cib</code> reset myNewConfig</pre></div><p>
      The previous command makes it easier to modify any existing resources
      later.
     </p></li><li class="step"><p>
      Make your changes as usual. After you have created the shadow
      configuration, all changes go there. To save all your changes, use the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)# <code class="command">commit</code></pre></div></li><li class="step"><p>
      If you need the live cluster configuration again, switch back with the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm(myNewConfig)configure# <code class="command">cib</code> use live
<code class="prompt custom">crm(live)# </code></pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-debugging" data-id-title="Debugging Your Configuration Changes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.8 </span><span class="title-name">Debugging Your Configuration Changes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-debugging">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before loading your configuration changes back into the cluster, it is
    recommended to review your changes with <code class="command">ptest</code>. The
    <code class="command">ptest</code> command can show a diagram of actions that will
    be induced by committing the changes. You need the
    <span class="package">graphviz</span> package to display the diagrams. The
    following example is a transcript, adding a monitor operation:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code> fence-bob
primitive fence-bob stonith:apcsmart \
        params hostlist="bob"
<code class="prompt custom">crm(live)configure# </code><code class="command">monitor</code> fence-bob 120m:60s
<code class="prompt custom">crm(live)configure# </code><code class="command">show</code> changed
primitive fence-bob stonith:apcsmart \
        params hostlist="bob" \
        op monitor interval="120m" timeout="60s"
<code class="prompt custom">crm(live)configure# </code><span class="strong"><strong>ptest</strong></span>
<code class="prompt custom">crm(live)configure# </code>commit</pre></div></section><section class="sect2" id="sec-ha-manual-config-diagram" data-id-title="Cluster Diagram"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.9 </span><span class="title-name">Cluster Diagram</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-diagram">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To output a cluster diagram, use the command
    <code class="command">crm</code> <code class="command">configure graph</code>. It displays
    the current configuration on its current window, therefore requiring
    X11.
   </p><p>
    If you prefer Scalable Vector Graphics (SVG), use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure graph dot config.svg svg</pre></div></section><section class="sect2" id="sec-ha-manual-config-crm-corosync" data-id-title="Managing Corosync Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.10 </span><span class="title-name">Managing Corosync Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-crm-corosync">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Corosync is the underlying messaging layer for most HA clusters. The
   <code class="command">corosync</code> subcommand provides commands for editing and
   managing the Corosync configuration.
  </p><p>
   For example, to list the status of the cluster, use
   <code class="command">status</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> corosync status
Printing ring status.
Local node ID 175704363
RING ID 0
        id      = 10.121.9.43
        status  = ring 0 active with no faults
Quorum information
------------------
Date:             Thu May  8 16:41:56 2014
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          175704363
Ring ID:          4032
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   2
Highest expected: 2
Total votes:      2
Quorum:           2
Flags:            Quorate

Membership information
----------------------
    Nodeid      Votes Name
 175704363          1 alice.example.com (local)
 175704619          1 bob.example.com</pre></div><p>
   The <code class="command">diff</code> command is very helpful: It compares the
   Corosync configuration on all nodes (if not stated otherwise) and
   prints the difference between:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> corosync diff
--- bob
+++ alice
@@ -46,2 +46,2 @@
-       expected_votes: 2
-       two_node: 1
+       expected_votes: 1
+       two_node: 0</pre></div><p>
   For more details, see
   <a class="link" href="http://crmsh.nongnu.org/crm.8.html#cmdhelp_corosync" target="_blank">http://crmsh.nongnu.org/crm.8.html#cmdhelp_corosync</a>.
  </p></section><section class="sect2" id="sec-ha-config-crm-setpwd" data-id-title="Setting Passwords Independent of cib.xml"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">5.5.11 </span><span class="title-name">Setting Passwords Independent of <code class="filename">cib.xml</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-setpwd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_cli.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If your cluster configuration contains sensitive information, such
   as passwords, it should be stored in local files. That way, these
   parameters will never be logged or leaked in support reports.
  </p><p>
   Before using <code class="command">secret</code>, better run the
   <code class="command">show</code> command first to get an overview of all your
   resources:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show
primitive mydb mysql \
   params replication_user=admin ...</pre></div><p>
   To set a password for the above <code class="literal">mydb</code>
   resource, use the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource secret mydb set passwd linux
INFO: syncing /var/lib/heartbeat/lrm/secrets/mydb/passwd to [your node list]</pre></div><p>
   You can get the saved password back with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource secret mydb show passwd
linux</pre></div><p>
   Note that the parameters need to be synchronized between nodes; the
   <code class="command">crm resource secret</code> command will take care of that. We
   highly recommend to only use this command to manage secret parameters.
  </p></section></section><section class="sect1" id="sec-ha-config-basics-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">5.6 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_config_basics.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.8.2.1"><span class="term"><a class="link" href="http://crmsh.github.io/" target="_blank">http://crmsh.github.io/</a>
    </span></dt><dd><p>
      Home page of the crm shell (crmsh), the advanced command line
      interface for High Availability cluster management.
     </p></dd><dt id="id-1.4.4.3.8.2.2"><span class="term"><a class="link" href="http://crmsh.github.io/documentation" target="_blank">http://crmsh.github.io/documentation</a>
    </span></dt><dd><p>
      Holds several documents about the crm shell, including a
      <em class="citetitle">Getting Started</em> tutorial for basic cluster
      setup with crmsh and the comprehensive
      <em class="citetitle">Manual</em> for the crm shell. The latter is
      available at <a class="link" href="http://crmsh.github.io/man-2.0/" target="_blank">http://crmsh.github.io/man-2.0/</a>.
      Find the tutorial at
      <a class="link" href="http://crmsh.github.io/start-guide/" target="_blank">http://crmsh.github.io/start-guide/</a>.
     </p></dd><dt id="id-1.4.4.3.8.2.3"><span class="term"><a class="link" href="http://clusterlabs.org/" target="_blank">http://clusterlabs.org/</a>
    </span></dt><dd><p>
      Home page of Pacemaker, the cluster resource manager shipped with
      SUSE Linux Enterprise High Availability.
     </p></dd><dt id="id-1.4.4.3.8.2.4"><span class="term"><a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>
    </span></dt><dd><p>
      Holds several comprehensive manuals and some shorter documents
      explaining general concepts. For example:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <em class="citetitle">Pacemaker Explained</em>: Contains comprehensive and very detailed information
        for reference.
       </p></li><li class="listitem"><p>
        <em class="citetitle">Colocation Explained</em>
       </p></li><li class="listitem"><p>
        <em class="citetitle">Ordering Explained</em>
       </p></li></ul></div></dd></dl></div></section></section><section xml:lang="en" class="chapter" id="sec-ha-config-basics-resources" data-id-title="Configuring Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">6 </span><span class="title-name">Configuring Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    As a cluster administrator, you need to create cluster resources for
    every resource or application you run on servers in your cluster. Cluster
    resources can include Web sites, e-mail servers, databases, file systems,
    virtual machines, and any other server-based applications or services you
    want to make available to users at all times.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-resources-types" data-id-title="Types of Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.1 </span><span class="title-name">Types of Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-types">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following types of resources can be created:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.3.3.1"><span class="term">Primitives</span></dt><dd><p>
      A primitive resource, the most basic type of resource.
     </p></dd><dt id="id-1.4.4.4.3.3.2"><span class="term">Groups</span></dt><dd><p>
      Groups contain a set of resources that need to be located together,
      started sequentially and stopped in the reverse order.
     </p></dd><dt id="id-1.4.4.4.3.3.3"><span class="term">Clones</span></dt><dd><p>
      Clones are resources that can be active on multiple hosts. Any
      resource can be cloned, provided the respective resource agent
      supports it.
     </p><p>
      Promotable clones (also known as multi-state resources) are a
      special type of clone resources that can be promoted.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-config-basics-raclasses" data-id-title="Supported Resource Agent Classes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.2 </span><span class="title-name">Supported Resource Agent Classes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-raclasses">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For each cluster resource you add, you need to define the standard that
    the resource agent conforms to. Resource agents abstract the services
    they provide and present an accurate status to the cluster, which allows
    the cluster to be non-committal about the resources it manages. The
    cluster relies on the resource agent to react appropriately when given a
    start, stop or monitor command.
   </p><p>
    Typically, resource agents come in the form of shell scripts. SUSE Linux Enterprise High Availability
    supports the following classes of resource agents:
   </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-resources-ocf-ra"><span class="term">Open Cluster Framework (OCF) resource agents</span></dt><dd><p>
       OCF RA agents are best suited for use with High Availability, especially when
       you need promotable clone resources or special monitoring abilities. The
       agents are generally located in
       <code class="filename">/usr/lib/ocf/resource.d/<em class="replaceable">provider</em>/</code>.
       Their functionality is similar to that of LSB scripts. However, the
       configuration is always done with environmental variables that allow
       them to accept and process parameters easily.
       OCF specifications have strict definitions of which exit codes must
       be returned by actions. See <a class="xref" href="#sec-ha-errorcodes" title="10.3. OCF Return Codes and Failure Recovery">Section 10.3, “OCF Return Codes and Failure Recovery”</a>. The
       cluster follows these specifications exactly.
      </p><p>
       All OCF Resource Agents are required to have at least the actions
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">status</code>, <code class="literal">monitor</code> and
       <code class="literal">meta-data</code>. The <code class="literal">meta-data</code> action
       retrieves information about how to configure the agent. For example,
       to know more about the <code class="literal">IPaddr</code> agent by
       the provider <code class="literal">heartbeat</code>, use the following command:
      </p><div class="verbatim-wrap"><pre class="screen">OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/IPaddr meta-data</pre></div><p>
       The output is information in XML format, including several sections
       (general description, available parameters, available actions for the
       agent).
      </p><p>
       Alternatively, use the crmsh to view information on OCF resource
       agents. For details, see <a class="xref" href="#sec-ha-manual-config-ocf" title="5.5.3. Displaying Information about OCF Resource Agents">Section 5.5.3, “Displaying Information about OCF Resource Agents”</a>.
      </p></dd><dt id="id-1.4.4.4.4.4.2"><span class="term">Linux Standards Base (LSB) scripts</span></dt><dd><p>
       LSB resource agents are generally provided by the operating
       system/distribution and are found in
       <code class="filename">/etc/init.d</code>. To be used with the cluster, they
       must conform to the LSB init script specification. For example, they
       must have several actions implemented, which are, at minimum,
       <code class="literal">start</code>, <code class="literal">stop</code>,
       <code class="literal">restart</code>, <code class="literal">reload</code>,
       <code class="literal">force-reload</code> and <code class="literal">status</code>. For
       more information, see
       <a class="link" href="http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html" target="_blank">http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html</a>.
      </p><p>
       The configuration of those services is not standardized. If you
       intend to use an LSB script with High Availability, make sure that you
       understand how the relevant script is configured. You can often find
       information about this in the documentation of the relevant package
       in
       <code class="filename">/usr/share/doc/packages/<em class="replaceable">PACKAGENAME</em></code>.
      </p></dd><dt id="id-1.4.4.4.4.4.3"><span class="term">systemd</span></dt><dd><p>
       Pacemaker can manage systemd services if they
       are present. Instead of init scripts, systemd has unit files.
       Generally, the services (or unit files) are provided by the operating
       system. In case you want to convert existing init scripts, find more
       information at
       <a class="link" href="http://0pointer.de/blog/projects/systemd-for-admins-3.html" target="_blank">http://0pointer.de/blog/projects/systemd-for-admins-3.html</a>.
      </p></dd><dt id="id-1.4.4.4.4.4.4"><span class="term">Service</span></dt><dd><p>
       There are currently many types of system
       services that exist in parallel: <code class="literal">LSB</code> (belonging to
       System V init), <code class="literal">systemd</code> and (in some
       distributions) <code class="literal">upstart</code>. Therefore, Pacemaker
       supports a special alias that figures out which one
       applies to a given cluster node. This is particularly useful when the
       cluster contains a mix of systemd, upstart and LSB services.
       Pacemaker tries to find the named service in the following order:
       as an LSB (SYS-V) init script, a systemd unit file or an Upstart
       job.
      </p></dd><dt id="id-1.4.4.4.4.4.5"><span class="term">Nagios</span></dt><dd><p>
       Monitoring plug-ins (formerly called Nagios plug-ins) allow to
       monitor services on remote hosts. Pacemaker can do remote monitoring
       with the monitoring plug-ins if they are present. For detailed
       information, see
       <a class="xref" href="#sec-ha-config-basics-remote-nagios" title="9.1. Monitoring Services on Remote Hosts with Monitoring Plug-ins">Section 9.1, “Monitoring Services on Remote Hosts with Monitoring Plug-ins”</a>.
      </p></dd><dt id="id-1.4.4.4.4.4.6"><span class="term">STONITH (fencing) resource agents</span></dt><dd><p>
       This class is used exclusively for fencing related resources. For
       more information, see <a class="xref" href="#cha-ha-fencing" title="Chapter 12. Fencing and STONITH">Chapter 12, <em>Fencing and STONITH</em></a>.
      </p></dd></dl></div><p>
    The agents supplied with SUSE Linux Enterprise High Availability are written to OCF
    specifications.
   </p></section><section class="sect1" id="sec-ha-config-basics-timeouts" data-id-title="Timeout Values"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.3 </span><span class="title-name">Timeout Values</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-timeouts">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Timeouts values for resources can be influenced by the following
   parameters:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="varname">op_defaults</code> (global timeout for operations),
    </p></li><li class="listitem"><p>
     a specific timeout value defined in a resource template,
    </p></li><li class="listitem"><p>
     a specific timeout value defined for a resource.
    </p></li></ul></div><div id="id-1.4.4.4.5.4" data-id-title="Priority of Values" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Priority of Values</div><p>
    If a <span class="emphasis"><em>specific</em></span> value is defined for a resource, it
    takes precedence over the global default. A specific value for a
    resource also takes precedence over a value that is defined in a
    resource template.
   </p></div><p>
   Getting timeout values right is very important. Setting them too low
   will result in a lot of (unnecessary) fencing operations for the
   following reasons:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     If a resource runs into a timeout, it fails and the cluster will try
     to stop it.
    </p></li><li class="listitem"><p>
     If stopping the resource also fails (for example, because the timeout
     for stopping is set too low), the cluster will fence the node. It
     considers the node where this happens to be out of control.
    </p></li></ol></div><p>
   You can adjust the global default for operations and set any specific
   timeout values with both crmsh and Hawk2. The best practice for
   determining and setting timeout values is as follows:
  </p><div class="procedure" id="id-1.4.4.4.5.8" data-id-title="Determining Timeout Values"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.1: </span><span class="title-name">Determining Timeout Values </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.5.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check how long it takes your resources to start and stop (under load).
    </p></li><li class="step"><p>
     If needed, add the <code class="varname">op_defaults</code> parameter and set
     the (default) timeout value accordingly:
    </p><ol type="a" class="substeps"><li class="step"><p>
       For example, set <code class="literal">op_defaults</code> to
       <code class="literal">60</code> seconds:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">op_defaults timeout=60</code></pre></div></li><li class="step"><p>
       For resources that need longer periods of time, define individual
       timeout values.
      </p></li></ol></li><li class="step"><p>
     When configuring operations for a resource, add separate
     <code class="literal">start</code> and <code class="literal">stop</code> operations. When
     configuring operations with Hawk2, it will provide useful timeout
     proposals for those operations.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-config-basics-resources-management" data-id-title="Creating Primitive Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.4 </span><span class="title-name">Creating Primitive Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-management">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Before you can use a resource in the cluster, it must be set up. For
    example, to use an Apache server as a cluster resource, set
    up the Apache server first and complete the Apache configuration before
    starting the respective resource in your cluster.
   </p><p>
    If a resource has specific environment requirements, make sure they are
    present and identical on all cluster nodes. This kind of configuration
    is not managed by SUSE Linux Enterprise High Availability. You must do this yourself.
   </p><p>
    You can create primitive resources using either Hawk2 or crmsh.
   </p><div id="id-1.4.4.4.6.5" data-id-title="Do Not Touch Services Managed by the Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Do Not Touch Services Managed by the Cluster</div><p>
     When managing a resource with SUSE Linux Enterprise High Availability, the same resource must not
     be started or stopped otherwise (outside of the cluster, for example
     manually or on boot or reboot). The High Availability software is responsible
     for all service start or stop actions.
    </p><p>
     If you need to execute testing or maintenance tasks after the services
     are already running under cluster control, make sure to put the
     resources, nodes, or the whole cluster into maintenance mode before you
     touch any of them manually. For details, see
     <a class="xref" href="#sec-ha-maint-overview" title="27.2. Different Options for Maintenance Tasks">Section 27.2, “Different Options for Maintenance Tasks”</a>.
    </p></div><div id="id-1.4.4.4.6.6" data-id-title="Resource IDs and Node Names" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Resource IDs and Node Names</div><p>Cluster resources and cluster nodes should be named differently.
     Otherwise Hawk2 will fail.</p></div><section class="sect2" id="sec-conf-hawk2-rsc-primitive" data-id-title="Creating Primitive Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.4.1 </span><span class="title-name">Creating Primitive Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-primitive">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To create the most basic type of resource, proceed as follows:
    </p><div class="procedure" id="pro-conf-hawk2-primitive-add" data-id-title="Adding a Primitive Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.2: </span><span class="title-name">Adding a Primitive Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-primitive-add">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add
       Resource</span> › <span class="guimenu">Primitive</span>.
      </p></li><li class="step"><p>
       Enter a unique <span class="guimenu">Resource ID</span>.
      </p></li><li class="step"><p>
       In case a resource template exists on which you want to base the resource
       configuration, select the respective <span class="guimenu">Template</span>. For
       details about configuring templates, see
       <a class="xref" href="#pro-conf-hawk2-template-add" title="Adding a Resource Template">Procedure 6.6, “Adding a Resource Template”</a>.
      </p></li><li class="step" id="step-ha-config-hawk2-primitive-start"><p>
       Select the resource agent <span class="guimenu">Class</span> you want to use:
       <code class="literal">lsb</code>, <code class="literal">ocf</code>,
       <code class="literal">service</code>, <code class="literal">stonith</code>, or
       <code class="literal">systemd</code>. For more information, see
       <a class="xref" href="#sec-ha-config-basics-raclasses" title="6.2. Supported Resource Agent Classes">Section 6.2, “Supported Resource Agent Classes”</a>.
      </p></li><li class="step"><p>
       If you selected <code class="literal">ocf</code> as class, specify the
       <span class="guimenu">Provider</span> of your OCF resource agent. The OCF
       specification allows multiple vendors to supply the same resource agent.
      </p></li><li class="step"><p>
       From the <span class="guimenu">Type</span> list, select the resource agent you want
       to use (for example, <span class="guimenu">IPaddr</span> or
       <span class="guimenu">Filesystem</span>). A short description for this resource
       agent is displayed.
      </p><p>
       With that, you have specified the resource basics.
      </p><div id="id-1.4.4.4.6.7.3.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        The selection you get in the <span class="guimenu">Type</span> list depends on the
        <span class="guimenu">Class</span> (and for OCF resources also on the
        <span class="guimenu">Provider</span>) you have chosen.
       </p></div><div class="figure" id="id-1.4.4.4.6.7.3.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-basic.png"><img src="images/hawk2-primitive-basic.png" width="100%" alt="Hawk2—Primitive Resource" title="Hawk2—Primitive Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.1: </span><span class="title-name">Hawk2—Primitive Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.6.7.3.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
       After you have specified the resource basics, Hawk2 shows
       the following categories. Either keep these categories as suggested by Hawk2,
       or edit them as required.
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.6.7.3.9.2.1"><span class="term">Parameters (Instance Attributes)</span></dt><dd><p>
          Determines which instance of a service the resource controls. For more
          information, refer to <a class="xref" href="#sec-ha-config-basics-inst-attr" title="6.13. Instance Attributes (Parameters)">Section 6.13, “Instance Attributes (Parameters)”</a>.
         </p><p>
          When creating a resource, Hawk2 automatically shows any required
          parameters. Edit them to get a valid resource configuration.
         </p></dd><dt id="id-1.4.4.4.6.7.3.9.2.2"><span class="term">Operations</span></dt><dd><p>
          Needed for resource monitoring. For more information, refer to
          <a class="xref" href="#sec-ha-config-basics-operations" title="6.14. Resource Operations">Section 6.14, “Resource Operations”</a>.
         </p><p>
          When creating a resource, Hawk2 displays the most important resource
          operations (<code class="literal">monitor</code>, <code class="literal">start</code>, and
          <code class="literal">stop</code>).
         </p></dd><dt id="id-1.4.4.4.6.7.3.9.2.3"><span class="term">Meta Attributes</span></dt><dd><p>
          Tells the CRM how to treat a specific resource. For more information,
          refer to <a class="xref" href="#sec-ha-config-basics-meta-attr" title="6.12. Resource Options (Meta Attributes)">Section 6.12, “Resource Options (Meta Attributes)”</a>.
         </p><p>
          When creating a resource, Hawk2 automatically lists the important meta
          attributes for that resource (for example, the
          <code class="literal">target-role</code> attribute that defines the initial state of
          a resource. By default, it is set to <code class="literal">Stopped</code>, so the
          resource will not start immediately).
         </p></dd><dt id="id-1.4.4.4.6.7.3.9.2.4"><span class="term">Utilization</span></dt><dd><p>
          Tells the CRM what capacity a certain resource requires from a node. For
          more information, refer to
          <a class="xref" href="#sec-config-hawk2-utilization" title="7.10.1. Placing Resources Based on Their Load Impact with Hawk2">Section 7.10.1, “Placing Resources Based on Their Load Impact with Hawk2”</a>.
         </p></dd></dl></div></li><li class="step"><p>
       Click <span class="guimenu">Create</span> to finish the configuration. A message at the
       top of the screen shows if the action has been successful.
      </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-create" data-id-title="Creating Primitive Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.4.2 </span><span class="title-name">Creating Primitive Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-create">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-ha-manual-config-create"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code> tool:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
       Configure a primitive IP address:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive myIP IPaddr \
      params ip=127.0.0.99 op monitor interval=60s</code></pre></div><p>
       The previous command configures a <span class="quote">“<span class="quote">primitive</span>”</span> with the
       name <code class="literal">myIP</code>. You need to choose a class (here
       <code class="literal">ocf</code>), provider (<code class="literal">heartbeat</code>), and
       type (<code class="literal">IPaddr</code>). Furthermore, this primitive expects
       other parameters like the IP address. Change the address to your
       setup.
      </p></li><li class="step"><p>
       Display and review the changes you have made:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
       Commit your changes to take effect:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources-advanced-groups" data-id-title="Creating Resource Groups"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.5 </span><span class="title-name">Creating Resource Groups</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-groups">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Some cluster resources depend on other components or resources. They
    require that each component or resource starts in a specific order and
    runs together on the same server with resources it depends on. To
    simplify this configuration, you can use cluster resource groups.
   </p><p>
    You can create resource groups using either Hawk2 or crmsh.
   </p><div class="complex-example"><div class="example" id="ex-ha-config-resource-group" data-id-title="Resource Group for a Web Server"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 6.1: </span><span class="title-name">Resource Group for a Web Server </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-resource-group">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     An example of a resource group would be a Web server that requires an
     IP address and a file system. In this case, each component is a
     separate resource that is combined into a cluster resource group. The
     resource group would run on one or more servers. In case of a software
     or hardware malfunction, the group would fail over to another server
     in the cluster, similar to an individual cluster resource.
    </p></div></div></div><div class="figure" id="id-1.4.4.4.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/webserver_groupresource_a.png"><img src="images/webserver_groupresource_a.png" width="63%" alt="Group Resource" title="Group Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.2: </span><span class="title-name">Group Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.7.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div><p>
    Groups have the following properties:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.7.7.1"><span class="term">Starting and Stopping</span></dt><dd><p>
       Resources are started in the order they appear in and stopped in the
       reverse order.
      </p></dd><dt id="id-1.4.4.4.7.7.2"><span class="term">Dependency</span></dt><dd><p>
       If a resource in the group cannot run anywhere, then none of the
       resources located after that resource in the group is allowed to
       run.
      </p></dd><dt id="id-1.4.4.4.7.7.3"><span class="term">Contents</span></dt><dd><p>
       Groups may only contain a collection of primitive cluster resources.
       Groups must contain at least one resource, otherwise the
       configuration is not valid. To refer to the child of a group
       resource, use the child's ID instead of the group's ID.
      </p></dd><dt id="id-1.4.4.4.7.7.4"><span class="term">Constraints</span></dt><dd><p>
       Although it is possible to reference the group's children in
       constraints, it is usually preferable to use the group's name
       instead.
      </p></dd><dt id="id-1.4.4.4.7.7.5"><span class="term">Stickiness</span></dt><dd><p>
       Stickiness is additive in groups. Every <span class="emphasis"><em>active</em></span>
       member of the group will contribute its stickiness value to the
       group's total. So if the default
       <code class="literal">resource-stickiness</code> is <code class="literal">100</code> and
       a group has seven members (ﬁve of which are active), the group as
       a whole will prefer its current location with a score of
       <code class="literal">500</code>.
      </p></dd><dt id="id-1.4.4.4.7.7.6"><span class="term">Resource Monitoring</span></dt><dd><p>
       To enable resource monitoring for a group, you must configure
       monitoring separately for each resource in the group that you want
       monitored.
      </p></dd></dl></div><section class="sect2" id="sec-conf-hawk2-rsc-group" data-id-title="Creating Resource Groups with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.1 </span><span class="title-name">Creating Resource Groups with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-group">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.4.7.8.2" data-id-title="Empty Groups" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Empty Groups</div><p>
      Groups must contain at least one resource, otherwise the configuration is
      not valid. While creating a group, Hawk2 allows you to create more
      primitives and add them to the group.
     </p></div><div class="procedure" id="pro-conf-hawk2-group" data-id-title="Adding a Resource Group"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.3: </span><span class="title-name">Adding a Resource Group </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-group">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> › <span class="guimenu">Group</span>.
      </p></li><li class="step"><p>
       Enter a unique <span class="guimenu">Group ID</span>.
      </p></li><li class="step"><p>
       To define the group members, select one or multiple entries in the list of
       <span class="guimenu">Children</span>. Re-sort group members by dragging and
       dropping them into the order you want by using the <span class="quote">“<span class="quote">handle</span>”</span>
       icon on the right.
      </p></li><li class="step"><p>
       If needed, modify or add <span class="guimenu">Meta Attributes</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Create</span> to finish the configuration. A message at
       the top of the screen shows if the action has been successful.
      </p></li></ol></div></div><div class="figure" id="id-1.4.4.4.7.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-rsc-group.png"><img src="images/hawk2-rsc-group.png" width="100%" alt="Hawk2—Resource Group" title="Hawk2—Resource Group"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.3: </span><span class="title-name">Hawk2—Resource Group </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.7.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-group" data-id-title="Configuring Resource Groups with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.5.2 </span><span class="title-name">Configuring Resource Groups with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-group">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The following example creates two primitives (an IP address and an
     e-mail resource):
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Run the <code class="command">crm</code> command as system administrator. The
       prompt changes to <code class="literal">crm(live)</code>.
      </p></li><li class="step"><p>
       Configure the primitives:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive Public-IP ocf:heartbeat:IPaddr2 \
    params ip=1.2.3.4 \
    op monitor interval=10s</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive Email systemd:postfix \
    op monitor interval=10s</code></pre></div></li><li class="step"><p>
       Group the primitives with their relevant identifiers in the correct
       order:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group g-mailsvc Public-IP Email</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources-advanced-clones" data-id-title="Creating Clone Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.6 </span><span class="title-name">Creating Clone Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-clones">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You may want certain resources to run simultaneously on multiple nodes
    in your cluster. To do this you must configure a resource as a clone.
    Examples of resources that might be configured as clones include
    cluster file systems like OCFS2. You can clone any
    resource provided. This is supported by the resource's Resource
    Agent. Clone resources may even be configured differently depending on
    which nodes they are hosted.
   </p><p>
    There are three types of resource clones:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.8.4.1"><span class="term">Anonymous Clones</span></dt><dd><p>
       These are the simplest type of clones. They behave identically
       anywhere they are running. Because of this, there can only be one
       instance of an anonymous clone active per machine.
      </p></dd><dt id="id-1.4.4.4.8.4.2"><span class="term">Globally Unique Clones</span></dt><dd><p>
       These resources are distinct entities. An instance of the clone
       running on one node is not equivalent to another instance on another
       node; nor would any two instances on the same node be equivalent.
      </p></dd><dt id="id-1.4.4.4.8.4.3"><span class="term">Promotable Clones (Multi-state Resources)</span></dt><dd><p>
       Active instances of these resources are divided into two states,
       active and passive. These are also sometimes called primary and
       secondary. Promotable clones can be either
       anonymous or globally unique. See also
       <a class="xref" href="#sec-ha-config-basics-resources-advanced-masters" title="6.7. Creating Promotable Clones (Multi-state Resources)">Section 6.7, “Creating Promotable Clones (Multi-state Resources)”</a>.
      </p></dd></dl></div><p>
    Clones must contain exactly one group or one regular resource.
   </p><p>
    When configuring resource monitoring or constraints, clones have
    different requirements than simple resources. For details, see
     <em class="citetitle">Pacemaker Explained</em>, available from <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>.
   </p><p>
    You can create clone resources using either Hawk2 or crmsh.
   </p><section class="sect2" id="sec-conf-hawk2-rsc-clone" data-id-title="Creating Clone Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.1 </span><span class="title-name">Creating Clone Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-clone">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.4.8.8.2" data-id-title="Child Resources for Clones" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Child Resources for Clones</div><p>
      Clones can either contain a primitive or a group as child resources. In
      Hawk2, child resources cannot be created or modified while creating a
      clone. Before adding a clone, create child resources and configure them as desired.
     </p></div><div class="procedure" id="pro-conf-hawk2-clone" data-id-title="Adding a Clone Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.4: </span><span class="title-name">Adding a Clone Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-clone">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> › <span class="guimenu">Clone</span>.
      </p></li><li class="step"><p>
       Enter a unique <span class="guimenu">Clone ID</span>.
      </p></li><li class="step"><p>
       From the <span class="guimenu">Child Resource</span> list, select the primitive or
       group to use as a sub-resource for the clone.
      </p></li><li class="step"><p>
       If needed, modify or add <span class="guimenu">Meta Attributes</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Create</span> to finish the configuration. A message at
       the top of the screen shows if the action has been successful.
      </p></li></ol></div></div><div class="figure" id="id-1.4.4.4.8.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-clone.png"><img src="images/hawk2-clone.png" width="100%" alt="Hawk2—Clone Resource" title="Hawk2—Clone Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.4: </span><span class="title-name">Hawk2—Clone Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.8.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-clone" data-id-title="Creating Clone Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.6.2 </span><span class="title-name">Creating Clone Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clone">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      To create an anonymous clone resource, first create a primitive
      resource and then refer to it with the <code class="command">clone</code>
      command. Do the following:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
        interactive shell:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
        Configure the primitive, for example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive Apache apache</code></pre></div></li><li class="step"><p>
        Clone the primitive:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">clone cl-apache Apache</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources-advanced-masters" data-id-title="Creating Promotable Clones (Multi-state Resources)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.7 </span><span class="title-name">Creating Promotable Clones (Multi-state Resources)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-advanced-masters">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Promotable clones (formerly known as multi-state resources) are a
    specialization of clones. They allow the
    instances to be in one of two operating modes (primary or
    secondary). Promotable clones must contain
    exactly one group or one regular resource.
   </p><p>
    When configuring resource monitoring or constraints, promotable
    clones have different requirements than simple resources. For
    details, see  <em class="citetitle">Pacemaker Explained</em>, available from
    <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>.
   </p><p>
    You can create promotable clones using either Hawk2 or crmsh.
   </p><section class="sect2" id="sec-conf-hawk2-rsc-ms" data-id-title="Creating Promotable Clones with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.1 </span><span class="title-name">Creating Promotable Clones with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-ms">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.4.9.5.2" data-id-title="Child Resources for Multi-state Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Child Resources for Multi-state Resources</div><p>
      Multi-state resources can either contain a primitive or a group as child
      resources. In Hawk2, child resources cannot be created or modified while
      creating a multi-state resource. Before adding a multi-state resource,
      create child resources and configure them as desired. For details, refer to
      <a class="xref" href="#sec-conf-hawk2-rsc-primitive" title="6.4.1. Creating Primitive Resources with Hawk2">Section 6.4.1, “Creating Primitive Resources with Hawk2”</a> or
      <a class="xref" href="#sec-conf-hawk2-rsc-group" title="6.5.1. Creating Resource Groups with Hawk2">Section 6.5.1, “Creating Resource Groups with Hawk2”</a>.
     </p></div><div class="procedure" id="pro-conf-hawk2-ms" data-id-title="Adding a Multi-state Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.5: </span><span class="title-name">Adding a Multi-state Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-ms">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> › <span class="guimenu">Multi-state</span>.
      </p></li><li class="step"><p>
       Enter a unique <span class="guimenu">Multi-state ID</span>.
      </p></li><li class="step"><p>
       From the <span class="guimenu">Child Resource</span> list, select the primitive or
       group to use as a sub-resource for the multi-state resource.
      </p></li><li class="step"><p>
       If needed, modify or add <span class="guimenu">Meta Attributes</span>.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Create</span> to finish the configuration. A message at
       the top of the screen shows if the action has been successful.
      </p></li></ol></div></div><div class="figure" id="id-1.4.4.4.9.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-multi-state.png"><img src="images/hawk2-multi-state.png" width="100%" alt="Hawk2—Multi-state Resource" title="Hawk2—Multi-state Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.5: </span><span class="title-name">Hawk2—Multi-state Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.9.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-clone-stateful" data-id-title="Creating Promotable Clones with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.7.2 </span><span class="title-name">Creating Promotable Clones with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-clone-stateful">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To create a promotable clone resource, first create a primitive resource
     and then the promotable clone resource. The promotable clone resource must
     support at least promote and demote operations.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
       interactive shell:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
       Configure the primitive. Change the intervals if needed:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive my-rsc ocf:myCorp:myAppl \
    op monitor interval=60 \
    op monitor interval=61 role=Master</code></pre></div></li><li class="step"><p>
       Create the promotable clone resource:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms ms-rsc my-rsc</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-resources-templates" data-id-title="Creating Resource Templates"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.8 </span><span class="title-name">Creating Resource Templates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-resources-templates">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If you want to create lots of resources with similar configurations,
    defining a resource template is the easiest way. After having been
    defined, it can be referenced in primitives—or in certain types
    of constraints, as described in
    <a class="xref" href="#sec-ha-config-basics-constraints-templates" title="7.3. Resource Templates and Constraints">Section 7.3, “Resource Templates and Constraints”</a>.
   </p><p>
    If a template is referenced in a primitive, the primitive will inherit
    all operations, instance attributes (parameters), meta attributes, and
    utilization attributes defined in the template. Additionally, you can
    define specific operations or attributes for your primitive. If any of
    these are defined in both the template and the primitive, the values
    defined in the primitive will take precedence over the ones defined in
    the template.
   </p><p>
    You can create resource templates using either Hawk2 or crmsh.
   </p><section class="sect2" id="sec-conf-hawk2-rsc-template" data-id-title="Creating Resource Templates with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.8.1 </span><span class="title-name">Creating Resource Templates with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Resource templates are configured like primitive resources.
     </p><div class="procedure" id="pro-conf-hawk2-template-add" data-id-title="Adding a Resource Template"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.6: </span><span class="title-name">Adding a Resource Template </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-template-add">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> › <span class="guimenu">Template</span>.
      </p></li><li class="step"><p>
       Enter a unique <span class="guimenu">Resource ID</span>.
      </p></li><li class="step"><p>
       Follow the instructions in <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.2, “Adding a Primitive Resource”</a>,
       starting from
       <a class="xref" href="#step-ha-config-hawk2-primitive-start" title="Step 5">Step 5</a>.
      </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-rsc-template" data-id-title="Creating Resource Templates with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.8.2 </span><span class="title-name">Creating Resource Templates with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-rsc-template">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Use the <code class="command">rsc_template</code> command to get familiar with the syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure rsc_template</code>
 usage: rsc_template &lt;name&gt; [&lt;class&gt;:[&lt;provider&gt;:]]&lt;type&gt;
         [params &lt;param&gt;=&lt;value&gt; [&lt;param&gt;=&lt;value&gt;...]]
         [meta &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
         [utilization &lt;attribute&gt;=&lt;value&gt; [&lt;attribute&gt;=&lt;value&gt;...]]
         [operations id_spec
             [op op_type [&lt;attribute&gt;=&lt;value&gt;...] ...]]</pre></div><p>
     For example, the following command creates a new resource template with
     the name <code class="literal">BigVM</code> derived from the
     <code class="literal">ocf:heartbeat:Xen</code> resource and some default values
     and operations:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">rsc_template BigVM ocf:heartbeat:Xen \
    params allow_mem_management="true" \
    op monitor timeout=60s interval=15s \
    op stop timeout=10m \
    op start timeout=10m</code></pre></div><p>
     Once you defined the new resource template, you can use it in primitives
     or reference it in order, colocation, or rsc_ticket constraints. To
     reference the resource template, use the <code class="literal">@</code> sign:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive MyVM1 @BigVM \
    params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1"</code></pre></div><p>
     The new primitive MyVM1 is going to inherit everything from the BigVM
     resource templates. For example, the equivalent of the above two would
     be:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive MyVM1 Xen \
    params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1" \
    params allow_mem_management="true" \
    op monitor timeout=60s interval=15s \
    op stop timeout=10m \
    op start timeout=10m</code></pre></div><p>
     If you want to overwrite some options or operations, add them to your
     (primitive) definition. For example, the following new primitive MyVM2
     doubles the timeout for monitor operations but leaves others untouched:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive MyVM2 @BigVM \
    params xmfile="/etc/xen/shared-vm/MyVM2" name="MyVM2" \
    op monitor timeout=120s interval=30s</code></pre></div><p>
     A resource template may be referenced in constraints to stand for all
     primitives which are derived from that template. This helps to produce a
     more concise and clear cluster configuration. Resource template
     references are allowed in all constraints except location constraints.
     Colocation constraints may not contain more than one template reference.
    </p></section></section><section class="sect1" id="sec-ha-conf-stonith-rsc" data-id-title="Creating STONITH Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.9 </span><span class="title-name">Creating STONITH Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-conf-stonith-rsc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.4.4.11.2" data-id-title="No Support Without STONITH" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support Without STONITH</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You must have a node fencing
        mechanism for your cluster.</p></li><li class="listitem"><p>The global cluster options
          <code class="systemitem">stonith-enabled</code> and
          <code class="systemitem">startup-fencing</code> must be set to
          <code class="literal">true</code>.
          When you change them, you lose support.</p></li></ul></div></div><p>
    By default, the global cluster option <code class="literal">stonith-enabled</code> is
    set to <code class="literal">true</code>. If no STONITH resources have been defined,
    the cluster will refuse to start any resources. Configure one or more
    STONITH resources to complete the STONITH setup. While STONITH resources
    are configured similarly to other resources, their behavior is different in
    some respects. For details refer to <a class="xref" href="#sec-ha-fencing-config" title="12.3. STONITH Resources and Configuration">Section 12.3, “STONITH Resources and Configuration”</a>.
   </p><section class="sect2" id="sec-conf-hawk2-rsc-stonith" data-id-title="Creating STONITH Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.9.1 </span><span class="title-name">Creating STONITH Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To add a STONITH resource for SBD, for libvirt (KVM/Xen) or for
    vCenter/ESX Server, the easiest way is to use the Hawk2 wizard.
   </p><div class="procedure" id="pro-conf-hawk2-stonith" data-id-title="Adding a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.7: </span><span class="title-name">Adding a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add
      Resource</span> › <span class="guimenu">Primitive</span>.
     </p></li><li class="step"><p>
      Enter a unique <span class="guimenu">Resource ID</span>.
     </p></li><li class="step"><p>
      From the <span class="guimenu">Class</span> list, select the resource agent class
      <span class="guimenu">stonith</span>.
     </p></li><li class="step"><p>
      From the <span class="guimenu">Type</span> list, select the STONITH plug-in to
      control your STONITH device. A short description for this plug-in is
      displayed.
     </p></li><li class="step"><p>
      Hawk2 automatically shows the required <span class="guimenu">Parameters</span> for
      the resource. Enter values for each parameter.
     </p></li><li class="step"><p>
      Hawk2 displays the most important resource <span class="guimenu">Operations</span>
      and proposes default values. If you do not modify any settings here,
      Hawk2 adds the proposed operations and their default values when you
      confirm.
     </p></li><li class="step"><p>
      If there is no reason to change them, keep the default <span class="guimenu">Meta
      Attributes</span> settings.
     </p><div class="figure" id="id-1.4.4.4.11.4.3.9.2"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-stonith.png"><img src="images/hawk2-primitive-stonith.png" width="100%" alt="Hawk2—STONITH Resource" title="Hawk2—STONITH Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.6: </span><span class="title-name">Hawk2—STONITH Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.11.4.3.9.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
      Confirm your changes to create the STONITH resource.
     </p><p>
      A message at the top of the screen shows if the action has been
      successful.
     </p></li></ol></div></div><p>
    To complete your fencing configuration, add constraints. For more details,
    refer to <a class="xref" href="#cha-ha-fencing" title="Chapter 12. Fencing and STONITH">Chapter 12, <em>Fencing and STONITH</em></a>.
   </p></section><section class="sect2" id="sec-ha-manual-create-stonith" data-id-title="Creating STONITH Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.9.2 </span><span class="title-name">Creating STONITH Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-create-stonith">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code></pre></div></li><li class="step"><p>
      Get a list of all STONITH types with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">ra list stonith</code>
apcmaster                  apcmastersnmp              apcsmart
baytech                    bladehpi                   cyclades
drac3                      external/drac5             external/dracmc-telnet
external/hetzner           external/hmchttp           external/ibmrsa
external/ibmrsa-telnet     external/ipmi              external/ippower9258
external/kdumpcheck        external/libvirt           external/nut
external/rackpdu           external/riloe             external/sbd
external/vcenter           external/vmware            external/xen0
external/xen0-ha           fence_legacy               ibmhmc
ipmilan                    meatware                   nw_rpc100s
rcd_serial                 rps10                      suicide
wti_mpc                    wti_nps</pre></div></li><li class="step" id="st-ha-manual-create-stonith-type"><p>
      Choose a STONITH type from the above list and view the list of
      possible options. Use the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">ra info stonith:external/ipmi</code>
IPMI STONITH external device (stonith:external/ipmi)

ipmitool based power management. Apparently, the power off
method of ipmitool is intercepted by ACPI which then makes
a regular shutdown. If case of a split brain on a two-node
it may happen that no node survives. For two-node clusters
use only the reset method.

Parameters (* denotes required, [] the default):

hostname (string): Hostname
    The name of the host to be managed by this STONITH device.
...</pre></div></li><li class="step"><p>
      Create the STONITH resource with the <code class="literal">stonith</code>
      class, the type you have chosen in
      <a class="xref" href="#st-ha-manual-create-stonith-type" title="Step 3">Step 3</a>,
      and the respective parameters if needed, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive my-stonith stonith:external/ipmi \
    params hostname="alice" \
    ipaddr="192.168.1.221" \
    userid="admin" passwd="secret" \
    op monitor interval=60m timeout=120s</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-monitoring" data-id-title="Configuring Resource Monitoring"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.10 </span><span class="title-name">Configuring Resource Monitoring</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-monitoring">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you want to ensure that a resource is running, you must configure
   resource monitoring for it. You can configure resource monitoring using
   either Hawk2 or crmsh.
  </p><p>
   If the resource monitor detects a failure, the following takes place:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Log file messages are generated, according to the configuration
     specified in the <code class="literal">logging</code> section of
     <code class="filename">/etc/corosync/corosync.conf</code>.
    </p></li><li class="listitem"><p>
     The failure is reflected in the cluster management tools (Hawk2,
     <code class="command">crm status</code>), and in the CIB status section.
    </p></li><li class="listitem"><p>
     The cluster initiates noticeable recovery actions which may include
     stopping the resource to repair the failed state and restarting the
     resource locally or on another node. The resource also may not be
     restarted, depending on the configuration and state of the cluster.
    </p></li></ul></div><p>
   If you do not configure resource monitoring, resource failures after a
   successful start will not be communicated, and the cluster will always
   show the resource as healthy.
  </p><p>
    Usually, resources are only
    monitored by the cluster while they are running. However, to detect
    concurrency violations, also configure monitoring for resources which are
    stopped. For resource monitoring, specify a timeout and/or start delay
    value, and an interval. The interval tells the CRM how often it should check
    the resource status. You can also set particular parameters such as
    <code class="literal">timeout</code> for <code class="literal">start</code> or
    <code class="literal">stop</code> operations.
   </p><p>
    For more information about monitor operation parameters, see
    <a class="xref" href="#sec-ha-config-basics-operations" title="6.14. Resource Operations">Section 6.14, “Resource Operations”</a>.
   </p><section class="sect2" id="sec-conf-hawk2-rsc-monitor" data-id-title="Configuring Resource Monitoring with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.10.1 </span><span class="title-name">Configuring Resource Monitoring with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-monitor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-operations" data-id-title="Adding and Modifying an Operation"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 6.8: </span><span class="title-name">Adding and Modifying an Operation </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-operations">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      Add a resource as described in
      <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.2, “Adding a Primitive Resource”</a> or select an existing
      primitive to edit.
     </p><p>
      Hawk2 automatically shows the most important
      <span class="guimenu">Operations</span> (<code class="literal">start</code>,
      <code class="literal">stop</code>, <code class="literal">monitor</code>) and proposes default
      values.
     </p><p>
      To see the attributes belonging to each proposed value, hover the mouse
      pointer over the respective value.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-monitor-op.png"><img src="images/hawk2-monitor-op.png" width="60%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      To change the suggested <code class="literal">timeout</code> values for the
      <code class="literal">start</code> or <code class="literal">stop</code> operation:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Click the pen icon next to the operation.
       </p></li><li class="step"><p>
        In the dialog that opens, enter a different value for the
        <code class="literal">timeout</code> parameter, for example <code class="literal">10</code>,
        and confirm your change.
       </p></li></ol></li><li class="step"><p>
      To change the suggested <span class="guimenu">interval</span> value for the
      <code class="literal">monitor</code> operation:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Click the pen icon next to the operation.
       </p></li><li class="step"><p>
        In the dialog that opens, enter a different value for the monitoring
        <code class="literal">interval</code>.
       </p></li><li class="step"><p>
        To configure resource monitoring in the case that the resource is
        stopped:
       </p><ol type="i" class="substeps"><li class="step"><p>
          Select the <code class="literal">role</code> entry from the empty drop-down box
          below.
         </p></li><li class="step"><p>
          From the <code class="literal">role</code> drop-down box, select
          <code class="literal">Stopped</code>.
         </p></li><li class="step"><p>
          Click <span class="guimenu">Apply</span> to confirm your changes and to close
          the dialog for the operation.
         </p></li></ol></li></ol></li><li class="step"><p>
      Confirm your changes in the resource configuration screen. A message at
      the top of the screen shows if the action has been successful.
     </p></li></ol></div></div><p>
    To view resource failures, switch to the <span class="guimenu">Status</span> screen in
    Hawk2 and select the resource you are interested in. In the
    <span class="guimenu">Operations</span> column click the arrow down icon and select
    <span class="guimenu">Recent Events</span>. The dialog that opens lists recent actions
    performed for the resource. Failures are displayed in red. To view the
    resource details, click the magnifier icon in the
    <span class="guimenu">Operations</span> column.
   </p><div class="figure" id="id-1.4.4.4.12.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-rsc-details.png"><img src="images/hawk2-rsc-details.png" width="60%" alt="Hawk2—Resource Details" title="Hawk2—Resource Details"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 6.7: </span><span class="title-name">Hawk2—Resource Details </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.12.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-monitor" data-id-title="Configuring Resource Monitoring with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">6.10.2 </span><span class="title-name">Configuring Resource Monitoring with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-monitor">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To monitor a resource, there are two possibilities: either define a
    monitor operation with the <code class="command">op</code> keyword or use the
    <code class="command">monitor</code> command. The following example configures an
    Apache resource and monitors it every 60 seconds with the
    <code class="literal">op</code> keyword:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive apache apache \
  params ... \
  <span class="emphasis"><em>op monitor interval=60s timeout=30s</em></span></code></pre></div><p>
    The same can be done with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive apache apache \
   params</code> ...
<code class="prompt custom">crm(live)configure# </code><code class="command">monitor apache 60s:30s</code></pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.12.9.6.1"><span class="term">Monitoring Stopped Resources</span></dt><dd><p>
      Usually, resources are only monitored by the cluster as long as they
      are running. However, to detect concurrency violations, also configure
      monitoring for resources which are stopped. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive dummy1 Dummy \
    op monitor interval="300s" role="Stopped" timeout="10s" \
    op monitor interval="30s" timeout="10s"</code></pre></div><p>
      This configuration triggers a monitoring operation every
      <code class="literal">300</code> seconds for the resource
      <code class="literal">dummy1</code> when it is in
      <code class="literal">role="Stopped"</code>. When running, it will be monitored
      every <code class="literal">30</code> seconds.
     </p></dd><dt id="id-1.4.4.4.12.9.6.2"><span class="term">Probing</span></dt><dd><p>
      The CRM executes an initial monitoring for each resource on every
      node, the so-called <code class="literal">probe</code>. A probe is also executed
      after the cleanup of a resource. If multiple monitoring operations are
      defined for a resource, the CRM will select the one with the smallest
      interval and will use its timeout value as default timeout for
      probing. If no monitor operation is configured, the cluster-wide
      default applies. The default is <code class="literal">20</code> seconds (if not
      specified otherwise by configuring the <code class="varname">op_defaults</code>
      parameter). If you do not want to rely on the automatic calculation or
      the <code class="systemitem">op_defaults</code> value, define a specific
      monitoring operation for the <span class="emphasis"><em>probing</em></span> of this
      resource. Do so by adding a monitoring operation with the
      <code class="literal">interval</code> set to <code class="literal">0</code>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive rsc1 ocf:pacemaker:Dummy \
    op monitor interval="0" timeout="60"</code></pre></div><p>
      The probe of <code class="systemitem">rsc1</code> will time out in
      <code class="literal">60s</code>, independent of the global timeout defined in
      <code class="varname">op_defaults</code>, or any other operation timeouts
      configured. If you did not set <code class="literal">interval="0"</code> for
      specifying the probing of the respective resource, the CRM will
      automatically check for any other monitoring operations defined for
      that resource and will calculate the timeout value for probing as
      described above.
     </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-manual-config-load" data-id-title="Loading Resources from a File"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.11 </span><span class="title-name">Loading Resources from a File</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-load">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Parts or all of the configuration can be loaded from a local file or a
    network URL. Three different methods can be defined:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.13.3.1"><span class="term"><code class="option">replace</code></span></dt><dd><p>
       This option replaces the current configuration with the new source
       configuration.
      </p></dd><dt id="id-1.4.4.4.13.3.2"><span class="term"><code class="option">update</code></span></dt><dd><p>
       This option tries to import the source configuration. It adds new items
       or updates existing items to the current configuration.
      </p></dd><dt id="id-1.4.4.4.13.3.3"><span class="term"><code class="option">push</code></span></dt><dd><p>
       This option imports the content from the source into the current
       configuration (same as <code class="option">update</code>). However, it removes
       objects that are not available in the new configuration.
      </p></dd></dl></div><p>
    To load the new configuration from the file <code class="filename">mycluster-config.txt</code>
    use the following syntax:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure load push mycluster-config.txt</code></pre></div></section><section class="sect1" id="sec-ha-config-basics-meta-attr" data-id-title="Resource Options (Meta Attributes)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.12 </span><span class="title-name">Resource Options (Meta Attributes)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-meta-attr">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For each resource you add, you can define options. Options are used by
   the cluster to decide how your resource should behave—they tell
   the CRM how to treat a specific resource. Resource options can be set
   with the <code class="command">crm_resource --meta</code> command or with
   Hawk2 as described in
   <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.2, “Adding a Primitive Resource”</a>.
  </p><p>
   The following resource options are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.14.4.1"><span class="term"><code class="literal">priority</code></span></dt><dd><p>
      If not all resources can be active, the cluster stops lower
      priority resources to keep higher priority resources active.
     </p><p>
      The default value is <code class="literal">0</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.2"><span class="term"><code class="literal">target-role</code></span></dt><dd><p>
      In what state should the cluster attempt to keep this resource?
      Allowed values: <code class="literal">Stopped</code>, <code class="literal">Started</code>,
      <code class="literal">Unpromoted</code>, <code class="literal">Promoted</code>.
     </p><p>
      The default value is <code class="literal">Started</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.3"><span class="term"><code class="literal">is-managed</code></span></dt><dd><p>
      Is the cluster allowed to start and stop the resource? Allowed
      values: <code class="literal">true</code>, <code class="literal">false</code>. If the
      value is set to <code class="literal">false</code>, the status of the
      resource is still monitored and any failures are reported. This is
      different from setting a resource to
      <code class="literal">maintenance="true"</code>.
     </p><p>
      The default value is <code class="literal">true</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.4"><span class="term"><code class="literal">maintenance</code></span></dt><dd><p>
      Can the resources be touched manually? Allowed values:
      <code class="literal">true</code>, <code class="literal">false</code>. If set to
      <code class="literal">true</code>, all resources become unmanaged: the
      cluster stops monitoring them and does not know their
      status. You can stop or restart cluster resources without
      the cluster attempting to restart them.
     </p><p>
      The default value is <code class="literal">false</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.5"><span class="term"><code class="literal">resource-stickiness</code></span></dt><dd><p>
      How much does the resource prefer to stay where it is?
     </p><p>
      The default value is <code class="literal">1</code> for individual clone instances,
      and <code class="literal">0</code> for all other resources.
     </p></dd><dt id="id-1.4.4.4.14.4.6"><span class="term"><code class="literal">migration-threshold</code></span></dt><dd><p>
      How many failures should occur for this resource on a node before
      making the node ineligible to host this resource?
     </p><p>
      The default value is <code class="literal">INFINITY</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.7"><span class="term"><code class="literal">multiple-active</code></span></dt><dd><p>
      What should the cluster do if it ever finds the resource active on
      more than one node? Allowed values: <code class="literal">block</code> (mark
      the resource as unmanaged), <code class="literal">stop_only</code>,
      <code class="literal">stop_start</code>.
     </p><p>
      The default value is <code class="literal">stop_start</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.8"><span class="term"><code class="literal">failure-timeout</code></span></dt><dd><p>
      How many seconds to wait before acting as if the failure did not
      occur (and potentially allowing the resource back to the node on
      which it failed)?
     </p><p>
      The default value is <code class="literal">0</code> (disabled).
     </p></dd><dt id="id-1.4.4.4.14.4.9"><span class="term"><code class="literal">allow-migrate</code></span></dt><dd><p>
      Whether to allow live migration for resources that support
      <code class="literal">migrate_to</code> and <code class="literal">migrate_from</code>
      actions. If the value is set to <code class="literal">true</code>, the resource can
      be migrated without loss of state. If the value is set to <code class="literal">false</code>,
      the resource will be shut down on the first node and restarted on the second node.
     </p><p>
      The default value is <code class="literal">true</code> for
      <code class="literal">ocf:pacemaker:remote</code> resources, and
      <code class="literal">false</code> for all other resources.
     </p></dd><dt id="id-1.4.4.4.14.4.10"><span class="term"><code class="literal">remote-node</code></span></dt><dd><p>
      The name of the remote node this resource defines. This both
      enables the resource as a remote node and defines the unique name
      used to identify the remote node. If no other parameters are set,
      this value is also assumed as the host name to connect to at the
      <code class="varname">remote-port</code> port.
     </p><p>
      This option is disabled by default.
     </p><div id="id-1.4.4.4.14.4.10.2.3" data-id-title="Use unique IDs" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Use unique IDs</div><p>
       This value must not overlap with any existing resource or node IDs.
      </p></div></dd><dt id="id-1.4.4.4.14.4.11"><span class="term"><code class="literal">remote-port</code></span></dt><dd><p>
      Custom port for the guest connection to pacemaker_remote.
     </p><p>
      The default value is <code class="literal">3121</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.12"><span class="term"><code class="literal">remote-addr</code></span></dt><dd><p>
      The IP address or host name to connect to if the remote node's
      name is not the host name of the guest.
     </p><p>
      The default value is the value set by <code class="literal">remote-node</code>.
     </p></dd><dt id="id-1.4.4.4.14.4.13"><span class="term"><code class="literal">remote-connect-timeout</code></span></dt><dd><p>
      How long before a pending guest connection times out?
     </p><p>
      The default value is <code class="literal">60s</code>.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-config-basics-inst-attr" data-id-title="Instance Attributes (Parameters)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.13 </span><span class="title-name">Instance Attributes (Parameters)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-inst-attr">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The scripts of all resource classes can be given parameters which
   determine how they behave and which instance of a service they control.
   If your resource agent supports parameters, you can add them with the
   <code class="command">crm_resource</code> command or with
   Hawk2 as described in
   <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.2, “Adding a Primitive Resource”</a>. In the
   <code class="command">crm</code> command line utility and in Hawk2, instance
   attributes are called <code class="literal">params</code> or
   <code class="literal">Parameter</code>, respectively. The list of instance
   attributes supported by an OCF script can be found by executing the
   following command as <code class="systemitem">root</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm ra info <em class="replaceable">[class:[provider:]]resource_agent</em></code></pre></div><p>
   or (without the optional parts):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm ra info <em class="replaceable">resource_agent</em></code></pre></div><p>
   The output lists all the supported attributes, their purpose and default
   values.
  </p><div id="id-1.4.4.4.15.7" data-id-title="Instance Attributes for Groups, Clones or Promotable Clones" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Instance Attributes for Groups, Clones or Promotable Clones</div><p>
    Note that groups, clones and promotable clone resources do not have instance
    attributes. However, any instance attributes set will be inherited by
    the group's, clone's or promotable clone's children.
   </p></div></section><section class="sect1" id="sec-ha-config-basics-operations" data-id-title="Resource Operations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">6.14 </span><span class="title-name">Resource Operations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-operations">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   By default, the cluster will not ensure that your resources are still
   healthy. To instruct the cluster to do this, you need to add a monitor
   operation to the resource's definition. Monitor operations can be
   added for all classes or resource agents. For more information, refer to
   <a class="xref" href="#sec-ha-config-basics-monitoring" title="6.10. Configuring Resource Monitoring">Section 6.10, “Configuring Resource Monitoring”</a>.
  </p><div class="table" id="id-1.4.4.4.16.3" data-id-title="Resource Operation Properties"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 6.1: </span><span class="title-name">Resource Operation Properties </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.4.16.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_configuring_resources.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Operation
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">id</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Your name for the action. Must be unique. (The ID is not shown).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">name</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The action to perform. Common values: <code class="literal">monitor</code>,
        <code class="literal">start</code>, <code class="literal">stop</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">interval</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        How frequently to perform the operation. Unit: seconds
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">timeout</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        How long to wait before declaring the action has failed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">requires</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        What conditions need to be satisfied before this action occurs.
        Allowed values: <code class="literal">nothing</code>,
        <code class="literal">quorum</code>, <code class="literal">fencing</code>. The default
        depends on whether fencing is enabled and if the resource's class
        is <code class="literal">stonith</code>. For STONITH resources, the
        default is <code class="literal">nothing</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">on-fail</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The action to take if this action ever fails. Allowed values:
       </p>
       <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          <code class="literal">ignore</code>: Pretend the resource did not fail.
         </p></li><li class="listitem"><p>
          <code class="literal">block</code>: Do not perform any further operations
          on the resource.
         </p></li><li class="listitem"><p>
          <code class="literal">stop</code>: Stop the resource and do not start it
          elsewhere.
         </p></li><li class="listitem"><p>
          <code class="literal">restart</code>: Stop the resource and start it again
          (possibly on a different node).
         </p></li><li class="listitem"><p>
          <code class="literal">fence</code>: Bring down the node on which the
          resource failed (STONITH).
         </p></li><li class="listitem"><p>
          <code class="literal">standby</code>: Move <span class="emphasis"><em>all</em></span>
          resources away from the node on which the resource failed.
         </p></li></ul></div>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">enabled</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        If <code class="literal">false</code>, the operation is treated as if it does
        not exist. Allowed values: <code class="literal">true</code>,
        <code class="literal">false</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">role</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Run the operation only if the resource has this role.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        <code class="literal">record-pending</code>
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Can be set either globally or for individual resources. Makes the
        CIB reflect the state of <span class="quote">“<span class="quote">in-flight</span>”</span> operations on
        resources.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        <code class="literal">description</code>
       </p>
      </td><td>
       <p>
        Description of the operation.
       </p>
      </td></tr></tbody></table></div></div></section></section><section xml:lang="en" class="chapter" id="sec-ha-config-basics-constraints" data-id-title="Configuring Resource Constraints"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">7 </span><span class="title-name">Configuring Resource Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Having all the resources configured is only part of the job. Even if the
    cluster knows all needed resources, it might still not be able to handle
    them correctly. Resource constraints let you specify which cluster nodes
    resources can run on, what order resources will load, and what other
    resources a specific resource is dependent on.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-constraints-types" data-id-title="Types of Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.1 </span><span class="title-name">Types of Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-types">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three different kinds of constraints available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.5.3.3.1"><span class="term">Resource Location
    </span></dt><dd><p>
       Location constraints define on which nodes a resource may be
       run, may not be run or is preferred to be run.
      </p></dd><dt id="id-1.4.4.5.3.3.2"><span class="term">Resource Colocation</span></dt><dd><p>
       Colocation constraints tell the cluster which resources may or
       may not run together on a node.
      </p></dd><dt id="id-1.4.4.5.3.3.3"><span class="term">Resource Order</span></dt><dd><p>
       Order constraints define the sequence of actions.
      </p></dd></dl></div><div id="id-1.4.4.5.3.4" data-id-title="Restrictions for Constraints and Certain Types of Resources" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Restrictions for Constraints and Certain Types of Resources</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Do not create colocation constraints for <span class="emphasis"><em>members</em></span> of a resource
       group. Create a colocation constraint pointing to the resource group as a whole instead. All
       other types of constraints are safe to use for members of a resource group.</p></li><li class="listitem"><p>Do not use any constraints on a resource that has a clone resource or a promotable clone
       resource applied to it. The constraints must apply to the clone or promotable clone resource, not
       to the child resource.</p></li></ul></div></div></section><section class="sect1" id="sec-ha-config-basics-constraints-scores" data-id-title="Scores and Infinity"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.2 </span><span class="title-name">Scores and Infinity</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-scores">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When defining constraints, you also need to deal with scores. Scores of
   all kinds are integral to how the cluster works. Practically everything
   from migrating a resource to deciding which resource to stop in a
   degraded cluster is achieved by manipulating scores in some way. Scores
   are calculated on a per-resource basis and any node with a negative
   score for a resource cannot run that resource. After calculating the
   scores for a resource, the cluster then chooses the node with the
   highest score.
  </p><p>
   <code class="literal">INFINITY</code> is currently deﬁned as
   <code class="literal">1,000,000</code>. Additions or subtractions with it stick to
   the following three basic rules:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Any value + INFINITY = INFINITY
    </p></li><li class="listitem"><p>
     Any value - INFINITY = -INFINITY
    </p></li><li class="listitem"><p>
     INFINITY - INFINITY = -INFINITY
    </p></li></ul></div><p>
   When defining resource constraints, you specify a score for each
   constraint. The score indicates the value you are assigning to this
   resource constraint. Constraints with higher scores are applied before
   those with lower scores. By creating additional location constraints
   with different scores for a given resource, you can specify an order for
   the nodes that a resource will fail over to.
  </p></section><section class="sect1" id="sec-ha-config-basics-constraints-templates" data-id-title="Resource Templates and Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.3 </span><span class="title-name">Resource Templates and Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-templates">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have defined a resource template (see
   <a class="xref" href="#sec-ha-config-basics-resources-templates" title="6.8. Creating Resource Templates">Section 6.8, “Creating Resource Templates”</a>), it can be
   referenced in the following types of constraints:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     order constraints,
    </p></li><li class="listitem"><p>
     colocation constraints,
    </p></li><li class="listitem"><p>
     rsc_ticket constraints (for Geo clusters).
    </p></li></ul></div><p>
   However, colocation constraints must not contain more than one reference
   to a template. Resource sets must not contain a reference to a template.
  </p><p>
   Resource templates referenced in constraints stand for all primitives
   which are derived from that template. This means, the constraint applies
   to all primitive resources referencing the resource template.
   Referencing resource templates in constraints is an alternative to
   resource sets and can simplify the cluster configuration considerably.
   For details about resource sets, refer to
   <a class="xref" href="#pro-hawk2-constraints-sets" title="Using a Resource Set for Constraints">Procedure 7.4, “Using a Resource Set for Constraints”</a>.
  </p></section><section class="sect1" id="sec-ha-conf-add-location-constraints" data-id-title="Adding Location Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.4 </span><span class="title-name">Adding Location Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-conf-add-location-constraints">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A location constraint determines on which node a resource may be run, is
  preferably run, or may not be run. An example of a location constraint is to
  place all resources related to a certain database on the same node. This type
  of constraint may be added multiple times for each resource. All location
  constraints are evaluated for a given resource.
 </p><p>
  You can add location constraints using either Hawk2 or crmsh.
 </p><section class="sect2" id="sec-conf-hawk2-cons-loc" data-id-title="Adding Location Constraints with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.1 </span><span class="title-name">Adding Location Constraints with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-loc">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-constraints-location" data-id-title="Adding a Location Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.1: </span><span class="title-name">Adding a Location Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-location">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Constraint</span> › <span class="guimenu">Location</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step" id="step-hawk2-loc-rsc"><p>
     From the list of <span class="guimenu">Resources</span> select the resource or
     resources for which to define the constraint.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. The score indicates the value you are
     assigning to this resource constraint. Positive values indicate the
     resource can run on the <span class="guimenu">Node</span> you specify in the next
     step. Negative values mean it should not run on that node. Constraints
     with higher scores are applied before those with lower scores.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To force the resources to run on the node, click the arrow
       icon and select <code class="literal">Always</code>. This sets the score to
       <code class="literal">INFINITY</code>.
      </p></li><li class="listitem"><p>
       If you never want the resources to run on the node, click the arrow icon
       and select <code class="literal">Never</code>. This sets the score to
       <code class="literal">-INFINITY</code>, meaning that the resources must not run on
       the node.
      </p></li><li class="listitem"><p>
       To set the score to <code class="literal">0</code>, click the arrow icon and
       select <code class="literal">Advisory</code>. This disables the constraint. This
       is useful when you want to set resource discovery but do not want to
       constrain the resources.
      </p></li></ul></div></li><li class="step"><p>
     Select a <span class="guimenu">Node</span>.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.4.4.5.6.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-loc-constraint.png"><img src="images/hawk2-loc-constraint.png" width="100%" alt="Hawk2—Location Constraint" title="Hawk2—Location Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.1: </span><span class="title-name">Hawk2—Location Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.6.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-constraints-locational" data-id-title="Adding Location Constraints with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.4.2 </span><span class="title-name">Adding Location Constraints with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-locational">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">location</code> command defines on which nodes a
   resource may be run, may not be run or is preferred to be run.
  </p><p>
   A simple example that expresses a preference to run the
   resource <code class="literal">fs1</code> on the node with the name
   <code class="systemitem">alice</code> to 100 would be the
   following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location loc-fs1 fs1 100: alice</code></pre></div><p>
   Another example is a location with ping:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive ping ping \
  params name=ping dampen=5s multiplier=100 host_list="r1 r2"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">clone cl-ping ping meta interleave=true</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">location loc-node_pref internal_www \
  rule 50: #uname eq alice \
  rule ping: defined ping</code></pre></div><p>
   The parameter <em class="parameter">host_list</em> is a space-separated list
   of hosts to ping and count.
   Another use case for location constraints are grouping primitives as a
   <span class="emphasis"><em>resource set</em></span>. This can be useful if several
   resources depend on, for example, a ping attribute for network
   connectivity. In former times, the <code class="literal">-inf/ping</code> rules
   needed to be duplicated several times in the configuration, making it
   unnecessarily complex.
  </p><p>
   The following example creates a resource set
   <code class="varname">loc-alice</code>, referencing the virtual IP addresses
   <code class="varname">vip1</code> and <code class="varname">vip2</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive vip1 IPaddr2 params ip=192.168.1.5</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive vip2 IPaddr2 params ip=192.168.1.6</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">location loc-alice { vip1 vip2 } inf: alice</code></pre></div><p>
   In some cases it is much more efficient and convenient to use resource
   patterns for your <code class="command">location</code> command. A resource
   pattern is a regular expression between two slashes. For example, the
   above virtual IP addresses can be all matched with the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location loc-alice /vip.*/ inf: alice</code></pre></div></section></section><section class="sect1" id="sec-ha-conf-add-colocation-constraints" data-id-title="Adding Colocation Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.5 </span><span class="title-name">Adding Colocation Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-conf-add-colocation-constraints">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  A colocation constraint tells the cluster which resources may or may not
  run together on a node. As a colocation constraint defines a dependency
  between resources, you need at least two resources to create a colocation
  constraint.
 </p><p>
  You can add colocation constraints using either Hawk2 or crmsh.
 </p><section class="sect2" id="sec-conf-hawk2-cons-col" data-id-title="Adding Colocation Constraints with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.1 </span><span class="title-name">Adding Colocation Constraints with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-col">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-constraints-colocation" data-id-title="Adding a Colocation Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.2: </span><span class="title-name">Adding a Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-colocation">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Constraint</span> › <span class="guimenu">Colocation</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. The score determines the location
     relationship between the resources. Positive values indicate that the
     resources should run on the same node. Negative values indicate that the
     resources should not run on the same node. The score will be combined with
     other factors to decide where to put the resource.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To force the resources to run on the same node, click the
       arrow icon and select <code class="literal">Always</code>. This sets the score to
       <code class="literal">INFINITY</code>.
      </p></li><li class="listitem"><p>
       If you never want the resources to run on the same node, click the arrow
       icon and select <code class="literal">Never</code>. This sets the score to
       <code class="literal">-INFINITY</code>, meaning that the resources must not run on
       the same node.
      </p></li></ul></div></li><li class="step"><p>
     To define the resources for the constraint:
    </p><ol type="a" class="substeps"><li class="step" id="step-hawk2-col-rsc"><p>
       From the drop-down box in the <span class="guimenu">Resources</span> category,
       select a resource (or a template).
      </p><p>
       The resource is added and a new empty drop-down box appears beneath.
      </p></li><li class="step"><p>
       Repeat this step to add more resources.
      </p><p>
       As the topmost resource depends on the next resource and so on, the
       cluster will first decide where to put the last resource, then place the
       depending ones based on that decision. If the constraint cannot be
       satisﬁed, the cluster may not allow the dependent resource to run.
      </p></li><li class="step"><p>
       To swap the order of resources within the colocation constraint, click
       the arrow up icon next to a resource to swap it with the entry above.
      </p></li></ol></li><li class="step"><p>
     If needed, specify further parameters for each resource (such as
     <code class="literal">Started</code>, <code class="literal">Stopped</code>,
     <code class="literal">Master</code>, <code class="literal">Slave</code>,
     <code class="literal">Promote</code>, <code class="literal">Demote</code>): Click the empty
     drop-down box next to the resource and select the desired entry.
    </p></li><li class="step"><p>
     Click <span class="guimenu">Create</span> to finish the configuration. A message at
     the top of the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.4.4.5.7.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-col-constraint.png"><img src="images/hawk2-col-constraint.png" width="100%" alt="Hawk2—Colocation Constraint" title="Hawk2—Colocation Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.2: </span><span class="title-name">Hawk2—Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.7.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-constraints-collocational" data-id-title="Adding Colocation Constraints with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.5.2 </span><span class="title-name">Adding Colocation Constraints with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-collocational">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     The <code class="command">colocation</code> command is used to define what
     resources should run on the same or on different hosts.
    </p><p>
     You can set a score of either +inf or -inf, defining
     resources that must always or must never run on the same node.
     You can also use non-infinite scores. In that case the colocation
     is called <span class="emphasis"><em>advisory</em></span> and the cluster may decide not
     to follow them in favor of not stopping other resources if there is a
     conflict.
    </p><p>
     For example, to always run the resources <code class="literal">resource1</code>
     and <code class="literal">resource2</code> on the same host, use the following constraint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation coloc-2resource inf: resource1 resource2</code></pre></div><p>
   For a primary/secondary configuration, it is necessary to know if the
   current node is a primary in addition to running the resource locally.
  </p></section></section><section class="sect1" id="sec-ha-conf-add-order-constraints" data-id-title="Adding Order Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.6 </span><span class="title-name">Adding Order Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-conf-add-order-constraints">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Order constraints can be used to start or stop a service right before or
  after a different resource meets a special condition, such as being started,
  stopped, or promoted to primary. For example, you cannot mount a file system
  before the device is available to a system. As an order constraint defines
  a dependency between resources, you need at least two resources to create
  an order constraint.
 </p><p>
  You can add order constraints using either Hawk2 or crmsh.
 </p><section class="sect2" id="sec-conf-hawk2-cons-order" data-id-title="Adding Order Constraints with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6.1 </span><span class="title-name">Adding Order Constraints with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-order">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-constraints-order" data-id-title="Adding an Order Constraint"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.3: </span><span class="title-name">Adding an Order Constraint </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-order">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add Constraint</span> › <span class="guimenu">Order</span>.
    </p></li><li class="step"><p>
     Enter a unique <span class="guimenu">Constraint ID</span>.
    </p></li><li class="step"><p>
     Enter a <span class="guimenu">Score</span>. If the score is greater than zero, the
     order constraint is mandatory, otherwise it is optional.
    </p><p>
     Some often-used values can also be set via the drop-down box:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If you want to make the order constraint mandatory, click the arrow icon
       and select <code class="literal">Mandatory</code>.
      </p></li><li class="listitem"><p>
       If you want the order constraint to be a suggestion only, click the
       arrow icon and select <code class="literal">Optional</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">Serialize</code>: To ensure that no two stop/start actions
       occur concurrently for the resources, click the arrow icon and select
       <code class="literal">Serialize</code>. This makes sure that one resource must
       complete starting before the other can be started. A typical use case
       are resources that put a high load on the host during start-up.
      </p></li></ul></div></li><li class="step"><p>
     For order constraints, you can usually keep the option
     <span class="guimenu">Symmetrical</span> enabled. This specifies that resources are
     stopped in reverse order.
    </p></li><li class="step"><p>
     To define the resources for the constraint:
    </p><ol type="a" class="substeps"><li class="step" id="step-hawk2-order-rsc"><p>
       From the drop-down box in the <span class="guimenu">Resources</span> category,
       select a resource (or a template).
      </p><p>
       The resource is added and a new empty drop-down box appears beneath.
      </p></li><li class="step"><p>
       Repeat this step to add more resources.
      </p><p>
       The topmost resource will start first, then the second, etc. Usually the
       resources will be stopped in reverse order.
      </p></li><li class="step"><p>
       To swap the order of resources within the order constraint, click the
       arrow up icon next to a resource to swap it with the entry above.
      </p></li></ol></li><li class="step"><p>
     If needed, specify further parameters for each resource (like
     <code class="literal">Started</code>, <code class="literal">Stopped</code>,
     <code class="literal">Master</code>, <code class="literal">Slave</code>,
     <code class="literal">Promote</code>, <code class="literal">Demote</code>): Click the empty
     drop-down box next to the resource and select the desired entry.
    </p></li><li class="step"><p>
     Confirm your changes to finish the configuration. A message at the top of
     the screen shows if the action has been successful.
    </p></li></ol></div></div><div class="figure" id="id-1.4.4.5.8.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-order-constraint.png"><img src="images/hawk2-order-constraint.png" width="100%" alt="Hawk2—Order Constraint" title="Hawk2—Order Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.3: </span><span class="title-name">Hawk2—Order Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.8.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-constraints-ordering" data-id-title="Adding Order Constraints with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.6.2 </span><span class="title-name">Adding Order Constraints with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-ordering">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">order</code> command defines a sequence of action.
  </p><p>
   For example, to always start <code class="literal">resource1</code> before
   <code class="literal">resource2</code>, use the following constraint:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order res1_before_res2 Mandatory: resource1 resource2</code></pre></div></section></section><section class="sect1" id="sec-ha-config-basics-constraints-rscset" data-id-title="Using Resource Sets to Define Constraints"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.7 </span><span class="title-name">Using Resource Sets to Define Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-rscset">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      As an alternative format for defining location, colocation, or order
      constraints, you can use <code class="literal">resource sets</code>, where
      primitives are grouped together in one set. Previously this was
      possible either by defining a resource group (which could not always
      accurately express the design), or by defining each relationship as an
      individual constraint. The latter caused a constraint explosion as the
      number of resources and combinations grew. The configuration via
      resource sets is not necessarily less verbose, but is easier to
      understand and maintain.
     </p><p>
      You can configure resource sets using either Hawk2 or crmsh.
     </p><section class="sect2" id="sec-conf-hawk2-cons-set" data-id-title="Using Resource Sets to Define Constraints with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.7.1 </span><span class="title-name">Using Resource Sets to Define Constraints with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-cons-set">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-constraints-sets" data-id-title="Using a Resource Set for Constraints"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.4: </span><span class="title-name">Using a Resource Set for Constraints </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-constraints-sets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         To use a resource set within a location constraint:
        </p><ol type="a" class="substeps"><li class="step"><p>
           Proceed as outlined in <a class="xref" href="#pro-hawk2-constraints-location" title="Adding a Location Constraint">Procedure 7.1, “Adding a Location Constraint”</a>,
           apart from
           <a class="xref" href="#step-hawk2-loc-rsc" title="Step 4">Step 4</a>: Instead
           of selecting a single resource, select multiple resources by pressing
           <span class="keycap">Ctrl</span> or <span class="keycap">Shift</span> and mouse
           click. This creates a resource set within the location constraint.
          </p></li><li class="step"><p>
           To remove a resource from the location constraint, press
           <span class="keycap">Ctrl</span> and click the resource again to deselect
           it.
          </p></li></ol></li><li class="step"><p>
         To use a resource set within a colocation or order constraint:
        </p><ol type="a" class="substeps"><li class="step"><p>
           Proceed as described in
           <a class="xref" href="#pro-hawk2-constraints-colocation" title="Adding a Colocation Constraint">Procedure 7.2, “Adding a Colocation Constraint”</a> or
           <a class="xref" href="#pro-hawk2-constraints-order" title="Adding an Order Constraint">Procedure 7.3, “Adding an Order Constraint”</a>, apart from the
           step where you define the resources for the constraint
           (<a class="xref" href="#step-hawk2-col-rsc" title="Step 5.a">Step 5.a</a> or
           <a class="xref" href="#step-hawk2-order-rsc" title="Step 6.a">Step 6.a</a>):
          </p></li><li class="step"><p>
           Add multiple resources.
          </p></li><li class="step"><p>
           To create a resource set, click the chain icon next to a resource to
           link it to the resource above. A resource set is visualized by a frame
           around the resources belonging to a set.
          </p></li><li class="step"><p>
           You can combine multiple resources in a resource set or create multiple
           resource sets.
          </p><div class="figure" id="id-1.4.4.5.9.4.2.3.2.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-constraint-set.png"><img src="images/hawk2-constraint-set.png" width="100%" alt="Hawk2—Two Resource Sets in a Colocation Constraint" title="Hawk2—Two Resource Sets in a Colocation Constraint"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 7.4: </span><span class="title-name">Hawk2—Two Resource Sets in a Colocation Constraint </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.9.4.2.3.2.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
           To unlink a resource from the resource above, click the scissors icon
           next to the resource.
          </p></li></ol></li><li class="step"><p>
         Confirm your changes to finish the constraint configuration.
        </p></li></ol></div></div></section><section class="sect2" id="sec-ha-config-basics-constraints-rscset-constraints" data-id-title="Using Resource Sets for Defining Constraints"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.7.2 </span><span class="title-name">Using Resource Sets for Defining Constraints</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-rscset-constraints">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="complex-example"><div class="example" id="ex-config-basic-resourceset-loc" data-id-title="A Resource Set for Location Constraints"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.1: </span><span class="title-name">A Resource Set for Location Constraints </span></span><a title="Permalink" class="permalink" href="#ex-config-basic-resourceset-loc">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
       For example, you can use the following configuration of a resource
       set (<code class="varname">loc-alice</code>) in the crmsh to place
       two virtual IPs (<code class="varname">vip1</code> and <code class="varname">vip2</code>)
       on the same node, <code class="varname">alice</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive vip1 IPaddr2 params ip=192.168.1.5</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive vip2 IPaddr2 params ip=192.168.1.6</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">location loc-alice { vip1 vip2 } inf: alice</code></pre></div></div></div></div><p>
      If you want to use resource sets to replace a configuration of
      colocation constraints, consider the following two examples:
     </p><div class="example" id="id-1.4.4.5.9.5.4" data-id-title="A Chain of Colocated Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.2: </span><span class="title-name">A Chain of Colocated Resources </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.9.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_colocation id="coloc-1" rsc="B" with-rsc="A" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-2" rsc="C" with-rsc="B" score="INFINITY"/&gt;
     &lt;rsc_colocation id="coloc-3" rsc="D" with-rsc="C" score="INFINITY"/&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same configuration expressed by a resource set:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
    &lt;rsc_colocation id="coloc-1" score="INFINITY" &gt;
     &lt;resource_set id="colocated-set-example" sequential="true"&gt;
      &lt;resource_ref id="A"/&gt;
      &lt;resource_ref id="B"/&gt;
      &lt;resource_ref id="C"/&gt;
      &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
    &lt;/rsc_colocation&gt;
&lt;/constraints&gt;</pre></div><p>
      If you want to use resource sets to replace a configuration of
      order constraints, consider the following two examples:
     </p><div class="example" id="id-1.4.4.5.9.5.8" data-id-title="A Chain of Ordered Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.3: </span><span class="title-name">A Chain of Ordered Resources </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.9.5.8">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1" first="A" then="B" /&gt;
     &lt;rsc_order id="order-2" first="B" then="C" /&gt;
     &lt;rsc_order id="order-3" first="C" then="D" /&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      The same purpose can be achieved by using a resource set with ordered
      resources:
     </p><div class="example" id="id-1.4.4.5.9.5.10" data-id-title="A Chain of Ordered Resources Expressed as Resource Set"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.4: </span><span class="title-name">A Chain of Ordered Resources Expressed as Resource Set </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.9.5.10">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;constraints&gt;
     &lt;rsc_order id="order-1"&gt;
     &lt;resource_set id="ordered-set-example" sequential="true"&gt;
     &lt;resource_ref id="A"/&gt;
     &lt;resource_ref id="B"/&gt;
     &lt;resource_ref id="C"/&gt;
     &lt;resource_ref id="D"/&gt;
     &lt;/resource_set&gt;
     &lt;/rsc_order&gt;
&lt;/constraints&gt;</pre></div></div></div><p>
      Sets can be either ordered (<code class="literal">sequential=true</code>) or
      unordered (<code class="literal">sequential=false</code>). Furthermore, the
      <code class="literal">require-all</code> attribute can be used to switch between
      <code class="literal">AND</code> and <code class="literal">OR</code> logic.
     </p></section><section class="sect2" id="sec-ha-manual-config-constraints-weak-bond" data-id-title="Collocating Sets for Resources Without Dependency"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.7.3 </span><span class="title-name">Collocating Sets for Resources Without Dependency</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-constraints-weak-bond">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Sometimes it is useful to place a group of resources on the same node
      (defining a colocation constraint), but without having hard
      dependencies between the resources. For example, you want two
      resources to be placed on the same node, but you do
      <span class="emphasis"><em>not</em></span> want the cluster to restart the other one if
      one of them fails.
     </p><p>
      This can be achieved on the crm shell by using the <code class="command">weak-bond</code> command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure assist weak-bond <em class="replaceable">resource1</em> <em class="replaceable">resource2</em></code></pre></div><p>
      The <code class="command">weak-bond</code> command creates a dummy
      resource and a colocation constraint with the given resources
      automatically.
     </p></section></section><section class="sect1" id="sec-ha-config-basics-failover" data-id-title="Specifying Resource Failover Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.8 </span><span class="title-name">Specifying Resource Failover Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-failover">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource will be automatically restarted if it fails. If that cannot
    be achieved on the current node, or it fails <code class="literal">N</code> times
    on the current node, it tries to fail over to another node. Each time
    the resource fails, its fail count is raised. You can define several
    failures for resources (a <code class="literal">migration-threshold</code>), after
    which they will migrate to a new node. If you have more than two nodes
    in your cluster, the node a particular resource fails over to is chosen
    by the High Availability software.
   </p><p>
    However, you can specify the node a resource will fail over to by
    configuring one or several location constraints and a
    <code class="literal">migration-threshold</code> for that resource.
   </p><p>
    You can specify resource failover nodes using either Hawk2 or crmsh.
   </p><div class="complex-example"><div class="example" id="ex-ha-config-basics-failover" data-id-title="Migration Threshold—Process Flow"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 7.5: </span><span class="title-name">Migration Threshold—Process Flow </span></span><a title="Permalink" class="permalink" href="#ex-ha-config-basics-failover">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     For example, let us assume you have configured a location constraint
     for resource <code class="literal">rsc1</code> to preferably run on
     <code class="literal">alice</code>. If it fails there,
     <code class="literal">migration-threshold</code> is checked and compared to the
     fail count. If <code class="literal">failcount</code> &gt;= migration-threshold, then the resource is
     migrated to the node with the next best preference.
    </p><p>
     After the threshold has been reached, the node will no longer be
     allowed to run the failed resource until the resource's fail count is
     reset. This can be done manually by the cluster administrator or by
     setting a <code class="literal">failure-timeout</code> option for the resource.
    </p><p>
     For example, a setting of <code class="literal">migration-threshold=2</code> and
     <code class="literal">failure-timeout=60s</code> would cause the resource to
     migrate to a new node after two failures. It would be allowed to move
     back (depending on the stickiness and constraint scores) after one
     minute.
    </p></div></div></div><p>
    There are two exceptions to the migration threshold concept, occurring
    when a resource either fails to start or fails to stop:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Start failures set the fail count to <code class="literal">INFINITY</code> and
      thus always cause an immediate migration.
     </p></li><li class="listitem"><p>
      Stop failures cause fencing (when <code class="literal">stonith-enabled</code>
      is set to <code class="literal">true</code> which is the default).
     </p><p>
      In case there is no STONITH resource defined (or
      <code class="literal">stonith-enabled</code> is set to
      <code class="literal">false</code>), the resource will not migrate.
     </p></li></ul></div><section class="sect2" id="sec-conf-hawk2-failover" data-id-title="Specifying Resource Failover Nodes with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.8.1 </span><span class="title-name">Specifying Resource Failover Nodes with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-failover" data-id-title="Specifying a Failover Node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.5: </span><span class="title-name">Specifying a Failover Node </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-failover">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       Configure a location constraint for the resource as described in
       <a class="xref" href="#pro-hawk2-constraints-location" title="Adding a Location Constraint">Procedure 7.1, “Adding a Location Constraint”</a>.
      </p></li><li class="step"><p>
       Add the <code class="literal">migration-threshold</code> meta attribute to the
       resource as described in
       <a class="xref" href="#pro-conf-hawk2-rsc-modify" title="Modifying Parameters, Operations, or Meta Attributes for a Resource">Procedure 8.1: Modifying Parameters, Operations, or Meta Attributes for a Resource</a>,
       <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 5">Step 5</a>
       and enter a value for the migration-threshold. The value should be
       positive and less than INFINITY.
      </p></li><li class="step"><p>
       If you want to automatically expire the fail count for a resource, add the
       <code class="literal">failure-timeout</code> meta attribute to the resource as
       described in
       <a class="xref" href="#pro-conf-hawk2-primitive-add" title="Adding a Primitive Resource">Procedure 6.2: Adding a Primitive Resource</a>,
       <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 5">Step 5</a>
       and enter a <span class="guimenu">Value</span> for the
       <code class="literal">failure-timeout</code>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-failover-node.png"><img src="images/hawk2-failover-node.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       If you want to specify additional failover nodes with preferences for a
       resource, create additional location constraints.
      </p></li></ol></div></div><p>
     Instead of letting the fail count for a resource expire automatically, you
     can also clean up fail counts for a resource manually at any time. Refer to
     <a class="xref" href="#sec-conf-hawk2-manage-cleanup" title="8.5.1. Cleaning Up Cluster Resources with Hawk2">Section 8.5.1, “Cleaning Up Cluster Resources with Hawk2”</a> for details.
    </p></section><section class="sect2" id="sec-ha-manual-config-failover" data-id-title="Specifying Resource Failover Nodes with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.8.2 </span><span class="title-name">Specifying Resource Failover Nodes with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-failover">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     To determine a resource failover, use the meta attribute
     <code class="literal">migration-threshold</code>. If the fail count exceeds
     <code class="literal">migration-threshold</code> on all nodes, the resource
     remains stopped. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">location rsc1-alice rsc1 100: alice</code></pre></div><p>
     Normally, <code class="literal">rsc1</code> prefers to run on <code class="literal">alice</code>.
     If it fails there, <code class="literal">migration-threshold</code> is checked and compared
     to the fail count. If <code class="literal">failcount</code> &gt;= <code class="literal">migration-threshold</code>,
     then the resource is migrated to the node with the next best preference.
    </p><p>
     Start failures set the fail count to inf depend on the
     <code class="option">start-failure-is-fatal</code> option. Stop failures cause
     fencing. If there is no STONITH defined, the resource will not migrate.
    </p></section></section><section class="sect1" id="sec-ha-config-basics-failback" data-id-title="Specifying Resource Failback Nodes (Resource Stickiness)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.9 </span><span class="title-name">Specifying Resource Failback Nodes (Resource Stickiness)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-failback">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    A resource might fail back to its original node when that node is back
    online and in the cluster. To prevent a resource from
    failing back to the node that it was running on, or
    to specify a different node for the resource to fail back to,
    change its resource stickiness value. You can
    either specify resource stickiness when you are creating a resource or
    afterward.
   </p><p>
    Consider the following implications when specifying resource stickiness
    values:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.5.11.4.1"><span class="term">Value is <code class="literal">0</code>:</span></dt><dd><p>
       The resource is placed optimally in the
       system. This may mean that it is moved when a <span class="quote">“<span class="quote">better</span>”</span>
       or less loaded node becomes available. The option is almost
       equivalent to automatic failback, except that the resource may be
       moved to a node that is not the one it was previously active on.
       This is the Pacemaker default.
      </p></dd><dt id="id-1.4.4.5.11.4.2"><span class="term">Value is greater than <code class="literal">0</code>:</span></dt><dd><p>
       The resource will prefer to remain in its current location, but may
       be moved if a more suitable node is available. Higher values indicate
       a stronger preference for a resource to stay where it is.
      </p></dd><dt id="id-1.4.4.5.11.4.3"><span class="term">Value is less than <code class="literal">0</code>:</span></dt><dd><p>
       The resource prefers to move away from its current location. Higher
       absolute values indicate a stronger preference for a resource to be
       moved.
      </p></dd><dt id="id-1.4.4.5.11.4.4"><span class="term">Value is <code class="literal">INFINITY</code>:</span></dt><dd><p>
       The resource will always remain in its current location unless forced
       off because the node is no longer eligible to run the resource (node
       shutdown, node standby, reaching the
       <code class="literal">migration-threshold</code>, or configuration change).
       This option is almost equivalent to completely disabling automatic
       failback.
      </p></dd><dt id="id-1.4.4.5.11.4.5"><span class="term">Value is <code class="literal">-INFINITY</code>:</span></dt><dd><p>
       The resource will always move away from its current location.
      </p></dd></dl></div><section class="sect2" id="sec-config-hawk2-failback" data-id-title="Specifying Resource Failback Nodes with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.9.1 </span><span class="title-name">Specifying Resource Failback Nodes with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-config-hawk2-failback">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-stickiness" data-id-title="Specifying Resource Stickiness"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.6: </span><span class="title-name">Specifying Resource Stickiness </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-stickiness">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2:
      </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       Add the <code class="literal">resource-stickiness</code> meta attribute to the
       resource as described in
       <a class="xref" href="#pro-conf-hawk2-rsc-modify" title="Modifying Parameters, Operations, or Meta Attributes for a Resource">Procedure 8.1: Modifying Parameters, Operations, or Meta Attributes for a Resource</a>,
       <a class="xref" href="#step-hawk2-rsc-modify-params" title="Step 5">Step 5</a>.
      </p></li><li class="step"><p>
       Specify a value between <code class="literal">-INFINITY</code> and
       <code class="literal">INFINITY</code> for <code class="literal">resource-stickiness</code>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-rsc-stickiness.png"><img src="images/hawk2-rsc-stickiness.png" width="100%" alt="Image" title="Image"/></a></div></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-config-basics-utilization" data-id-title="Placing Resources Based on Their Load Impact"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.10 </span><span class="title-name">Placing Resources Based on Their Load Impact</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-utilization">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Not all resources are equal. Some, such as Xen guests, require that
    the node hosting them meets their capacity requirements. If resources
    are placed such that their combined need exceed the provided capacity,
    the resources diminish in performance (or even fail).
   </p><p>
    To take this into account, SUSE Linux Enterprise High Availability allows you to specify the
    following parameters:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      The capacity a certain node <span class="emphasis"><em>provides</em></span>.
     </p></li><li class="listitem"><p>
      The capacity a certain resource <span class="emphasis"><em>requires</em></span>.
     </p></li><li class="listitem"><p>
      An overall strategy for placement of resources.
     </p></li></ol></div><p>
    You can configure these setting using either Hawk2 or crmsh:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Hawk2: <a class="xref" href="#sec-config-hawk2-utilization" title="7.10.1. Placing Resources Based on Their Load Impact with Hawk2">Section 7.10.1, “Placing Resources Based on Their Load Impact with Hawk2”</a>
     </p></li><li class="listitem"><p>
      crmsh: <a class="xref" href="#sec-ha-manual-config-utilization" title="7.10.2. Placing Resources Based on Their Load Impact with crmsh">Section 7.10.2, “Placing Resources Based on Their Load Impact with crmsh”</a>
     </p></li></ul></div><p>
    A node is considered eligible for a resource if it has sufficient free
    capacity to satisfy the resource's requirements. The nature of the
    capacities is completely irrelevant for the High Availability software; it only makes
    sure that all capacity requirements of a resource are satisfied before
    moving a resource to a node.
   </p><p>
    To manually configure the resource's requirements and the capacity a
    node provides, use utilization attributes. You can name the utilization
    attributes according to your preferences and define as many name/value
    pairs as your configuration needs. However, the attribute's values must
    be integers.
   </p><p>
    If multiple resources with utilization attributes are grouped or have
    colocation constraints, SUSE Linux Enterprise High Availability takes that into account. If
    possible, the resources is placed on a node that can fulfill
    <span class="emphasis"><em>all</em></span> capacity requirements.
   </p><div id="id-1.4.4.5.12.10" data-id-title="Utilization Attributes for Groups" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Utilization Attributes for Groups</div><p>
     It is impossible to set utilization attributes directly for a resource
     group. However, to simplify the configuration for a group, you can add
     a utilization attribute with the total capacity needed to any of the
     resources in the group.
    </p></div><p>
    SUSE Linux Enterprise High Availability also provides the means to detect and configure both node
    capacity and resource requirements automatically:
   </p><p>
    The <code class="systemitem">NodeUtilization</code> resource agent checks the
    capacity of a node (regarding CPU and RAM).
    To configure automatic detection, create a clone resource of the
    following class, provider, and type:
    <code class="literal">ocf:pacemaker:NodeUtilization</code>. One instance of the
    clone should be running on each node. After the instance has started, a
    utilization section will be added to the node's configuration in CIB.
   </p><p>
    For automatic detection of a resource's minimal requirements (regarding
    RAM and CPU) the <code class="systemitem">Xen</code> resource agent has been
    improved. Upon start of a <code class="systemitem">Xen</code> resource, it will
    reflect the consumption of RAM and CPU. Utilization attributes will
    automatically be added to the resource configuration.
   </p><div id="id-1.4.4.5.12.14" data-id-title="Different Resource Agents for Xen and libvirt" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Different Resource Agents for Xen and libvirt</div><p>
     The <code class="systemitem">ocf:heartbeat:Xen</code> resource agent should not be
     used with <code class="literal">libvirt</code>, as <code class="literal">libvirt</code> expects
     to be able to modify the machine description file.
    </p><p>
     For <code class="literal">libvirt</code>, use the
     <code class="systemitem">ocf:heartbeat:VirtualDomain</code> resource agent.
    </p></div><p>
    Apart from detecting the minimal requirements, the High Availability software also allows
    to monitor the current utilization via the
    <code class="systemitem">VirtualDomain</code> resource agent. It detects CPU
    and RAM use of the virtual machine. To use this feature, configure a
    resource of the following class, provider and type:
    <code class="literal">ocf:heartbeat:VirtualDomain</code>. The following instance
    attributes are available: <code class="varname">autoset_utilization_cpu</code> and
    <code class="varname">autoset_utilization_hv_memory</code>. Both default to
    <code class="literal">true</code>. This updates the utilization values in the CIB
    during each monitoring cycle.
   </p><p>
    Independent of manually or automatically configuring capacity and
    requirements, the placement strategy must be specified with the
    <code class="literal">placement-strategy</code> property (in the global cluster
    options). The following values are available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.5.12.17.1"><span class="term"><code class="literal">default</code> (default value)</span></dt><dd><p>
       Utilization values are not considered. Resources are allocated
       according to location scoring. If scores are equal, resources are
       evenly distributed across nodes.
      </p></dd><dt id="id-1.4.4.5.12.17.2"><span class="term"><code class="literal">utilization</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. However,
       load-balancing is still done based on the number of resources
       allocated to a node.
      </p></dd><dt id="id-1.4.4.5.12.17.3"><span class="term"><code class="literal">minimal</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to concentrate the resources on as few nodes as possible
       (to achieve power savings on the remaining nodes).
      </p></dd><dt id="id-1.4.4.5.12.17.4"><span class="term"><code class="literal">balanced</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to distribute the resources evenly, thus optimizing resource
       performance.
      </p></dd></dl></div><div id="id-1.4.4.5.12.18" data-id-title="Configuring Resource Priorities" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Configuring Resource Priorities</div><p>
     The available placement strategies are best-effort—they do not
     yet use complex heuristic solvers to always reach optimum allocation
     results. Ensure that resource priorities are properly set so that
     your most important resources are scheduled first.
    </p></div><section class="sect2" id="sec-config-hawk2-utilization" data-id-title="Placing Resources Based on Their Load Impact with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.10.1 </span><span class="title-name">Placing Resources Based on Their Load Impact with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-config-hawk2-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Utilization attributes are used to configure both the resource's
    requirements and the capacity a node provides. You first need to configure a
    node's capacity before you can configure the capacity a resource requires.
   </p><div class="procedure" id="pro-hawk2-utilization-node" data-id-title="Configuring the Capacity a Node Provides"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.7: </span><span class="title-name">Configuring the Capacity a Node Provides </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-utilization-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>.
     </p></li><li class="step"><p>
      On the <span class="guimenu">Nodes</span> tab, select the node whose capacity you
      want to configure.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Operations</span> column, click the arrow down icon and
      select <span class="guimenu">Edit</span>.
     </p><p>
      The <span class="guimenu">Edit Node</span> screen opens.
     </p></li><li class="step"><p>
      Below <span class="guimenu">Utilization</span>, enter a name for a utilization
      attribute into the empty drop-down box.
     </p><p>
      The name can be arbitrary (for example, <code class="literal">RAM_in_GB</code>).
     </p></li><li class="step"><p>
      Click the <span class="guimenu">Add</span> icon to add the attribute.
     </p></li><li class="step"><p>
      In the empty text box next to the attribute, enter an attribute value. The
      value must be an integer.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-utilization-node.png"><img src="images/hawk2-utilization-node.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Add as many utilization attributes as you need and add values for all of
      them.
     </p></li><li class="step"><p>
      Confirm your changes. A message at the top of the screen shows if the
      action has been successful.
     </p></li></ol></div></div><div class="procedure" id="pro-hawk2-utilization-rsc" data-id-title="Configuring the Capacity a Resource Requires"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.8: </span><span class="title-name">Configuring the Capacity a Resource Requires </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-utilization-rsc">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     Configure the capacity a certain resource requires from a node either when
     creating a primitive resource or when editing an existing primitive resource.
    </p><p>
     Before you can add utilization attributes to a resource, you need to have
     set utilization attributes for your cluster nodes as described in
     <a class="xref" href="#pro-hawk2-utilization-node" title="Configuring the Capacity a Node Provides">Procedure 7.7</a>.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      To add a utilization attribute to an existing resource: Go to <span class="guimenu">Manage</span> › <span class="guimenu">Status</span> and open
      the resource configuration dialog as described in
      <a class="xref" href="#sec-conf-hawk2-manage-edit" title="8.2.1. Editing Resources and Groups with Hawk2">Section 8.2.1, “Editing Resources and Groups with Hawk2”</a>.
     </p><p>
      If you create a new resource: Go to <span class="guimenu">Configuration</span> › <span class="guimenu">Add Resource</span> and proceed as described in
      <a class="xref" href="#sec-conf-hawk2-rsc-primitive" title="6.4.1. Creating Primitive Resources with Hawk2">Section 6.4.1, “Creating Primitive Resources with Hawk2”</a>.
     </p></li><li class="step"><p>
      In the resource configuration dialog, go to the
      <span class="guimenu">Utilization</span> category.
     </p></li><li class="step"><p>
      From the empty drop-down box, select one of the utilization attributes
      that you have configured for the nodes in
      <a class="xref" href="#pro-hawk2-utilization-node" title="Configuring the Capacity a Node Provides">Procedure 7.7</a>.
     </p></li><li class="step"><p>
      In the empty text box next to the attribute, enter an attribute value. The
      value must be an integer.
     </p></li><li class="step"><p>
      Add as many utilization attributes as you need and add values for all of
      them.
     </p></li><li class="step"><p>
      Confirm your changes. A message at the top of the screen shows if the
      action has been successful.
     </p></li></ol></div></div><p>
    After you have configured the capacities your nodes provide and the
    capacities your resources require, set the placement strategy in
    the global cluster options. Otherwise the capacity configurations have no
    effect. Several strategies are available to schedule the load: for example,
    you can concentrate it on as few nodes as possible, or balance it evenly
    over all available nodes.
   </p><div class="procedure" id="pro-ha-config-hawk2-placement" data-id-title="Setting the Placement Strategy"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.9: </span><span class="title-name">Setting the Placement Strategy </span></span><a title="Permalink" class="permalink" href="#pro-ha-config-hawk2-placement">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Cluster Configuration</span> to open the respective screen. It shows global
      cluster options and resource and operation defaults.
     </p></li><li class="step"><p>
      From the empty drop-down box in the upper part of the screen, select
      <code class="literal">placement-strategy</code>.
     </p><p>
      By default, its value is set to <span class="guimenu">Default</span>, which means
      that utilization attributes and values are not considered.
     </p></li><li class="step"><p>
      Depending on your requirements, set <span class="guimenu">Placement Strategy</span>
      to the appropriate value.
     </p></li><li class="step"><p>
      Confirm your changes.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-utilization" data-id-title="Placing Resources Based on Their Load Impact with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">7.10.2 </span><span class="title-name">Placing Resources Based on Their Load Impact with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-utilization">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To configure the resource's requirements and the capacity a node
    provides, use utilization attributes.
    You can name the utilization attributes according to your preferences
    and define as many name/value pairs as your configuration needs. In
    certain cases, some agents update the utilization themselves, for
    example the <code class="systemitem">VirtualDomain</code>.
   </p><p>
    In the following example, we assume that you already have a basic
    configuration of cluster nodes and resources. You now additionally want
    to configure the capacities a certain node provides and the capacity a
    certain resource requires.
   </p><div class="procedure" id="id-1.4.4.5.12.20.4" data-id-title="Adding Or Modifying Utilization Attributes With crm"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 7.10: </span><span class="title-name">Adding Or Modifying Utilization Attributes With <code class="command">crm</code> </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.5.12.20.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
      To specify the capacity a node <span class="emphasis"><em>provides</em></span>, use the
      following command and replace the placeholder
      <em class="replaceable">NODE_1</em> with the name of your node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">node <em class="replaceable">NODE_1</em> utilization hv_memory=16384 cpu=8</code></pre></div><p>
      With these values, <em class="replaceable">NODE_1</em> would be assumed
      to provide 16GB of memory and 8 CPU cores to resources.
     </p></li><li class="step"><p>
      To specify the capacity a resource <span class="emphasis"><em>requires</em></span>, use:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive xen1 Xen ... \
     utilization hv_memory=4096 cpu=4</code></pre></div><p>
      This would make the resource consume 4096 of those memory units from
      <em class="replaceable">NODE_1</em>, and 4 of the CPU units.
     </p></li><li class="step"><p>
      Configure the placement strategy with the <code class="command">property</code>
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> ...</pre></div><p>
      The following values are available:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.5.12.20.4.5.4.1"><span class="term"><code class="literal">default</code> (default value)</span></dt><dd><p>
       Utilization values are not considered. Resources are allocated
       according to location scoring. If scores are equal, resources are
       evenly distributed across nodes.
      </p></dd><dt id="id-1.4.4.5.12.20.4.5.4.2"><span class="term"><code class="literal">utilization</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. However,
       load-balancing is still done based on the number of resources
       allocated to a node.
      </p></dd><dt id="id-1.4.4.5.12.20.4.5.4.3"><span class="term"><code class="literal">minimal</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to concentrate the resources on as few nodes as possible
       (to achieve power savings on the remaining nodes).
      </p></dd><dt id="id-1.4.4.5.12.20.4.5.4.4"><span class="term"><code class="literal">balanced</code>
     </span></dt><dd><p>
       Utilization values are considered when deciding if a node has enough
       free capacity to satisfy a resource's requirements. An attempt is
       made to distribute the resources evenly, thus optimizing resource
       performance.
      </p></dd></dl></div><div id="id-1.4.4.5.12.20.4.5.5" data-id-title="Configuring Resource Priorities" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Configuring Resource Priorities</div><p>
     The available placement strategies are best-effort—they do not
     yet use complex heuristic solvers to always reach optimum allocation
     results. Ensure that resource priorities are properly set so that
     your most important resources are scheduled first.
    </p></div></li><li class="step"><p>
      Commit your changes before leaving crmsh:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    The following example demonstrates a three node cluster of equal nodes,
    with 4 virtual machines:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">node alice utilization hv_memory="4000"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">node bob utilization hv_memory="4000"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">node charlie utilization hv_memory="4000"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive xenA Xen \
    utilization hv_memory="3500" meta priority="10" \
    params xmfile="/etc/xen/shared-vm/vm1"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive xenB Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm2"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive xenC Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm3"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive xenD Xen \
    utilization hv_memory="1000" meta priority="5" \
    params xmfile="/etc/xen/shared-vm/vm4"</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">property placement-strategy="minimal"</code></pre></div><p>
    With all three nodes up, xenA will be placed onto a node first, followed
    by xenD. xenB and xenC would either be allocated together or one of them
    with xenD.
   </p><p>
    If one node failed, too little total memory would be available to host
    them all. xenA would be ensured to be allocated, as would xenD. However,
    only one of xenB or xenC could still be placed, and since their priority
    is equal, the result is not defined yet. To resolve this ambiguity as
    well, you would need to set a higher priority for either one.
   </p></section></section><section class="sect1" id="sec-ha-config-basics-constraints-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">7.11 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-constraints-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_resource_constraints.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For more information on configuring constraints and detailed background
    information about the basic concepts of ordering and colocation, refer
    to the following documents at <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <em class="citetitle">Pacemaker Explained</em>, chapter <em class="citetitle">Resource Constraints</em>
     </p></li><li class="listitem"><p>
      <em class="citetitle">Colocation Explained</em>
     </p></li><li class="listitem"><p>
      <em class="citetitle">Ordering Explained</em>
     </p></li></ul></div></section></section><section xml:lang="en" class="chapter" id="cha-ha-manage-resources" data-id-title="Managing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">8 </span><span class="title-name">Managing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#cha-ha-manage-resources">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    After configuring the resources in the cluster, use the cluster management tools
    to start, stop, clean up, remove or migrate the resources. This chapter describes
    how to use Hawk2 or crmsh for resource management tasks.
   </p></div></div></div></div><section class="sect1" id="sec-ha-resource-show" data-id-title="Showing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.1 </span><span class="title-name">Showing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-show">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-ha-manual-config-show" data-id-title="Showing Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.1.1 </span><span class="title-name">Showing Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-show">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    When administering a cluster the command <code class="command">crm configure show</code>
    lists the current CIB objects like cluster configuration, global options,
    primitives, and others:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show</code>
node 178326192: alice
node 178326448: bob
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20
primitive stonith-sbd stonith:external/sbd \
        params pcmk_delay_max=30
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version=1.1.15-17.1-e174ec8 \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=true \
        placement-strategy=balanced \
        standby-mode=true
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true</pre></div><p>
    If you have lots of resources, the output of <code class="command">show</code>
    is too verbose. To restrict the output, use the name of the resource.
    For example, to list the properties of the primitive
    <code class="systemitem">admin_addr</code> only, append the resource name to
    <code class="command">show</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show admin_addr</code>
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20</pre></div><p>
    However, in some cases, you want to limit the output of specific resources
    even more. This can be achieved with <span class="emphasis"><em>filters</em></span>. Filters
    limit the output to specific components. For example, to list the
    nodes only, use <code class="literal">type:node</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show type:node</code>
node 178326192: alice
node 178326448: bob</pre></div><p>If you are also interested in primitives, use the
   <code class="literal">or</code> operator:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show type:node or type:primitive</code>
node 178326192: alice
node 178326448: bob
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20
primitive stonith-sbd stonith:external/sbd \
        params pcmk_delay_max=30</pre></div><p>
    Furthermore, to search for an object that starts with a certain string,
    use this notation:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure show type:primitive and 'admin*'</code>
primitive admin_addr IPaddr2 \
        params ip=192.168.2.1 \
        op monitor interval=10 timeout=20</pre></div><p>
    To list all available types, enter <code class="command">crm configure show type:</code>
    and press the <span class="keycap">→|</span> key. The Bash completion will give
    you a list of all types.
   </p></section></section><section class="sect1" id="sec-ha-resource-edit" data-id-title="Editing Resources and Groups"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.2 </span><span class="title-name">Editing Resources and Groups</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-edit">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can edit resources or groups using either Hawk2 or crmsh.
  </p><section class="sect2" id="sec-conf-hawk2-manage-edit" data-id-title="Editing Resources and Groups with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.2.1 </span><span class="title-name">Editing Resources and Groups with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-edit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If you have created a resource, you can edit its configuration at any time
   by adjusting parameters, operations, or meta attributes as needed.
  </p><div class="procedure" id="pro-conf-hawk2-rsc-modify" data-id-title="Modifying Parameters, Operations, or Meta Attributes for a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.1: </span><span class="title-name">Modifying Parameters, Operations, or Meta Attributes for a Resource </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-rsc-modify">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     On the Hawk2 <span class="guimenu">Status</span> screen, go to the
     <span class="guimenu">Resources</span> list.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column, click the arrow down icon
     next to the resource or group you want to modify and select
     <span class="guimenu">Edit</span>.
    </p><p>
     The resource configuration screen opens.
    </p><div class="figure" id="id-1.4.4.6.4.3.3.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-primitive-edit.png"><img src="images/hawk2-primitive-edit.png" width="100%" alt="Hawk2—Editing A Primitive Resource" title="Hawk2—Editing A Primitive Resource"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.1: </span><span class="title-name">Hawk2—Editing A Primitive Resource </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.4.3.3.4.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     At the top of the configuration screen, you can select operations to perform.
    </p><p>
     If you edit a primitive resource, the following operations are available:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Copying the resource.
      </p></li><li class="listitem"><p>
       Renaming the resource (changing its ID).
      </p></li><li class="listitem"><p>
       Deleting the resource.
      </p></li></ul></div><p>
     If you edit a group, the following operations are available:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creating a new primitive which will be added to this group.
      </p></li><li class="listitem"><p>
       Renaming the group (changing its ID).
      </p></li><li class="listitem"><p>
       Re-sort group members by dragging and dropping them into the order you
       want using the <span class="quote">“<span class="quote">handle</span>”</span> icon on the right.
      </p></li></ul></div></li><li class="step" id="step-hawk2-rsc-modify-params"><p>
     To add a new parameter, operation, or meta attribute, select an entry from
     the empty drop-down box.
    </p></li><li class="step"><p>
     To edit any values in the <span class="guimenu">Operations</span> category, click
     the <span class="guimenu">Edit</span> icon of the respective entry, enter a
     different value for the operation, and click <span class="guimenu">Apply</span>.
    </p></li><li class="step"><p>
     When you are finished, click the <span class="guimenu">Apply</span> button in the
     resource configuration screen to confirm your changes to the parameters,
     operations, or meta attributes.
    </p><p>
     A message at the top of the screen shows if the action has been
     successful.
    </p></li></ol></div></div></section><section class="sect2" id="sec-conf-crm-manage-edit" data-id-title="Editing Groups with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.2.2 </span><span class="title-name">Editing Groups with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-conf-crm-manage-edit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To change the order of a group member, use the
  <code class="command">modgroup</code> command from the
  <code class="command">configure</code> subcommand. For example, use the following commands to
  move the primitive <code class="literal">Email</code> before
  <code class="literal">Public-IP</code>.
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-mailsvc add Email before Public-IP</code></pre></div><p>
  In case you want to remove a resource from a group (for example,
  <code class="literal">Email</code>), use this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup g-mailsvc remove Email</code></pre></div></section></section><section class="sect1" id="sec-ha-resource-start" data-id-title="Starting Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.3 </span><span class="title-name">Starting Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-start">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you start a cluster resource, make sure it is set up correctly. For
   example, if you use an Apache server as a cluster resource, set up the
   Apache server first. Complete the Apache configuration before starting the
   respective resource in your cluster.
  </p><div id="id-1.4.4.6.5.3" data-id-title="Do Not Touch Services Managed by the Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Do Not Touch Services Managed by the Cluster</div><p>
    When managing a resource via the High Availability software, the resource must not be started
    or stopped otherwise (outside the cluster, for example manually or on
    boot or reboot). The High Availability software is responsible for all service start
    or stop actions.
   </p><p>
    However, if you want to check if the service is configured properly, start
    it manually, but make sure that it is stopped again before the High Availability software takes
    over.
   </p><p>
    For interventions in resources that are currently managed by the cluster,
    set the resource to <code class="literal">maintenance mode</code> first. For details,
    see <a class="xref" href="#pro-ha-maint-mode-rsc-hawk2" title="Putting a Resource into Maintenance Mode with Hawk2">Procedure 27.5, “Putting a Resource into Maintenance Mode with Hawk2”</a>.
   </p></div><p>
   You can start a cluster resource using either Hawk2 or crmsh.
  </p><section class="sect2" id="sec-conf-hawk2-manage-start" data-id-title="Starting Cluster Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.3.1 </span><span class="title-name">Starting Cluster Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When creating a resource with Hawk2, you can set its initial state with
   the <code class="literal">target-role</code> meta attribute. If you set its value to
   <code class="literal">stopped</code>, the resource does not start automatically after
   being created.
  </p><div class="procedure" id="pro-hawk2-rsc-start" data-id-title="Starting A New Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.2: </span><span class="title-name">Starting A New Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-start">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     Select the resource to start. In its <span class="guimenu">Operations</span> column
     click the <span class="guimenu">Start</span> icon. To continue, confirm the message
     that appears.
    </p><p>
     When the resource has started, Hawk2 changes the resource's
     <span class="guimenu">Status</span> to green and shows on which node it is running.
    </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-start" data-id-title="Starting Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.3.2 </span><span class="title-name">Starting Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-start">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To start a new cluster resource you need the respective identifier.
   Proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
     interactive shell:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code></pre></div></li><li class="step"><p>
     Switch to the resource level:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource</code></pre></div></li><li class="step"><p>
     Start the resource with <code class="command">start</code> and press the
     <span class="keycap">→|</span> key to show all known resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code><code class="command">start <em class="replaceable">ID</em></code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-resource-stop" data-id-title="Stopping Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.4 </span><span class="title-name">Stopping Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-stop">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="sec-ha-manual-config-stop" data-id-title="Stopping Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.4.1 </span><span class="title-name">Stopping Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-stop">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To stop one or more existing cluster resources you need the respective identifier(s).
   Proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
     interactive shell:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code></pre></div></li><li class="step"><p>
     Switch to the resource level:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource</code></pre></div></li><li class="step"><p>
     Stop the resource with <code class="command">stop</code> and press the
     <span class="keycap">→|</span> key to show all known resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code><code class="command">stop <em class="replaceable">ID</em></code></pre></div><p>
     You can stop multiple resources at once:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code><code class="command">stop <em class="replaceable">ID1</em> <em class="replaceable">ID2</em></code> ...</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-resource-clean" data-id-title="Cleaning Up Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.5 </span><span class="title-name">Cleaning Up Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-clean">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A resource is automatically restarted if it fails, but each failure
   increases the resource's fail count.
  </p><p>
   If a <code class="literal">migration-threshold</code> has been set for the resource,
   the node will no longer run the resource when the number of failures reaches
   the migration threshold.
  </p><p>
   By default, fail counts are not automatically reset. You can configure a fail count
   to be reset automatically by setting a <code class="literal">failure-timeout</code> option for the
   resource, or you can manually reset the fail count using either Hawk2 or crmsh.
  </p><section class="sect2" id="sec-conf-hawk2-manage-cleanup" data-id-title="Cleaning Up Cluster Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.5.1 </span><span class="title-name">Cleaning Up Cluster Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-cleanup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-clean" data-id-title="Cleaning Up A Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.3: </span><span class="title-name">Cleaning Up A Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-clean">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     Go to the resource to clean up. In the <span class="guimenu">Operations</span>
     column click the arrow down button and select <span class="guimenu">Cleanup</span>.
     To continue, confirm the message that appears.
    </p><p>
     This executes the command <code class="command">crm resource cleanup</code> and
     cleans up the resource on all nodes.
    </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-cleanup" data-id-title="Cleaning Up Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.5.2 </span><span class="title-name">Cleaning Up Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-cleanup">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.4.6.7.6.2" data-id-title="Cleaning up a resource with crmsh"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.4: </span><span class="title-name">Cleaning up a resource with crmsh </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.7.6.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a shell and log in as user <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Get a list of all your resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource status</code>
Full List of Resources
   * admin-ip      (ocf:heartbeat:IPaddr2):    Started
   * stonith-sbd   (stonith:external/sbd):     Started
   * Resource Group: dlm-clvm:
     * dlm:        (ocf:pacemaker:controld)    Started
     * clvm:       (ocf:heartbeat:lvmlockd)    Started</pre></div></li><li class="step"><p>
        Show the fail count of a resource:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource failcount <em class="replaceable">RESOURCE</em> show <em class="replaceable">NODE</em></code></pre></div><p>
        For example, to show the fail count of the resource <code class="literal">dlm</code> on node
        <code class="literal">alice</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource failcount dlm show alice</code>
scope=status name=fail-count-dlm value=2</pre></div></li><li class="step"><p>
      Clean up the resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource cleanup <em class="replaceable">RESOURCE</em></code></pre></div><p>
        This command cleans up the resource on all nodes. If the resource is part of a group,
        crmsh also cleans up the other resources in the group.
      </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-resource-remove" data-id-title="Removing Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.6 </span><span class="title-name">Removing Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-remove">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To remove a resource from the cluster, follow either the
   Hawk2 or crmsh procedure below to avoid configuration errors:
  </p><section class="sect2" id="sec-conf-hawk2-manage-remove" data-id-title="Removing Cluster Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.6.1 </span><span class="title-name">Removing Cluster Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-remove">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-rsc-rm" data-id-title="Removing a Cluster Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.5: </span><span class="title-name">Removing a Cluster Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-rm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     Clean up the resource on all nodes as described in
     <a class="xref" href="#pro-hawk2-clean" title="Cleaning Up A Resource">Procedure 8.3, “Cleaning Up A Resource”</a>.
    </p></li><li class="step"><p>
     Stop the resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>. The list
       of <span class="guimenu">Resources</span> also shows the
       <span class="guimenu">Status</span>.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column click the
       <span class="guimenu">Stop</span> button next to the resource.
      </p></li><li class="step"><p>
       To continue, confirm the message that appears.
      </p><p>
       The <span class="guimenu">Status</span> column will reflect the change when the
       resource is stopped.
      </p></li></ol></li><li class="step"><p>
     Delete the resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       From the left navigation bar, select
       <span class="guimenu">Configuration</span> › <span class="guimenu">Edit
       Configuration</span>.
      </p></li><li class="step"><p>
       In the list of <span class="guimenu">Resources</span>, go to the respective
       resource. From the <span class="guimenu">Operations</span> column click the
       <span class="guimenu">Delete</span> icon next to the resource.
      </p></li><li class="step"><p>
       To continue, confirm the message that appears.
      </p></li></ol></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-remove" data-id-title="Removing Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.6.2 </span><span class="title-name">Removing Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-remove">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.4.6.8.4.2" data-id-title="Removing a cluster resource with crmsh"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.6: </span><span class="title-name">Removing a cluster resource with crmsh </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.8.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> and start the <code class="command">crm</code>
      interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code></pre></div></li><li class="step"><p>
      Get a list of your resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource status</code>
Full List of Resources:
  * admin-ip     (ocf:heartbeat:IPaddr2):     Started
  * stonith-sbd  (stonith:external/sbd):      Started
  * nfsserver    (ocf:heartbeat:nfsserver):   Started</pre></div></li><li class="step"><p>
        Stop the resource you want to remove:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">resource stop <em class="replaceable">RESOURCE</em></code></pre></div></li><li class="step"><p>
      Delete the resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)# </code><code class="command">configure delete <em class="replaceable">RESOURCE</em></code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-resource-migrate" data-id-title="Migrating Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.7 </span><span class="title-name">Migrating Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-resource-migrate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The cluster will fail over (migrate) resources automatically in case of software or hardware
   failures, according to certain parameters you can define (for example,
   migration threshold or resource stickiness). You can also manually migrate a
   resource to another node in the cluster, or move the resource
   away from the current node and let the cluster decide where to put it.
  </p><p>
   You can migrate a cluster resource using either Hawk2 or crmsh.
  </p><section class="sect2" id="sec-conf-hawk2-manage-migrate" data-id-title="Migrating Cluster Resources with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.7.1 </span><span class="title-name">Migrating Cluster Resources with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-hawk2-rsc-migrate" data-id-title="Manually Migrating a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.7: </span><span class="title-name">Manually Migrating a Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-migrate">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     In the list of <span class="guimenu">Resources</span>, select the respective
     resource.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column click the arrow down button
     and select <span class="guimenu">Migrate</span>.
    </p></li><li class="step"><p>
     In the window that opens you have the following choices:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="guimenu">Away from current node</span>: This creates a location
       constraint with a <code class="literal">-INFINITY</code> score for the current
       node.
      </p></li><li class="listitem"><p>
       Alternatively, you can move the resource to another node. This creates a
       location constraint with an <code class="literal">INFINITY</code> score for the
       destination node.
      </p></li></ul></div></li><li class="step"><p>
     Confirm your choice.
    </p></li></ol></div></div><p>
   To allow a resource to move back again, proceed as follows:
  </p><div class="procedure" id="pro-hawk2-rsc-migrate-back" data-id-title="Unmigrating a Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.8: </span><span class="title-name">Unmigrating a Resource </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-rsc-migrate-back">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>. The list of
     <span class="guimenu">Resources</span> also shows the <span class="guimenu">Status</span>.
    </p></li><li class="step"><p>
     In the list of <span class="guimenu">Resources</span>, go to the respective
     resource.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column click the arrow down button
     and select <span class="guimenu">Clear</span>. To continue, confirm the message
     that appears.
    </p><p>
     Hawk2 uses the <code class="command">crm_resource --clear</code>
     command. The resource can move back to its original location or it may
     stay where it is (depending on resource stickiness).
    </p></li></ol></div></div><p>
   For more information, see  <em class="citetitle">Pacemaker Explained</em>, available from
   <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>. Refer to section
   <em class="citetitle">Resource Migration</em>.
  </p></section><section class="sect2" id="sec-ha-manual-config-migrate" data-id-title="Migrating Cluster Resources with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.7.2 </span><span class="title-name">Migrating Cluster Resources with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-migrate">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Use the <code class="command">move</code> command for this task. For example,
    to migrate the resource <code class="literal">ipaddress1</code> to a cluster node
    named <code class="systemitem">bob</code>, use these
    commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource</code>
<code class="prompt custom">crm(live)resource# </code><code class="command">move ipaddress1 bob</code></pre></div></section></section><section class="sect1" id="sec-ha-config-basics-tags" data-id-title="Grouping Resources by Using Tags"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">8.8 </span><span class="title-name">Grouping Resources by Using Tags</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-tags">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Tags are a way to refer to multiple resources at once, without creating any
   colocation or ordering relationship between them. This can be useful for
   grouping conceptually related resources. For example, if you have
   several resources related to a database, create a tag called
   <code class="literal">databases</code> and add all resources related to the
   database to this tag. This allows you to stop or start them all with a
   single command.
  </p><p>
   Tags can also be used in constraints. For example, the following
   location constraint <code class="literal">loc-db-prefer</code> applies to the set
   of resources tagged with <code class="literal">databases</code>:
  </p><div class="verbatim-wrap"><pre class="screen">location loc-db-prefer databases 100: alice</pre></div><p>
   You can create tags using either Hawk2 or crmsh.
  </p><section class="sect2" id="sec-conf-hawk2-rsc-tag" data-id-title="Grouping Resources by Using Tags with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.8.1 </span><span class="title-name">Grouping Resources by Using Tags with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-rsc-tag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="pro-conf-hawk2-tag" data-id-title="Adding a Tag"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 8.9: </span><span class="title-name">Adding a Tag </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-tag">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2:
     </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Add
      Resource</span> › <span class="guimenu">Tag</span>.
     </p></li><li class="step"><p>
      Enter a unique <span class="guimenu">Tag ID</span>.
     </p></li><li class="step"><p>
      From the <span class="guimenu">Objects</span> list, select the resources you want to
      refer to with the tag.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Create</span> to finish the configuration. A message at
      the top of the screen shows if the action has been successful.
     </p></li></ol></div></div><div class="figure" id="id-1.4.4.6.10.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-tag.png"><img src="images/hawk2-tag.png" width="100%" alt="Hawk2—Tag" title="Hawk2—Tag"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 8.2: </span><span class="title-name">Hawk2—Tag </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.6.10.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-ha-manual-config-tag" data-id-title="Grouping Resources by Using Tags with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">8.8.2 </span><span class="title-name">Grouping Resources by Using Tags with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-tag">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_managing_resources.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For example, if you have
    several resources related to a database, create a tag called
    <code class="literal">databases</code> and add all resources related to the
    database to this tag:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure tag databases: db1 db2 db3</code></pre></div><p>
    This allows you to start them all with a single command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource start databases</code></pre></div><p>
    Similarly, you can stop them all too:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource stop databases</code></pre></div></section></section></section><section xml:lang="en" class="chapter" id="sec-ha-config-basics-remote" data-id-title="Managing Services on Remote Hosts"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">9 </span><span class="title-name">Managing Services on Remote Hosts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_remote_hosts.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The possibilities for monitoring and managing services on remote hosts
    has become increasingly important during the last few years.
    SUSE Linux Enterprise High Availability 11 SP3 offered fine-grained monitoring of services on
    remote hosts via monitoring plug-ins. The recent addition of the
    <code class="literal">pacemaker_remote</code> service now allows SUSE Linux Enterprise High Availability
    15 SP2 to fully manage and monitor resources on remote hosts
    just as if they were a real cluster node—without the need to
    install the cluster stack on the remote machines.
   </p></div></div></div></div><section class="sect1" id="sec-ha-config-basics-remote-nagios" data-id-title="Monitoring Services on Remote Hosts with Monitoring Plug-ins"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.1 </span><span class="title-name">Monitoring Services on Remote Hosts with Monitoring Plug-ins</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote-nagios">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_remote_hosts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Monitoring of virtual machines can be done with the VM agent (which only
    checks if the guest shows up in the hypervisor), or by external scripts
    called from the VirtualDomain or Xen agent. Up to now, more fine-grained
    monitoring was only possible with a full setup of the High Availability stack
    within the virtual machines.
   </p><p>
    By providing support for monitoring plug-ins (formerly named Nagios
    plug-ins), SUSE Linux Enterprise High Availability now also allows you to monitor services on
    remote hosts. You can collect external statuses on the guests without
    modifying the guest image. For example, VM guests might run Web services
    or simple network resources that need to be accessible. With the Nagios
    resource agents, you can now monitor the Web service or the network
    resource on the guest. If these services are not reachable anymore,
    SUSE Linux Enterprise High Availability triggers a restart or migration of the respective guest.
   </p><p>
    If your guests depend on a service (for example, an NFS server to be
    used by the guest), the service can either be an ordinary resource,
    managed by the cluster, or an external service that is monitored with
    Nagios resources instead.
   </p><p>
    To configure the Nagios resources, the following packages must be
    installed on the host:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="package">monitoring-plugins</span>
     </p></li><li class="listitem"><p>
      <span class="package">monitoring-plugins-metadata</span>
     </p></li></ul></div><p>
    YaST or Zypper will resolve any dependencies on further packages,
    if required.
   </p><p>
    A typical use case is to configure the monitoring plug-ins as resources
    belonging to a resource container, which usually is a VM. The container
    will be restarted if any of its resources has failed. Refer to
    <a class="xref" href="#ex-ha-nagios-config" title="Configuring Resources for Monitoring Plug-ins">Example 9.1, “Configuring Resources for Monitoring Plug-ins”</a> for a configuration example.
    Alternatively, Nagios resource agents can also be configured as ordinary
    resources to use them for monitoring hosts or services via the network.
   </p><div class="complex-example"><div class="example" id="ex-ha-nagios-config" data-id-title="Configuring Resources for Monitoring Plug-ins"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 9.1: </span><span class="title-name">Configuring Resources for Monitoring Plug-ins </span></span><a title="Permalink" class="permalink" href="#ex-ha-nagios-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_remote_hosts.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">primitive vm1 VirtualDomain \
    params hypervisor="qemu:///system" config="/etc/libvirt/qemu/vm1.xml" \
    op start interval="0" timeout="90" \
    op stop interval="0" timeout="90" \
    op monitor interval="10" timeout="30"
primitive vm1-sshd nagios:check_tcp \
    params hostname="vm1" port="22" \ <span class="callout" id="co-nagios-hostname">1</span>
    op start interval="0" timeout="120" \ <span class="callout" id="co-nagios-startinterval">2</span>
    op monitor interval="10"
group g-vm1-and-services vm1 vm1-sshd \
    meta container="vm1" <span class="callout" id="co-nagios-container">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-hostname"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The supported parameters are the same as the long options of a
       monitoring plug-in. Monitoring plug-ins connect to services with the
       parameter <code class="literal">hostname</code>. Therefore the attribute's
       value must be a resolvable host name or an IP address.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-startinterval"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       As it takes some time to get the guest operating system up and its
       services running, the start timeout of the monitoring resource must
       be long enough.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-nagios-container"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       A cluster resource container of type
       <code class="literal">ocf:heartbeat:Xen</code>,
       <code class="literal">ocf:heartbeat:VirtualDomain</code> or
       <code class="literal">ocf:heartbeat:lxc</code>. It can either be a VM or a
       Linux Container.
      </p></td></tr></table></div><p>
     The example above contains only one resource for the
     <code class="literal">check_tcp</code>plug-in, but multiple resources for
     different plug-in types can be configured (for example,
     <code class="literal">check_http</code> or <code class="literal">check_udp</code>).
    </p><p>
     If the host names of the services are the same, the
     <code class="literal">hostname</code> parameter can also be specified for the
     group, instead of adding it to the individual primitives. For example:
    </p><div class="verbatim-wrap"><pre class="screen">group g-vm1-and-services vm1 vm1-sshd vm1-httpd \
     meta container="vm1" \
     params hostname="vm1"</pre></div><p>
     If any of the services monitored by the monitoring plug-ins fail within
     the VM, the cluster will detect that and restart the container resource
     (the VM). Which action to take in this case can be configured by
     specifying the <code class="literal">on-fail</code> attribute for the service's
     monitoring operation. It defaults to
     <code class="literal">restart-container</code>.
    </p><p>
     Failure counts of services will be taken into account when considering
     the VM's migration-threshold.
    </p></div></div></div></section><section class="sect1" id="sec-ha-config-basics-remote-pace-remote" data-id-title="Managing Services on Remote Nodes with pacemaker_remote"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">9.2 </span><span class="title-name">Managing Services on Remote Nodes with <code class="literal">pacemaker_remote</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-remote-pace-remote">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_remote_hosts.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    With the <code class="literal">pacemaker_remote</code> service, High Availability clusters
    can be extended to virtual nodes or remote bare-metal machines. They do
    not need to run the cluster stack to become members of the cluster.
   </p><p>
    SUSE Linux Enterprise High Availability can now launch virtual environments (KVM and LXC), plus
    the resources that live within those virtual environments without
    requiring the virtual environments to run Pacemaker or Corosync.
   </p><p>
    For the use case of managing both virtual machines as cluster resources
    plus the resources that live within the VMs, you can now use the
    following setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The <span class="quote">“<span class="quote">normal</span>”</span> (bare-metal) cluster nodes run
      SUSE Linux Enterprise High Availability.
     </p></li><li class="listitem"><p>
      The virtual machines run the <code class="literal">pacemaker_remote</code>
      service (almost no configuration required on the VM's side).
     </p></li><li class="listitem"><p>
      The cluster stack on the <span class="quote">“<span class="quote">normal</span>”</span> cluster nodes launches
      the VMs and connects to the <code class="literal">pacemaker_remote</code>
      service running on the VMs to integrate them as remote nodes into the
      cluster.
     </p></li></ul></div><p>
    As the remote nodes do not have the cluster stack installed, this has
    the following implications:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Remote nodes do not take part in quorum.
     </p></li><li class="listitem"><p>
      Remote nodes cannot become the DC.
     </p></li><li class="listitem"><p>
      Remote nodes are not bound by the scalability limits (Corosync
      has a member limit of 32 nodes).
     </p></li></ul></div><p>
    Find more information about the <code class="literal">remote_pacemaker</code>
    service, including multiple use cases with detailed setup instructions
    in <span class="intraxref">Article “Pacemaker Remote Quick Start”</span>.
   </p></section></section><section class="chapter" id="cha-ha-agents" data-id-title="Adding or Modifying Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Adding or Modifying Resource Agents</span></span> <a title="Permalink" class="permalink" href="#cha-ha-agents">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    All tasks that need to be managed by a cluster must be available as a
    resource. There are two major groups here to consider: resource agents
    and STONITH agents. For both categories, you can add your own
    agents, extending the abilities of the cluster to your own needs.
   </p></div></div></div></div><section class="sect1" id="sec-ha-stonithagents" data-id-title="STONITH Agents"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">STONITH Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-stonithagents">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A cluster sometimes detects that one of the nodes is behaving strangely
   and needs to remove it. This is called <span class="emphasis"><em>fencing</em></span> and
   is commonly done with a STONITH resource. 
  </p><div id="id-1.4.4.8.3.3" data-id-title="External SSH/STONITH Are Not Supported" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: External SSH/STONITH Are Not Supported</div><p>
    It is impossible to know how SSH might react to other system problems.
    For this reason, external SSH/STONITH agents (like
    <code class="literal">stonith:external/ssh</code>) are not supported for
    production environments. If you still want to use such agents for
    testing, install the
    <span class="package">libglue-devel</span> package.
   </p></div><p>
   To get a list of all currently available STONITH devices (from the
   software side), use the command <code class="command">crm ra list stonith</code>.
   If you do not find your favorite agent, install the
   <span class="package">-devel</span> package.
   For more information on STONITH devices and resource agents, 
   see <a class="xref" href="#cha-ha-fencing" title="Chapter 12. Fencing and STONITH">Chapter 12, <em>Fencing and STONITH</em></a>.
  </p><p>
   As of yet there is no documentation about writing STONITH agents. If
   you want to write new STONITH agents, consult the examples available
   in the source of the
   <span class="package">cluster-glue</span> package.
  </p></section><section class="sect1" id="sec-ha-writingresourceagents" data-id-title="Writing OCF Resource Agents"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Writing OCF Resource Agents</span></span> <a title="Permalink" class="permalink" href="#sec-ha-writingresourceagents">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_agents.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All OCF resource agents (RAs) are available in
   <code class="filename">/usr/lib/ocf/resource.d/</code>, see
   <a class="xref" href="#sec-ha-config-basics-raclasses" title="6.2. Supported Resource Agent Classes">Section 6.2, “Supported Resource Agent Classes”</a> for more information.
   Each resource agent must supported the following operations to control
   it:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.8.4.3.1"><span class="term"><code class="command">start</code>
    </span></dt><dd><p>
      start or enable the resource
     </p></dd><dt id="id-1.4.4.8.4.3.2"><span class="term"><code class="command">stop</code>
    </span></dt><dd><p>
      stop or disable the resource
     </p></dd><dt id="id-1.4.4.8.4.3.3"><span class="term"><code class="command">status</code>
    </span></dt><dd><p>
      returns the status of the resource
     </p></dd><dt id="id-1.4.4.8.4.3.4"><span class="term"><code class="command">monitor</code>
    </span></dt><dd><p>
      similar to <code class="command">status</code>, but checks also for unexpected
      states
     </p></dd><dt id="id-1.4.4.8.4.3.5"><span class="term"><code class="command">validate</code>
    </span></dt><dd><p>
      validate the resource's configuration
     </p></dd><dt id="id-1.4.4.8.4.3.6"><span class="term"><code class="command">meta-data</code>
    </span></dt><dd><p>
      returns information about the resource agent in XML
     </p></dd></dl></div><p>
   The general procedure of how to create an OCF RA is like the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Load the file
     <code class="filename">/usr/lib/ocf/resource.d/pacemaker/Dummy</code> as a
     template.
    </p></li><li class="step"><p>
     Create a new subdirectory for each new resource agents to avoid naming
     contradictions. For example, if you have a resource group
     <code class="systemitem">kitchen</code> with the resource
     <code class="systemitem">coffee_machine</code>, add this resource to the directory
     <code class="filename">/usr/lib/ocf/resource.d/kitchen/</code>. To access this
     RA, execute the command <code class="command">crm</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure primitive coffee_1 ocf:coffee_machine:kitchen ...</pre></div></li><li class="step"><p>
     Implement the different shell functions and save your file under a
     different name.
    </p></li></ol></div></div><p>
   More details about writing OCF resource agents can be found at
   <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a> in the guide
   <em class="citetitle">Pacemaker Administration</em>. Find
   special information about several concepts at
   <a class="xref" href="#cha-ha-concepts" title="Chapter 1. Product Overview">Chapter 1, <em>Product Overview</em></a>.
  </p></section><section class="sect1" id="sec-ha-errorcodes" data-id-title="OCF Return Codes and Failure Recovery"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">OCF Return Codes and Failure Recovery</span></span> <a title="Permalink" class="permalink" href="#sec-ha-errorcodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  According to the OCF specification, there are strict definitions of the
  exit codes an action must return. The cluster always checks the return
  code against the expected result. If the result does not match the
  expected value, then the operation is considered to have failed and a
  recovery action is initiated. There are three types of failure recovery:
 </p><div class="table" id="id-1.4.4.8.5.4" data-id-title="Failure Recovery Types"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 10.1: </span><span class="title-name">Failure Recovery Types </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.8.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Recovery Type
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Description
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Action Taken by the Cluster
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       A transient error occurred.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Restart the resource or move it to a new location.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       A non-transient error occurred. The error may be specific to the
       current node.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       Move the resource elsewhere and prevent it from being retried on the
       current node.
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       fatal
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       A non-transient error occurred that will be common to all cluster
       nodes. This means a bad configuration was specified.
      </p>
     </td><td>
      <p>
       Stop the resource and prevent it from being started on any cluster
       node.
      </p>
     </td></tr></tbody></table></div></div><p>
  Assuming an action is considered to have failed, the following table
  outlines the different OCF return codes. It also shows the type of
  recovery the cluster will initiate when the respective error code is
  received.
 </p><div class="table" id="id-1.4.4.8.5.7" data-id-title="OCF Return Codes"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 10.2: </span><span class="title-name">OCF Return Codes </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.8.5.7">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_error_codes.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/><col class="4"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF Return Code
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF Alias
      </p>
     </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Description
      </p>
     </th><th style="border-bottom: 1px solid ; ">
      <p>
       Recovery Type
      </p>
     </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       0
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_SUCCESS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Success. The command completed successfully. This is the expected
       result for all start, stop, promote and demote commands.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       1
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­GENERIC
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       Generic <span class="quote">“<span class="quote">there was a problem</span>”</span> error code.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       2
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_ARGS
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource’s configuration is not valid on this machine (for
       example, it refers to a location/tool not found on the node).
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       3
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_­ERR_­UN­IMPLEMENTED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The requested action is not implemented.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       4
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_PERM
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource agent does not have sufficient privileges to complete
       the task.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       5
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­INSTALLED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The tools required by the resource are not installed on this machine.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       hard
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       6
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_ERR_­CONFIGURED
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource’s configuration is invalid (for example, required
       parameters are missing).
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       fatal
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       7
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_NOT_­RUNNING
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is not running. The cluster will not attempt to stop a
       resource that returns this for any action.
      </p>
      <p>
       This OCF return code may or may not require resource
       recovery—it depends on what is the expected resource status.
       If unexpected, then <code class="literal">soft</code> recovery.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       N/A
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       8
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_RUNNING_­MASTER
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is running in Master mode.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       9
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       OCF_FAILED_­MASTER
      </p>
     </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
      <p>
       The resource is in Master mode but has failed. The resource will be
       demoted, stopped and then started (and possibly promoted) again.
      </p>
     </td><td style="border-bottom: 1px solid ; ">
      <p>
       soft
      </p>
     </td></tr><tr><td style="border-right: 1px solid ; ">
      <p>
       other
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       N/A
      </p>
     </td><td style="border-right: 1px solid ; ">
      <p>
       Custom error code.
      </p>
     </td><td>
      <p>
       soft
      </p>
     </td></tr></tbody></table></div></div></section></section><section xml:lang="en" class="chapter" id="cha-ha-monitor-clusters" data-id-title="Monitoring Clusters"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">11 </span><span class="title-name">Monitoring Clusters</span></span> <a title="Permalink" class="permalink" href="#cha-ha-monitor-clusters">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_monitoring_clusters.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter describes how to monitor a cluster's health and view its history.
   </p></div></div></div></div><section class="sect1" id="sec-conf-hawk2-monitor" data-id-title="Monitoring Cluster Status"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.1 </span><span class="title-name">Monitoring Cluster Status</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 has different screens for monitoring single clusters and multiple
  clusters: the <span class="guimenu">Status</span> and the <span class="guimenu">Dashboard</span>
  screen.
 </p><section class="sect2" id="sec-conf-hawk2-manage-monitor-status" data-id-title="Monitoring a Single Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.1.1 </span><span class="title-name">Monitoring a Single Cluster</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-monitor-status">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To monitor a single cluster, use the <span class="guimenu">Status</span> screen. After
   you have logged in to Hawk2, the <span class="guimenu">Status</span> screen is
   displayed by default. An icon in the upper right corner shows the cluster
   status at a glance. For further details, have a look at the following
   categories:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.9.3.4.3.1"><span class="term">Errors</span></dt><dd><p>
      If errors have occurred, they are shown at the top of the page.
     </p></dd><dt id="id-1.4.4.9.3.4.3.2"><span class="term">Resources</span></dt><dd><p>
      Shows the configured resources including their <span class="guimenu">Status</span>,
      <span class="guimenu">Name</span> (ID), <span class="guimenu">Location</span> (node on which
      they are running), and resource agent <span class="guimenu">Type</span>. From the
      <span class="guimenu">Operations</span> column, you can start or stop a resource,
      trigger several actions, or view details. Actions that can be triggered
      include setting the resource to maintenance mode (or removing maintenance
      mode), migrating it to a different node, cleaning up the resource,
      showing any recent events, or editing the resource.
     </p></dd><dt id="id-1.4.4.9.3.4.3.3"><span class="term">Nodes</span></dt><dd><p>
      Shows the nodes belonging to the cluster site you are logged in to,
      including the nodes' <span class="guimenu">Status</span> and
      <span class="guimenu">Name</span>. In the <span class="guimenu">Maintenance</span> and
      <span class="guimenu">Standby</span> columns, you can set or remove the
      <code class="literal">maintenance</code> or <code class="literal">standby</code> flag for a
      node. The <span class="guimenu">Operations</span> column allows you to view recent
      events for the node or further
      details: for example, if a <code class="literal">utilization</code>,
      <code class="literal">standby</code> or <code class="literal">maintenance</code> attribute is
      set for the respective node.
     </p></dd><dt id="id-1.4.4.9.3.4.3.4"><span class="term">Tickets</span></dt><dd><p>
      Only shown if tickets have been configured (for use with Geo
      clustering).
     </p></dd></dl></div><div class="figure" id="id-1.4.4.9.3.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-cluster-status-main.png"><img src="images/hawk2-cluster-status-main.png" width="100%" alt="Hawk2—Cluster Status" title="Hawk2—Cluster Status"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.1: </span><span class="title-name">Hawk2—Cluster Status </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.9.3.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></section><section class="sect2" id="sec-conf-hawk2-manage-monitor-dash" data-id-title="Monitoring Multiple Clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.1.2 </span><span class="title-name">Monitoring Multiple Clusters</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-manage-monitor-dash">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To monitor multiple clusters, use the Hawk2 <span class="guimenu">Dashboard</span>.
   The cluster information displayed in the <span class="guimenu">Dashboard</span> screen
   is stored on the server side. It is synchronized between the cluster nodes (if
   passwordless SSH access between the cluster nodes has been configured). For
   details, see <a class="xref" href="#sec-crmreport-nonroot-ssh" title="D.2. Configuring a Passwordless SSH Account">Section D.2, “Configuring a Passwordless SSH Account”</a>. However, the
   machine running Hawk2 does not even need to be part of any
   cluster for that purpose—it can be a separate, unrelated system.
  </p><p>
   In addition to the general
   <a class="xref" href="#sec-conf-hawk2-req" title="5.4.1. Hawk2 Requirements">Hawk2 Requirements</a>, the following
   prerequisites need to be fulfilled to monitor multiple clusters with
   Hawk2:
  </p><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Prerequisites </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.9.3.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     All clusters to be monitored from Hawk2's <span class="guimenu">Dashboard</span>
     must be running SUSE Linux Enterprise High Availability 15 SP2.
    </p></li><li class="listitem"><p>
     If you did not replace the self-signed certificate for Hawk2 on every
     cluster node with your own certificate (or a certificate signed by an
     official Certificate Authority) yet, do the following: Log in to Hawk2 on
     <span class="emphasis"><em>every</em></span> node in <span class="emphasis"><em>every</em></span> cluster
     at least once. Verify the certificate (or add an exception in the
     browser to bypass the warning). Otherwise Hawk2 cannot connect to the
     cluster.
    </p></li></ul></div><div class="procedure" id="pro-conf-hawk2-dashboard" data-id-title="Monitoring Multiple Clusters with the Dashboard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.1: </span><span class="title-name">Monitoring Multiple Clusters with the Dashboard </span></span><a title="Permalink" class="permalink" href="#pro-conf-hawk2-dashboard">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Dashboard</span>.
    </p><p>
    Hawk2 shows an overview of the resources and nodes on the current
    cluster site. In addition, it shows any <span class="guimenu">Tickets</span> that
    have been configured for use with a Geo cluster. If you need information
    about the icons used in this view, click <span class="guimenu">Legend</span>. To
    search for a resource ID, enter the name (ID) into the <span class="guimenu">Search</span>
    text box. To only show specific nodes, click the filter icon and select a
    filtering option.
   </p><div class="figure" id="id-1.4.4.9.3.5.5.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-dashboard-site1.png"><img src="images/hawk2-dashboard-site1.png" width="95%" alt="Hawk2 Dashboard with One Cluster Site (amsterdam)" title="Hawk2 Dashboard with One Cluster Site (amsterdam)"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.2: </span><span class="title-name">Hawk2 Dashboard with One Cluster Site (<code class="literal">amsterdam</code>) </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.9.3.5.5.3.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_monitor_i.xml" title="Edit source document"> </a></div></div></div></li><li class="step"><p>
     To add dashboards for multiple clusters:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Add Cluster</span>.
      </p></li><li class="step"><p>
       Enter the <span class="guimenu">Cluster name</span> with which to identify the
       cluster in the <span class="guimenu">Dashboard</span>. For example,
       <code class="literal">berlin</code>.
      </p></li><li class="step"><p>
       Enter the fully qualified host name of one of the nodes in the second
       cluster. For example, <code class="literal">charlie</code>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-dashboard-add-cluster.png"><img src="images/hawk2-dashboard-add-cluster.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Click <span class="guimenu">Add</span>. Hawk2 will display a second tab for the
       newly added cluster site with an overview of its nodes and resources.
      </p><div id="id-1.4.4.9.3.5.5.4.2.4.2" data-id-title="Connection Error" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Connection Error</div><p>If instead you are prompted to log in to this node by entering a
        password, you probably did not connect to this node yet and have not
        replaced the self-signed certificate. In that case, even after entering
        the password, the connection will fail with the following message:
        <code class="literal">Error connecting to server. Retrying every 5 seconds... '</code>.
       </p><p>
        To proceed, see <a class="xref" href="#vle-trouble-hawk2-cert">Replacing the Self-Signed Certificate</a>.
       </p></div></li></ol></li><li class="step"><p>
     To view more details for a cluster site or to manage it, switch to the
     site's tab and click the chain icon.
    </p><p>
     Hawk2 opens the <span class="guimenu">Status</span> view for this site in a new
     browser window or tab. From there, you can administer this part of the
     Geo cluster.
    </p></li><li class="step"><p>
     To remove a cluster from the dashboard, click the <code class="literal">x</code>
     icon on the right-hand side of the cluster's details.
    </p></li></ol></div></div></section></section><section class="sect1" id="sec-conf-health" data-id-title="Verifying Cluster Health"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.2 </span><span class="title-name">Verifying Cluster Health</span></span> <a title="Permalink" class="permalink" href="#sec-conf-health">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_health_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  You can check the health of a cluster using either Hawk2 or crmsh.
 </p><section class="sect2" id="sec-conf-hawk2-health" data-id-title="Verifying Cluster Health with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.2.1 </span><span class="title-name">Verifying Cluster Health with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-health">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_health_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides a wizard which checks and detects issues with your cluster.
  After the analysis is complete, Hawk2 creates a cluster report with further
  details. To verify cluster health and generate the report, Hawk2 requires
  passwordless SSH access between the nodes. Otherwise it can only collect data
  from the current node. If you have set up your cluster with the bootstrap scripts,
  provided by the
  <code class="systemitem">ha-cluster-bootstrap</code>
  package, passwordless SSH access is already configured. In case you need to
  configure it manually, see <a class="xref" href="#sec-crmreport-nonroot-ssh" title="D.2. Configuring a Passwordless SSH Account">Section D.2, “Configuring a Passwordless SSH Account”</a>.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to Hawk2:
   </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
    From the left navigation bar, select <span class="guimenu">Configuration</span> › <span class="guimenu">Wizards</span>.
   </p></li><li class="step"><p>
    Expand the <span class="guimenu">Basic</span> category.
   </p></li><li class="step"><p>
    Select the <span class="guimenu">Verify health and configuration</span> wizard.
   </p></li><li class="step"><p>
    Confirm with <span class="guimenu">Verify</span>.
   </p></li><li class="step"><p>
    Enter the root password for your cluster and click
    <span class="guimenu">Apply</span>. Hawk2 will generate the report.
   </p></li></ol></div></div></section><section class="sect2" id="sec-ha-manual-config-cli-health" data-id-title="Getting Health Status with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.2.2 </span><span class="title-name">Getting Health Status with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-manual-config-cli-health">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_health_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The <span class="quote">“<span class="quote">health</span>”</span> status of a cluster or node can be displayed
  with so called <span class="emphasis"><em>scripts</em></span>. A script can perform
  different tasks—they are not targeted to health. However, for
  this subsection, we focus on how to get the health status.
 </p><p>
  To get all the details about the <code class="command">health</code> command, use
  <code class="command">describe</code>:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm script describe health</code></pre></div><p>
  It shows a description and a list of all parameters and their default
  values. To execute a script, use <code class="command">run</code>:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm script run health</code></pre></div><p>
  If you prefer to run only one step from the suite, the
  <code class="command">describe</code> command lists all available steps in the
  <em class="citetitle">Steps</em> category.
 </p><p>
  For example, the following command executes the first step of the
  <code class="command">health</code> command. The output is stored in the
  <code class="filename">health.json</code> file for further investigation:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm script run health statefile='health.json'</code></pre></div><p>It is also possible to run the above commands with
  <code class="command">crm cluster health</code>.</p><p>
  For additional information regarding scripts, see
  <a class="link" href="http://crmsh.github.io/scripts/" target="_blank">http://crmsh.github.io/scripts/</a>.
 </p></section></section><section class="sect1" id="sec-conf-hawk2-history" data-id-title="Viewing the Cluster History"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.3 </span><span class="title-name">Viewing the Cluster History</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Hawk2 provides the following possibilities to view past events on the
  cluster (on different levels and in varying detail):
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-recent" title="11.3.1. Viewing Recent Events of Nodes or Resources">Section 11.3.1, “Viewing Recent Events of Nodes or Resources”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-explorer" title="11.3.2. Using the History Explorer for Cluster Reports">Section 11.3.2, “Using the History Explorer for Cluster Reports”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-conf-hawk2-history-transitions" title="11.3.3. Viewing Transition Details in the History Explorer">Section 11.3.3, “Viewing Transition Details in the History Explorer”</a>
   </p></li></ul></div><p>
  You can also view cluster history information using crmsh:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-ha-config-crm-history" title="11.3.4. Retrieving History Information with crmsh">Section 11.3.4, “Retrieving History Information with crmsh”</a>
   </p></li></ul></div><section class="sect2" id="sec-conf-hawk2-history-recent" data-id-title="Viewing Recent Events of Nodes or Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.1 </span><span class="title-name">Viewing Recent Events of Nodes or Resources</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-recent">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Monitoring</span> › <span class="guimenu">Status</span>. It lists
     <span class="guimenu">Resources</span> and <span class="guimenu">Nodes</span>.
    </p></li><li class="step"><p>
     To view recent events of a resource:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Resources</span> and select the respective resource.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column for the resource, click the
       arrow down button and select <span class="guimenu">Recent events</span>.
      </p><p>
       Hawk2 opens a new window and displays a table view of the latest
       events.
      </p></li></ol></li><li class="step"><p>
     To view recent events of a node:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Nodes</span> and select the respective node.
      </p></li><li class="step"><p>
       In the <span class="guimenu">Operations</span> column for the node, select
       <span class="guimenu">Recent events</span>.
      </p><p>
       Hawk2 opens a new window and displays a table view of the latest
       events.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-node-events.png"><img src="images/hawk2-node-events.png" width="90%" alt="Image" title="Image"/></a></div></div></li></ol></li></ol></div></div></section><section class="sect2" id="sec-conf-hawk2-history-explorer" data-id-title="Using the History Explorer for Cluster Reports"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.2 </span><span class="title-name">Using the History Explorer for Cluster Reports</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-explorer">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   From the left navigation bar, select <span class="guimenu">Troubleshooting</span> › <span class="guimenu">History</span> to access the
   <span class="guimenu">History Explorer</span>.
   The <span class="guimenu">History Explorer</span> allows you to create detailed
   cluster reports and view transition information. It provides the following
   options:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.9.5.8.3.1"><span class="term"><span class="guimenu">Generate</span>
    </span></dt><dd><p>
      Create a cluster report for a certain time. Hawk2 calls the
      <code class="command">crm report</code> command to generate the report.
     </p></dd><dt id="id-1.4.4.9.5.8.3.2"><span class="term"><span class="guimenu">Upload</span>
    </span></dt><dd><p>
      Allows you to upload <code class="literal">crm report</code> archives that have
      either been created with the crm shell directly or even on a different
      cluster.
     </p></dd></dl></div><p>
   After reports have been generated or uploaded, they are shown below
   <span class="guimenu">Reports</span>. From the list of reports, you can show a
   report's details, download or delete the report.
  </p><div class="figure" id="id-1.4.4.9.5.8.5"><div class="figure-contents"><div class="mediaobject"><a href="images/hawk2-history-explorer-main.png"><img src="images/hawk2-history-explorer-main.png" width="90%" alt="Hawk2—History Explorer Main View" title="Hawk2—History Explorer Main View"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 11.3: </span><span class="title-name">Hawk2—History Explorer Main View </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.9.5.8.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div><div class="procedure" id="pro-hawk2-history-report" data-id-title="Generating or Uploading a Cluster Report"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.2: </span><span class="title-name">Generating or Uploading a Cluster Report </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-history-report">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Troubleshooting</span> › <span class="guimenu">History</span>.
    </p><p>
     The <span class="guimenu">History Explorer</span> screen opens in the
     <span class="guimenu">Generate</span> view. By default, the suggested time frame for
     a report is the last hour.
    </p></li><li class="step"><p>
     To create a cluster report:
    </p><ol type="a" class="substeps"><li class="step"><p>
       To immediately start a report, click <span class="guimenu">Generate</span>.
      </p></li><li class="step"><p>
       To modify the time frame for the report, click anywhere on the suggested
       time frame and select another option from the drop-down box. You can
       also enter a <span class="guimenu">Custom</span> start date, end date and hour,
       respectively. To start the report, click <span class="guimenu">Generate</span>.
      </p><p>
       After the report has finished, it is shown below
       <span class="guimenu">Reports</span>.
      </p></li></ol></li><li class="step"><p>
     To upload a cluster report, the <code class="command">crm report</code> archive must
     be located on a file system that you can access with Hawk2. Proceed as
     follows:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Switch to the <span class="guimenu">Upload</span> tab.
      </p></li><li class="step"><p>
       <span class="guimenu">Browse</span> for the cluster report archive and click
       <span class="guimenu">Upload</span>.
      </p><p>
       After the report is uploaded, it is shown below
       <span class="guimenu">Reports</span>.
      </p></li></ol></li><li class="step"><p>
     To download or delete a report, click the respective icon next to the
     report in the <span class="guimenu">Operations</span> column.
    </p></li><li class="step"><p>
     To view
     <a class="xref" href="#il-hawk2-history-report-details" title="Report Details in History Explorer">Report Details in History Explorer</a>,
     click the report's name or select <span class="guimenu">Show</span> from the
     <span class="guimenu">Operations</span> column.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-history-report-details.png"><img src="images/hawk2-history-report-details.png" width="90%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Return to the list of reports by clicking the <span class="guimenu">Reports</span>
     button.
    </p></li></ol></div></div><div class="itemizedlist" id="il-hawk2-history-report-details" data-id-title="Report Details in History Explorer"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Report Details in History Explorer </span></span><a title="Permalink" class="permalink" href="#il-hawk2-history-report-details">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>
     Name of the report.
    </p></li><li class="listitem"><p>
     Start time of the report.
    </p></li><li class="listitem"><p>
     End time of the report.
    </p></li><li class="listitem"><p>
     Number of transitions plus time line of all transitions in the cluster
     that are covered by the report. To learn how to view more details for a
     transition, see
     <a class="xref" href="#sec-conf-hawk2-history-transitions" title="11.3.3. Viewing Transition Details in the History Explorer">Section 11.3.3</a>.
    </p></li><li class="listitem"><p>
     Node events.
    </p></li><li class="listitem"><p>
     Resource events.
    </p></li></ul></div></section><section class="sect2" id="sec-conf-hawk2-history-transitions" data-id-title="Viewing Transition Details in the History Explorer"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.3 </span><span class="title-name">Viewing Transition Details in the History Explorer</span></span> <a title="Permalink" class="permalink" href="#sec-conf-hawk2-history-transitions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For each transition, the cluster saves a copy of the state which it provides
   as input to <code class="systemitem">pacemaker-schedulerd</code>.
   The path to this archive is logged. All
   <code class="filename">pe-*</code> files are generated on the Designated
   Coordinator (DC). As the DC can change in a cluster, there may be
   <code class="filename">pe-*</code> files from several nodes. Any <code class="filename">pe-*</code>
   files are saved snapshots of the CIB, used as input of calculations by <code class="systemitem">pacemaker-schedulerd</code>.
  </p><p>
   In Hawk2, you can display the name of each <code class="filename">pe-*</code>
   file plus the time and node on which it was created. In addition, the
   <span class="guimenu">History Explorer</span> can visualize the following details,
   based on the respective <code class="filename">pe-*</code> file:
  </p><div class="variablelist" id="vl-hawk2-history-transition-details" data-id-title="Transition Details in the History Explorer"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">Transition Details in the History Explorer </span></span><a title="Permalink" class="permalink" href="#vl-hawk2-history-transition-details">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.4.4.9.5.9.4.2"><span class="term"><span class="guimenu">Details</span>
    </span></dt><dd><p>
      Shows snippets of logging data that belongs to the transition. Displays
      the output of the following command (including the resource agents' log
      messages):
     </p><div class="verbatim-wrap"><pre class="screen">crm history transition <em class="replaceable">peinput</em></pre></div></dd><dt id="id-1.4.4.9.5.9.4.3"><span class="term"><span class="guimenu">Configuration</span>
    </span></dt><dd><p>
      Shows the cluster configuration at the time that the
      <code class="filename">pe-*</code> file was created.
     </p></dd><dt id="id-1.4.4.9.5.9.4.4"><span class="term"><span class="guimenu">Diff</span>
    </span></dt><dd><p>
      Shows the differences of configuration and status between the selected
      <code class="filename">pe-*</code> file and the following one.
     </p></dd><dt id="id-1.4.4.9.5.9.4.5"><span class="term"><span class="guimenu">Log</span>
    </span></dt><dd><p>
      Shows snippets of logging data that belongs to the transition. Displays
      the output of the following command:
     </p><div class="verbatim-wrap"><pre class="screen">crm history transition log <em class="replaceable">peinput</em></pre></div><p>
      This includes details from the following daemons:
      <code class="systemitem">pacemaker-schedulerd</code>,
      <code class="systemitem">pacemaker-controld</code>, and
      <code class="systemitem">pacemaker-execd</code>.
     </p></dd><dt id="id-1.4.4.9.5.9.4.6"><span class="term"><span class="guimenu">Graph</span>
    </span></dt><dd><p>
      Shows a graphical representation of the transition. If you click
      <span class="guimenu">Graph</span>, the calculation is simulated (exactly as done by
      <code class="systemitem">pacemaker-schedulerd</code>) and a graphical
      visualization is generated.
     </p></dd></dl></div><div class="procedure" id="pro-hawk2-history-transitions" data-id-title="Viewing Transition Details"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.3: </span><span class="title-name">Viewing Transition Details </span></span><a title="Permalink" class="permalink" href="#pro-hawk2-history-transitions">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Troubleshooting</span> › <span class="guimenu">History</span>.
    </p><p>
     If reports have already been generated or uploaded, they are shown in the
     list of <span class="guimenu">Reports</span>. Otherwise generate or upload a report
     as described in
     <a class="xref" href="#pro-hawk2-history-report" title="Generating or Uploading a Cluster Report">Procedure 11.2</a>.
    </p></li><li class="step"><p>
     Click the report's name or select <span class="guimenu">Show</span> from the
     <span class="guimenu">Operations</span> column to open the
     <a class="xref" href="#il-hawk2-history-report-details" title="Report Details in History Explorer">Report Details in History Explorer</a>.
    </p></li><li class="step"><p>
     To access the transition details, you need to select a transition point in
     the transition time line that is shown below. Use the
     <span class="guimenu">Previous</span> and <span class="guimenu">Next</span> icons and the
     <span class="guimenu">Zoom In</span> and <span class="guimenu">Zoom Out</span> icons to find
     the transition that you are interested in.
    </p></li><li class="step"><p>
     To display the name of a <code class="filename">pe-input*</code> file plus the time
     and node on which it was created, hover the mouse pointer over a
     transition point in the time line.
    </p></li><li class="step"><p>
     To view the <a class="xref" href="#vl-hawk2-history-transition-details" title="Transition Details in the History Explorer">Transition Details in the History Explorer</a>, click
     the transition point for which you want to know more.
    </p></li><li class="step"><p>
     To show <span class="guimenu">Details</span>, <span class="guimenu">Configuration</span>,
     <span class="guimenu">Diff</span>, <span class="guimenu">Logs</span> or
     <span class="guimenu">Graph</span>, click the respective buttons to show the content
     described in <a class="xref" href="#vl-hawk2-history-transition-details" title="Transition Details in the History Explorer">Transition Details in the History Explorer</a>.
    </p></li><li class="step"><p>
     To return to the list of reports, click the <span class="guimenu">Reports</span>
     button.
    </p></li></ol></div></div></section><section class="sect2" id="sec-ha-config-crm-history" data-id-title="Retrieving History Information with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">11.3.4 </span><span class="title-name">Retrieving History Information with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-crm-history">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_hawk2_history_i.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Investigating the cluster history is a complex task. To simplify this
   task, crmsh contains the <code class="command">history</code> command with its
   subcommands. It is assumed SSH is configured correctly.
  </p><p>
   Each cluster moves states, migrates resources, or starts important
   processes. All these actions can be retrieved by subcommands of
   <code class="command">history</code>.
  </p><p>
   By default, all <code class="command">history</code> commands look at the events of
   the last hour. To change this time frame, use the
   <code class="command">limit</code> subcommand. The syntax is:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm history</code>
<code class="prompt custom">crm(live)history# </code><code class="command">limit <em class="replaceable">FROM_TIME</em> [<em class="replaceable">TO_TIME</em>]</code></pre></div><p>
   Some valid examples include:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.9.5.10.7.1"><span class="term"><code class="command">limit</code><code class="literal">4:00pm</code>
    , </span><span class="term"><code class="command">limit</code><code class="literal">16:00</code>
    </span></dt><dd><p>
      Both commands mean the same, today at 4pm.
     </p></dd><dt id="id-1.4.4.9.5.10.7.2"><span class="term"><code class="command">limit</code><code class="literal">2012/01/12 6pm</code>
    </span></dt><dd><p>
      January 12th 2012 at 6pm
     </p></dd><dt id="id-1.4.4.9.5.10.7.3"><span class="term"><code class="command">limit</code><code class="literal">"Sun 5 20:46"</code>
    </span></dt><dd><p>
      In the current year of the current month at Sunday the 5th at 8:46pm
     </p></dd></dl></div><p>
   Find more examples and how to create time frames at
   <a class="link" href="http://labix.org/python-dateutil" target="_blank">http://labix.org/python-dateutil</a>.
  </p><p>
   The <code class="command">info</code> subcommand shows all the parameters which are
   covered by the <code class="command">crm report</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">info</code>
Source: live
Period: 2012-01-12 14:10:56 - end
Nodes: alice
Groups:
Resources:</pre></div><p>
   To limit <code class="command">crm report</code> to certain parameters view the
   available options with the subcommand <code class="command">help</code>.
  </p><p>
   To narrow down the level of detail, use the subcommand
   <code class="command">detail</code> with a level:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">detail 1</code></pre></div><p>
   The higher the number, the more detailed your report will be. Default is
   <code class="literal">0</code> (zero).
  </p><p>
   After you have set above parameters, use <code class="command">log</code> to show
   the log messages.
  </p><p>
   To display the last transition, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">transition -1</code>
INFO: fetching new logs, please wait ...</pre></div><p>
   This command fetches the logs and runs <code class="command">dotty</code> (from the
   <span class="package">graphviz</span> package) to show the
   transition graph. The shell opens the log file which you can browse with
   the <span class="keycap">↓</span> and <span class="keycap">↑</span> cursor keys.
  </p><p>
   If you do not want to open the transition graph, use the
   <code class="option">nograph</code> option:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">transition -1 nograph</code></pre></div></section></section><section class="sect1" id="sec-ha-config-basics-monitor-health" data-id-title="Monitoring System Health"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">11.4 </span><span class="title-name">Monitoring System Health</span></span> <a title="Permalink" class="permalink" href="#sec-ha-config-basics-monitor-health">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_monitoring_clusters.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To prevent a node from running out of disk space and thus being unable to
   manage any resources that have been assigned to it, SUSE Linux Enterprise High Availability
   provides a resource agent,
   <code class="systemitem">ocf:pacemaker:SysInfo</code>. Use it to monitor a
   node's health with regard to disk partitions.
   The SysInfo RA creates a node attribute named
   <code class="literal">#health_disk</code> which will be set to
   <code class="literal">red</code> if any of the monitored disks' free space is below
   a specified limit.
  </p><p>
   To define how the CRM should react in case a node's health reaches a
   critical state, use the global cluster option
   <code class="systemitem">node-health-strategy</code>.
  </p><div class="procedure" id="pro-ha-health-monitor" data-id-title="Configuring System Health Monitoring"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 11.4: </span><span class="title-name">Configuring System Health Monitoring </span></span><a title="Permalink" class="permalink" href="#pro-ha-health-monitor">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_monitoring_clusters.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To automatically move resources away from a node in case the node runs
    out of disk space, proceed as follows:
   </p><ol class="procedure" type="1"><li class="step"><p>
     Configure an <code class="systemitem">ocf:pacemaker:SysInfo</code> resource:
    </p><div class="verbatim-wrap"><pre class="screen">primitive sysinfo ocf:pacemaker:SysInfo \
     params disks="/tmp /var"<span class="callout" id="co-disks">1</span> min_disk_free="100M"<span class="callout" id="co-min-disk-free">2</span> disk_unit="M"<span class="callout" id="co-disk-unit">3</span> \
     op monitor interval="15s"</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disks"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Which disk partitions to monitor. For example,
       <code class="filename">/tmp</code>, <code class="filename">/usr</code>,
       <code class="filename">/var</code>, and <code class="filename">/dev</code>. To specify
       multiple partitions as attribute values, separate them with a blank.
      </p><div id="id-1.4.4.9.6.4.3.3.1.2" data-id-title="/ File System Always Monitored" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: <code class="filename">/</code> File System Always Monitored</div><p>
        You do not need to specify the root partition
        (<code class="filename">/</code>) in <code class="literal">disks</code>. It is always
        monitored by default.
       </p></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-min-disk-free"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minimum free disk space required for those partitions.
       Optionally, you can specify the unit to use for measurement (in the
       example above, <code class="literal">M</code> for megabytes is used). If not
       specified, <code class="systemitem">min_disk_free</code> defaults to the
       unit defined in the <code class="systemitem">disk_unit</code> parameter.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-disk-unit"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The unit in which to report the disk space.
      </p></td></tr></table></div></li><li class="step"><p>
     To complete the resource configuration, create a clone of
     <code class="systemitem">ocf:pacemaker:SysInfo</code> and start it on each
     cluster node.
    </p></li><li class="step"><p>
     Set the <code class="systemitem">node-health-strategy</code> to
     <code class="literal">migrate-on-red</code>:
    </p><div class="verbatim-wrap"><pre class="screen">property node-health-strategy="migrate-on-red"</pre></div><p>
     In case of a <code class="systemitem">#health_disk</code> attribute set to
     <code class="literal">red</code>, the <code class="systemitem">pacemaker-schedulerd</code> adds <code class="literal">-INF</code>
     to the resources' score for that node. This will cause any resources to
     move away from this node. The STONITH resource will be the last
     one to be stopped but even if the STONITH resource is not running
     anymore, the node can still be fenced. Fencing has direct access to the
     CIB and will continue to work.
    </p></li></ol></div></div><p>
   After a node's health status has turned to <code class="literal">red</code>, solve
   the issue that led to the problem. Then clear the <code class="literal">red</code>
   status to make the node eligible again for running resources. Log in to
   the cluster node and use one of the following methods:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Execute the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm node status-attr <em class="replaceable">NODE</em> delete #health_disk</code></pre></div></li><li class="listitem"><p>
     Restart the cluster services on that node.
    </p></li><li class="listitem"><p>
     Reboot the node.
    </p></li></ul></div><p>
   The node will be returned to service and can run resources again.
  </p></section></section><section class="chapter" id="cha-ha-fencing" data-id-title="Fencing and STONITH"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">12 </span><span class="title-name">Fencing and STONITH</span></span> <a title="Permalink" class="permalink" href="#cha-ha-fencing">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Fencing is a very important concept in computer clusters for HA (High
    Availability). A cluster sometimes detects that one of the nodes is
    behaving strangely and needs to remove it. This is called
    <span class="emphasis"><em>fencing</em></span> and is commonly done with a STONITH
    resource. Fencing may be defined as a method to bring an HA cluster to a
    known state.
   </p><p>
    Every resource in a cluster has a state attached. For example:
    <span class="quote">“<span class="quote">resource r1 is started on alice</span>”</span>. In an HA cluster, such
    a state implies that <span class="quote">“<span class="quote">resource r1 is stopped on all nodes except
    alice</span>”</span>, because the cluster must make sure that every resource
    may be started on only one node. Every node must report every change
    that happens to a resource. The cluster state is thus a collection of
    resource states and node states.
   </p><p>
    When the state of a node or resource cannot be established with
    certainty, fencing comes in. Even when the cluster is not aware of what
    is happening on a given node, fencing can ensure that the node does not
    run any important resources.
   </p></div></div></div></div><section class="sect1" id="sec-ha-fencing-classes" data-id-title="Classes of Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.1 </span><span class="title-name">Classes of Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-classes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are two classes of fencing: resource level and node level fencing.
   The latter is the primary subject of this chapter.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.10.3.3.1"><span class="term">Resource Level Fencing</span></dt><dd><p>Resource level fencing ensures exclusive access to a given resource.
      Common examples of this are changing the zoning of the node from a SAN fiber
      channel switch (thus locking the node out of access to its disks) or methods
      like SCSI reserve. For examples, refer to <a class="xref" href="#sec-ha-storage-protect-rsc-fencing" title="13.10. Additional Mechanisms for Storage Protection">Section 13.10, “Additional Mechanisms for Storage Protection”</a>.
     </p></dd><dt id="id-1.4.4.10.3.3.2"><span class="term">Node Level Fencing</span></dt><dd><p>
      Node level fencing prevents a failed node from accessing shared resources
      entirely. This is usually done in a simple and abrupt way: reset or power
      off the node.
     </p></dd></dl></div><div class="table" id="tab-fencing-classes" data-id-title="Classes of fencing"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 12.1: </span><span class="title-name">Classes of fencing </span></span><a title="Permalink" class="permalink" href="#tab-fencing-classes">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Classes</th><th style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Methods</th><th style="text-align: center; border-right: 1px solid ; border-bottom: 1px solid ; ">Options</th><th style="text-align: center; border-bottom: 1px solid ; ">Examples</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="4">Node fencing<a href="#ftn.id-1.4.4.10.3.4.2.2.1.1.1" class="footnote"><sup class="footnote" id="id-1.4.4.10.3.4.2.2.1.1.1">[a]</sup></a> (reboot or shutdown)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">Remote management</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">In-node</td><td style="border-bottom: 1px solid ; ">ILO, DRAC, IPMI</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">External</td><td style="border-bottom: 1px solid ; ">HMC, vCenter, EC2</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">SBD and watchdog</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Disk-based</td><td style="border-bottom: 1px solid ; " rowspan="2">hpwdt, iTCO_wdt, ipmi_wdt, softdog</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Diskless</td></tr><tr><td style="border-right: 1px solid ; " rowspan="4">I/O fencing (locking or reservation)</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " rowspan="2">Pure locking</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">In-cluster</td><td style="border-bottom: 1px solid ; ">SFEX</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">External</td><td style="border-bottom: 1px solid ; ">SCSI2 reservation, SCSI3 reservation</td></tr><tr><td style="border-right: 1px solid ; " rowspan="2">Built-in locking</td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Cluster-based</td><td style="border-bottom: 1px solid ; ">Cluster MD, LVM with lvmlockd and DLM</td></tr><tr><td style="border-right: 1px solid ; ">Cluster-handled</td><td>MD-RAID</td></tr></tbody><tbody class="footnotes"><tr><td colspan="4"><div id="ftn.id-1.4.4.10.3.4.2.2.1.1.1" class="footnote"><p><a href="#id-1.4.4.10.3.4.2.2.1.1.1" class="para"><sup class="para">[a] </sup></a>Mandatory in High Availability clusters</p></div></td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-fencing-nodes" data-id-title="Node Level Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.2 </span><span class="title-name">Node Level Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In a Pacemaker cluster, the implementation of node level fencing is STONITH
   (Shoot The Other Node in the Head). SUSE Linux Enterprise High Availability
   includes the <code class="command">stonith</code> command line tool, an extensible
   interface for remotely powering down a node in the cluster. For an
   overview of the available options, run <code class="command">stonith --help</code>
   or refer to the man page of <code class="command">stonith</code> for more
   information.
  </p><section class="sect2" id="sec-ha-fencing-nodes-devices" data-id-title="STONITH Devices"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.2.1 </span><span class="title-name">STONITH Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes-devices">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To use node level fencing, you first need to have a fencing device. To
    get a list of STONITH devices which are supported by SUSE Linux Enterprise High Availability, run
    one of the following commands on any of the nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>stonith -L</pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm ra list stonith</pre></div><p>
    STONITH devices may be classified into the following categories:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.10.4.3.7.1"><span class="term">Power Distribution Units (PDU)</span></dt><dd><p>
       Power Distribution Units are an essential element in managing power
       capacity and functionality for critical network, server and data
       center equipment. They can provide remote load monitoring of
       connected equipment and individual outlet power control for remote
       power recycling.
      </p></dd><dt id="id-1.4.4.10.4.3.7.2"><span class="term">Uninterruptible Power Supplies (UPS)</span></dt><dd><p>
       A stable power supply provides emergency power to connected equipment
       by supplying power from a separate source if a utility
       power failure occurs.
      </p></dd><dt id="id-1.4.4.10.4.3.7.3"><span class="term">Blade Power Control Devices</span></dt><dd><p>
       If you are running a cluster on a set of blades, then the power
       control device in the blade enclosure is the only candidate for
       fencing. Of course, this device must be capable of managing single
       blade computers.
      </p></dd><dt id="id-1.4.4.10.4.3.7.4"><span class="term">Lights-out Devices</span></dt><dd><p>
       Lights-out devices (IBM RSA, HP iLO, Dell DRAC) are becoming
       increasingly popular and may even become standard in off-the-shelf
       computers. However, they are inferior to UPS devices, because they
       share a power supply with their host (a cluster node). If a node
       stays without power, the device supposed to control it would be
       useless. In that case, the CRM would continue its attempts to fence
       the node indefinitely while all other resource operations would wait
       for the fencing/STONITH operation to complete.
      </p></dd><dt id="id-1.4.4.10.4.3.7.5"><span class="term">Testing Devices</span></dt><dd><p>
       Testing devices are used exclusively for testing purposes. They are
       usually more gentle on the hardware. Before the cluster goes into
       production, they must be replaced with real fencing devices.
      </p></dd></dl></div><p>
    The choice of the STONITH device depends mainly on your budget and the
    kind of hardware you use.
   </p></section><section class="sect2" id="sec-ha-fencing-nodes-implementation" data-id-title="STONITH Implementation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.2.2 </span><span class="title-name">STONITH Implementation</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-nodes-implementation">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The STONITH implementation of SUSE® Linux Enterprise High Availability consists of two
    components:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.10.4.4.3.1"><span class="term">pacemaker-fenced</span></dt><dd><p>
       <code class="systemitem">pacemaker-fenced</code> is a daemon which can be accessed by local processes or over
       the network. It accepts the commands which correspond to fencing
       operations: reset, power-off, and power-on. It can also check the
       status of the fencing device.
      </p><p>
       The <code class="systemitem">pacemaker-fenced</code> daemon runs on every node in the High Availability cluster. The
       <code class="systemitem">pacemaker-fenced</code> instance running on the DC node receives a fencing request
       from the <code class="systemitem">pacemaker-controld</code>. It
       is up to this and other <code class="systemitem">pacemaker-fenced</code> programs to carry
       out the desired fencing operation.
      </p></dd><dt id="id-1.4.4.10.4.4.3.2"><span class="term">STONITH Plug-ins</span></dt><dd><p>
       For every supported fencing device there is a STONITH plug-in which
       is capable of controlling said device. A STONITH plug-in is the
       interface to the fencing device. The STONITH plug-ins contained in
       the <span class="package">cluster-glue</span> package reside in
       <code class="filename">/usr/lib64/stonith/plugins</code> on each node.
       (If you installed the
       <span class="package">fence-agents</span> package, too,
       the plug-ins contained there are installed in
       <code class="filename">/usr/sbin/fence_*</code>.) All STONITH plug-ins look
       the same to <code class="systemitem">pacemaker-fenced</code>,
       but are quite different on the other side, reflecting the nature of the
       fencing device.
      </p><p>
       Some plug-ins support more than one device. A typical example is
       <code class="literal">ipmilan</code> (or <code class="literal">external/ipmi</code>)
       which implements the IPMI protocol and can control any device which
       supports this protocol.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-fencing-config" data-id-title="STONITH Resources and Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.3 </span><span class="title-name">STONITH Resources and Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To set up fencing, you need to configure one or more STONITH
   resources—the <code class="systemitem">pacemaker-fenced</code> daemon requires no configuration. All
   configuration is stored in the CIB. A STONITH resource is a resource of
   class <code class="literal">stonith</code> (see
   <a class="xref" href="#sec-ha-config-basics-raclasses" title="6.2. Supported Resource Agent Classes">Section 6.2, “Supported Resource Agent Classes”</a>). STONITH resources
   are a representation of STONITH plug-ins in the CIB. Apart from the
   fencing operations, the STONITH resources can be started, stopped and
   monitored, like any other resource. Starting or stopping STONITH
   resources means loading and unloading the STONITH device driver on a
   node. Starting and stopping are thus only administrative operations and
   do not translate to any operation on the fencing device itself. However,
   monitoring does translate to logging it to the device (to verify that the
   device will work in case it is needed). When a STONITH resource fails
   over to another node it enables the current node to talk to the STONITH
   device by loading the respective driver.
  </p><p>
   STONITH resources can be configured like any other resource. For
   details how to do so with your preferred cluster management tool:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hawk2: <a class="xref" href="#sec-conf-hawk2-rsc-stonith" title="6.9.1. Creating STONITH Resources with Hawk2">Section 6.9.1, “Creating STONITH Resources with Hawk2”</a>
    </p></li><li class="listitem"><p>
     crmsh: <a class="xref" href="#sec-ha-manual-create-stonith" title="6.9.2. Creating STONITH Resources with crmsh">Section 6.9.2, “Creating STONITH Resources with crmsh”</a>
    </p></li></ul></div><p>
   The list of parameters (attributes) depends on the respective STONITH
   type. To view a list of parameters for a specific device, use the
   <code class="command">stonith</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">stonith-device-type</em> -n</pre></div><p>
   For example, to view the parameters for the <code class="literal">ibmhmc</code>
   device type, enter the following:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t ibmhmc -n</pre></div><p>
   To get a short help text for the device, use the <code class="option">-h</code>
   option:
  </p><div class="verbatim-wrap"><pre class="screen">stonith -t <em class="replaceable">stonith-device-type</em> -h</pre></div><section class="sect2" id="sec-ha-fencing-config-examples" data-id-title="Example STONITH Resource Configurations"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">12.3.1 </span><span class="title-name">Example STONITH Resource Configurations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-config-examples">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In the following, find some example configurations written in the syntax
    of the <code class="command">crm</code> command line tool. To apply them, put the
    sample in a text file (for example, <code class="filename">sample.txt</code>) and
    run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> &lt; sample.txt</pre></div><p>
    For more information about configuring resources with the
    <code class="command">crm</code> command line tool, refer to
    <a class="xref" href="#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a>.
   </p><div class="complex-example"><div class="example" id="id-1.4.4.10.5.11.5" data-id-title="Configuration of an IBM RSA Lights-out Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 12.1: </span><span class="title-name">Configuration of an IBM RSA Lights-out Device </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.10.5.11.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     An IBM RSA lights-out device might be configured like this:
    </p><div class="verbatim-wrap"><pre class="screen">configure
primitive st-ibmrsa-1 stonith:external/ibmrsa-telnet \
params nodename=alice ip_address=192.168.0.101 \
username=USERNAME password=PASSW0RD
primitive st-ibmrsa-2 stonith:external/ibmrsa-telnet \
params nodename=bob ip_address=192.168.0.102 \
username=USERNAME password=PASSW0RD
location l-st-alice st-ibmrsa-1 -inf: alice
location l-st-bob st-ibmrsa-2 -inf: bob
commit</pre></div><p>
     In this example, location constraints are used for the following
     reason: There is always a certain probability that the STONITH
     operation is going to fail. Therefore, a STONITH operation on the
     node which is the executioner as well is not reliable. If the node is
     reset, it cannot send the notification about the fencing operation
     outcome. The only way to do that is to assume that the operation is
     going to succeed and send the notification beforehand. But if the
     operation fails, problems could arise. Therefore, by convention,
     <code class="systemitem">pacemaker-fenced</code> refuses to terminate its host.
    </p></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.10.5.11.6" data-id-title="Configuration of a UPS Fencing Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 12.2: </span><span class="title-name">Configuration of a UPS Fencing Device </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.10.5.11.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The configuration of a UPS type fencing device is similar to the
     examples above. The details are not covered here. All UPS devices
     employ the same mechanics for fencing. How the device is accessed
     varies. Old UPS devices only had a serial port, usually connected at
     1200baud using a special serial cable. Many new ones still have a
     serial port, but often they also use a USB or Ethernet interface. The
     kind of connection you can use depends on what the plug-in supports.
    </p><p>
     For example, compare the <code class="literal">apcmaster</code> with the
     <code class="literal">apcsmart</code> device by using the <code class="command">stonith
     -t</code> <em class="replaceable">stonith-device-type</em> -n command:
    </p><div class="verbatim-wrap"><pre class="screen">stonith -t apcmaster -h</pre></div><p>
     returns the following information:
    </p><div class="verbatim-wrap"><pre class="screen">STONITH Device: apcmaster - APC MasterSwitch (via telnet)
NOTE: The APC MasterSwitch accepts only one (telnet)
connection/session a time. When one session is active,
subsequent attempts to connect to the MasterSwitch will fail.
For more information see http://www.apc.com/
List of valid parameter names for apcmaster STONITH device:
        ipaddr
        login
        password
For Config info [-p] syntax, give each of the above parameters in order as
the -p value.
Arguments are separated by white space.
Config file [-F] syntax is the same as -p, except # at the start of a line
denotes a comment</pre></div><p>
     With
    </p><div class="verbatim-wrap"><pre class="screen">stonith -t apcsmart -h</pre></div><p>
     you get the following output:
    </p><div class="verbatim-wrap"><pre class="screen">STONITH Device: apcsmart - APC Smart UPS
(via serial port - NOT USB!).
Works with higher-end APC UPSes, like
Back-UPS Pro, Smart-UPS, Matrix-UPS, etc.
(Smart-UPS may have to be &gt;= Smart-UPS 700?).
See http://www.networkupstools.org/protocols/apcsmart.html
for protocol compatibility details.
For more information see http://www.apc.com/
List of valid parameter names for apcsmart STONITH device:
ttydev
hostlist</pre></div><p>
     The first plug-in supports APC UPS with a network port and telnet
     protocol. The second plug-in uses the APC SMART protocol over the
     serial line, which is supported by many APC UPS product
     lines.
    </p></div></div></div><div class="complex-example"><div class="example" id="ex-ha-fencing-kdump" data-id-title="Configuration of a Kdump Device"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 12.3: </span><span class="title-name">Configuration of a Kdump Device </span></span><a title="Permalink" class="permalink" href="#ex-ha-fencing-kdump">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>Kdump belongs to the <a class="xref" href="#sec-ha-fencing-special" title="12.5. Special Fencing Devices">Special Fencing Devices</a> and is in fact the opposite of a fencing device.
     The plug-in checks if a Kernel dump is in progress on a node. If so, it
     returns true, and acts <span class="emphasis"><em>as if</em></span> the node has been fenced.
    </p><p>
     The Kdump plug-in must be used in concert with another, real STONITH
     device, for example, <code class="literal">external/ipmi</code>. For the fencing
     mechanism to work properly, you must specify that Kdump is checked before
     a real STONITH device is triggered. Use <code class="command">crm configure
     fencing_topology</code> to specify the order of the fencing devices as
     shown in the following procedure.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Use the <code class="literal">stonith:fence_kdump</code> resource agent (provided
     by the package <span class="package">fence-agents</span>)
     to monitor all nodes with the Kdump function enabled. Find a
     configuration example for the resource below:
    </p><div class="verbatim-wrap"><pre class="screen">configure
  primitive st-kdump stonith:fence_kdump \
    params nodename="alice "\ <span class="callout" id="co-ha-fenc-kdump-nodename">1</span>
    pcmk_host_check="static-list" \
    pcmk_reboot_action="off" \
    pcmk_monitor_action="metadata" \
    pcmk_reboot_retries="1" \
    timeout="60"
commit</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-fenc-kdump-nodename"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Name of the node to be monitored. If you need to monitor more than one
       node, configure more STONITH resources. To prevent a specific node
       from using a fencing device, add location constraints.
      </p></td></tr></table></div><p>
     The fencing action will be started after the timeout of the resource.
    </p></li><li class="step"><p>
     In <code class="filename">/etc/sysconfig/kdump</code> on each node, configure
     <code class="literal">KDUMP_POSTSCRIPT</code> to send a notification to all nodes
     when the Kdump process is finished. For example:
    </p><div class="verbatim-wrap"><pre class="screen">KDUMP_POSTSCRIPT="/usr/lib/fence_kdump_send -i <em class="replaceable">INTERVAL</em> -p <em class="replaceable">PORT</em> -c 1 alice bob charlie"</pre></div><p>
     The node that does a Kdump will restart automatically after Kdump has
     finished.
    </p></li><li class="step"><p>
     Run either <code class="command">systemctl restart kdump.service</code> or <code class="command">mkdumprd</code>.
     Either of these commands will detect that <code class="filename">/etc/sysconfig/kdump</code>
     was modified, and will regenerate the <code class="filename">initrd</code> to include the
     library <code class="literal">fence_kdump_send</code> with network enabled.
    </p></li><li class="step"><p>
     Open a port in the firewall for the <code class="literal">fence_kdump</code> resource.
     The default port is <code class="literal">7410</code>.
    </p></li><li class="step"><p>
     To achieve that Kdump is checked before triggering a real fencing
     mechanism (like <code class="literal">external/ipmi</code>),
     use a configuration similar to the following:</p><div class="verbatim-wrap"><pre class="screen">fencing_topology \
  alice: kdump-node1 ipmi-node1 \
  bob: kdump-node2 ipmi-node2</pre></div><p>For more details on <code class="option">fencing_topology</code>:
    </p><div class="verbatim-wrap"><pre class="screen">crm configure help fencing_topology</pre></div></li></ol></div></div></div></div></div></section></section><section class="sect1" id="sec-ha-fencing-monitor" data-id-title="Monitoring Fencing Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.4 </span><span class="title-name">Monitoring Fencing Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Like any other resource, the STONITH class agents also support the
   monitoring operation for checking status.
  </p><div id="id-1.4.4.10.6.3" data-id-title="Monitoring STONITH Resources" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Monitoring STONITH Resources</div><p>
    Monitor STONITH resources regularly, yet sparingly. For most devices a
    monitoring interval of at least 1800 seconds (30 minutes) should
    suffice.
   </p></div><p>
   Fencing devices are an indispensable part of an HA cluster, but the less
   you need to use them, the better. Power management equipment is often
   affected by too much broadcast traffic. Some devices cannot handle more
   than ten or so connections per minute. Some get confused if two clients
   try to connect at the same time. Most cannot handle more than one session
   at a time.
  </p><p>
   Checking the status of fencing devices once every few hours should
   usually be enough. The probability that a fencing operation needs to be
   performed and the power switch fails is low.
  </p><p>
   For detailed information on how to configure monitor operations, refer to

   <a class="xref" href="#sec-ha-manual-config-monitor" title="6.10.2. Configuring Resource Monitoring with crmsh">Section 6.10.2, “Configuring Resource Monitoring with crmsh”</a> for the command line
   approach.
  </p></section><section class="sect1" id="sec-ha-fencing-special" data-id-title="Special Fencing Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.5 </span><span class="title-name">Special Fencing Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-special">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In addition to plug-ins which handle real STONITH devices, there are
   special purpose STONITH plug-ins.
  </p><div id="id-1.4.4.10.7.3" data-id-title="For Testing Only" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: For Testing Only</div><p>
    Some STONITH plug-ins mentioned below are for demonstration and
    testing purposes only. Do not use any of the following devices in
    real-life scenarios because this may lead to data corruption and
    unpredictable results:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">external/ssh </code>
     </p></li><li class="listitem"><p>
      <code class="literal">ssh </code>
     </p></li></ul></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-fence-kdump"><span class="term"><code class="literal">fence_kdump</code>
    </span></dt><dd><p>
      This plug-in checks if a Kernel dump is in progress on a node. If so,
      it returns <code class="literal">true</code>, and acts as if the node has been
      fenced. The node cannot run any resources during the dump anyway. This
      avoids fencing a node that is already down but doing a dump, which
      takes some time. The plug-in must be used in concert with another,
      real STONITH device.
     </p><p>
      For configuration details, see <a class="xref" href="#ex-ha-fencing-kdump" title="Configuration of a Kdump Device">Example 12.3, “Configuration of a Kdump Device”</a>.
     </p></dd><dt id="id-1.4.4.10.7.4.2"><span class="term"><code class="literal">external/sbd</code>
    </span></dt><dd><p>
      This is a self-fencing device. It reacts to a so-called <span class="quote">“<span class="quote">poison
      pill</span>”</span> which can be inserted into a shared disk. On
      shared-storage connection loss, it stops the node from operating.
      Learn how to use this STONITH agent to implement storage-based
      fencing in
      <a class="xref" href="#cha-ha-storage-protect" title="Chapter 13. Storage Protection and SBD">Chapter 13</a>,
      <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the cluster to use SBD">Procedure 13.7, “Configuring the cluster to use SBD”</a>. See also
      <a class="link" href="http://www.linux-ha.org/wiki/SBD_Fencing" target="_blank">http://www.linux-ha.org/wiki/SBD_Fencing</a> for more
      details.
     </p><div id="id-1.4.4.10.7.4.2.2.2" data-id-title="external/sbd and DRBD" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="literal">external/sbd</code> and DRBD</div><p>
       The <code class="literal">external/sbd</code> fencing mechanism requires that
       the SBD partition is readable directly from each node. Thus, a DRBD*
       device must not be used for an SBD partition.
      </p><p>
       However, you can use the fencing mechanism for a DRBD cluster,
       provided the SBD partition is located on a shared disk that is not
       mirrored or replicated.
      </p></div></dd><dt id="id-1.4.4.10.7.4.3"><span class="term"><code class="literal">external/ssh</code>
    </span></dt><dd><p>
      Another software-based <span class="quote">“<span class="quote">fencing</span>”</span> mechanism. The nodes
      must be able to log in to each other as <code class="systemitem">root</code> without passwords.
      It takes a single parameter, <code class="literal">hostlist</code>, specifying
      the nodes that it will target. As it is not able to reset a truly
      failed node, it must not be used for real-life clusters—for
      testing and demonstration purposes only. Using it for shared storage
      would result in data corruption.
     </p></dd><dt id="id-1.4.4.10.7.4.4"><span class="term"><code class="literal">meatware</code>
    </span></dt><dd><p>
      <code class="literal">meatware</code> requires help from the user to operate.
      Whenever invoked, <code class="literal">meatware</code> logs a CRIT severity
      message which shows up on the node's console. The operator then
      confirms that the node is down and issues a
      <code class="command">meatclient(8)</code> command. This tells
      <code class="literal">meatware</code> to inform the cluster that the node should
      be considered dead. See
      <code class="filename">/usr/share/doc/packages/cluster-glue/README.meatware</code>
      for more information.
     </p></dd><dt id="id-1.4.4.10.7.4.5"><span class="term"><code class="literal">suicide</code>
    </span></dt><dd><p>
      This is a software-only device, which can reboot a node it is running
      on, using the <code class="command">reboot</code> command. This requires action
      by the node's operating system and can fail under certain
      circumstances. Therefore avoid using this device whenever possible.
      However, it is safe to use on one-node clusters.
     </p></dd><dt id="id-1.4.4.10.7.4.6"><span class="term">Diskless SBD</span></dt><dd><p>This configuration is useful if you want a fencing mechanism without
     shared storage. In this diskless mode, SBD fences nodes by using the
     hardware watchdog without relying on any shared device.
     However, diskless SBD cannot handle a split brain scenario for
     a two-node cluster. Use this option only for clusters with <span class="emphasis"><em>more than two</em></span> nodes.</p></dd></dl></div><p>
   <code class="literal">suicide</code>

   is the only exception to the <span class="quote">“<span class="quote">I do not shoot my host</span>”</span> rule.
  </p></section><section class="sect1" id="sec-ha-fencing-recommend" data-id-title="Basic Recommendations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.6 </span><span class="title-name">Basic Recommendations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-recommend">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Check the following list of recommendations to avoid common mistakes:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Do not configure several power switches in parallel.
    </p></li><li class="listitem"><p>
     To test your STONITH devices and their configuration, pull the plug
     once from each node and verify that fencing the node does takes place.
    </p></li><li class="listitem"><p>
     Test your resources under load and verify the timeout values are
     appropriate. Setting timeout values too low can trigger (unnecessary)
     fencing operations. For details, refer to
     <a class="xref" href="#sec-ha-config-basics-timeouts" title="6.3. Timeout Values">Section 6.3, “Timeout Values”</a>.
    </p></li><li class="listitem"><p>
     Use appropriate fencing devices for your setup. For details, also refer
     to <a class="xref" href="#sec-ha-fencing-special" title="12.5. Special Fencing Devices">Section 12.5, “Special Fencing Devices”</a>.
    </p></li><li class="listitem"><p>
     Configure one or more STONITH resources. By default, the global
     cluster option <code class="literal">stonith-enabled</code> is set to
     <code class="literal">true</code>. If no STONITH resources have been defined,
     the cluster will refuse to start any resources.
    </p></li><li class="listitem"><p>
     Do not set the global cluster option
     <code class="systemitem">stonith-enabled</code> to <code class="literal">false</code>
     for the following reasons:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Clusters without STONITH enabled are not supported.
      </p></li><li class="listitem"><p>
       DLM/OCFS2 will block forever waiting for a fencing operation that
       will never happen.
      </p></li></ul></div></li><li class="listitem"><p>
     Do not set the global cluster option
     <code class="systemitem">startup-fencing</code> to <code class="literal">false</code>.
     By default, it is set to <code class="literal">true</code> for the following
     reason: If a node is in an unknown state during cluster start-up, the
     node will be fenced once to clarify its status.
    </p></li></ul></div></section><section class="sect1" id="sec-ha-fencing-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">12.7 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-fencing-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_fencing.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.10.9.2.1"><span class="term"><code class="filename">/usr/share/doc/packages/cluster-glue</code>
    </span></dt><dd><p>
      In your installed system, this directory contains README files for
      many STONITH plug-ins and devices.
     </p></dd><dt id="id-1.4.4.10.9.2.2"><span class="term"><a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>
    </span></dt><dd><p>
       <em class="citetitle">Pacemaker Explained</em>: Explains the concepts used to configure Pacemaker.
      Contains comprehensive and detailed information for reference.
     </p></dd><dt id="id-1.4.4.10.9.2.3"><span class="term"><a class="link" href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html" target="_blank">http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html</a>
    </span></dt><dd><p>
      Article explaining the concepts of split brain, quorum and fencing in
      HA clusters.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-ha-storage-protect" data-id-title="Storage Protection and SBD"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">13 </span><span class="title-name">Storage Protection and SBD</span></span> <a title="Permalink" class="permalink" href="#cha-ha-storage-protect">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    SBD (STONITH Block Device) provides a node fencing mechanism for
    Pacemaker-based clusters through the exchange of messages via shared block
    storage (SAN, iSCSI, FCoE, etc.). This isolates the fencing
    mechanism from changes in firmware version or dependencies on specific
    firmware controllers. SBD needs a watchdog on each node to ensure that misbehaving
    nodes are really stopped. Under certain conditions, it is also possible to use
    SBD without shared storage, by running it in diskless mode.
   </p><p>
    The <span class="package">ha-cluster-bootstrap</span> scripts provide an automated
    way to set up a cluster with the option of using SBD as fencing mechanism.
    For details, see the <span class="intraxref">Article “Installation and Setup Quick Start”</span>. However,
    manually setting up SBD provides you with more options regarding the
    individual settings.
   </p><p>
    This chapter explains the concepts behind SBD. It guides you through
    configuring the components needed by SBD to protect your cluster from
    potential data corruption in case of a split brain scenario.
   </p><p>
    In addition to node level fencing, you can use additional mechanisms for storage
    protection, such as LVM exclusive activation or OCFS2 file locking support
    (resource level fencing). They protect your system against administrative or
    application faults.
   </p></div></div></div></div><section class="sect1" id="sec-ha-storage-protect-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>SBD expands to <span class="emphasis"><em>Storage-Based Death</em></span> or
        <span class="emphasis"><em>STONITH Block Device</em></span>.
      </p><p>
        The highest priority of the High Availability cluster stack is to protect the integrity
        of data. This is achieved by preventing uncoordinated concurrent access
        to data storage. The cluster stack takes care of this using several
        control mechanisms.
      </p><p>
        However, network partitioning or software malfunction could potentially
        cause scenarios where several DCs are elected in a cluster. If this
        so-called split brain scenario were allowed to unfold, data corruption
        might occur.
      </p><p>
        Node fencing via STONITH is the primary mechanism to prevent this.
        Using SBD as a node fencing mechanism is one way of shutting down nodes
        without using an external power off device in case of a split brain scenario.
      </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">SBD Components and Mechanisms </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.11.3.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.4.4.11.3.6.2"><span class="term">SBD Partition</span></dt><dd><p> In an environment where all nodes have access to shared storage, a
      small partition of the device is formatted for use with SBD. The size of
      the partition depends on the block size of the used disk (for example,
      1 MB for standard SCSI disks with 512 byte block size or
      4 MB for DASD disks with 4 kB block size). The initialization
      process creates a message layout on the device with slots for up to 255
      nodes.</p></dd><dt id="id-1.4.4.11.3.6.3"><span class="term">SBD Daemon</span></dt><dd><p> After the respective SBD daemon is configured, it is brought online
      on each node before the rest of the cluster stack is started. It is
      terminated after all other cluster components have been shut down, thus
      ensuring that cluster resources are never activated without SBD
      supervision. </p></dd><dt id="id-1.4.4.11.3.6.4"><span class="term">Messages</span></dt><dd><p>
      The daemon automatically allocates one of the message slots on the
      partition to itself, and constantly monitors it for messages addressed
      to itself. Upon receipt of a message, the daemon immediately complies
      with the request, such as initiating a power-off or reboot cycle for
      fencing.
     </p><p>
      Also, the daemon constantly monitors connectivity to the storage device, and
      terminates itself in case the partition becomes unreachable. This
      guarantees that it is not disconnected from fencing messages. If the
      cluster data resides on the same logical unit in a different partition,
      this is not an additional point of failure: The workload will terminate
      anyway if the storage connectivity has been lost.
     </p></dd><dt id="id-1.4.4.11.3.6.5"><span class="term">Watchdog</span></dt><dd><p>
      Whenever SBD is used, a correctly working watchdog is crucial.
      Modern systems support a <span class="emphasis"><em>hardware watchdog</em></span>
      that needs to be <span class="quote">“<span class="quote">tickled</span>”</span> or <span class="quote">“<span class="quote">fed</span>”</span> by a
      software component. The software component (in this case, the SBD daemon)
      <span class="quote">“<span class="quote">feeds</span>”</span> the watchdog by regularly writing a service pulse
      to the watchdog. If the daemon stops feeding the watchdog, the hardware
      will enforce a system restart. This protects against failures of the SBD
      process itself, such as dying, or becoming stuck on an I/O error.
     </p></dd></dl></div><p>
   If Pacemaker integration is activated, SBD will not self-fence if device
   majority is lost. For example, your cluster contains three nodes: A, B, and
   C. Because of a network split, A can only see itself while B and C can
   still communicate. In this case, there are two cluster partitions: one
   with quorum because of being the majority (B, C), and one without (A).
   If this happens while the majority of fencing devices are unreachable,
   node A would immediately commit suicide, but nodes B and C would
   continue to run.
   </p></section><section class="sect1" id="sec-ha-storage-protect-steps" data-id-title="Overview of Manually Setting Up SBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.2 </span><span class="title-name">Overview of Manually Setting Up SBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-steps">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following steps are necessary to manually set up storage-based protection.
  They must be executed as <code class="systemitem">root</code>. Before you start, check <a class="xref" href="#sec-ha-storage-protect-req" title="13.3. Requirements">Section 13.3, “Requirements”</a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     <a class="xref" href="#sec-ha-storage-protect-watchdog" title="13.6. Setting Up the Watchdog">Setting Up the Watchdog</a>
    </p></li><li class="step"><p>Depending on your scenario, either use SBD with one to three devices or in diskless mode.
     For an outline, see <a class="xref" href="#sec-ha-storage-protect-fencing-number" title="13.4. Number of SBD Devices">Section 13.4, “Number of SBD Devices”</a>. The detailed setup
     is described in:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <a class="xref" href="#sec-ha-storage-protect-fencing-setup" title="13.7. Setting Up SBD with Devices">Setting Up SBD with Devices</a>
      </p></li><li class="listitem"><p>
       <a class="xref" href="#sec-ha-storage-protect-diskless-sbd" title="13.8. Setting Up Diskless SBD">Setting Up Diskless SBD</a>
      </p></li></ul></div></li><li class="step"><p>
     <a class="xref" href="#sec-ha-storage-protect-test" title="13.9. Testing SBD and Fencing">Testing SBD and Fencing</a>
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-storage-protect-req" data-id-title="Requirements"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.3 </span><span class="title-name">Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-req">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You can use up to three SBD devices for storage-based fencing.
     When using one to three devices, the shared storage must be accessible from all nodes.</p></li><li class="listitem"><p>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <code class="filename">/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</code>.
     </p></li><li class="listitem"><p>The shared storage can be connected via Fibre Channel (FC),
     Fibre Channel over Ethernet (FCoE), or even iSCSI. </p></li><li class="listitem"><p> The shared storage segment <span class="emphasis"><em>must not</em></span>
     use host-based RAID, LVM, or DRBD*. DRBD can be split, which is
     problematic for SBD, as there cannot be two states in SBD.
     Cluster multi-device (Cluster MD) cannot be used for SBD.
    </p></li><li class="listitem"><p> However, using storage-based RAID and multipathing is
     recommended for increased reliability. </p></li><li class="listitem"><p>An SBD device can be shared between different clusters, as
     long as no more than 255 nodes share the device. </p></li><li class="listitem"><p>For clusters with more than two nodes, you can also use SBD in
    <span class="emphasis"><em>diskless</em></span> mode.
   </p></li></ul></div></section><section class="sect1" id="sec-ha-storage-protect-fencing-number" data-id-title="Number of SBD Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.4 </span><span class="title-name">Number of SBD Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-fencing-number">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p> SBD supports the use of up to three devices: </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.11.6.3.1"><span class="term">One Device</span></dt><dd><p>
      The most simple implementation. It is appropriate for clusters where
      all of your data is on the same shared storage.
     </p></dd><dt id="id-1.4.4.11.6.3.2"><span class="term">Two Devices</span></dt><dd><p>
      This configuration is primarily useful for environments that use
      host-based mirroring but where no third storage device is available.
      SBD will not terminate itself if it loses access to one mirror leg,
      allowing the cluster to continue. However, since SBD does not have
      enough knowledge to detect an asymmetric split of the storage, it
      will not fence the other side while only one mirror leg is available.
      Thus, it cannot automatically tolerate a second failure while one of
      the storage arrays is down.
     </p></dd><dt id="id-1.4.4.11.6.3.3"><span class="term">Three Devices</span></dt><dd><p>
      The most reliable configuration. It is resilient against outages of
      one device—be it because of failures or maintenance. SBD
      will terminate itself only if more than one device is lost and if required,
      depending on the status of the cluster partition or node. If at least
      two devices are still accessible, fencing messages can be successfully
      transmitted.
     </p><p>
      This configuration is suitable for more complex scenarios where
      storage is not restricted to a single array. Host-based mirroring
      solutions can have one SBD per mirror leg (not mirrored itself), and
      an additional tie-breaker on iSCSI.
     </p></dd><dt id="id-1.4.4.11.6.3.4"><span class="term">Diskless</span></dt><dd><p>This configuration is useful if you want a fencing mechanism without
     shared storage. In this diskless mode, SBD fences nodes by using the
     hardware watchdog without relying on any shared device.
     However, diskless SBD cannot handle a split brain scenario for
     a two-node cluster. Use this option only for clusters with <span class="emphasis"><em>more than two</em></span> nodes.</p></dd></dl></div></section><section class="sect1" id="sec-ha-storage-protect-watchdog-timings" data-id-title="Calculation of Timeouts"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.5 </span><span class="title-name">Calculation of Timeouts</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-watchdog-timings">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      When using SBD as a fencing mechanism, it is vital to consider the timeouts
      of all components, because they depend on each other.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.11.7.3.1"><span class="term">Watchdog Timeout</span></dt><dd><p>
        This timeout is set during initialization of the SBD device. It depends
        mostly on your storage latency. The majority of devices must be successfully
        read within this time. Otherwise, the node might self-fence.
       </p><div id="id-1.4.4.11.7.3.1.2.2" data-id-title="Multipath or iSCSI Setup" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Multipath or iSCSI Setup</div><p>
          If your SBD device(s) reside on a multipath setup or iSCSI, the timeout
          should be  set to the time required to detect a path failure and switch
          to the next path.
          </p><p>
           This also means that in <code class="filename">/etc/multipath.conf</code> the
           value of  <code class="literal">max_polling_interval</code> must be less than
           <code class="literal">watchdog</code> timeout.
         </p></div></dd><dt id="id-1.4.4.11.7.3.2"><span class="term"><code class="literal">msgwait</code> Timeout</span></dt><dd><p>
        This timeout is set during initialization of the SBD device. It defines
        the time after which a message written to a node's slot on the SBD device
        is considered delivered. The timeout should be long enough for the node to
        detect that it needs to self-fence.
       </p><p>
        However, if the <code class="literal">msgwait</code> timeout is relatively long,
        a fenced cluster node might rejoin before the fencing action returns.
        This can be mitigated by setting the <code class="varname">SBD_DELAY_START</code>
        parameter in the SBD configuration, as described in
        <a class="xref" href="#pro-ha-storage-protect-sbd-config" title="Editing the SBD Configuration File">Procedure 13.4</a>
        in
        <a class="xref" href="#st-ha-storage-protect-sbd-delay-start" title="Step 3">Step 3</a>.
       </p></dd><dt id="id-1.4.4.11.7.3.3"><span class="term"><code class="literal">stonith-timeout</code> in the CIB</span></dt><dd><p>
        This timeout is set in the CIB as a global cluster property. It defines
        how long to wait for the STONITH action (reboot, on, off) to complete.
       </p></dd><dt id="id-1.4.4.11.7.3.4"><span class="term"><code class="literal">stonith-watchdog-timeout</code> in the CIB</span></dt><dd><p>
        This timeout is set in the CIB as a global cluster property. If not set
        explicitly, it defaults to <code class="literal">0</code>, which is appropriate for
        using SBD with one to three devices. For SBD in diskless mode, this timeout
        must <span class="emphasis"><em>not</em></span> be <code class="literal">0</code>. For details, see
        <a class="xref" href="#pro-ha-storage-protect-confdiskless" title="Configuring Diskless SBD">Procedure 13.8, “Configuring Diskless SBD”</a>.</p></dd></dl></div><p>
   If you change the watchdog timeout, you need to adjust the other two timeouts
   as well. The following <span class="quote">“<span class="quote">formula</span>”</span> expresses the relationship
   between these three values:
  </p><div class="example" id="ex-ha-storage-protect-sbd-timings" data-id-title="Formula for Timeout Calculation"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 13.1: </span><span class="title-name">Formula for Timeout Calculation </span></span><a title="Permalink" class="permalink" href="#ex-ha-storage-protect-sbd-timings">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">Timeout (msgwait) &gt;= (Timeout (watchdog) * 2)
stonith-timeout &gt;= Timeout (msgwait) + 20%</pre></div></div></div><p>
    For example, if you set the watchdog timeout to <code class="literal">120</code>,
    set the <code class="literal">msgwait</code> timeout to at least <code class="literal">240</code> and the
    <code class="literal">stonith-timeout</code> to at least <code class="literal">288</code>.
   </p><p>
     If you use the <span class="package">ha-cluster-bootstrap</span> scripts to set up a
     cluster and to initialize the SBD device, the relationship between these
     timeouts is automatically considered.
    </p></section><section class="sect1" id="sec-ha-storage-protect-watchdog" data-id-title="Setting Up the Watchdog"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.6 </span><span class="title-name">Setting Up the Watchdog</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-watchdog">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p> SUSE Linux Enterprise High Availability ships with several kernel modules that provide
   hardware-specific watchdog drivers. For a list of the most commonly used
   ones, see <a class="xref" href="#tab-ha-storage-protect-watchdog-drivers" title="Commonly used watchdog drivers">Commonly used watchdog drivers</a>.
 </p><p>
  For clusters in production environments we recommend to use a hardware-specific
  watchdog driver. However, if no watchdog matches your hardware,
  <code class="systemitem">softdog</code> can be used as kernel
  watchdog module.
 </p><p>
   SUSE Linux Enterprise High Availability uses the SBD daemon as the software component that <span class="quote">“<span class="quote">feeds</span>”</span>
   the watchdog.</p><section class="sect2" id="sec-ha-storage-protect-hw-watchdog" data-id-title="Using a Hardware Watchdog"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.6.1 </span><span class="title-name">Using a Hardware Watchdog</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-hw-watchdog">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>Finding the right watchdog kernel module for a given system is not
    trivial. Automatic probing fails very often. As a result, lots of modules
    are already loaded before the right one gets a chance.</p><p>
     The following table lists some commonly used watchdog drivers. However, this is
     not a complete list of supported drivers. If your hardware is not listed here,
     you can also find a list of choices in the directories
     <code class="filename">/lib/modules/<em class="replaceable">KERNEL_VERSION</em>/kernel/drivers/watchdog</code>
     and
     <code class="filename">/lib/modules/<em class="replaceable">KERNEL_VERSION</em>/kernel/drivers/ipmi</code>.
     Alternatively, ask your hardware or
     system vendor for details on system-specific watchdog configuration.
    </p><div class="table" id="tab-ha-storage-protect-watchdog-drivers" data-id-title="Commonly used watchdog drivers"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 13.1: </span><span class="title-name">Commonly used watchdog drivers </span></span><a title="Permalink" class="permalink" href="#tab-ha-storage-protect-watchdog-drivers">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Hardware</th><th style="border-bottom: 1px solid ; ">Driver</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HP</td><td style="border-bottom: 1px solid ; "><code class="systemitem">hpwdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Dell, Lenovo (Intel TCO)</td><td style="border-bottom: 1px solid ; "><code class="systemitem">iTCO_wdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Fujitsu</td><td style="border-bottom: 1px solid ; "><code class="systemitem">ipmi_watchdog</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">LPAR on IBM Power</td><td style="border-bottom: 1px solid ; "><code class="systemitem">pseries-wdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VM on IBM z/VM</td><td style="border-bottom: 1px solid ; "><code class="systemitem">vmwatchdog</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Xen VM (DomU)</td><td style="border-bottom: 1px solid ; "><code class="systemitem">xen_xdt</code></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">VM on VMware vSphere</td><td style="border-bottom: 1px solid ; "><code class="systemitem">wdat_wdt</code></td></tr><tr><td style="border-right: 1px solid ; ">Generic</td><td><code class="systemitem">softdog</code></td></tr></tbody></table></div></div><div id="id-1.4.4.11.8.5.5" data-id-title="Accessing the Watchdog Timer" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Accessing the Watchdog Timer</div><p>Some hardware vendors ship systems management software that uses the
     watchdog for system resets (for example, HP ASR daemon). If the watchdog is
     used by SBD, disable such software. No other software must access the
     watchdog timer. </p></div><div class="procedure" id="pro-ha-storage-protect-watchdog" data-id-title="Loading the Correct Kernel Module"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.1: </span><span class="title-name">Loading the Correct Kernel Module </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-watchdog">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>To make sure the correct watchdog module is loaded, proceed as follows:</p><ol class="procedure" type="1"><li class="step"><p>List the drivers that have been installed with your kernel version:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rpm</code> -ql kernel-<em class="replaceable">VERSION</em> | <code class="command">grep</code> watchdog</pre></div></li><li class="step" id="st-ha-storage-listwatchdog-modules"><p>List any watchdog modules that are currently loaded in the kernel:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">egrep</code> "(wd|dog)"</pre></div></li><li class="step"><p>If you get a result, unload the wrong module:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rmmod</code> <em class="replaceable">WRONG_MODULE</em></pre></div></li><li class="step"><p> Enable the watchdog module that matches your hardware: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo</code> <em class="replaceable">WATCHDOG_MODULE</em> &gt; /etc/modules-load.d/watchdog.conf
<code class="prompt root"># </code><code class="command">systemctl</code> restart systemd-modules-load</pre></div></li><li class="step"><p>Test whether the watchdog module is loaded correctly:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">grep</code> dog</pre></div></li><li class="step"><p>Verify if the watchdog device is available and works:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ls -l</code> /dev/watchdog*
<code class="prompt root"># </code><code class="command">sbd</code> query-watchdog</pre></div><p> If your watchdog device is not available, stop here and check the
      module name and options. Maybe use another driver. </p></li><li class="step"><p>
      Verify if the watchdog device works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd</code> -w <em class="replaceable">WATCHDOG_DEVICE</em> test-watchdog</pre></div></li><li class="step"><p>
      Reboot your machine to make sure there are no conflicting kernel modules. For example,
      if you find the message <code class="literal">cannot register ...</code> in your log, this would indicate
      such conflicting modules. To ignore such modules, refer to <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist" target="_blank">https://documentation.suse.com/sles/html/SLES-all/cha-mod.html#sec-mod-modprobe-blacklist</a>.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-storage-protect-sw-watchdog" data-id-title="Using the Software Watchdog (softdog)"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.6.2 </span><span class="title-name">Using the Software Watchdog (softdog)</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-sw-watchdog">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For clusters in production environments we recommend to use a hardware-specific watchdog
    driver. However, if no watchdog matches your hardware, <code class="systemitem">softdog</code> can be used as kernel watchdog module. </p><div id="id-1.4.4.11.8.6.3" data-id-title="Softdog Limitations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Softdog Limitations</div><p>
     The softdog driver assumes that at least one CPU is still running. If all
     CPUs are stuck, the code in the softdog driver that should reboot the system
     will never be executed. In contrast, hardware watchdogs keep working even
     if all CPUs are stuck.
    </p></div><div class="procedure" id="pro-ha-storage-protect-sw-watchdog" data-id-title="Loading the Softdog Kernel Module"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.2: </span><span class="title-name">Loading the Softdog Kernel Module </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sw-watchdog">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Enable the softdog watchdog:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">echo</code> softdog &gt; /etc/modules-load.d/watchdog.conf
<code class="prompt root"># </code><code class="command">systemctl</code> restart systemd-modules-load</pre></div></li><li class="step"><p>Test whether the softdog watchdog module is loaded correctly:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsmod</code> | <code class="command">grep</code> softdog</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-storage-protect-fencing-setup" data-id-title="Setting Up SBD with Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.7 </span><span class="title-name">Setting Up SBD with Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-fencing-setup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following steps are necessary for setup:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Initializing the SBD Devices</a>
        </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-config" title="Editing the SBD Configuration File">Editing the SBD Configuration File</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-services" title="Enabling and Starting the SBD Service">Enabling and Starting the SBD Service</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-sbd-test" title="Testing the SBD Devices">Testing the SBD Devices</a>
    </p></li><li class="step"><p>
     <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the cluster to use SBD">Configuring the cluster to use SBD</a>
    </p></li></ol></div></div><p>
    Before you start, make sure the block device or devices you want to use for
    SBD meet the requirements specified in <a class="xref" href="#sec-ha-storage-protect-req" title="13.3. Requirements">Section 13.3</a>.
  </p><p>
   When setting up the SBD devices, you need to take several timeout values into
   account. For details, see <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="13.5. Calculation of Timeouts">Section 13.5, “Calculation of Timeouts”</a>.
  </p><p>
   The node will terminate itself if the SBD daemon running on it has not
   updated the watchdog timer fast enough. After having set the timeouts, test
   them in your specific environment.
  </p><div class="procedure" id="pro-ha-storage-protect-sbd-create" data-id-title="Initializing the SBD Devices"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.3: </span><span class="title-name">Initializing the SBD Devices </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-create">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To use SBD with shared storage, you must first create the messaging
    layout on one to three block devices. The <code class="command">sbd create</code> command
    will write a metadata header to the specified device or devices. It will also
    initialize the messaging slots for up to 255 nodes. If executed without any
    further options, the command will use the default timeout settings.</p><div id="id-1.4.4.11.9.7.3" data-id-title="Overwriting Existing Data" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Overwriting Existing Data</div><p> Make sure the device or devices you want to use for SBD do not hold any
       important data. When you execute the <code class="command">sbd create</code>
       command, roughly the first megabyte of the specified block devices
       will be overwritten without further requests or backup.
      </p></div><ol class="procedure" type="1"><li class="step"><p>Decide which block device or block devices to use for SBD.</p></li><li class="step"><p>Initialize the SBD device with the following command: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> create</code></pre></div><p> To use more than one device for SBD, specify the <code class="option">-d</code> option multiple times, for
      example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em> -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID3</em> create</code></pre></div></li><li class="step"><p>If your SBD device resides on a multipath group, use the <code class="option">-1</code>
      and <code class="option">-4</code> options to adjust the timeouts to use for SBD. For
      details, see <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="13.5. Calculation of Timeouts">Section 13.5, “Calculation of Timeouts”</a>.
      All timeouts are given in seconds:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> -4 180</code><span class="callout" id="co-ha-sbd-msgwait">1</span> <code class="command">-1 90</code><span class="callout" id="co-ha-sbd-watchdog">2</span> <code class="command">create</code></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-msgwait"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p> The <code class="option">-4</code> option is used to specify the
         <code class="literal">msgwait</code> timeout. In the example above, it is set to
         <code class="literal">180</code> seconds. </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-watchdog"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p> The <code class="option">-1</code> option is used to specify the
         <code class="literal">watchdog</code> timeout. In the example above, it is set
        to <code class="literal">90</code> seconds. The minimum allowed value for the
        emulated watchdog is <code class="literal">15</code> seconds. </p></td></tr></table></div></li><li class="step"><p>Check what has been written to the device: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> dump</code>
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10
==Header on disk /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> is dumped</pre></div><p> As you can see, the timeouts are also stored in the header, to ensure
    that all participating nodes agree on them. </p></li></ol></div></div><p>
    After you have initialized the SBD devices, edit the SBD configuration file,
    then enable and start the respective services for the changes to take effect.
   </p><div class="procedure" id="pro-ha-storage-protect-sbd-config" data-id-title="Editing the SBD Configuration File"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.4: </span><span class="title-name">Editing the SBD Configuration File </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-config">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Open the file <code class="filename">/etc/sysconfig/sbd</code>.</p></li><li class="step"><p>Search for the following parameter: <em class="parameter">SBD_DEVICE</em>.
     </p><p>It specifies the devices to monitor and to use for exchanging SBD messages.
     </p><p> Edit this line by replacing /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>
     with your SBD device:</p><div class="verbatim-wrap"><pre class="screen">SBD_DEVICE="/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>"</pre></div><p> If you need to specify multiple devices in the first line, separate them with semicolons
     (the order of the devices does not matter):</p><div class="verbatim-wrap"><pre class="screen">SBD_DEVICE="/dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em>;/dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em>;/dev/disk/by-id/<em class="replaceable">DEVICE_ID3</em>"</pre></div><p> If the SBD device is not accessible, the daemon fails to start and inhibits
     cluster start-up. </p></li><li class="step" id="st-ha-storage-protect-sbd-delay-start"><p>Search for the following parameter: <em class="parameter">SBD_DELAY_START</em>.</p><p>
      Enables or disables a delay. Set <em class="parameter">SBD_DELAY_START</em>
      to <code class="literal">yes</code> if <code class="literal">msgwait</code> is relatively
      long, but your cluster nodes boot very fast.
      Setting this parameter to <code class="literal">yes</code> delays the start of
      SBD on boot. This is sometimes necessary with virtual machines.
    </p><p>
      The default delay length is the same as the <code class="literal">msgwait</code> timeout value.
      Alternatively, you can specify an integer, in seconds, instead of <code class="literal">yes</code>.
    </p><p>
      If you enable <em class="parameter">SBD_DELAY_START</em>, you must also check the SBD service file
      to ensure that the value of <code class="literal">TimeoutStartSec</code> is greater than the value of
      <em class="parameter">SBD_DELAY_START</em>. For more information, see
      <a class="link" href="https://www.suse.com/support/kb/doc/?id=000019356" target="_blank">https://www.suse.com/support/kb/doc/?id=000019356</a>.
    </p></li></ol></div></div><p>After you have added your SBD devices to the SBD configuration file,
  enable the SBD daemon. The SBD daemon is a critical piece
  of the cluster stack. It needs to be running when the cluster stack is running.
  Thus, the <code class="systemitem">sbd</code> service is started as a dependency whenever
  the cluster services are started.</p><div class="procedure" id="pro-ha-storage-protect-sbd-services" data-id-title="Enabling and Starting the SBD Service"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.5: </span><span class="title-name">Enabling and Starting the SBD Service </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-services">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>On each node, enable the SBD service:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable sbd</pre></div><p>It will be started together with the Corosync service whenever the
     cluster services are started.</p></li><li class="step"><p>Restart the cluster services on each node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster restart</pre></div><p> This automatically triggers the start of the SBD daemon. </p></li></ol></div></div><p>
   As a next step, test the SBD devices as described in <a class="xref" href="#pro-ha-storage-protect-sbd-test" title="Testing the SBD Devices">Procedure 13.6</a>.
  </p><div class="procedure" id="pro-ha-storage-protect-sbd-test" data-id-title="Testing the SBD Devices"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.6: </span><span class="title-name">Testing the SBD Devices </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-sbd-test">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p> The following command will dump the node slots and their current
      messages from the SBD device: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> list</code></pre></div><p> Now you should see all cluster nodes that have ever been started with SBD listed here.
     For example, if you have a two-node cluster, the message slot should show
      <code class="literal">clear</code> for both nodes:</p><div class="verbatim-wrap"><pre class="screen">0       alice        clear
1       bob          clear</pre></div></li><li class="step"><p> Try sending a test message to one of the nodes: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sbd -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> message alice test</code></pre></div></li><li class="step"><p> The node will acknowledge the receipt of the message in the system
      log files: </p><div class="verbatim-wrap"><pre class="screen">May 03 16:08:31 alice sbd[66139]: /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>: notice: servant:
Received command test from bob on disk /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></pre></div><p> This confirms that SBD is indeed up and running on the node and
      that it is ready to receive messages. </p></li></ol></div></div><p>
   As a final step, you need to adjust the cluster configuration as described in
   <a class="xref" href="#pro-ha-storage-protect-fencing" title="Configuring the cluster to use SBD">Procedure 13.7</a>.
  </p><div class="procedure" id="pro-ha-storage-protect-fencing" data-id-title="Configuring the cluster to use SBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.7: </span><span class="title-name">Configuring the cluster to use SBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-fencing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>Enter the following:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-enabled="true" <span class="callout" id="co-ha-sbd-st-enabled">1</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-watchdog-timeout=0 <span class="callout" id="co-ha-sbd-watchdog-timeout">2</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-timeout="40s" <span class="callout" id="co-ha-sbd-st-timeout">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-st-enabled"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This is the default configuration, because clusters without STONITH are not supported.
       But in case STONITH has been deactivated for testing purposes,
       make sure this parameter is set to <code class="literal">true</code> again.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-watchdog-timeout"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>If not explicitly set, this value defaults to <code class="literal">0</code>,
        which is appropriate for use of SBD with one to three devices.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-st-timeout"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       To calculate the <em class="parameter">stonith-timeout</em>, refer to
       <a class="xref" href="#sec-ha-storage-protect-watchdog-timings" title="13.5. Calculation of Timeouts">Section 13.5, “Calculation of Timeouts”</a>.
       A <code class="systemitem">stonith-timeout</code> value of <code class="literal">40</code>
       would be appropriate if the <code class="literal">msgwait</code> timeout value for
       SBD was set to <code class="literal">30</code> seconds.</p></td></tr></table></div></li><li class="step" id="st-ha-storage-protect-fencing-static-random"><p>
    Configure the SBD STONITH resource. You do not need to clone this resource.
   </p><p>
    For a two-node cluster, in case of split brain, there will be fencing issued from
    each node to the other as expected. To prevent both nodes from being reset at practically
    the same time, it is recommended to apply the following fencing
    delays to help one of the nodes, or even the preferred node, win the fencing match.
    For clusters with more than two nodes, you do not need to apply these delays.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.11.9.15.5.3.1"><span class="term">Priority fencing delay</span></dt><dd><p>
        The <code class="literal">priority-fencing-delay</code> cluster property is disabled by
        default. By configuring a delay value, if the other node is lost and it has
        the higher total resource priority, the fencing targeting it will be delayed
        for the specified amount of time. This means that in case of split-brain,
        the more important node wins the fencing match .
      </p><p>
        Resources that matter can be configured with priority meta attribute. On
        calculation, the priority values of the resources or instances that are running
        on each node are summed up to be accounted. A promoted resource instance takes the
        configured base priority plus one, so that it receives a higher value than any
        unpromoted instance.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property priority-fencing-delay=30</pre></div><p>
        Even if <code class="literal">priority-fencing-delay</code> is used, we still
        recommend also using <code class="literal">pcmk_delay_base</code> or
        <code class="literal">pcmk_delay_max</code> as described below to address any
        situations where the nodes happen to have equal priority.
        The value of <code class="literal">priority-fencing-delay</code> should be significantly
        greater than the maximum of <code class="literal">pcmk_delay_base</code> / <code class="literal">pcmk_delay_max</code>,
        and preferably twice the maximum.
       </p></dd><dt id="id-1.4.4.11.9.15.5.3.2"><span class="term">Predictable static delay</span></dt><dd><p>This parameter adds a static delay before executing STONITH actions.
      To prevent the nodes from getting reset at the same time under split-brain of
      a two-node cluster, configure separate fencing resources with different delay values.
      The preferred node can be marked with the parameter to be targeted with a longer
      fencing delay, so that it wins any fencing match.
      To make this succeed, it is essential to create two primitive STONITH
      devices for each node. In the following configuration, alice will win
      and survive in case of a split brain scenario:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> st-sbd-alice stonith:external/sbd params \
pcmk_host_list=alice pcmk_delay_base=20
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> st-sbd-bob stonith:external/sbd params \
pcmk_host_list=bob pcmk_delay_base=0</pre></div></dd><dt id="id-1.4.4.11.9.15.5.3.3"><span class="term">Dynamic random delay</span></dt><dd><p>This parameter adds a random delay for STONITH actions on the fencing device.
       Rather than a static delay targeting a specific node, the parameter
       <em class="parameter">pcmk_delay_max</em> adds a random delay for any fencing
       with the fencing resource to prevent double reset. Unlike
       <em class="parameter">pcmk_delay_base</em>, this parameter can be specified for
       an unified fencing resource targeting multiple nodes.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> stonith_sbd stonith:external/sbd
params pcmk_delay_max=30</pre></div><div id="id-1.4.4.11.9.15.5.3.3.2.3" data-id-title="pcmk_delay_max might not prevent double reset in a split-brain scenario" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: <em class="parameter">pcmk_delay_max</em> might not prevent double reset
       in a split-brain scenario</div><p>
        The lower the value of <em class="parameter">pcmk_delay_max</em>, the higher
        the chance that a double reset might still occur.
       </p><p>
        If your aim is to have a predictable survivor, use a priority fencing delay
        or predictable static delay.
       </p></div></dd></dl></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     Submit your changes with <code class="command">commit</code> and leave the crm live
     configuration with <code class="command">quit</code>.
    </p></li></ol></div></div><p> After the resource has started, your cluster is successfully
    configured for use of SBD. It will use this method in case a
    node needs to be fenced.</p></section><section class="sect1" id="sec-ha-storage-protect-diskless-sbd" data-id-title="Setting Up Diskless SBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.8 </span><span class="title-name">Setting Up Diskless SBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-diskless-sbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>SBD can be operated in a diskless mode. In this mode, a watchdog device
    will be used to reset the node in the following cases: if it loses quorum,
    if any monitored daemon is lost and not recovered, or if Pacemaker decides
    that the node requires fencing. Diskless SBD is based on
    <span class="quote">“<span class="quote">self-fencing</span>”</span> of a node, depending on the status of the cluster,
    the quorum and some reasonable assumptions. No STONITH SBD resource
    primitive is needed in the CIB.
   </p><div id="id-1.4.4.11.10.3" data-id-title="Do not block Corosync traffic in the local firewall" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Do not block Corosync traffic in the local firewall</div><p>
     Diskless SBD relies on reformed membership and loss of quorum to achieve
     fencing. Corosync traffic must be able to pass through all network interfaces,
     including the loopback interface, and must not be blocked by a local firewall.
     Otherwise, Corosync cannot reform a new membership, which can cause a
     split-brain scenario that cannot be handled by diskless SBD fencing.
    </p></div><div id="id-1.4.4.11.10.4" data-id-title="Number of Cluster Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Number of Cluster Nodes</div><p>
         Do <span class="emphasis"><em>not</em></span> use diskless SBD as a fencing mechanism
         for two-node clusters.
         Use diskless SBD only for clusters with three or more nodes.
         SBD in diskless mode cannot handle split brain scenarios for two-node clusters.
         If you want to use diskless SBD for two-node clusters, use QDevice as
         described in <a class="xref" href="#cha-ha-qdevice" title="Chapter 14. QDevice and QNetd">Chapter 14, <em>QDevice and QNetd</em></a>.
      </p></div><div class="procedure" id="pro-ha-storage-protect-confdiskless" data-id-title="Configuring Diskless SBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.8: </span><span class="title-name">Configuring Diskless SBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-storage-protect-confdiskless">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Open the file <code class="filename">/etc/sysconfig/sbd</code> and use
      the following entries:</p><div class="verbatim-wrap"><pre class="screen">SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5</pre></div><p>
       The <code class="varname">SBD_DEVICE</code> entry is not needed as no shared
       disk is used. When this parameter is missing, the <code class="systemitem">sbd</code>
       service does not start any watcher process for SBD devices.
      </p><p>
        If you need to delay the start of SBD on boot, change <code class="varname">SBD_DELAY_START</code>
        to <code class="literal">yes</code>. The default delay length is double the value of
        <code class="varname">SBD_WATCHDOG_TIMEOUT</code>. Alternatively, you can specify an integer,
        in seconds, instead of <code class="literal">yes</code>.
      </p><div id="id-1.4.4.11.10.5.2.5" data-id-title="SBD_WATCHDOG_TIMEOUT for diskless SBD and QDevice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="literal">SBD_WATCHDOG_TIMEOUT</code> for diskless SBD and QDevice</div><p>
        If you use QDevice with diskless SBD, the <code class="literal">SBD_WATCHDOG_TIMEOUT</code>
        value must be greater than QDevice's <code class="literal">sync_timeout</code> value,
        or SBD will time out and fail to start.
       </p><p>
        The default value for <code class="literal">sync_timeout</code> is 30 seconds.
        Therefore, set <code class="literal">SBD_WATCHDOG_TIMEOUT</code> to a greater value,
        such as <code class="literal">35</code>.
       </p></div></li><li class="step"><p>On each node, enable the SBD service:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> enable sbd</pre></div><p>It will be started together with the Corosync service whenever the
      cluster services are started.</p></li><li class="step"><p>Restart the cluster services on each node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster restart</pre></div><p> This automatically triggers the start of the SBD daemon. </p></li><li class="step"><p>
       Check if the parameter <em class="parameter">have-watchdog=true</em> has
       been automatically set:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show | grep have-watchdog
         have-watchdog=true</pre></div></li><li class="step"><p>Run <code class="command">crm configure</code> and set the following cluster
      properties on the crm shell:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-enabled="true" <span class="callout" id="co-ha-sbd-stonith-enabled">1</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-watchdog-timeout=10<span class="callout" id="co-ha-sbd-diskless-watchdog-timeout">2</span>
<code class="prompt custom">crm(live)configure# </code><code class="command">property</code> stonith-timeout=15<span class="callout" id="co-ha-sbd-diskless-stonith-timeout">3</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-stonith-enabled"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       This is the default configuration, because clusters without STONITH are not supported.
       But in case STONITH has been deactivated for testing purposes,
       make sure this parameter is set to <code class="literal">true</code> again.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-diskless-watchdog-timeout"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>For diskless SBD, this parameter must not equal zero.
       It defines after how long it is assumed that the fencing target has already
       self-fenced. Use the following formula to calculate this timeout:
      </p><div class="verbatim-wrap"><pre class="screen">stonith-watchdog-timeout &gt;= (SBD_WATCHDOG_TIMEOUT * 2)</pre></div><p>
        If you set <em class="parameter">stonith-watchdog-timeout</em>
        to a negative value, Pacemaker automatically calculates this timeout
        and sets it to twice the value of <em class="parameter">SBD_WATCHDOG_TIMEOUT</em>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-sbd-diskless-stonith-timeout"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         This parameter must allow sufficient time for fencing to complete.
         For diskless SBD, use the following formula to calculate this timeout:
       </p><div class="verbatim-wrap"><pre class="screen">stonith-timeout &gt;= stonith-watchdog-timeout + 20%</pre></div><div id="id-1.4.4.11.10.5.6.3.3.3" data-id-title="Diskless SBD timeouts" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Diskless SBD timeouts</div><p>
          With diskless SBD, if the <code class="literal">stonith-timeout</code> value is smaller than the
          <code class="literal">stonith-watchdog-timeout</code> value, failed nodes can become stuck
          in an <code class="literal">UNCLEAN</code> state and block failover of active resources.
        </p></div></td></tr></table></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     Submit your changes with <code class="command">commit</code> and leave the crm live
     configuration with <code class="command">quit</code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-storage-protect-test" data-id-title="Testing SBD and Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.9 </span><span class="title-name">Testing SBD and Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-test">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>To test whether SBD works as expected for node fencing purposes, use one or all
    of the following methods:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.11.11.3.1"><span class="term">Manually Triggering Fencing of a Node</span></dt><dd><p>To trigger a fencing action for node <em class="replaceable">NODENAME</em>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node fence <em class="replaceable">NODENAME</em></pre></div><p>Check if the node is fenced and if the other nodes consider the node as fenced
      after the <em class="parameter">stonith-watchdog-timeout</em>.</p></dd><dt id="id-1.4.4.11.11.3.2"><span class="term">Simulating an SBD Failure</span></dt><dd><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Identify the process ID of the SBD inquisitor:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> status sbd
● sbd.service - Shared-storage based fencing daemon

   Loaded: loaded (/usr/lib/systemd/system/sbd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-04-17 15:24:51 CEST; 6 days ago
     Docs: man:sbd(8)
  Process: 1844 ExecStart=/usr/sbin/sbd $SBD_OPTS -p /var/run/sbd.pid watch (code=exited, status=0/SUCCESS)
 Main PID: 1859 (sbd)
    Tasks: 4 (limit: 4915)
   CGroup: /system.slice/sbd.service
           ├─<span class="strong"><strong>1859 sbd: inquisitor</strong></span>
[...]</pre></div></li><li class="step"><p>Simulate an SBD failure by terminating the SBD inquisitor process.
       In our example, the process ID of the SBD inquisitor is
         <code class="literal">1859</code>):</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">kill</code> -9 1859</pre></div><p>
        The node proactively self-fences. The other nodes notice the loss of
        the node and consider it has self-fenced after the
        <em class="parameter">stonith-watchdog-timeout</em>.
       </p></li></ol></div></div></dd><dt id="id-1.4.4.11.11.3.3"><span class="term">Triggering Fencing through a Monitor Operation Failure</span></dt><dd><p>With a normal configuration, a failure of a resource <span class="emphasis"><em>stop operation</em></span>
      will trigger fencing. To trigger fencing manually, you can produce a failure
      of a resource stop operation. Alternatively, you can temporarily change
      the configuration of a resource <span class="emphasis"><em>monitor operation</em></span>
      and produce a monitor failure as described below:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Configure an <code class="literal">on-fail=fence</code> property for a resource monitor
        operation:</p><div class="verbatim-wrap"><pre class="screen">op monitor interval=10 on-fail=fence</pre></div></li><li class="step"><p>Let the monitoring operation fail (for example, by terminating the respective
        daemon, if the resource relates to a service).</p><p>This failure triggers a fencing action.</p></li></ol></div></div></dd></dl></div></section><section class="sect1" id="sec-ha-storage-protect-rsc-fencing" data-id-title="Additional Mechanisms for Storage Protection"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.10 </span><span class="title-name">Additional Mechanisms for Storage Protection</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-rsc-fencing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>Apart from node fencing via STONITH there are other methods to achieve
    storage protection at a resource level. For example, SCSI-3 and SCSI-4 use
    persistent reservations whereas <code class="literal">sfex</code> provides a locking
    mechanism. Both methods are explained in the following subsections.
  </p><section class="sect2" id="sec-ha-storage-protect-sgpersist" data-id-title="Configuring an sg_persist Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.10.1 </span><span class="title-name">Configuring an sg_persist Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-sgpersist">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The SCSI specifications 3 and 4 define <span class="emphasis"><em>persistent reservations</em></span>.
    These are SCSI protocol features and can be used for I/O fencing and failover.
    This feature is implemented in the <code class="command">sg_persist</code> Linux
    command.
   </p><div id="id-1.4.4.11.12.4.4" data-id-title="SCSI Disk Compatibility" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: SCSI Disk Compatibility</div><p> Any backing disks for <code class="literal">sg_persist</code> must be SCSI
     disk compatible. <code class="literal">sg_persist</code> only works for devices like
     SCSI disks or iSCSI LUNs.
     
     Do <span class="emphasis"><em>not</em></span> use it for IDE, SATA, or any block devices
     which do not support the SCSI protocol. </p></div><p>Before you proceed, check if your disk supports
    persistent reservations. Use the following command (replace
     <em class="replaceable">DEVICE_ID</em> with your device name):</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sg_persist -n --in --read-reservation -d /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div><p>The result shows whether your disk supports persistent reservations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Supported disk:</p><div class="verbatim-wrap"><pre class="screen">PR generation=0x0, there is NO reservation held</pre></div></li><li class="listitem"><p>Unsupported disk:</p><div class="verbatim-wrap"><pre class="screen">PR in (Read reservation): command not supported
Illegal request, Invalid opcode</pre></div></li></ul></div><p>If you get an error message (like the one above), replace the old
    disk with an SCSI compatible disk. Otherwise proceed as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the primitive resource <code class="literal">sg_persist</code>,
      using a stable device name for the disk:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive sg sg_persist \
    params devs="/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>" reservation_type=3 \
    op monitor interval=60 timeout=60</code></pre></div></li><li class="step"><p> Add the <code class="literal">sg_persist</code> primitive to a master-slave
      group: 
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms</code> ms-sg sg \
    meta master-max=1 notify=true</pre></div></li><li class="step"><p>Test the setup. When the resource is promoted, you can
      mount and write to the disk partitions on the cluster node where
      the primary instance is running, but you cannot write on the cluster node
      where the secondary instance is running.</p></li><li class="step"><p> Add a file system primitive for Ext4, using a stable device name for
     the disk partition: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive ext4 Filesystem \
    params device="/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>" directory="/mnt/ext4" fstype=ext4</code></pre></div></li><li class="step"><p> Add the following order relationship plus a collocation between the
      <code class="literal">sg_persist</code> master and the file system resource: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order o-ms-sg-before-ext4 Mandatory: ms-sg:promote ext4:start</code>
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation col-ext4-with-sg-persist inf: ext4 ms-sg:Promoted</code></pre></div></li><li class="step"><p> Check all your changes with the <code class="command">show changed</code> command.
     </p></li><li class="step"><p> Commit your changes. </p></li></ol></div></div><p>For more information, refer to the <code class="command">sg_persist</code> man
    page.</p></section><section class="sect2" id="sec-ha-storage-protect-exstoract" data-id-title="Ensuring Exclusive Storage Activation with sfex"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">13.10.2 </span><span class="title-name">Ensuring Exclusive Storage Activation with <code class="literal">sfex</code></span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     
    This section introduces <code class="literal">sfex</code>, an additional low-level
    mechanism to lock access to shared storage exclusively to one node. Note
    that sfex does not replace STONITH. As sfex requires shared
    storage, it is recommended that the SBD node fencing mechanism described
    above is used on another partition of the storage.
   </p><p>
    By design, sfex cannot be used with workloads that require concurrency
    (such as OCFS2). It serves as a layer of protection for classic failover
    style workloads. This is similar to an SCSI-2 reservation in effect, but
    more general.
   </p><section class="sect3" id="sec-ha-storage-protect-exstoract-description" data-id-title="Overview"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">13.10.2.1 </span><span class="title-name">Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract-description">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In a shared storage environment, a small partition of the storage is set
     aside for storing one or more locks.
    </p><p>
     Before acquiring protected resources, the node must first acquire the
     protecting lock. The ordering is enforced by Pacemaker. The sfex
     component ensures that even if Pacemaker were subject to a split brain
     situation, the lock will never be granted more than once.
    </p><p>
     These locks must also be refreshed periodically, so that a node's death
     does not permanently block the lock and other nodes can proceed.
    </p></section><section class="sect3" id="sec-ha-storage-protect-exstoract-requirements" data-id-title="Setup"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">13.10.2.2 </span><span class="title-name">Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-exstoract-requirements">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     In the following, learn how to create a shared partition for use with
     sfex and how to configure a resource for the sfex lock in the CIB. A
     single sfex partition can hold any number of locks, and needs 1 KB
     of storage space allocated per lock.
     By default, <code class="command">sfex_init</code> creates one lock on the partition.
    </p><div id="id-1.4.4.11.12.5.5.3" data-id-title="Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Requirements</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        The shared partition for sfex should be on the same logical unit as
        the data you want to protect.
       </p></li><li class="listitem"><p>
        The shared sfex partition must not use host-based RAID or DRBD.
       </p></li><li class="listitem"><p>
        Using an LVM logical volume is possible.
       </p></li></ul></div></div><div class="procedure" id="id-1.4.4.11.12.5.5.4" data-id-title="Creating an sfex Partition"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.9: </span><span class="title-name">Creating an sfex Partition </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.11.12.5.5.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a shared partition for use with sfex. Note the name of this
       partition and use it as a substitute for
       <code class="filename">/dev/sfex</code> below.
      </p></li><li class="step"><p>
       Create the sfex metadata with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sfex_init</code> -n 1 /dev/sfex</pre></div></li><li class="step"><p>
       Verify that the metadata has been created correctly:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">sfex_stat</code> -i 1 /dev/sfex ; echo $?</pre></div><p>
       This should return <code class="literal">2</code>, since the lock is not
       currently held.
      </p></li></ol></div></div><div class="procedure" id="id-1.4.4.11.12.5.5.5" data-id-title="Configuring a Resource for the sfex Lock"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 13.10: </span><span class="title-name">Configuring a Resource for the sfex Lock </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.11.12.5.5.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       The sfex lock is represented via a resource in the CIB, configured as
       follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on-fail="fence"</pre></div></li><li class="step"><p>
       To protect resources via an sfex lock, create mandatory order and
       placement constraints between the resources to protect the sfex resource. If
       the resource to be protected has the ID
       <code class="literal">filesystem1</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> order-sfex-1 Mandatory: sfex_1 filesystem1
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-sfex-1 inf: filesystem1 sfex_1</pre></div></li><li class="step"><p>
       If using group syntax, add the sfex resource as the first resource to
       the group:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> LAMP sfex_1 filesystem1 apache ipaddr</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="sec-ha-storage-protect-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">13.11 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-protect-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_protection.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For more details, see <code class="command">man sbd</code>.
   </p></section></section><section class="chapter" id="cha-ha-qdevice" data-id-title="QDevice and QNetd"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">QDevice and QNetd</span></span> <a title="Permalink" class="permalink" href="#cha-ha-qdevice">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
            QDevice and QNetd participate in quorum decisions. With
            assistance from the arbitrator <code class="systemitem">corosync-qnetd</code>,
            <code class="systemitem">corosync-qdevice</code> provides
            a configurable number of votes, allowing a cluster to sustain
            more node failures than the standard quorum rules allow. We
            recommend deploying <code class="systemitem">corosync-qnetd</code>
            and <code class="systemitem">corosync-qdevice</code> for
            clusters with an even number of nodes, and especially for two-node clusters.
         </p></div></div></div></div><section class="sect1" id="sec-ha-qdevice-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         In comparison to calculating quora among cluster nodes, the
         QDevice-and-QNetd approach has the following benefits:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
               It provides better sustainability in case of node failures.
            </p></li><li class="listitem"><p>
               You can write your own heuristics scripts to affect votes.
               This is especially useful for complex setups, such as SAP applications.
            </p></li><li class="listitem"><p>
               It enables you to configure a QNetd server to provide
               votes for multiple clusters.
            </p></li><li class="listitem"><p>
               Allows using diskless SBD for two-node clusters.
            </p></li><li class="listitem"><p>
               It helps with quorum decisions for clusters with an even number of
               nodes under split-brain situations, especially for two-node clusters.
            </p></li></ul></div><p>
        A setup with QDevice/QNetd consists of the following components and
         mechanisms:
      </p><div class="variablelist"><div class="title-container"><div class="variablelist-title-wrap"><div class="variablelist-title"><span class="title-number-name"><span class="title-name">QDevice/QNetd Components and Mechanisms </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.12.3.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div><dl class="variablelist"><dt id="id-1.4.4.12.3.5.2"><span class="term">QNetd (<code class="systemitem">corosync-qnetd</code>)</span></dt><dd><p>
                  A systemd service (a daemon, the <span class="quote">“<span class="quote">QNetd server</span>”</span>) which
                  is not part of the cluster.
                  The systemd service provides a vote to the <code class="systemitem">corosync-qdevice</code> daemon.
               </p><p>
                  To improve security, <code class="systemitem">corosync-qnetd</code>
                  can work with TLS for client certificate checking.
               </p></dd><dt id="id-1.4.4.12.3.5.3"><span class="term">QDevice (<code class="systemitem">corosync-qdevice</code>)</span></dt><dd><p>
                  A systemd service (a daemon) on each cluster node running together with
                  Corosync. This is the client of <code class="systemitem">corosync-qnetd</code>.
                  Its primary use is to allow a cluster to sustain more node failures than
                  standard quorum rules allow.
               </p><p>
                  QDevice is designed to work with different arbitrators. However, currently,
                  only QNetd is supported.
               </p></dd><dt id="id-1.4.4.12.3.5.4"><span class="term">Algorithms</span></dt><dd><p>
                  QDevice supports different algorithms which determine the behaviour
                  how votes are assigned.
                  Currently, the following exist:
               </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                        FFSplit (<span class="quote">“<span class="quote">fifty-fifty split</span>”</span> is the default. It is used
                        for clusters with an even number of nodes. If the cluster splits
                        into two similar partitions, this algorithm provides one vote to one of
                        the partitions, based on the results of heuristics checks and
                        other factors.
                     </p></li><li class="listitem"><p>
                       LMS (<span class="quote">“<span class="quote">last man standing</span>”</span>) allows the only
                        remaining node that can see the QNetd server to get the votes.
                        So this algorithm is useful when a cluster with only one
                        active node should remain quorate.
                     </p></li></ul></div></dd><dt id="id-1.4.4.12.3.5.5"><span class="term">Heuristics</span></dt><dd><p>
                  QDevice supports a set of commands (<span class="quote">“<span class="quote">heuristics</span>”</span>).
                  The commands are executed locally on startup of cluster services,
                  cluster membership change, successful connect to <code class="systemitem">corosync-qnetd</code>, or optionally, at
                  regular times.
                  The heuristics can be set with the <em class="parameter">quorum.device.heuristics</em>
                  key (in the <code class="filename">corosync.conf</code> file) or with the
                  <code class="option">--qdevice-heuristics-mode</code> option.
                  Both know the values <code class="literal">off</code> (default),
                  <code class="literal">sync</code>, and <code class="literal">on</code>.
                  The difference between <code class="literal">sync</code> and <code class="literal">on</code>
                  is you can additionally execute the above commands regularly.
               </p><p>
                  Only if all commands executed successfully are the heuristics
                  considered to have passed; otherwise, they failed. The heuristics' result is
                  sent to <code class="systemitem">corosync-qnetd</code> where
                  it is used in calculations to determine which partition should be quorate.
               </p></dd><dt id="id-1.4.4.12.3.5.6"><span class="term">Tiebreaker</span></dt><dd><p>
                  This is used as a fallback if the cluster partitions are completely
                  equal even with the same heuristics results. It can be configured
                  to be the lowest, the highest, or a specific node ID.
               </p></dd></dl></div></section><section class="sect1" id="sec-ha-qdevice-require" data-id-title="Requirements and Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.2 </span><span class="title-name">Requirements and Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-require">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         Before setting up QDevice and QNetd, you need to prepare the
         environment as the following:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
               In addition to the cluster nodes, you have a separate machine
               which will become the QNetd server.
               See <a class="xref" href="#sec-ha-qdevice-setup-qnetd" title="14.3. Setting Up the QNetd Server">Section 14.3, “Setting Up the QNetd Server”</a>.
            </p></li><li class="listitem"><p>
               A different physical network than the one that Corosync uses.
               It is recommended for QDevice to reach the QNetd server.
               Ideally, the QNetd server should be in a separate rack than the
               main cluster, or at least on a separate PSU and not in the same
               network segment as the corosync ring or rings.
            </p></li></ul></div></section><section class="sect1" id="sec-ha-qdevice-setup-qnetd" data-id-title="Setting Up the QNetd Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.3 </span><span class="title-name">Setting Up the QNetd Server</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-setup-qnetd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         The QNetd server is not part of the cluster stack, it is also
         not a real member of your cluster. As such, you cannot move resources
         to this server.
      </p><p>
         The QNetd server is almost <span class="quote">“<span class="quote">state free</span>”</span>. Usually, you do not need to
         change anything in the configuration file <code class="filename">/etc/sysconfig/corosync-qnetd</code>.
         By default, the <span class="package">corosync-qnetd</span> service runs the daemon
         as user <code class="systemitem">coroqnetd</code>
         in the group <code class="systemitem">coroqnetd</code>. This avoids
         running the daemon as <code class="systemitem">root</code>.
      </p><p>
         To create a QNetd server, proceed as follows:
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
               On the machine that will become the  QNetd server, install
               SUSE Linux Enterprise Server 15 SP2.
            </p></li><li class="step"><p>
           Enable the SUSE Linux Enterprise High Availability using the command listed in
           <code class="command">SUSEConnect --list-extensions</code>.
          </p></li><li class="step"><p>
               Install the <span class="package">corosync-qnetd</span> package:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper</code> install corosync-qnetd</pre></div><p>
               You do not need to manually start the
               <code class="systemitem">corosync-qnetd</code> service.
               It will be started automatically when you configure QDevice on
               the cluster.
      </p></li></ol></div></div><p>
         Your QNetd server is ready to accept connections from a QDevice client
         <code class="systemitem">corosync-qdevice</code>.
         Further configuration is not needed.
      </p></section><section class="sect1" id="sec-ha-qdevice-qdevice" data-id-title="Connecting QDevice Clients to the QNetd Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.4 </span><span class="title-name">Connecting QDevice Clients to the QNetd Server</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-qdevice">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         After you set up your QNetd server, you can set up and run the clients.
         You can connect the clients to the QNetd server during the installation
         of your cluster, or you can add them later. This procedure documents how
         to add them later.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
             On all nodes, install the <span class="package">corosync-qdevice</span> package:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper</code> install corosync-qdevice</pre></div></li><li class="step"><p>
             On one of the nodes, run the following command to configure QDevice:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster init qdevice
Do you want to configure QDevice (y/n)? <code class="command">y</code>
HOST or IP of the QNetd server to be used []<code class="command"><em class="replaceable">QNETD_SERVER</em></code>
TCP PORT of QNetd server [5403]
QNetd decision ALGORITHM (ffsplit/lms) [ffsplit]
QNetd TIE_BREAKER (lowest/highest/valid node id) [lowest]
Whether using TLS on QDevice/QNetd (on/off/required) [on]
Heuristics COMMAND to run with absolute path; For multiple commands, use ";" to separate []</pre></div><p>
             Confirm with <code class="literal">y</code> that you want to configure QDevice,
             then enter the host name or IP address of the QNetd server. For the
             remaining fields, you can accept the default values or change them
             if required.
            </p><div id="id-1.4.4.12.6.3.2.4" data-id-title="SBD_WATCHDOG_TIMEOUT for diskless SBD and QDevice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: <code class="literal">SBD_WATCHDOG_TIMEOUT</code> for diskless SBD and QDevice</div><p>
              If you use QDevice with diskless SBD, the <code class="literal">SBD_WATCHDOG_TIMEOUT</code>
              value must be greater than QDevice's <code class="literal">sync_timeout</code> value,
              or SBD will time out and fail to start.
             </p><p>
              The default value for <code class="literal">sync_timeout</code> is 30 seconds.
              Therefore, in the file <code class="filename">/etc/sysconfig/sbd</code>, make sure
              that <code class="literal">SBD_WATCHDOG_TIMEOUT</code> is set to a greater value,
              such as <code class="literal">35</code>.
             </p></div></li></ol></div></div></section><section class="sect1" id="sec-ha-qdevice-heuristic" data-id-title="Setting Up a QDevice with Heuristics"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.5 </span><span class="title-name">Setting Up a QDevice with Heuristics</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-heuristic">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         If you need additional control over how votes are determined, use heuristics.
         Heuristics are a set of commands which are executed in parallel.
      </p><p>
         For this purpose, the command <code class="command">crm cluster init qdevice</code>
         provides the option <code class="option">--qdevice-heuristics</code>. You can
         pass one or more commands (separated by semicolon) with absolute paths.
      </p><p>
         For example, if your own command for heuristic checks is located at
         <code class="filename">/usr/sbin/my-script.sh</code> you can run it on
         one of your cluster nodes as follows:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster init qdevice --qnetd-hostname=charlie \
     --qdevice-heuristics=/usr/sbin/my-script.sh  \
     --qdevice-heuristics-mode=on</pre></div><p>
         The command or commands can be written in any language such as Shell, Python, or Ruby.
         If they succeed, they return <code class="literal">0</code> (zero), otherwise they return an error code.
      </p><p>
         You can also pass a set of commands. Only when all commands finish successfully (return code is zero), the heuristics have passed.
      </p><p>
         The <code class="option">--qdevice-heuristics-mode=on</code> option lets the heuristics
         commands run regularly.
      </p></section><section class="sect1" id="sec-ha-qdevice-status" data-id-title="Checking and Showing Quorum Status"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.6 </span><span class="title-name">Checking and Showing Quorum Status</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-status">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         You can query the quorum status on one of your cluster nodes as shown in <a class="xref" href="#ex-ha-qdevice-crm-corosync-status-quorum" title="Status of QDevice">Example 14.1, “Status of QDevice”</a>.
         It shows the status of your QDevice nodes.
      </p><div class="example" id="ex-ha-qdevice-crm-corosync-status-quorum" data-id-title="Status of QDevice"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 14.1: </span><span class="title-name">Status of QDevice </span></span><a title="Permalink" class="permalink" href="#ex-ha-qdevice-crm-corosync-status-quorum">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">corosync-quorumtool</code> <span class="callout" id="co-quorum-cmd">1</span>
 Quorum information
------------------
Date:             ...
Quorum provider:  corosync_votequorum
Nodes:            2 <span class="callout" id="co-quorum-nodesnumber">2</span>
Node ID:          3232235777 <span class="callout" id="co-quorum-nodeid">3</span>
Ring ID:          3232235777/8
Quorate:          Yes <span class="callout" id="co-quorum-quorate">4</span>

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
 3232235777         1    A,V,NMW 192.168.1.1 (local) <span class="callout" id="co-quorum-nodes">5</span>
 3232235778         1    A,V,NMW 192.168.1.2 <a class="xref" href="#co-quorum-nodes"><span class="callout">5</span></a>
         0          1            Qdevice</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-cmd"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  As an alternative with an identical result, you can also use
                  the <code class="command">crm corosync status quorum</code> command.
               </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-nodesnumber"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  The number of expected nodes we are expecting. In this example, it is a
                  two-node cluster.
               </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-nodeid"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  As the node ID is not explicitly specified in <code class="filename">corosync.conf</code>
                  this ID is a 32-bit integer representation of the IP address.
                  In this example, the value
                  <code class="literal">3232235777</code> stands for the IP address <code class="systemitem">192.168.1.1</code>.
               </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-quorate"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  The quorum status. In this case, the cluster has quorum.
               </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-nodes"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                  The status for each cluster node means:
               </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.12.8.3.3.5.2.1"><span class="term"><code class="literal">A</code> (Alive) or <code class="literal">NA</code> (not alive)</span></dt><dd><p>
                           Shows the connectivity status between QDevice and Corosync.
                           If there is a heartbeat between QDevice and Corosync,
                           it is shown as alive (A).
                        </p></dd><dt id="id-1.4.4.12.8.3.3.5.2.2"><span class="term"><code class="literal">V</code> (Vote) or <code class="literal">NV</code> (non vote)</span></dt><dd><p>
                           Shows if the quorum device has given a vote (letter <code class="literal">V</code>)
                           to the node.
                           A letter <code class="literal">V</code> means that both nodes can communicate
                           with each other. In a split-brain situation, one node would be
                           set to <code class="literal">V</code> and the other node would be set to
                           <code class="literal">NV</code>.
                        </p></dd><dt id="id-1.4.4.12.8.3.3.5.2.3"><span class="term"><code class="literal">MW</code> (Master wins) or
                           <code class="literal">NMW</code>(not master wins)</span></dt><dd><p>
                           Shows if the quorum device <em class="parameter">master_wins</em>
                           flag is set. By default, the flag is not set, so you see <code class="literal">NMW</code> (not master
                           wins)
                           See the man page votequorum_qdevice_master_wins(3) for more
                           information.
                        </p></dd><dt id="id-1.4.4.12.8.3.3.5.2.4"><span class="term"><code class="literal">NR</code> (not registered)</span></dt><dd><p>
                           Shows that the cluster is not using a quorum device.
                        </p></dd></dl></div></td></tr></table></div></div></div><p>
         If you want to query the status of the QNetd server, you get a similar output as
         shown in <a class="xref" href="#ex-ha-qdevice-crm-corosync-status-qnetd" title="Status of QNetd Server">Example 14.2, “Status of QNetd Server”</a>:
      </p><div class="example" id="ex-ha-qdevice-crm-corosync-status-qnetd" data-id-title="Status of QNetd Server"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 14.2: </span><span class="title-name">Status of QNetd Server </span></span><a title="Permalink" class="permalink" href="#ex-ha-qdevice-crm-corosync-status-qnetd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">corosync-qnetd-tool</code> <span class="callout" id="co-quorum-qnet-cmd">1</span>
Cluster "hacluster": <span class="callout" id="co-quorum-qnet-name">2</span>
    Algorithm:          Fifty-Fifty split <span class="callout" id="co-quorum-qnet-algo">3</span>
    Tie-breaker:        Node with lowest node ID
    Node ID 3232235777: <span class="callout" id="co-quorum-qnet-alice">4</span>
        Client address:         ::ffff:192.168.1.1:54732
        HB interval:            8000ms
        Configured node list:   3232235777, 3232235778
        Ring ID:                aa10ab0.8
        Membership node list:   3232235777, 3232235778
        Heuristics:             Undefined (membership: Undefined, regular: Undefined)
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 3232235778:
        Client address:         ::ffff:192.168.1.2:43016
        HB interval:            8000ms
        Configured node list:   3232235777, 3232235778
        Ring ID:                aa10ab0.8
        Membership node list:   3232235777, 3232235778
        Heuristics:             Undefined (membership: Undefined, regular: Undefined)
        TLS active:             Yes (client certificate verified)
        Vote:                   No change (ACK)</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-qnet-cmd"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                As an alternative with an identical result, you can also use
                the <code class="command">crm corosync status qnetd</code> command.
             </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-qnet-name"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The name of your cluster as set in the configuration file
                <code class="filename">/etc/corosync/corosync.conf</code> in the
                <code class="literal">totem.cluster_name</code> section.
             </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-qnet-algo"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                The algorithm currently used. In this example, it is <code class="literal">FFSplit</code>.
             </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-quorum-qnet-alice"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
                This is the entry for the node with the IP address
                <code class="systemitem">192.168.1.1</code>.
                TLS is active.
             </p></td></tr></table></div></div></div></section><section class="sect1" id="sec-ha-qdevice-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.7 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-qdevice-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_qdevice-qnetd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
         For additional information about QDevice and QNetd
         Man pages of corosync-qdevice(8), corosync-qnetd(8).
      </p></section></section><section class="chapter" id="cha-ha-acl" data-id-title="Access Control Lists"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">15 </span><span class="title-name">Access Control Lists</span></span> <a title="Permalink" class="permalink" href="#cha-ha-acl">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The cluster administration tools like crm shell (crmsh) or
    Hawk2 can be used by <code class="systemitem">root</code> or any user in the group
    <code class="systemitem">haclient</code>. By default, these
    users have full read/write access. To limit access or assign more
    fine-grained access rights, you can use <span class="emphasis"><em>Access control
    lists</em></span> (ACLs).
   </p><p>
    Access control lists consist of an ordered set of access rules. Each
    rule allows read or write access or denies access to a part of the
    cluster configuration. Rules are typically combined to produce a
    specific role, then users may be assigned to a role that matches their
    tasks.
   </p></div></div></div></div><div id="id-1.4.4.13.4" data-id-title="CIB Syntax Validation Version and ACL Differences" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: CIB Syntax Validation Version and ACL Differences</div><p>
   This ACL documentation only applies if your CIB is validated with the CIB
   syntax version <code class="literal">pacemaker-2.0</code> or higher. For details on
   how to check this and upgrade the CIB version, see
   <a class="xref" href="#note-ha-cib-upgrade" title="Note: Upgrading the CIB Syntax Version">Note: Upgrading the CIB Syntax Version</a>.
  </p></div><section class="sect1" id="sec-ha-acl-require" data-id-title="Requirements and Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.1 </span><span class="title-name">Requirements and Prerequisites</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-require">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you start using ACLs on your cluster, make sure the following
   conditions are fulfilled:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ensure you have the same users on all nodes in your cluster, either by
     using NIS, Active Directory, or by manually adding the same users to
     all nodes.
    </p></li><li class="listitem"><p>
     All users for whom you want to modify access rights with ACLs must
     belong to the <code class="systemitem">haclient</code>
     group.
    </p></li><li class="listitem"><p>
     All users need to run crmsh by its absolute path
     <code class="filename">/usr/sbin/crm</code>.
    </p></li><li class="listitem"><p>
     If non-privileged users want to run crmsh, their
     <code class="envar">PATH</code> variable needs to be extended with
     <code class="filename">/usr/sbin</code>.
    </p></li></ul></div><div id="id-1.4.4.13.5.4" data-id-title="Default Access Rights" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Default Access Rights</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      ACLs are an optional feature. By default, use of ACLs is disabled.
     </p></li><li class="listitem"><p>
      If ACLs are not enabled, <code class="systemitem">root</code> and all users belonging to the
      <code class="systemitem">haclient</code> group have full
      read/write access to the cluster configuration.
     </p></li><li class="listitem"><p>
      Even if ACLs are enabled and configured, both <code class="systemitem">root</code> and the
      default CRM owner <code class="systemitem">hacluster</code>
      <span class="emphasis"><em>always</em></span> have full access to the cluster
      configuration.
     </p></li></ul></div></div></section><section class="sect1" id="sec-ha-acl-basics" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.2 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-basics">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Access control lists consist of an ordered set of access rules. Each rule
   allows read or write access or denies access to a part of the cluster
   configuration. Rules are typically combined to produce a specific role,
   then users may be assigned to a role that matches their tasks. An ACL
   role is a set of rules which describe access rights to CIB. A rule
   consists of the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     an access right like <code class="literal">read</code>, <code class="literal">write</code>,
     or <code class="literal">deny</code>
    </p></li><li class="listitem"><p>
     a specification where to apply the rule. This specification can be a
     type, an ID reference, or an XPath expression.
     XPath is a language for selecting nodes in an XML document. Refer to
     <a class="link" href="http://en.wikipedia.org/wiki/XPath" target="_blank">http://en.wikipedia.org/wiki/XPath</a>.
    </p></li></ul></div><p>
   Usually, it is convenient to bundle ACLs into roles and assign a specific
   role to system users (ACL targets). There are these methods to create ACL
   roles:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#sec-ha-acl-config-xpath" title="15.7. Setting ACL Rules via XPath Expressions">Section 15.7, “Setting ACL Rules via XPath Expressions”</a>. You need to know the
     structure of the underlying XML to create ACL rules.
    </p></li><li class="listitem"><p>
     <a class="xref" href="#sec-ha-acl-config-tag" title="15.8. Setting ACL Rules via Abbreviations">Section 15.8, “Setting ACL Rules via Abbreviations”</a>. Create a shorthand syntax and
     ACL rules to apply to the matched objects.
    </p></li></ul></div></section><section class="sect1" id="sec-ha-acl-enable" data-id-title="Enabling Use of ACLs in Your Cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.3 </span><span class="title-name">Enabling Use of ACLs in Your Cluster</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-enable">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can start configuring ACLs, you need to
   <span class="emphasis"><em>enable</em></span> use of ACLs. To do so, use the following
   command in the crmsh:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property enable-acl=true</pre></div><p>
   Alternatively, use Hawk2 as described in
   <a class="xref" href="#pro-ha-acl-enable-hawk2" title="Enabling Use of ACLs with Hawk2">Procedure 15.1, “Enabling Use of ACLs with Hawk2”</a>.
  </p><div class="procedure" id="pro-ha-acl-enable-hawk2" data-id-title="Enabling Use of ACLs with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.1: </span><span class="title-name">Enabling Use of ACLs with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-enable-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Configuration</span>
     to display the global cluster options and their current values.
    </p></li><li class="step"><p>
     Below <span class="guimenu">Cluster Configuration</span> click the empty drop-down box
     and select <span class="guimenu">enable-acl</span> to add the parameter. It is added
     with its default value <code class="literal">No</code>.
    </p></li><li class="step"><p>
     Set its value to <code class="literal">Yes</code> and apply your changes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-acl-create-ro-monitor-role" data-id-title="Creating a Read-Only Monitor Role"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.4 </span><span class="title-name">Creating a Read-Only Monitor Role</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-create-ro-monitor-role">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following subsections describe how to configure read-only access
   by defining a <code class="systemitem">monitor</code> role either in Hawk2
   or crm shell.
  </p><section class="sect2" id="sec-ha-acl-config-hawk2" data-id-title="Creating a Read-Only Monitor Role with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.1 </span><span class="title-name">Creating a Read-Only Monitor Role with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-hawk2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following procedures show how to configure read-only access to the
    cluster configuration by defining a <code class="systemitem">monitor</code> role and
    assigning it to a user. Alternatively, you can use crmsh to do so,
    as described in <a class="xref" href="#pro-ha-acl-crm" title="Adding a Monitor Role and Assigning a User with crmsh">Procedure 15.4, “Adding a Monitor Role and Assigning a User with crmsh”</a>.
   </p><div class="procedure" id="pro-ha-acl-hawk2-role" data-id-title="Adding a Monitor Role with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.2: </span><span class="title-name">Adding a Monitor Role with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-hawk2-role">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      In the left navigation bar, select <span class="guimenu">Roles</span>.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Create</span>.
     </p></li><li class="step"><p>
      Enter a unique <span class="guimenu">Role ID</span>, for example,
      <code class="literal">monitor</code>.
     </p></li><li class="step"><p>
      As access <span class="guimenu">Right</span>, select <code class="literal">Read</code>.
     </p></li><li class="step"><p>
      As <span class="guimenu">Xpath</span>, enter the XPath expression
      <code class="literal">/cib</code>.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-acl-role.png"><img src="images/hawk2-acl-role.png" width="100%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Click <span class="guimenu">Create</span>.
     </p><p>
      This creates a new role with the name <code class="literal">monitor</code>, sets
      the <code class="literal">read</code> rights and applies this to all elements in
      the CIB by using the XPath expression<code class="literal">/cib</code>.
     </p></li><li class="step"><p>
      If necessary, add more rules by clicking the plus icon and specifying
      the respective parameters.
     </p></li><li class="step"><p>
      Sort the individual rules by using the arrow up or down buttons.
     </p></li></ol></div></div><div class="procedure" id="pro-ha-acl-hawk2-target" data-id-title="Assigning a Role to a Target with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.3: </span><span class="title-name">Assigning a Role to a Target with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-hawk2-target">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     To assign the role we created in <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 15.2</a> to a system user (target), proceed as follows:
    </p><ol class="procedure" type="1"><li class="step"><p>
      Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
      In the left navigation bar, select <span class="guimenu">Targets</span>.
     </p></li><li class="step"><p>
      To create a system user (ACL Target), click <span class="guimenu">Create</span> and
      enter a unique <span class="guimenu">Target ID</span>, for example, <code class="literal">tux</code>.
      Make sure this user belongs to the <code class="systemitem">haclient</code> group.
     </p></li><li class="step"><p>
      To assign a role to the target, select one or multiple <span class="guimenu">Roles</span>.
     </p><p>
      In our example, select the <code class="literal">monitor</code> role you created
      in <a class="xref" href="#pro-ha-acl-hawk2-role" title="Adding a Monitor Role with Hawk2">Procedure 15.2</a>.
     </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-acl-user-assign.png"><img src="images/hawk2-acl-user-assign.png" width="80%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
      Confirm your choice.
     </p></li></ol></div></div><p>
    To configure access rights for resources or constraints, you can also use
    the abbreviated syntax as explained in
    <a class="xref" href="#sec-ha-acl-config-tag" title="15.8. Setting ACL Rules via Abbreviations">Section 15.8, “Setting ACL Rules via Abbreviations”</a>.
   </p></section><section class="sect2" id="sec-ha-acl-config-crm" data-id-title="Creating a Read-Only Monitor Role with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.4.2 </span><span class="title-name">Creating a Read-Only Monitor Role with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-crm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following procedure shows how to configure a read-only access to the
    cluster configuration by defining a <code class="literal">monitor</code> role and
    assigning it to a user.
   </p><div class="procedure" id="pro-ha-acl-crm" data-id-title="Adding a Monitor Role and Assigning a User with crmsh"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 15.4: </span><span class="title-name">Adding a Monitor Role and Assigning a User with crmsh </span></span><a title="Permalink" class="permalink" href="#pro-ha-acl-crm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Start the interactive mode of crmsh:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code></pre></div></li><li class="step"><p>
      Define your ACL role(s):
     </p><ol type="a" class="substeps"><li class="step"><p>
        Use the <code class="command">role</code> command to define a new role:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">role</code> monitor read xpath:"/cib"</pre></div><p>
        The previous command creates a new role with the name
        <code class="literal">monitor</code>, sets the <code class="literal">read</code> rights
        and applies it to all elements in the CIB by using the XPath
        expression <code class="literal">/cib</code>. If necessary, you can add more
        access rights and XPath arguments.
       </p></li><li class="step"><p>
        Add additional roles as needed.
       </p></li></ol></li><li class="step"><p>
      Assign your roles to one or multiple ACL targets, which are the
      corresponding system users. Make sure they belong to the
      <code class="systemitem">haclient</code> group.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">acl_target</code> tux monitor</pre></div></li><li class="step"><p>
      Check your changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">show</code></pre></div></li><li class="step"><p>
      Commit your changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div><p>
    To configure access rights for resources or constraints, you can also use
    the abbreviated syntax as explained in
    <a class="xref" href="#sec-ha-acl-config-tag" title="15.8. Setting ACL Rules via Abbreviations">Section 15.8, “Setting ACL Rules via Abbreviations”</a>.
   </p></section></section><section class="sect1" id="sec-ha-acl-rm-user" data-id-title="Removing a User"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.5 </span><span class="title-name">Removing a User</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-user">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following subsections describe how to remove an existing user from
    ACL either in Hawk2 or crmsh.
   </p><section class="sect2" id="sec-ha-acl-rm-user-hawk2" data-id-title="Removing a User with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.5.1 </span><span class="title-name">Removing a User with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-user-hawk2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>To remove a user from ACL, proceed as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       In the left navigation bar, select <span class="guimenu">Targets</span>.
      </p></li><li class="step"><p>
       To remove a system user (ACL target), click the garbage bin icon under
       the <span class="guimenu">Operations</span> column.
      </p></li><li class="step"><p>
       Confirm the dialog box.
      </p></li></ol></div></div></section><section class="sect2" id="sec-ha-acl-rm-user-crmsh" data-id-title="Removing a User with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.5.2 </span><span class="title-name">Removing a User with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-user-crmsh">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p> To remove a user from ACL, replace the placeholder
     <em class="replaceable">USER</em> with the name of the user: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure delete <em class="replaceable">USERNAME</em></pre></div><p>
     As an alternative, you can use the <code class="command">edit</code> subcommand:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure edit <em class="replaceable">USERNAME</em></pre></div></section></section><section class="sect1" id="sec-ha-acl-rm-role" data-id-title="Removing an Existing Role"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.6 </span><span class="title-name">Removing an Existing Role</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-role">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following subsections describe how to remove an existing role in either Hawk2 or crmsh.
   </p><div id="id-1.4.4.13.10.3" data-id-title="Deleting Roles with Referenced Users" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Deleting Roles with Referenced Users</div><p>
     Keep in mind, no user should belong to this role. If there is still a reference to a user in the
     role, the role cannot be deleted.
     Delete the references to the users first before you delete the role.
    </p></div><section class="sect2" id="sec-ha-acl-rm-role-hawk2" data-id-title="Removing an Existing Role with Hawk2"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.1 </span><span class="title-name">Removing an Existing Role with Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-role-hawk2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>To remove a role, proceed as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Log in to Hawk2: </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
       In the left navigation bar, select <span class="guimenu">Roles</span>.
      </p></li><li class="step"><p>
       To remove a role, click the garbage bin icon under
       the <span class="guimenu">Operations</span> column.
      </p></li><li class="step"><p>
       Confirm the dialog box. If an error message appears, make sure your
       role is <span class="quote">“<span class="quote">empty</span>”</span> and does not reference users.
      </p></li></ol></div></div></section><section class="sect2" id="sec-ha-acl-rm-role-crmsh" data-id-title="Removing an Existing Role with crmsh"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">15.6.2 </span><span class="title-name">Removing an Existing Role with crmsh</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-rm-role-crmsh">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To remove an existing role, replace the placeholder <em class="replaceable">ROLE</em> with the name
    of the role:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure delete <em class="replaceable">ROLE</em></pre></div></section></section><section class="sect1" id="sec-ha-acl-config-xpath" data-id-title="Setting ACL Rules via XPath Expressions"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.7 </span><span class="title-name">Setting ACL Rules via XPath Expressions</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-xpath">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To manage ACL rules via XPath, you need to know the structure of the
   underlying XML. Retrieve the structure with the following command that
   shows your cluster configuration in XML (see
   <a class="xref" href="#ex-ha-acl-excerpt" title="Excerpt of a Cluster Configuration in XML">Example 15.1</a>):
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure show xml</pre></div><div class="example" id="ex-ha-acl-excerpt" data-id-title="Excerpt of a Cluster Configuration in XML"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 15.1: </span><span class="title-name">Excerpt of a Cluster Configuration in XML </span></span><a title="Permalink" class="permalink" href="#ex-ha-acl-excerpt">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">&lt;cib&gt;
  &lt;!-- ... --&gt;
  &lt;configuration&gt;
    &lt;crm_config&gt;
       &lt;cluster_property_set id="cib-bootstrap-options"&gt;
        &lt;nvpair name="stonith-enabled" value="true" id="cib-bootstrap-options-stonith-enabled"/&gt;
       [...]
      &lt;/cluster_property_set&gt;
    &lt;/crm_config&gt;
    &lt;nodes&gt;
      &lt;node id="175704363" uname="alice"/&gt;
      &lt;node id="175704619" uname="bob"/&gt;
    &lt;/nodes&gt;
    &lt;resources&gt; [...]  &lt;/resources&gt;
    &lt;constraints/&gt;
    &lt;rsc_defaults&gt; [...] &lt;/rsc_defaults&gt;
    &lt;op_defaults&gt; [...] &lt;/op_defaults&gt;
  &lt;configuration&gt;
&lt;/cib&gt;</pre></div></div></div><p>
   With the XPath language you can locate nodes in this XML document. For
   example, to select the root node (<code class="literal">cib</code>) use the XPath
   expression <code class="literal">/cib</code>. To locate the global cluster
   configurations, use the XPath expression
   <code class="literal">/cib/configuration/crm_config</code>.
  </p><p>
   As an example, <a class="xref" href="#tab-ha-acl-operator" title="Operator Role—Access Types and XPath Expressions">Table 15.1, “Operator Role—Access Types and XPath Expressions”</a> shows the
   parameters (access type and XPath expression) to create an
   <span class="quote">“<span class="quote">operator</span>”</span> role. Users with this role can only execute the
   tasks mentioned in the second column—they cannot reconfigure
   any resources (for example, change parameters or operations), nor change
   the configuration of colocation or order constraints.
  </p><div class="table" id="tab-ha-acl-operator" data-id-title="Operator Role—Access Types and XPath Expressions"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 15.1: </span><span class="title-name">Operator Role—Access Types and XPath Expressions </span></span><a title="Permalink" class="permalink" href="#tab-ha-acl-operator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Type
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        XPath/Explanation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//crm_config//nvpair[@name='maintenance-mode']</pre></div>
       <p>
        Turn cluster maintenance mode on or off.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//op_defaults//nvpair[@name='record-pending']</pre></div>
       <p>
        Choose whether pending operations are recorded.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//nodes/node//nvpair[@name='standby']</pre></div>
       <p>
        Set node in online or standby mode.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//resources//nvpair[@name='target-role']</pre></div>
       <p>
        Start, stop, promote or demote any resource.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//resources//nvpair[@name='maintenance']</pre></div>
       <p>
        Select if a resource should be put to maintenance mode or not.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Write
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <div class="verbatim-wrap"><pre class="screen">//constraints/rsc_location</pre></div>
       <p>
        Migrate/move resources from one node to another.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Read
       </p>
      </td><td>
       <div class="verbatim-wrap"><pre class="screen">/cib</pre></div>
       <p>
        View the status of the cluster.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-acl-config-tag" data-id-title="Setting ACL Rules via Abbreviations"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">15.8 </span><span class="title-name">Setting ACL Rules via Abbreviations</span></span> <a title="Permalink" class="permalink" href="#sec-ha-acl-config-tag">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_acl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For users who do not want to deal with the XML structure there is an
   easier method.
   
  </p><p>
   For example, consider the following XPath:
  </p><div class="verbatim-wrap"><pre class="screen">//*[@id="rsc1"]</pre></div><p>
   which locates all the XML nodes with the ID <code class="literal">rsc1</code>.
  </p><p>
   The abbreviated syntax is written like this:
  </p><div class="verbatim-wrap"><pre class="screen">ref:"rsc1"</pre></div><p>
   This also works for constraints. Here is the verbose XPath:
  </p><div class="verbatim-wrap"><pre class="screen">//constraints/rsc_location</pre></div><p>
   The abbreviated syntax is written like this:
  </p><div class="verbatim-wrap"><pre class="screen">type:"rsc_location"</pre></div><p>
   The abbreviated syntax can be used in crmsh and Hawk2. The CIB
   daemon knows how to apply the ACL rules to the matching objects.
  </p></section></section><section class="chapter" id="cha-ha-netbonding" data-id-title="Network Device Bonding"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">16 </span><span class="title-name">Network Device Bonding</span></span> <a title="Permalink" class="permalink" href="#cha-ha-netbonding">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
   For many systems, it is desirable to implement network connections that
   comply to more than the standard data security or availability
   requirements of a typical Ethernet device. In these cases, several
   Ethernet devices can be aggregated to a single bonding device.
   </p></div></div></div></div><p>
  The configuration of the bonding device is done by means of bonding module
  options. The behavior is determined through the mode of the bonding
  device. By default, this is <code class="systemitem">mode=active-backup</code>,
  which means that a different device will become active if the primary
  device fails.
 </p><p>
  When using Corosync, the bonding device is not managed by the cluster
  software. Therefore, the bonding device must be configured on each cluster
  node that might possibly need to access the bonding device.
 </p><section class="sect1" id="sec-ha-netbond-yast" data-id-title="Configuring Bonding Devices with YaST"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.1 </span><span class="title-name">Configuring Bonding Devices with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbond-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To configure a bonding device, you need to have multiple Ethernet devices
   that can be aggregated to a single bonding device. Proceed as follows:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start YaST as <code class="systemitem">root</code> and select <span class="guimenu">System</span> › <span class="guimenu">Network Settings</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Network Settings</span>, switch to the
     <span class="guimenu">Overview</span> tab, which shows the available devices.
    </p></li><li class="step"><p>
     Check if the Ethernet devices to be aggregate to a bonding device have
     an IP address assigned. If yes, change it:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Select the device to change and click <span class="guimenu">Edit</span>.
      </p></li><li class="step" id="step-bond-slave"><p>
       In the <span class="guimenu">Address</span> tab of the <span class="guimenu">Network Card
       Setup</span> dialog that opens, select the option <span class="guimenu">No Link
       and IP Setup (Bonding Slaves)</span>.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast_netw_bond.png"><img src="images/yast_netw_bond.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Click <span class="guimenu">Next</span> to return to the
       <span class="guimenu">Overview</span> tab in the <span class="guimenu">Network
       Settings</span> dialog.
      </p></li></ol></li><li class="step"><p>
     To add a new bonding device:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Click <span class="guimenu">Add</span> and set the <span class="guimenu">Device
       Type</span> to <span class="guimenu">Bond</span>. Proceed with
       <span class="guimenu">Next</span>.
      </p></li><li class="step"><p>
       Select how to assign the IP address to the bonding device. Three
       methods are at your disposal:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         No Link and IP Setup (Bonding Slaves)
        </p></li><li class="listitem"><p>
         Dynamic Address (with DHCP or Zeroconf)
        </p></li><li class="listitem"><p>
         Statically assigned IP Address
        </p></li></ul></div><p>
       Use the method that is appropriate for your environment. If
       Corosync manages virtual IP addresses, select
       <span class="guimenu">Statically assigned IP Address</span> and assign an IP
       address to the interface.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">Bond Slaves</span> tab.
      </p></li><li class="step"><p>
       To select the Ethernet devices that you want to include into the
       bond, activate the check box in front of the respective devices.
      </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast_netw_bond_slaves.png"><img src="images/yast_netw_bond_slaves.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
       Edit the <span class="guimenu">Bond Driver Options</span>. The following modes
       are available:
      </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.14.5.3.4.2.5.2.1"><span class="term"><code class="literal">balance-rr</code>
        </span></dt><dd><p>
          Provides load balancing and fault tolerance, at the cost of
          out-of-order packet transmission. This may cause delays, for
          example, for TCP reassembly.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.2"><span class="term"><code class="literal">active-backup</code>
        </span></dt><dd><p>
          Provides fault tolerance.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.3"><span class="term"><code class="literal">balance-xor</code>
        </span></dt><dd><p>
          Provides load balancing and fault tolerance.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.4"><span class="term"><code class="literal">broadcast</code>
        </span></dt><dd><p>
          Provides fault tolerance.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.5"><span class="term"><code class="literal">802.3ad</code>
        </span></dt><dd><p>
          Provides dynamic link aggregation if supported by the connected
          switch.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.6"><span class="term"><code class="literal">balance-tlb</code>
        </span></dt><dd><p>
          Provides load balancing for outgoing traffic.
         </p></dd><dt id="id-1.4.4.14.5.3.4.2.5.2.7"><span class="term"><code class="literal">balance-alb</code>
        </span></dt><dd><p>
          Provides load balancing for incoming and outgoing traffic, if the
          network devices used allow the modifying of the network device's
          hardware address while in use.
         </p></dd></dl></div></li><li class="step"><p>
       Make sure to add the parameter <code class="literal">miimon=100</code> to
       <span class="guimenu">Bond Driver Options</span>. Without this parameter, the
       link is not checked regularly, so the bonding driver might continue
       to lose packets on a faulty link.
      </p></li></ol></li><li class="step"><p>
     Click <span class="guimenu">Next</span> and leave YaST with
     <span class="guimenu">OK</span> to finish the configuration of the bonding
     device. YaST writes the configuration to
     <code class="filename">/etc/sysconfig/network/ifcfg-bond<em class="replaceable">DEVICENUMBER</em></code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-netbond-hotpug-yast" data-id-title="Hotplugging devices into a bond"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.2 </span><span class="title-name">Hotplugging devices into a bond</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbond-hotpug-yast">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Sometimes it is necessary to replace an interface in a bond with
   another one, for example, if the respective network device constantly
   fails. The solution is to set up hotplugging. It is also
   necessary to change the <code class="systemitem">udev</code> rules to match the
   device by bus ID instead of by MAC address. This enables you to replace
   defective hardware (a network card in the same slot but with a different
   MAC address), if the hardware allows for that.
  </p><div class="procedure" id="id-1.4.4.14.6.3" data-id-title="Hotplugging devices into a bond with YaST"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 16.1: </span><span class="title-name">Hotplugging devices into a bond with YaST </span></span><a title="Permalink" class="permalink" href="#id-1.4.4.14.6.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    If you prefer manual configuration instead, refer to the SUSE Linux Enterprise Server
    Administration Guide, chapter <em class="citetitle">Basic Networking</em>, section
    <em class="citetitle">Hotplugging of Bonding Slaves</em>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start YaST as <code class="systemitem">root</code> and select <span class="guimenu">System</span> › <span class="guimenu">Network Settings</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Network Settings</span>, switch to the
     <span class="guimenu">Overview</span> tab, which shows the already configured
     devices. If devices are already configured in a bond, the
     <span class="guimenu">Note</span> column shows it.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/yast_netw_bond_slaves_oview.png"><img src="images/yast_netw_bond_slaves_oview.png" width="75%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     For each of the Ethernet devices that have been aggregated to a bonding
     device, execute the following steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Select the device to change and click <span class="guimenu">Edit</span>. The
       <span class="guimenu">Network Card Setup</span> dialog opens.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">General</span> tab and make sure that
       <span class="guimenu">Activate device</span> is set to <code class="literal">On
       Hotplug</code>.
      </p></li><li class="step"><p>
       Switch to the <span class="guimenu">Hardware</span> tab.
      </p></li><li class="step"><p>
       For the <span class="guimenu">Udev rules</span>, click
       <span class="guimenu">Change</span> and select the <span class="guimenu">BusID</span>
       option.
      </p></li><li class="step"><p>
       Click <span class="guimenu">OK</span> and <span class="guimenu">Next</span> to return to
       the <span class="guimenu">Overview</span> tab in the <span class="guimenu">Network
       Settings</span> dialog. If you click the Ethernet device entry
       now, the bottom pane shows the device's details, including the bus
       ID.
      </p></li></ol></li><li class="step"><p>
     Click <span class="guimenu">OK</span> to confirm your changes and leave the
     network settings.
    </p></li></ol></div></div><p>
   At boot time, the network setup does not wait for the hotplug devices, but
   for the bond to become ready, which needs at least one available device.
   When one of the interfaces is removed from the system (unbind from
   NIC driver, <code class="command">rmmod</code> of the NIC driver or true PCI
   hotplug removal), the Kernel removes it from the bond automatically. When
   a new card is added to the system (replacement of the hardware in the
   slot), <code class="systemitem">udev</code> renames it by applying the bus-based
   persistent name rule and calls <code class="command">ifup</code> for it. The
   <code class="command">ifup</code> call automatically joins it into the bond.
  </p></section><section class="sect1" id="sec-ha-netbonding-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">16.3 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-netbonding-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_netbonding.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   All modes and many options are explained in detail in the
   <span class="guimenu">Linux Ethernet Bonding Driver HOWTO</span>. The file can be
   found at
   <code class="filename">/usr/src/linux/Documentation/networking/bonding.txt</code>
   after you have installed the package
   <code class="systemitem">kernel-source</code>.
  </p><p>
   For High Availability setups, the following options described therein are
   especially important: <code class="option">miimon</code> and
   <code class="option">use_carrier</code>.
  </p></section></section><section class="chapter" id="cha-ha-lb" data-id-title="Load Balancing"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">17 </span><span class="title-name">Load Balancing</span></span> <a title="Permalink" class="permalink" href="#cha-ha-lb">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
  <span class="emphasis"><em>Load Balancing</em></span> makes a cluster of servers appear as
  one large, fast server to outside clients. This apparent single server is
  called a <span class="emphasis"><em>virtual server</em></span>. It consists of one or more
  load balancers dispatching incoming requests and several real servers
  running the actual services. With a load balancing setup of SUSE Linux Enterprise High Availability, you
  can build highly scalable and highly available network services, such as
  Web, cache, mail, FTP, media and VoIP services.
 </p></div></div></div></div><section class="sect1" id="sec-ha-lb-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   SUSE Linux Enterprise High Availability supports two technologies for load balancing: Linux Virtual Server (LVS) and
   HAProxy. The key difference is Linux Virtual Server operates at OSI layer 4
   (Transport), configuring the network layer of kernel, while HAProxy
   operates at layer 7 (Application), running in user space. Thus Linux Virtual Server
   needs fewer resources and can handle higher loads, while HAProxy can
   inspect the traffic, do SSL termination and make dispatching decisions
   based on the content of the traffic.
  </p><p>
   On the other hand, Linux Virtual Server includes two different software:
   IPVS (IP Virtual Server) and KTCPVS (Kernel TCP Virtual Server).
   IPVS provides layer 4 load balancing whereas KTCPVS provides layer 7
   load balancing.
  </p><p>
   This section gives you a conceptual overview of load balancing
   in combination with high availability, then briefly introduces you
   to Linux Virtual Server and HAProxy. Finally, it points you to further reading.
  </p><p>
   The real servers and the load balancers may be interconnected by either
   high-speed LAN or by geographically dispersed WAN. The load balancers
   dispatch requests to the different servers. They make parallel services
   of the cluster appear as one virtual service on a single IP address (the
   virtual IP address or VIP). Request dispatching can use IP load balancing
   technologies or application-level load balancing technologies.
   Scalability of the system is achieved by transparently adding or removing
   nodes in the cluster.
  </p><p>
   High availability is provided by detecting node or service failures and
   reconfiguring the whole virtual server system appropriately, as usual.
  </p><p>
   There are several load balancing strategies. Here are some Layer 4
   strategies, suitable for Linux Virtual Server:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Round Robin. </span>
      The simplest strategy is to direct each connection to a different
      address, taking turns. For example, a DNS server can have several
      entries for a given host name. With DNS round robin, the DNS server
      will return all of them in a rotating order. Thus different clients
      will see different addresses.
     </p></li><li class="listitem"><p><span class="formalpara-title">Selecting the <span class="quote">“<span class="quote">best</span>”</span> server. </span>
      Although this has several drawbacks, balancing could be implemented
      with an <span class="quote">“<span class="quote">the first server who responds</span>”</span> or <span class="quote">“<span class="quote">the
      least loaded server</span>”</span> approach.
     </p></li><li class="listitem"><p><span class="formalpara-title">Balancing the number of connections per server. </span>
      A load balancer between users and servers can divide the number of
      users across multiple servers.
     </p></li><li class="listitem"><p><span class="formalpara-title">Geographical location. </span>
      It is possible to direct clients to a server nearby.
     </p></li></ul></div><p>
   Here are some Layer 7 strategies, suitable for HAProxy:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">URI. </span>
      Inspect the HTTP content and dispatch to a server most suitable for
      this specific URI.
     </p></li><li class="listitem"><p><span class="formalpara-title">URL parameter, RDP cookie. </span>
      Inspect the HTTP content for a session parameter, possibly in post
      parameters, or the RDP (remote desktop protocol) session cookie, and
      dispatch to the server serving this session.
     </p></li></ul></div><p>
   Although there is some overlap, HAProxy can be used in scenarios
   where LVS/<code class="command">ipvsadm</code> is not adequate and vice versa:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">SSL termination. </span>
      The front-end load balancers can handle the SSL layer. Thus the cloud
      nodes do not need to have access to the SSL keys, or could take
      advantage of SSL accelerators in the load balancers.
     </p></li><li class="listitem"><p><span class="formalpara-title">Application level. </span>
      HAProxy operates at the application level, allowing the load
      balancing decisions to be influenced by the content stream. This
      allows for persistence based on cookies and other such filters.
     </p></li></ul></div><p>
   On the other hand, LVS/<code class="command">ipvsadm</code> cannot be fully
   replaced by HAProxy:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     LVS supports <span class="quote">“<span class="quote">direct routing</span>”</span>, where the load balancer is
     only in the inbound stream, whereas the outbound traffic is routed to
     the clients directly. This allows for potentially much higher
     throughput in asymmetric environments.
    </p></li><li class="listitem"><p>
     LVS supports stateful connection table replication (via
     <code class="systemitem">conntrackd</code>). This allows for
     load balancer failover that is transparent to the client and server.
    </p></li></ul></div></section><section class="sect1" id="sec-ha-lb-lvs" data-id-title="Configuring Load Balancing with Linux Virtual Server"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.2 </span><span class="title-name">Configuring Load Balancing with Linux Virtual Server</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-lvs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following sections give an overview of the main LVS components and
   concepts. Then we explain how to set up Linux Virtual Server on SUSE Linux Enterprise High Availability.
  </p><section class="sect2" id="sec-ha-lvs-overview-director" data-id-title="Director"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.1 </span><span class="title-name">Director</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-director">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The main component of LVS is the ip_vs (or IPVS) Kernel code. It is part of
    the default Kernel and
    implements transport-layer load balancing inside the Linux Kernel
    (layer-4 switching). The node that runs a Linux Kernel including the
    IPVS code is called <span class="emphasis"><em>director</em></span>. The IPVS code running
    on the director is the essential feature of LVS.
   </p><p>
    When clients connect to the director, the incoming requests are
    load-balanced across all cluster nodes: The director forwards packets to
    the real servers, using a modified set of routing rules that make the
    LVS work. For example, connections do not originate or terminate on the
    director, it does not send acknowledgments. The director acts as a
    specialized router that forwards packets from end users to real servers
    (the hosts that run the applications that process the requests).
   </p></section><section class="sect2" id="sec-ha-lvs-overview-userspace" data-id-title="User Space Controller and Daemons"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.2 </span><span class="title-name">User Space Controller and Daemons</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-userspace">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The <code class="systemitem">ldirectord</code> daemon is a
    user space daemon for managing Linux Virtual Server and monitoring the real servers
    in an LVS cluster of load balanced virtual servers. A configuration
    file (see below) specifies the virtual services and their associated real servers and tells
    <code class="systemitem">ldirectord</code> how to configure the
    server as an LVS redirector. When the daemon is initialized, it creates
    the virtual services for the cluster.
   </p><p>
    By periodically requesting a known URL and checking the responses, the
    <code class="systemitem">ldirectord</code> daemon monitors the
    health of the real servers. If a real server fails, it will be removed
    from the list of available servers at the load balancer. When the
    service monitor detects that the dead server has recovered and is
    working again, it will add the server back to the list of available
    servers. In case that all real servers should be down, a fall-back
    server can be specified to which to redirect a Web service. Typically
    the fall-back server is localhost, presenting an emergency page about
    the Web service being temporarily unavailable.
   </p><p>
    The <code class="systemitem">ldirectord</code> uses the
    <code class="systemitem">ipvsadm</code> tool (package
    <span class="package">ipvsadm</span>) to manipulate the
    virtual server table in the Linux Kernel.
   </p></section><section class="sect2" id="sec-ha-lvs-overview-forwarding" data-id-title="Packet Forwarding"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.3 </span><span class="title-name">Packet Forwarding</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-forwarding">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    There are three different methods of how the director can send packets
    from the client to the real servers:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.15.4.6.3.1"><span class="term">Network Address Translation (NAT)</span></dt><dd><p>
       Incoming requests arrive at the virtual IP. They are forwarded to the
       real servers by changing the destination IP address and port to that
       of the chosen real server. The real server sends the response to the
       load balancer which in turn changes the destination IP address and
       forwards the response back to the client. Thus, the end user receives
       the replies from the expected source. As all traffic goes through the
       load balancer, it usually becomes a bottleneck for the cluster.
      </p></dd><dt id="id-1.4.4.15.4.6.3.2"><span class="term">IP Tunneling (IP-IP Encapsulation)</span></dt><dd><p>
       IP tunneling enables packets addressed to an IP address to be
       redirected to another address, possibly on a different network. The
       LVS sends requests to real servers through an IP tunnel (redirecting
       to a different IP address) and the real servers reply directly to the
       client using their own routing tables. Cluster members can be in
       different subnets.
      </p></dd><dt id="id-1.4.4.15.4.6.3.3"><span class="term">Direct Routing</span></dt><dd><p>
       Packets from end users are forwarded directly to the real server. The
       IP packet is not modified, so the real servers must be configured to
       accept traffic for the virtual server's IP address. The response from
       the real server is sent directly to the client. The real servers and
       load balancers need to be in the same physical network segment.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-lvs-overview-schedulers" data-id-title="Scheduling Algorithms"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.4 </span><span class="title-name">Scheduling Algorithms</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-overview-schedulers">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Deciding which real server to use for a new connection requested by a
    client is implemented using different algorithms. They are available as
    modules and can be adapted to specific needs. For an overview of
    available modules, refer to the <code class="command">ipvsadm(8)</code> man page.
    Upon receiving a connect request from a client, the director assigns a
    real server to the client based on a <span class="emphasis"><em>schedule</em></span>. The
    scheduler is the part of the IPVS Kernel code which decides which real
    server will get the next new connection.
   </p><p>More detailed description about Linux Virtual Server scheduling algorithms can be
      found at <a class="link" href="http://kb.linuxvirtualserver.org/wiki/IPVS" target="_blank">http://kb.linuxvirtualserver.org/wiki/IPVS</a>.
      Furthermore, search for <code class="option">--scheduler</code> in the
      <code class="command">ipvsadm</code> man page.
    </p><p>Related load balancing strategies for HAProxy can be found at
      <a class="link" href="http://www.haproxy.org/download/1.6/doc/configuration.txt" target="_blank">http://www.haproxy.org/download/1.6/doc/configuration.txt</a>.
    </p></section><section class="sect2" id="sec-ha-lvs-ldirectord" data-id-title="Setting Up IP Load Balancing with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.5 </span><span class="title-name">Setting Up IP Load Balancing with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    You can configure Kernel-based IP load balancing with the YaST IP
    Load Balancing module. It is a front-end for
    <code class="systemitem">ldirectord</code>.
   </p><p>
    To access the IP Load Balancing dialog, start YaST as <code class="systemitem">root</code>
    and select <span class="guimenu">High Availability</span> › <span class="guimenu">IP Load
    Balancing</span>. Alternatively, start the YaST
    cluster module as <code class="systemitem">root</code> on a command line with
    <code class="command">yast2 iplb</code>.
   </p><p>
    The default installation does not include the configuration file
    <code class="filename">/etc/ha.d/ldirectord.cf</code>.
    This file is created by the YaST module. The tabs available in the
    YaST module correspond to the structure of the
    <code class="filename">/etc/ha.d/ldirectord.cf</code> configuration file,
    defining global options and defining the options for the virtual
    services.
   </p><p>
    For an example configuration and the resulting processes between load
    balancers and real servers, refer to
    <a class="xref" href="#ex-ha-lvs-ldirectord" title="Simple ldirectord Configuration">Example 17.1, “Simple ldirectord Configuration”</a>.
   </p><div id="id-1.4.4.15.4.8.6" data-id-title="Global Parameters and Virtual Server Parameters" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Global Parameters and Virtual Server Parameters</div><p>
     If a certain parameter is specified in both the virtual server section
     and in the global section, the value defined in the virtual server
     section overrides the value defined in the global section.
    </p></div><div class="procedure" id="sec-ha-lvs-ldirectord-global" data-id-title="Configuring Global Parameters"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.1: </span><span class="title-name">Configuring Global Parameters </span></span><a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     The following procedure describes how to configure the most important
     global parameters. For more details about the individual parameters
     (and the parameters not covered here), click <span class="guimenu">Help</span> or
     refer to the <code class="systemitem">ldirectord</code> man
     page.
    </p><ol class="procedure" type="1"><li class="step"><p>
      With <span class="guimenu">Check Interval</span>, define the interval in which
      <code class="systemitem">ldirectord</code> will connect to
      each of the real servers to check if they are still online.
     </p></li><li class="step"><p>
      With <span class="guimenu">Check Timeout</span>, set the time in which the real
      server should have responded after the last check.
     </p></li><li class="step"><p>
      With <span class="guimenu">Failure Count </span> you can define how many times
      <code class="systemitem">ldirectord</code> will attempt to
      request the real servers until the check is considered failed.
     </p></li><li class="step"><p>
      With <span class="guimenu">Negotiate Timeout</span> define a timeout in seconds
      for negotiate checks.
     </p></li><li class="step"><p>
      In <span class="guimenu">Fallback</span>, enter the host name or IP address of
      the Web server onto which to redirect a Web service in case all real
      servers are down.
     </p></li><li class="step"><p>
      If you want the system to send alerts in case the connection status to
      any real server changes, enter a valid e-mail address in
      <span class="guimenu">Email Alert</span>.
     </p></li><li class="step"><p>
      With <span class="guimenu">Email Alert Frequency</span>, define after how many
      seconds the e-mail alert should be repeated if any of the real servers
      remains inaccessible.
     </p></li><li class="step"><p>
      In <span class="guimenu">Email Alert Status</span> specify the server states for
      which e-mail alerts should be sent. If you want to define more than
      one state, use a comma-separated list.
     </p></li><li class="step"><p>
      With <span class="guimenu">Auto Reload</span> define, if
      <code class="systemitem">ldirectord</code> should continuously
      monitor the configuration file for modification. If set to
      <code class="literal">yes</code>, the configuration is automatically reloaded
      upon changes.
     </p></li><li class="step"><p>
      With the <span class="guimenu">Quiescent</span> switch, define whether to remove
      failed real servers from the Kernel's LVS table or not. If set to
      <span class="guimenu">Yes</span>, failed servers are not removed. Instead their
      weight is set to <code class="literal">0</code> which means that no new
      connections will be accepted. Already established connections will
      persist until they time out.
     </p></li><li class="step"><p>
      If you want to use an alternative path for logging, specify a path for
      the log files in <span class="guimenu">Log File</span>. By default,
      <code class="systemitem">ldirectord</code> writes its log
      files to <code class="filename">/var/log/ldirectord.log</code>.
     </p></li></ol></div></div><div class="figure" id="fig-ha-lvs-yast-global"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_iplb_global.png"><img src="images/yast_iplb_global.png" width="65%" alt="YaST IP Load Balancing—Global Parameters" title="YaST IP Load Balancing—Global Parameters"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 17.1: </span><span class="title-name">YaST IP Load Balancing—Global Parameters </span></span><a title="Permalink" class="permalink" href="#fig-ha-lvs-yast-global">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div><div class="procedure" id="sec-ha-lvs-ldirectord-virtual" data-id-title="Configuring Virtual Services"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 17.2: </span><span class="title-name">Configuring Virtual Services </span></span><a title="Permalink" class="permalink" href="#sec-ha-lvs-ldirectord-virtual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     You can configure one or more virtual services by defining a couple of
     parameters for each. The following procedure describes how to configure
     the most important parameters for a virtual service. For more details
     about the individual parameters (and the parameters not covered here),
     click <span class="guimenu">Help</span> or refer to the
     <code class="systemitem">ldirectord</code> man page.
    </p><ol class="procedure" type="1"><li class="step"><p>
      In the YaST IP Load Balancing module, switch to the
      <span class="guimenu">Virtual Server Configuration</span> tab.
     </p></li><li class="step"><p>
      <span class="guimenu">Add</span> a new virtual server or <span class="guimenu">Edit</span>
      an existing virtual server. A new dialog shows the available options.
     </p></li><li class="step"><p>
      In <span class="guimenu">Virtual Server</span> enter the shared virtual IP
      address (IPv4 or IPv6) and port under which the load balancers and the
      real servers are accessible as LVS. Instead of IP address and port
      number you can also specify a host name and a service. Alternatively,
      you can also use a firewall mark. A firewall mark is a way of
      aggregating an arbitrary collection of <code class="literal">VIP:port</code>
      services into one virtual service.
     </p></li><li class="step"><p>
      To specify the <span class="guimenu">Real Servers</span>, you need to enter the
      IP addresses (IPv4, IPv6, or host names) of the servers, the ports (or
      service names) and the forwarding method. The forwarding method must
      either be <code class="literal">gate</code>, <code class="literal">ipip</code> or
      <code class="literal">masq</code>, see
      <a class="xref" href="#sec-ha-lvs-overview-forwarding" title="17.2.3. Packet Forwarding">Section 17.2.3, “Packet Forwarding”</a>.
     </p><p>
      Click the <span class="guimenu">Add</span> button and enter the required
      arguments for each real server.
     </p></li><li class="step"><p>
      As <span class="guimenu">Check Type</span>, select the type of check that should
      be performed to test if the real servers are still alive. For example,
      to send a request and check if the response contains an expected
      string, select <code class="literal">Negotiate</code>.
     </p></li><li class="step" id="step-ha-lvs-ldirectord-service"><p>
      If you have set the <span class="guimenu">Check Type</span> to
      <code class="literal">Negotiate</code>, you also need to define the type of
      service to monitor. Select it from the <span class="guimenu">Service</span>
      drop-down box.
     </p></li><li class="step"><p>
      In <span class="guimenu">Request</span>, enter the URI to the object that is
      requested on each real server during the check intervals.
     </p></li><li class="step"><p>
      If you want to check if the response from the real servers contains a
      certain string (<span class="quote">“<span class="quote">I'm alive</span>”</span> message), define a regular
      expression that needs to be matched. Enter the regular expression into
      <span class="guimenu">Receive</span>. If the response from a real server
      contains this expression, the real server is considered to be alive.
     </p></li><li class="step"><p>
      Depending on the type of <span class="guimenu">Service</span> you have selected
      in <a class="xref" href="#step-ha-lvs-ldirectord-service" title="Step 6">Step 6</a>, you also need to
      specify further parameters for authentication. Switch to the
      <span class="guimenu">Auth type</span> tab and enter the details like
      <span class="guimenu">Login</span>, <span class="guimenu">Password</span>,
      <span class="guimenu">Database</span>, or <span class="guimenu">Secret</span>. For more
      information, refer to the YaST help text or to the
      <code class="systemitem">ldirectord</code> man page.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Others</span> tab.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Scheduler</span> to be used for load balancing.
      For information on the available schedulers, refer to the
      <code class="command">ipvsadm(8)</code> man page.
     </p></li><li class="step"><p>
      Select the <span class="guimenu">Protocol</span> to be used.

      If the virtual service is specified as an IP address and port, it must
      be either <code class="literal">tcp</code> or <code class="literal">udp</code>. If the
      virtual service is specified as a firewall mark, the protocol must be
      <code class="literal">fwm</code>.
     </p></li><li class="step"><p>
      Define further parameters, if needed. Confirm your configuration with
      <span class="guimenu">OK</span>. YaST writes the configuration to
      <code class="filename">/etc/ha.d/ldirectord.cf</code>.
     </p></li></ol></div></div><div class="figure" id="fig-ha-lvs-yast-virtual"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_iplb_virtual.png"><img src="images/yast_iplb_virtual.png" width="65%" alt="YaST IP Load Balancing—Virtual Services" title="YaST IP Load Balancing—Virtual Services"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 17.2: </span><span class="title-name">YaST IP Load Balancing—Virtual Services </span></span><a title="Permalink" class="permalink" href="#fig-ha-lvs-yast-virtual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div><div class="complex-example"><div class="example" id="ex-ha-lvs-ldirectord" data-id-title="Simple ldirectord Configuration"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 17.1: </span><span class="title-name">Simple ldirectord Configuration </span></span><a title="Permalink" class="permalink" href="#ex-ha-lvs-ldirectord">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
     The values shown in <a class="xref" href="#fig-ha-lvs-yast-global" title="YaST IP Load Balancing—Global Parameters">Figure 17.1, “YaST IP Load Balancing—Global Parameters”</a> and
     <a class="xref" href="#fig-ha-lvs-yast-virtual" title="YaST IP Load Balancing—Virtual Services">Figure 17.2, “YaST IP Load Balancing—Virtual Services”</a>, would lead to the following
     configuration, defined in <code class="filename">/etc/ha.d/ldirectord.cf</code>:
    </p><div class="verbatim-wrap"><pre class="screen">autoreload = yes <span class="callout" id="co-ha-ldirectord-autoreload">1</span>
    checkinterval = 5 <span class="callout" id="co-ha-ldirectord-checkintervall">2</span>
    checktimeout = 3 <span class="callout" id="co-ha-ldirectord-checktimeout">3</span>
    quiescent = yes <span class="callout" id="co-ha-ldirectord-quiescent">4</span>
    virtual = 192.168.0.200:80 <span class="callout" id="co-ha-ldirectord-virtual">5</span>
    checktype = negotiate <span class="callout" id="co-ha-ldirectord-checktype">6</span>
    fallback = 127.0.0.1:80 <span class="callout" id="co-ha-ldirectord-fallback">7</span>
    protocol = tcp <span class="callout" id="co-ha-ldirectord-protocol">8</span>
    real = 192.168.0.110:80 gate <span class="callout" id="co-ha-ldirectord-real">9</span>
    real = 192.168.0.120:80 gate <a class="xref" href="#co-ha-ldirectord-real"><span class="callout">9</span></a>
    receive = "still alive" <span class="callout" id="co-ha-ldirectord-receive">10</span>
    request = "test.html" <span class="callout" id="co-ha-ldirectord-request">11</span>
    scheduler = wlc <span class="callout" id="co-ha-ldirectord-scheduler">12</span>
    service = http <span class="callout" id="co-ha-ldirectord-service">13</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-autoreload"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines that <code class="systemitem">ldirectord</code>
       should continuously check the configuration file for modification.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checkintervall"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Interval in which <code class="systemitem">ldirectord</code>
       will connect to each of the real servers to check if they are still
       online.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checktimeout"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Time in which the real server should have responded after the last
       check.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-quiescent"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines not to remove failed real servers from the Kernel's LVS
       table, but to set their weight to <code class="literal">0</code> instead.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-virtual"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Virtual IP address (VIP) of the LVS. The LVS is available at port
       <code class="literal">80</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-checktype"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Type of check that should be performed to test if the real servers
       are still alive.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-fallback"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Server onto which to redirect a Web service all real servers for this
       service are down.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-protocol"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Protocol to be used.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-real"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Two real servers defined, both available at port
       <code class="literal">80</code>. The packet forwarding method is
       <code class="literal">gate</code>, meaning that direct routing is used.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-receive"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Regular expression that needs to be matched in the response string
       from the real server.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-request"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       URI to the object that is requested on each real server during the
       check intervals.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-scheduler"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Selected scheduler to be used for load balancing.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-ldirectord-service"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Type of service to monitor.
      </p></td></tr></table></div><p>
     This configuration would lead to the following process flow: The
     <code class="systemitem">ldirectord</code> will connect to each
     real server once every 5 seconds
     (<a class="xref" href="#co-ha-ldirectord-checkintervall"><span class="callout">2</span></a>)
     and request <code class="literal">192.168.0.110:80/test.html</code> or
     <code class="literal">192.168.0.120:80/test.html</code> as specified in
     <a class="xref" href="#co-ha-ldirectord-real"><span class="callout">9</span></a>
     and
     <a class="xref" href="#co-ha-ldirectord-request"><span class="callout">11</span></a>.
     If it does not receive the expected <code class="literal">still alive</code>
     string
     (<a class="xref" href="#co-ha-ldirectord-receive"><span class="callout">10</span></a>)
     from a real server within 3 seconds
     (<a class="xref" href="#co-ha-ldirectord-checktimeout"><span class="callout">3</span></a>)
     of the last check, it will remove the real server from the available
     pool. However, because of the <code class="literal">quiescent=yes</code> setting
     (<a class="xref" href="#co-ha-ldirectord-quiescent"><span class="callout">4</span></a>),
     the real server will not be removed from the LVS table. Instead its
     weight will be set to <code class="literal">0</code> so that no new connections
     to this real server will be accepted. Already established connections
     will be persistent until they time out.
    </p></div></div></div></section><section class="sect2" id="sec-ha-lvs-further" data-id-title="Further Setup"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">17.2.6 </span><span class="title-name">Further Setup</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lvs-further">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_lvs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Apart from the configuration of
    <code class="systemitem">ldirectord</code> with YaST, you
    need to make sure the following conditions are fulfilled to complete the
    LVS setup:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The real servers are set up correctly to provide the needed services.
     </p></li><li class="listitem"><p>
      The load balancing server (or servers) must be able to route traffic
      to the real servers using IP forwarding. The network configuration of
      the real servers depends on which packet forwarding method you have
      chosen.
     </p></li><li class="listitem"><p>
      To prevent the load balancing server (or servers) from becoming a
      single point of failure for the whole system, you need to set up one
      or several backups of the load balancer. In the cluster configuration,
      configure a primitive resource for
      <code class="systemitem">ldirectord</code>, so that
      <code class="systemitem">ldirectord</code> can fail over to
      other servers in case of hardware failure.
     </p></li><li class="listitem"><p>
      As the backup of the load balancer also needs the
      <code class="systemitem">ldirectord</code> configuration file
      to fulfill its task, make sure the
      <code class="filename">/etc/ha.d/ldirectord.cf</code> is available on all
      servers that you want to use as backup for the load balancer. You can
      synchronize the configuration file with Csync2 as described in
      <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
     </p></li></ul></div></section></section><section class="sect1" id="sec-ha-lb-haproxy" data-id-title="Configuring Load Balancing with HAProxy"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.3 </span><span class="title-name">Configuring Load Balancing with HAProxy</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-haproxy">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_lb_haproxy.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  The following section gives an overview of the HAProxy and how to
  set up on High Availability. The load balancer distributes all requests to its
  back-end servers. It is configured as active/passive, meaning if one
  server fails, the passive server becomes active. In such a scenario, the user
  will not notice any interruption.
 </p><p>
  In this section, we will use the following setup:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    A load balancer, with the IP address
    <code class="systemitem">192.168.1.99</code>.
   </p></li><li class="listitem"><p>
    A virtual, floating IP address
    <code class="systemitem">192.168.1.99</code>.
   </p></li><li class="listitem"><p>
    Our servers (usually for Web content)
    <code class="systemitem">www.example1.com</code> (IP:
    <code class="systemitem">192.168.1.200</code>) and
    <code class="systemitem">www.example2.com</code> (IP:
    <code class="systemitem">192.168.1.201</code>)
   </p></li></ul></div><p>
  To configure HAProxy, use the following procedure:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Install the <span class="package">haproxy</span> package.
   </p></li><li class="step"><p>
    Create the file <code class="filename">/etc/haproxy/haproxy.cfg</code> with the
    following contents:
   </p><div class="verbatim-wrap"><pre class="screen">global <span class="callout" id="co-ha-lb-global">1</span>
  maxconn 256
  daemon

defaults <span class="callout" id="co-ha-lb-defaults">2</span>
  log     global
  mode    http
  option  httplog
  option  dontlognull
  retries 3
  option redispatch
  maxconn 2000
  timeout connect   5000  <span class="callout" id="co-ha-lb-timeout-connect">3</span>
  timeout client    50s   <span class="callout" id="co-ha-lb-timeout-client">4</span>
  timeout server    50000 <span class="callout" id="co-ha-lb-timeout-server">5</span>

frontend LB
  bind 192.168.1.99:80 <span class="callout" id="co-ha-lb-listen">6</span>
  reqadd X-Forwarded-Proto:\ http
  default_backend LB

backend LB
  mode http
  stats enable
  stats hide-version
  stats uri /stats
  stats realm Haproxy\ Statistics
  stats auth haproxy:password	<span class="callout" id="co-ha-lb-stats-auth">7</span>
  balance roundrobin	<span class="callout" id="co-ha-lb-balance">8</span>
  option  httpclose
  option forwardfor
  cookie LB insert
  option httpchk GET /robots.txt HTTP/1.0
  server web1-srv 192.168.1.200:80 cookie web1-srv check
  server web2-srv 192.168.1.201:80 cookie web2-srv check</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-global"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which contains process-wide and OS-specific options.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.15.5.8.2.4.1.2.1"><span class="term"><code class="option">maxconn</code>
       </span></dt><dd><p>
         Maximum per-process number of concurrent connections.
        </p></dd><dt id="id-1.4.4.15.5.8.2.4.1.2.2"><span class="term"><code class="option">daemon</code>
       </span></dt><dd><p>
         Recommended mode, HAProxy runs in the background.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-defaults"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which sets default parameters for all other sections
      following its declaration. Some important lines:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.15.5.8.2.4.2.2.1"><span class="term"><code class="option">redispatch</code>
       </span></dt><dd><p>
         Enables or disables session redistribution in case of connection
         failure.
        </p></dd><dt id="id-1.4.4.15.5.8.2.4.2.2.2"><span class="term"><code class="option">log</code>
       </span></dt><dd><p>
         Enables logging of events and traffic.
        </p></dd><dt id="id-1.4.4.15.5.8.2.4.2.2.3"><span class="term"><code class="literal">mode http</code>
       </span></dt><dd><p>
         Operates in HTTP mode (recommended mode for HAProxy). In this
         mode, a request will be analyzed before a connection to any server
         is performed. Request that are not RFC-compliant will be rejected.
        </p></dd><dt id="id-1.4.4.15.5.8.2.4.2.2.4"><span class="term"><code class="literal">option forwardfor</code>
       </span></dt><dd><p>
         Adds the HTTP <code class="option">X-Forwarded-For</code> header into the
         request. You need this option if you want to preserve the client's
         IP address.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-connect"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time to wait for a connection attempt to a server
      to succeed.
     </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-client"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time of inactivity on the client side.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-timeout-server"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>The maximum time of inactivity on the server side.</p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-listen"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
      Section which combines front-end and back-end sections in one.
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.15.5.8.2.4.6.2.1"><span class="term"><code class="literal">balance leastconn</code>
       </span></dt><dd><p>
         Defines the load balancing algorithm, see
         <a class="link" href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4-balance" target="_blank">http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#4-balance</a>.
        </p></dd><dt id="id-1.4.4.15.5.8.2.4.6.2.2"><span class="term"><code class="literal">stats enable</code>
       , </span><span class="term"><code class="literal">stats auth</code>
       </span></dt><dd><p>
         Enables statistics reporting (by <code class="literal">stats enable</code>).
         The <code class="option">auth</code> option logs statistics with
         authentication to a specific account.
        </p></dd></dl></div></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-stats-auth"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Credentials for HAProxy Statistic report page.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-ha-lb-balance"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Load balancing will work in a round-robin process.
      </p></td></tr></table></div></li><li class="step"><p>
    Test your configuration file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">haproxy</code> -f /etc/haproxy/haproxy.cfg -c</pre></div></li><li class="step"><p>
    Add the following line to Csync2's configuration file
    <code class="filename">/etc/csync2/csync2.cfg</code> to make sure the
    HAProxy configuration file is included:
   </p><div class="verbatim-wrap"><pre class="screen">include /etc/haproxy/haproxy.cfg</pre></div></li><li class="step"><p>
    Synchronize it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -f /etc/haproxy/haproxy.cfg
<code class="prompt root"># </code><code class="command">csync2</code> -xv</pre></div><div id="id-1.4.4.15.5.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     The Csync2 configuration part assumes that the HA nodes were
     configured using the bootstrap scripts provided by the crm shell. For details,
     see the Installation and Setup Quick Start.
    </p></div></li><li class="step"><p>
    Make sure HAProxy is disabled on both load balancers
    (<code class="systemitem">alice</code> and
    <code class="systemitem">bob</code>) as it is started by
    Pacemaker:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable haproxy</pre></div></li><li class="step"><p>
    Configure a new CIB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code>
<code class="prompt custom">crm(live)# </code><code class="command">cib new haproxy-config</code>
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">primitive haproxy systemd:haproxy \
    op start timeout=120 interval=0 \
    op stop timeout=120 interval=0 \
    op monitor timeout=100 interval=5s \
    meta target-role=Started</code>
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">primitive</code> vip IPaddr2 \
    params ip=192.168.1.99 nic=eth0 cidr_netmask=23 broadcast=192.168.1.255 \
    op monitor interval=5s timeout=120 on-fail=restart
<code class="prompt custom">crm(haproxy-config)# </code><code class="command">group</code> g-haproxy vip haproxy</pre></div></li><li class="step"><p>
    Verify the new CIB and correct any errors:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(haproxy-config)# </code><code class="command">verify</code></pre></div></li><li class="step"><p>
    Commit the new CIB:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(haproxy-config)# </code><code class="command">cib</code> use live
<code class="prompt custom">crm(live)# </code><code class="command">cib</code> commit haproxy-config</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-lb-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">17.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-lb-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_loadbalancing.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="link" href="http://www.haproxy.org" target="_blank">http://www.haproxy.org</a></p></li><li class="listitem"><p>Project home page at <a class="link" href="http://www.linuxvirtualserver.org/" target="_blank">http://www.linuxvirtualserver.org/</a>.
  </p></li><li class="listitem"><p>For more information about <code class="systemitem">ldirectord</code>, refer to its
        comprehensive man page.</p></li><li class="listitem"><p>LVS Knowledge Base: <a class="link" href="http://kb.linuxvirtualserver.org/wiki/Main_Page" target="_blank">http://kb.linuxvirtualserver.org/wiki/Main_Page</a></p></li></ul></div></section></section><section class="chapter" id="cha-ha-geo" data-id-title="Geo Clusters (Multi-Site Clusters)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">18 </span><span class="title-name">Geo Clusters (Multi-Site Clusters)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-geo">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_geocluster.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Apart from local clusters and metro area clusters, SUSE® Linux Enterprise High Availability
    15 SP2 also supports geographically dispersed clusters (Geo
    clusters, sometimes also called multi-site clusters). That means you can
    have multiple, geographically dispersed sites with a local cluster each.
    Failover between these clusters is coordinated by a higher level entity,
    the so-called <code class="literal">booth</code>. For details on how to
    use and set up Geo clusters, refer to <span class="intraxref">Article “Geo Clustering Quick Start”</span> and
   <span class="intraxref">Book “Geo Clustering Guide”</span>.
    </p></div></div></div></div></section></div><div class="part" id="part-storage" data-id-title="Storage and Data Replication"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part III </span><span class="title-name">Storage and Data Replication </span></span><a title="Permalink" class="permalink" href="#part-storage">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-storage-dlm"><span class="title-number">19 </span><span class="title-name">Distributed Lock Manager (DLM)</span></a></span></li><dd class="toc-abstract"><p>
    The Distributed Lock Manager (DLM) in the kernel is the base component used
    by OCFS2, GFS2, Cluster MD, and Cluster LVM (lvmlockd) to provide active-active
    storage at each respective layer.
   </p></dd><li><span class="chapter"><a href="#cha-ha-ocfs2"><span class="title-number">20 </span><span class="title-name">OCFS2</span></a></span></li><dd class="toc-abstract"><p>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    OCFS2 allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with Corosync and the Distributed
    Lock Manager (DLM).
   </p></dd><li><span class="chapter"><a href="#cha-ha-gfs2"><span class="title-number">21 </span><span class="title-name">GFS2</span></a></span></li><dd class="toc-abstract"><p>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating-mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </p><p>
    SUSE recommends OCFS2 over GFS2 for your cluster environments if
    performance is one of your major requirements. Our tests have revealed
    that OCFS2 performs better as compared to GFS2 in such settings.
   </p></dd><li><span class="chapter"><a href="#cha-ha-drbd"><span class="title-number">22 </span><span class="title-name">DRBD</span></a></span></li><dd class="toc-abstract"><p>
    The <span class="emphasis"><em>distributed replicated block device</em></span> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with Corosync,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </p></dd><li><span class="chapter"><a href="#cha-ha-clvm"><span class="title-number">23 </span><span class="title-name">Cluster Logical Volume Manager (Cluster LVM)</span></a></span></li><dd class="toc-abstract"><p>
    When managing shared storage on a cluster, every node must be informed
    about changes to the storage subsystem. Logical Volume
    Manager (LVM) supports transparent management of volume groups
    across the whole cluster. Volume groups shared among multiple hosts
    can be managed using the same commands as local storage.
   </p></dd><li><span class="chapter"><a href="#cha-ha-cluster-md"><span class="title-number">24 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></a></span></li><dd class="toc-abstract"><p>The cluster multi-device (Cluster MD) is a software based RAID
   storage solution for a cluster. Currently, Cluster MD provides the redundancy of
   RAID1 mirroring to the cluster. With SUSE Linux Enterprise High Availability 15 SP2, RAID10 is
   included as a technology preview. If you want to try RAID10, replace <code class="literal">mirror</code>
   with <code class="literal">10</code> in the related <code class="command">mdadm</code> command.
   This chapter shows you how to create and use Cluster MD.
   </p></dd><li><span class="chapter"><a href="#cha-ha-samba"><span class="title-number">25 </span><span class="title-name">Samba Clustering</span></a></span></li><dd class="toc-abstract"><p>
    A clustered Samba server provides a High Availability solution in your
    heterogeneous networks. This chapter explains some background
    information and how to set up a clustered Samba server.
   </p></dd><li><span class="chapter"><a href="#cha-ha-rear"><span class="title-number">26 </span><span class="title-name">Disaster Recovery with ReaR (Relax-and-Recover)</span></a></span></li><dd class="toc-abstract"><p>
    Relax-and-Recover (<span class="quote">“<span class="quote">ReaR</span>”</span>) is a disaster recovery framework for use by
    system administrators. It is a collection of Bash scripts that need to
    be adjusted to the specific production environment that is to be
    protected in case of disaster.
   </p><p>
    No disaster recovery solution will  work out-of-the-box. Therefore
    it is essential to take preparations <span class="emphasis"><em>before</em></span> any
    disaster happens.
   </p></dd></ul></div><section class="chapter" id="cha-ha-storage-dlm" data-id-title="Distributed Lock Manager (DLM)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">19 </span><span class="title-name">Distributed Lock Manager (DLM)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-storage-dlm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The Distributed Lock Manager (DLM) in the kernel is the base component used
    by OCFS2, GFS2, Cluster MD, and Cluster LVM (lvmlockd) to provide active-active
    storage at each respective layer.
   </p></div></div></div></div><section class="sect1" id="sec-ha-storage-dlm-protocol" data-id-title="Protocols for DLM Communication"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.1 </span><span class="title-name">Protocols for DLM Communication</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-dlm-protocol">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To avoid single points of failure, redundant communication paths are important
   for High Availability clusters. This is also true for DLM communication. If network bonding
   (Link Aggregation Control Protocol, LACP) cannot be used for any reason, we
   highly recommend defining a redundant communication channel (a second ring)
   in Corosync. For details, see <a class="xref" href="#pro-ha-installation-setup-channel2" title="Defining a Redundant Communication Channel">Procedure 4.3, “Defining a Redundant Communication Channel”</a>.
  </p><p>
   DLM communicates through port 21064 using either the TCP or SCTP protocol,
   depending on the configuration in <code class="filename">/etc/corosync/corosync.conf</code>:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If <span class="guimenu">rrp_mode</span> is set to <code class="literal">none</code> (which
     means redundant ring configuration is disabled), DLM automatically uses
     TCP. However, without a redundant communication channel, DLM communication
     will fail if the TCP link is down.
   </p></li><li class="listitem"><p>
     If <span class="guimenu">rrp_mode</span> is set to <code class="literal">passive</code> (which
     is the typical setting), and a second communication ring in <code class="filename">/etc/corosync/corosync.conf</code>
     is configured correctly, DLM automatically uses SCTP. In this case, DLM
     messaging has the redundancy capability provided by SCTP.
   </p></li></ul></div></section><section class="sect1" id="sec-ha-storage-generic-dlm-config" data-id-title="Configuring DLM Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">19.2 </span><span class="title-name">Configuring DLM Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-storage-generic-dlm-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   DLM uses the cluster membership services from Pacemaker which run in user
   space. Therefore, DLM needs to be configured as a clone resource that is
   present on each node in the cluster.
  </p><div id="id-1.4.5.3.4.3" data-id-title="DLM Resource for Several Solutions" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: DLM Resource for Several Solutions</div><p>
    As OCFS2, GFS2, Cluster MD, and Cluster LVM (lvmlockd) all use DLM, it is
    enough to configure one resource for DLM. As the DLM resource runs on all
    nodes in the cluster it is configured as a clone resource.
   </p><p>
    If you have a setup that includes both OCFS2 and Cluster LVM, configuring
    <span class="emphasis"><em>one</em></span> DLM resource for both OCFS2 and Cluster LVM is enough.
    In this case, configure DLM using <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 19.1, “Configuring a Base Group for DLM”</a>.
   </p><p>
    However, if you need to keep the resources that use DLM independent from one
    another (such as multiple OCFS2 mount points), use separate colocation and
    order constraints instead of a group. In this case, configure DLM using
    <a class="xref" href="#pro-dlm-multiple-resources" title="Configuring an independent DLM resource">Procedure 19.2, “Configuring an independent DLM resource”</a>.
   </p></div><div class="procedure" id="pro-dlm-resources" data-id-title="Configuring a Base Group for DLM"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.1: </span><span class="title-name">Configuring a Base Group for DLM </span></span><a title="Permalink" class="permalink" href="#pro-dlm-resources">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    This configuration consists of a base group that includes several primitives
    and a base clone. Both base group and base clone can be used in various
    scenarios afterward (for both OCFS2 and Cluster LVM, for example). You only need
    to extend the base group with the respective primitives as needed. As the
    base group has internal colocation and ordering, this simplifies the
    overall setup as you do not need to specify several individual groups,
    clones and their dependencies.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to a node as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm configure</code>.
    </p></li><li class="step"><p>
     Create the primitive resource for DLM:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dlm ocf:pacemaker:controld \
  op monitor interval="60" timeout="60"</pre></div></li><li class="step"><p>
     Create a base group for the <code class="literal">dlm</code> resource and further storage-related
     resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-storage dlm</pre></div></li><li class="step"><p>
     Clone the <code class="literal">g-storage</code> group so that it runs on all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code> <code class="command">clone</code> cl-storage g-storage \
  meta interleave=true target-role=Started</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div><div id="id-1.4.5.3.4.5" data-id-title="Failure When Disabling STONITH" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Failure When Disabling STONITH</div><p>
    Clusters without STONITH are not supported. If you set the global cluster
    option <code class="varname">stonith-enabled</code> to <code class="literal">false</code> for
    testing or troubleshooting purposes, the DLM resource and all services
    depending on it (such as Cluster LVM, GFS2, and OCFS2) will fail to start.
   </p></div><div class="procedure" id="pro-dlm-multiple-resources" data-id-title="Configuring an independent DLM resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 19.2: </span><span class="title-name">Configuring an independent DLM resource </span></span><a title="Permalink" class="permalink" href="#pro-dlm-multiple-resources">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_storage_basics.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    This configuration consists of a primitive and a clone, but no group.
    By adding colocation and order constraints, you can avoid introducing
    dependencies between multiple resources that use DLM (such as multiple OCFS2
    mount points).
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to a node as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm configure</code>.
    </p></li><li class="step"><p>
     Create the primitive resource for DLM:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> dlm ocf:pacemaker:controld \
  op start timeout=90 interval=0 \
  op stop timeout=100 interval=0 \
  op monitor interval=60 timeout=60</pre></div></li><li class="step"><p>
     Clone the <code class="literal">dlm</code> resource so that it runs on all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code> <code class="command">clone</code> cl-dlm dlm meta interleave=true</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-ocfs2" data-id-title="OCFS2"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">20 </span><span class="title-name">OCFS2</span></span> <a title="Permalink" class="permalink" href="#cha-ha-ocfs2">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    OCFS2 allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with Corosync and the Distributed
    Lock Manager (DLM).
   </p></div></div></div></div><section class="sect1" id="sec-ha-ocfs2-features" data-id-title="Features and Benefits"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.1 </span><span class="title-name">Features and Benefits</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-features">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   OCFS2 can be used for the following storage solutions for example:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     General applications and workloads.
    </p></li><li class="listitem"><p>
     Xen image store in a cluster. Xen virtual machines and
     virtual servers can be stored on OCFS2 volumes that are mounted by
     cluster servers. This provides quick and easy portability of Xen
     virtual machines between servers.
    </p></li><li class="listitem"><p>
     LAMP (Linux, Apache, MySQL, and PHP | Perl |
     Python) stacks.
    </p></li></ul></div><p>
   As a high-performance, symmetric and parallel cluster file system,
   OCFS2 supports the following functions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </p></li><li class="listitem"><p>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </p></li><li class="listitem"><p>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </p></li><li class="listitem"><p>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </p></li></ul></div><p>
   OCFS2 also provides the following capabilities:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Metadata caching.
    </p></li><li class="listitem"><p>
     Metadata journaling.
    </p></li><li class="listitem"><p>
     Cross-node file data consistency.
    </p></li><li class="listitem"><p>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </p></li><li class="listitem"><p>
     Support for up to 32 cluster nodes.
    </p></li><li class="listitem"><p>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </p></li></ul></div><div id="id-1.4.5.4.3.8" data-id-title="Support for OCFS2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Support for OCFS2</div><p>
    OCFS2 is only supported by SUSE when used with the pcmk (Pacemaker) stack,
    as provided by SUSE Linux Enterprise High Availability. SUSE does not provide support for OCFS2 in
    combination with the o2cb stack.
   </p></div></section><section class="sect1" id="sec-ha-ocfs2-utils" data-id-title="OCFS2 Packages and Management Utilities"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.2 </span><span class="title-name">OCFS2 Packages and Management Utilities</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-utils">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The OCFS2 Kernel module (<code class="literal">ocfs2</code>) is installed
   automatically in SUSE Linux Enterprise High Availability 15 SP2. To use
   OCFS2, make sure the following packages are installed on each node in
   the cluster: <span class="package">ocfs2-tools</span> and
   the matching <span class="package">ocfs2-kmp-*</span>
   packages for your Kernel.
  </p><p>
   The <span class="package">ocfs2-tools</span> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </p><div class="table" id="id-1.4.5.4.4.4" data-id-title="OCFS2 Utilities"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 20.1: </span><span class="title-name">OCFS2 Utilities </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.4.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        OCFS2 Utility
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        debugfs.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Examines the state of the OCFS file system for debugging.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        fsck.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Checks the file system for errors and optionally repairs errors.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mkfs.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Creates an OCFS2 file system on a device, usually a partition on
        a shared physical or logical disk.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mounted.ocfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Detects and lists all OCFS2 volumes on a clustered system.
        Detects and lists all nodes on the system that have mounted an
        OCFS2 device or lists all OCFS2 devices.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        tunefs.ocfs2
       </p>
      </td><td>
       <p>
        Changes OCFS2 file system parameters, including the volume
        label, number of node slots, journal size for all node slots, and
        volume size.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-ocfs2-create-service" data-id-title="Configuring OCFS2 Services and a STONITH Resource"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.3 </span><span class="title-name">Configuring OCFS2 Services and a STONITH Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-create-service">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM and a STONITH resource.
  </p><p>
   The following procedure uses the <code class="command">crm</code> shell to
   configure the cluster resources. Alternatively, you can also use
   Hawk2 to configure the resources as described in
   <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="20.6. Configuring OCFS2 Resources With Hawk2">Section 20.6, “Configuring OCFS2 Resources With Hawk2”</a>.
  </p><div class="procedure" id="pro-ocfs2-stonith" data-id-title="Configuring a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.1: </span><span class="title-name">Configuring a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.4.5.4.5.4.2" data-id-title="STONITH Device Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: STONITH Device Needed</div><p>
     You need to configure a fencing device. Without a STONITH
     mechanism (like <code class="literal">external/sbd</code>) in place the
     configuration will fail.
    </p></div><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Create an SBD partition as described in
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 13.3, “Initializing the SBD Devices”</a>.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure <code class="literal">external/sbd</code> as the fencing device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div><p>
    For details on configuring the resource for DLM, see <a class="xref" href="#sec-ha-storage-generic-dlm-config" title="19.2. Configuring DLM Cluster Resources">Section 19.2, “Configuring DLM Cluster Resources”</a>.
  </p></section><section class="sect1" id="sec-ha-ocfs2-create" data-id-title="Creating OCFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.4 </span><span class="title-name">Creating OCFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured a DLM cluster resource as described in
   <a class="xref" href="#sec-ha-ocfs2-create-service" title="20.3. Configuring OCFS2 Services and a STONITH Resource">Section 20.3, “Configuring OCFS2 Services and a STONITH Resource”</a>, configure your system to
   use OCFS2 and create OCFs2 volumes.
  </p><div id="id-1.4.5.4.6.3" data-id-title="OCFS2 Volumes for Application and Data Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: OCFS2 Volumes for Application and Data Files</div><p>
    We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </p></div><p>
   Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space.
  </p><p>
   Then create and format the OCFS2 volume with the
   <code class="command">mkfs.ocfs2</code> as described in
   <a class="xref" href="#pro-ocfs2-volume" title="Creating and Formatting an OCFS2 Volume">Procedure 20.2, “Creating and Formatting an OCFS2 Volume”</a>. The most important parameters for the
   command are listed in <a class="xref" href="#tab-ha-ofcs2-mkfs-ocfs2-params" title="Important OCFS2 Parameters">Table 20.2, “Important OCFS2 Parameters”</a>.
   For more information and the command syntax, refer to the
   <code class="command">mkfs.ocfs2</code> man page.
  </p><div class="table" id="tab-ha-ofcs2-mkfs-ocfs2-params" data-id-title="Important OCFS2 Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 20.2: </span><span class="title-name">Important OCFS2 Parameters </span></span><a title="Permalink" class="permalink" href="#tab-ha-ofcs2-mkfs-ocfs2-params">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        OCFS2 Parameter
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description and Recommendation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Volume Label (<code class="option">-L</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <code class="command">tunefs.ocfs2</code> utility to modify the label as
        needed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Cluster Size (<code class="option">-C</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Number of Node Slots (<code class="option">-N</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such
        as the journals. Nodes that access the volume
        can be a combination of little-endian architectures (such as AMD64/Intel 64)
        and big-endian architectures (such as S/390x).
       </p>
       <p>
        Node-specific files are called local files. A node slot
        number is appended to the local file. For example:
        <code class="literal">journal:0000</code> belongs to whatever node is assigned
        to slot number <code class="literal">0</code>.
       </p>
       <p>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <code class="command">tunefs.ocfs2</code> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased, and one node slot will consume about 100 MiB disk space.
       </p>
       <p>
        In case the <code class="option">-N</code> parameter is not specified, the
        number of node slots is decided based on the size of the file system.
        For the default value, refer to the <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Block Size (<code class="option">-b</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <code class="command">mkfs.ocfs2</code> man
        page.

       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Specific Features On/Off (<code class="option">--fs-features</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A comma separated list of feature flags can be provided, and
        <code class="systemitem">mkfs.ocfs2</code> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <code class="literal">no</code> to the name.
       </p>
       <p>
        For an overview of all available flags, refer to the
        <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Pre-Defined Features (<code class="option">--fs-feature-level</code>)
       </p>
      </td><td>
       <p>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <code class="command">mkfs.ocfs2</code> man page.
       </p>
      </td></tr></tbody></table></div></div><p>
   If you do not specify any features when creating and formatting
   the volume with <code class="command">mkfs.ocfs2</code>, the following features are
   enabled by default: <code class="option">backup-super</code>,
   <code class="option">sparse</code>, <code class="option">inline-data</code>,
   <code class="option">unwritten</code>, <code class="option">metaecc</code>,
   <code class="option">indexed-dirs</code>, and <code class="option">xattr</code>.
  </p><div class="procedure" id="pro-ocfs2-volume" data-id-title="Creating and Formatting an OCFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.2: </span><span class="title-name">Creating and Formatting an OCFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-volume">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Execute the following steps only on <span class="emphasis"><em>one</em></span> of the
    cluster nodes.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Create and format the volume using the <code class="command">mkfs.ocfs2</code>
     utility. For information about the syntax for this command, refer to
     the <code class="command">mkfs.ocfs2</code> man page.
    </p><p>
     For example, to create a new OCFS2 file system
     that supports up to 32 cluster nodes,
     enter the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ocfs2 -N 32 /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div><p>
      Always use a stable device name (for example:
      <code class="filename">/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</code>).
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-mount" data-id-title="Mounting OCFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.5 </span><span class="title-name">Mounting OCFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-mount">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <a class="xref" href="#pro-ocfs2-mount-cluster" title="Mounting an OCFS2 Volume with the Cluster Resource Manager">Procedure 20.4, “Mounting an OCFS2 Volume with the Cluster Resource Manager”</a>.
  </p><p>
   To mount multiple OCFS2 volumes, see <a class="xref" href="#pro-ocfs2-mount-multiple" title="Mounting multiple OCFS2 volumes with the cluster resource manager">Procedure 20.5, “Mounting multiple OCFS2 volumes with the cluster resource manager”</a>.
  </p><div class="procedure" id="pro-ocfs2-mount-manual" data-id-title="Manually Mounting an OCFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.3: </span><span class="title-name">Manually Mounting an OCFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-mount-manual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Mount the volume from the command line, using the
     <code class="command">mount</code> command.
    </p></li></ol></div></div><div id="id-1.4.5.4.7.5" data-id-title="Manually Mounted OCFS2 Devices" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Manually Mounted OCFS2 Devices</div><p>
    If you mount the OCFS2 file system manually for testing purposes,
    make sure to unmount it again before starting to use it by means of
    cluster resources.
   </p></div><div class="procedure" id="pro-ocfs2-mount-cluster" data-id-title="Mounting an OCFS2 Volume with the Cluster Resource Manager"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.4: </span><span class="title-name">Mounting an OCFS2 Volume with the Cluster Resource Manager </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-mount-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To mount an OCFS2 volume with the High Availability software, configure an
    OCFS2 file system resource in the cluster. The following procedure uses
    the <code class="command">crm</code> shell to configure the cluster resources.
    Alternatively, you can also use Hawk2 to configure the resources as
    described in <a class="xref" href="#sec-ha-ocfs2-rsc-hawk2" title="20.6. Configuring OCFS2 Resources With Hawk2">Section 20.6, “Configuring OCFS2 Resources With Hawk2”</a>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to a node as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm configure</code>.
    </p></li><li class="step"><p>
     Configure Pacemaker to mount the OCFS2 file system on every node in
     the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>" directory="/mnt/shared" fstype="ocfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</pre></div></li><li class="step"><p>
     Add the <code class="literal">ocfs2-1</code> primitive
     to the <code class="literal">g-storage</code> group you created in
     <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 19.1, “Configuring a Base Group for DLM”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-storage add ocfs2-1</pre></div><p>
     Because of the base group's internal colocation and ordering,
     the <code class="systemitem">ocfs2-1</code> resource will only start
     on nodes that also have a <code class="literal">dlm</code> resource already running.
    </p><div id="id-1.4.5.4.7.6.6.4" data-id-title="Do not use a group for multiple OCFS2 resources" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Do not use a group for multiple OCFS2 resources</div><p>
      Adding multiple OCFS2 resources to a group creates a dependency between
      the OCFS2 volumes. For example, if you created a group with
      <code class="command">crm configure group g-storage dlm ocfs2-1 ocfs2-2</code>,
      then stopping <code class="literal">ocfs2-1</code> will also stop
      <code class="literal">ocfs2-2</code>, and starting <code class="literal">ocfs2-2</code>
      will also start <code class="literal">ocfs2-1</code>.
     </p><p>
      To use multiple OCFS2 resources in the cluster, use colocation and order
      constraints as described in <a class="xref" href="#pro-ocfs2-mount-multiple" title="Mounting multiple OCFS2 volumes with the cluster resource manager">Procedure 20.5, “Mounting multiple OCFS2 volumes with the cluster resource manager”</a>.
     </p></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div><div class="procedure" id="pro-ocfs2-mount-multiple" data-id-title="Mounting multiple OCFS2 volumes with the cluster resource manager"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.5: </span><span class="title-name">Mounting multiple OCFS2 volumes with the cluster resource manager </span></span><a title="Permalink" class="permalink" href="#pro-ocfs2-mount-multiple">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To mount multiple OCFS2 volumes in the cluster, configure an OCFS2 file system
    resource for each volume, and colocate them with the <code class="literal">dlm</code>
    resource you created in <a class="xref" href="#pro-dlm-multiple-resources" title="Configuring an independent DLM resource">Procedure 19.2, “Configuring an independent DLM resource”</a>.
   </p><div id="id-1.4.5.4.7.7.3" class="admonition important compact"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><p>
     Do <span class="emphasis"><em>not</em></span> add multiple OCFS2 resources to a group with DLM.
     This creates a dependency between the OCFS2 volumes. For example, if
     <code class="literal">ocfs2-1</code> and <code class="literal">ocfs2-2</code> are in the same group,
     then stopping <code class="literal">ocfs2-1</code> will also stop <code class="literal">ocfs2-2</code>.
    </p></div><ol class="procedure" type="1"><li class="step"><p>
     Log in to a node as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm configure</code>.
    </p></li><li class="step"><p>
     Create the primitive for the first OCFS2 volume:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-1 Filesystem \
  params directory="/srv/ocfs2-1" fstype=ocfs2 device="/dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em>" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</pre></div></li><li class="step"><p>
     Create the primitive for the second OCFS2 volume:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-2 Filesystem \
  params directory="/srv/ocfs2-2" fstype=ocfs2 device="/dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em>" \
  op monitor interval=20 timeout=40 \
  op start timeout=60 interval=0 \
  op stop timeout=60 interval=0</pre></div></li><li class="step"><p>
     Clone the OCFS2 resources so that they can run on all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ocfs2-1 ocfs2-1 meta interleave=true
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ocfs2-2 ocfs2-2 meta interleave=true</pre></div></li><li class="step"><p>
     Add a colocation constraint for both OCFS2 resources so that they can only
     run on nodes where DLM is also running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> co-ocfs2-with-dlm inf: ( cl-ocfs2-1 cl-ocfs2-2 ) cl-dlm</pre></div></li><li class="step"><p>
     Add an order constraint for both OCFS2 resources so that they can only
     start after DLM is already running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-dlm-before-ocfs2 Mandatory: cl-dlm ( cl-ocfs2-1 cl-ocfs2-2 )</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with <code class="command">commit</code>
     and leave the crm live configuration with <code class="command">quit</code>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-rsc-hawk2" data-id-title="Configuring OCFS2 Resources With Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.6 </span><span class="title-name">Configuring OCFS2 Resources With Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-rsc-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Instead of configuring the DLM and the file system resource for OCFS2
   manually with the crm shell, you can also use the OCFS2 template
   in Hawk2's <span class="guimenu">Setup Wizard</span>.
  </p><div id="id-1.4.5.4.8.3" data-id-title="Differences Between Manual Configuration and Hawk2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Differences Between Manual Configuration and Hawk2</div><p>
    The OCFS2 template in the <span class="guimenu">Setup Wizard</span> does
    <span class="emphasis"><em>not</em></span> include the configuration of a STONITH
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a STONITH resource as
    described in <a class="xref" href="#pro-ocfs2-stonith" title="Configuring a STONITH Resource">Procedure 20.1, “Configuring a STONITH Resource”</a>.
   </p><p>
    Using the OCFS2 template in the Hawk2 <span class="guimenu">Setup
    Wizard</span> also leads to a slightly different resource
    configuration than the manual configuration described in
    <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 19.1, “Configuring a Base Group for DLM”</a> and
    <a class="xref" href="#pro-ocfs2-mount-cluster" title="Mounting an OCFS2 Volume with the Cluster Resource Manager">Procedure 20.4, “Mounting an OCFS2 Volume with the Cluster Resource Manager”</a>.
   </p></div><div class="procedure" id="pro-ha-ocfs2-rsc-hawk" data-id-title="Configuring OCFS2 Resources with Hawk2s Wizard"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 20.6: </span><span class="title-name">Configuring OCFS2 Resources with Hawk2's <span class="guimenu">Wizard</span> </span></span><a title="Permalink" class="permalink" href="#pro-ha-ocfs2-rsc-hawk">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in to Hawk2:
    </p><div class="verbatim-wrap"><pre class="screen">https://<em class="replaceable">HAWKSERVER</em>:7630/</pre></div></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Wizard</span>.
    </p></li><li class="step"><p>
     Expand the <span class="guimenu">File System</span> category and select
     <code class="literal">OCFS2 File System</code>.
    </p></li><li class="step"><p>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in Hawk2. After the last
     configuration step, <span class="guimenu">Verify</span> the values you have entered.
    </p><p>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </p><div class="informalfigure"><div class="mediaobject"><a href="images/hawk2-wizard-ocfs2-verify.png"><img src="images/hawk2-wizard-ocfs2-verify.png" width="70%" alt="Image" title="Image"/></a></div></div></li><li class="step"><p>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </p><p>
     A message on the screen shows if the action has been successful.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-ocfs2-quota" data-id-title="Using Quotas on OCFS2 File Systems"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.7 </span><span class="title-name">Using Quotas on OCFS2 File Systems</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-quota">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To use quotas on an OCFS2 file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <code class="literal">ursquota</code> (quota for individual users) or
   <code class="literal">grpquota</code> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <code class="command">tunefs.ocfs2</code>.
  </p><p>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   OCFS2 treats quota information as file system-internal metadata, you
   do not need to run the <code class="command">quotacheck</code>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </p><p>
   To enable enforcement of limits imposed on each user or group, run
   <code class="command">quotaon</code>(8) like you would do for any other file
   system.
  </p><p>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tuneable with
   <code class="command">tunefs.ocfs2</code>, options
   <code class="option">usrquota-sync-interval</code> and
   <code class="option">grpquota-sync-interval</code>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </p></section><section class="sect1" id="sec-ha-ocfs2-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">20.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-ocfs2-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_ocfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For more information about OCFS2, see the following links:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.10.3.1"><span class="term"><a class="link" href="https://ocfs2.wiki.kernel.org/" target="_blank">https://ocfs2.wiki.kernel.org/</a>
    </span></dt><dd><p>
      The OCFS2 project home page.
     </p></dd><dt id="id-1.4.5.4.10.3.2"><span class="term"><a class="link" href="http://oss.oracle.com/projects/ocfs2/" target="_blank">http://oss.oracle.com/projects/ocfs2/</a>
    </span></dt><dd><p>
      The former OCFS2 project home page at Oracle.
     </p></dd><dt id="id-1.4.5.4.10.3.3"><span class="term"><a class="link" href="http://oss.oracle.com/projects/ocfs2/documentation" target="_blank">http://oss.oracle.com/projects/ocfs2/documentation</a>
    </span></dt><dd><p>
      The project's former documentation home page.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-ha-gfs2" data-id-title="GFS2"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">21 </span><span class="title-name">GFS2</span></span> <a title="Permalink" class="permalink" href="#cha-ha-gfs2">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Global File System 2 or GFS2 is a shared disk file system for Linux
    computer clusters. GFS2 allows all nodes to have direct concurrent
    access to the same shared block storage. GFS2 has no disconnected
    operating-mode, and no client or server roles. All nodes in a GFS2
    cluster function as peers. GFS2 supports up to 32 cluster nodes. Using
    GFS2 in a cluster requires hardware to allow access to the shared
    storage, and a lock manager to control access to the storage.
   </p><p>
    SUSE recommends OCFS2 over GFS2 for your cluster environments if
    performance is one of your major requirements. Our tests have revealed
    that OCFS2 performs better as compared to GFS2 in such settings.
   </p></div></div></div></div><div id="id-1.4.5.5.3" data-id-title="GFS2 support" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: GFS2 support</div><p>
      SUSE only supports GFS2 in read-only mode. Write operations are not supported.
     </p></div><section class="sect1" id="sec-ha-gfs2-utils" data-id-title="GFS2 Packages and Management Utilities"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.1 </span><span class="title-name">GFS2 Packages and Management Utilities</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-utils">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To use GFS2, make sure
   <span class="package">gfs2-utils</span> and a matching
   <span class="package">gfs2-kmp-*</span> package for your
   Kernel is installed on each node of the cluster.
  </p><p>
   The <span class="package">gfs2-utils</span> package provides
   the following utilities for management of GFS2 volumes. For syntax
   information, see their man pages.
  </p><div class="table" id="id-1.4.5.5.4.4" data-id-title="GFS2 Utilities"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 21.1: </span><span class="title-name">GFS2 Utilities </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.5.4.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        GFS2 Utility
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        fsck.gfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Checks the file system for errors and optionally repairs errors.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        gfs2_jadd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Adds additional journals to a GFS2 file system.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        gfs2_grow
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Grow a GFS2 file system.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        mkfs.gfs2
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Create a GFS2 file system on a device, usually a shared device or
        partition.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        tunegfs2
       </p>
      </td><td>
       <p>
        Allows viewing and manipulating the GFS2 file system parameters such
        as <code class="varname">UUID</code>, <code class="varname">label</code>,
        <code class="varname">lockproto</code> and <code class="varname">locktable</code>.
       </p>
      </td></tr></tbody></table></div></div></section><section class="sect1" id="sec-ha-gfs2-create-service" data-id-title="Configuring GFS2 Services and a STONITH Resource"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.2 </span><span class="title-name">Configuring GFS2 Services and a STONITH Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-create-service">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Before you can create GFS2 volumes, you must configure DLM and a
   STONITH resource.
  </p><div class="procedure" id="pro-gfs2-stonith" data-id-title="Configuring a STONITH Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.1: </span><span class="title-name">Configuring a STONITH Resource </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-stonith">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.4.5.5.5.3.2" data-id-title="STONITH Device Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: STONITH Device Needed</div><p>
     You need to configure a fencing device. Without a STONITH
     mechanism (like <code class="literal">external/sbd</code>) in place the
     configuration will fail.
    </p></div><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Create an SBD partition as described in
     <a class="xref" href="#pro-ha-storage-protect-sbd-create" title="Initializing the SBD Devices">Procedure 13.3, “Initializing the SBD Devices”</a>.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure <code class="literal">external/sbd</code> as the fencing device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> sbd_stonith stonith:external/sbd \
    params pcmk_delay_max=30 meta target-role="Started"</pre></div></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div><p>
    For details on configuring the resource for DLM, see <a class="xref" href="#sec-ha-storage-generic-dlm-config" title="19.2. Configuring DLM Cluster Resources">Section 19.2, “Configuring DLM Cluster Resources”</a>.
  </p></section><section class="sect1" id="sec-ha-gfs2-create" data-id-title="Creating GFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.3 </span><span class="title-name">Creating GFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured DLM as cluster resources as described in
   <a class="xref" href="#sec-ha-gfs2-create-service" title="21.2. Configuring GFS2 Services and a STONITH Resource">Section 21.2, “Configuring GFS2 Services and a STONITH Resource”</a>, configure your system to
   use GFS2 and create GFS2 volumes.
  </p><div id="id-1.4.5.5.6.3" data-id-title="GFS2 Volumes for Application and Data Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: GFS2 Volumes for Application and Data Files</div><p>
    We recommend that you generally store application files and data files
    on different GFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </p></div><p>
   Before you begin, prepare the block devices you plan to use for your GFS2
   volumes. Leave the devices as free space.
  </p><p>
   Then create and format the GFS2 volume with the
   <code class="command">mkfs.gfs2</code> as described in
   <a class="xref" href="#pro-gfs2-volume" title="Creating and Formatting a GFS2 Volume">Procedure 21.2, “Creating and Formatting a GFS2 Volume”</a>. The most important parameters for the
   command are listed in <a class="xref" href="#tab-ha-gfs2-mkfs-gfs2-params" title="Important GFS2 Parameters">Table 21.2, “Important GFS2 Parameters”</a>. For
   more information and the command syntax, refer to the
   <code class="command">mkfs.gfs2</code> man page.
  </p><div class="table" id="tab-ha-gfs2-mkfs-gfs2-params" data-id-title="Important GFS2 Parameters"><div class="title-container"><div class="table-title-wrap"><div class="table-title"><span class="title-number-name"><span class="title-number">Table 21.2: </span><span class="title-name">Important GFS2 Parameters </span></span><a title="Permalink" class="permalink" href="#tab-ha-gfs2-mkfs-gfs2-params">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="table-contents"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="c1"/><col class="c2"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        GFS2 Parameter
       </p>
      </th><th style="border-bottom: 1px solid ; ">
       <p>
        Description and Recommendation
       </p>
      </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Lock Protocol Name (<code class="option">-p</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The name of the locking protocol to use. Acceptable locking
        protocols are lock_dlm (for shared storage) or if you are using GFS2
        as a local file system (1 node only), you can specify the
        lock_nolock protocol. If this option is not specified, lock_dlm
        protocol will be assumed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        Lock Table Name (<code class="option">-t</code>)
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The lock table field appropriate to the lock module you are using.
        It is
        <em class="replaceable">clustername</em>:<em class="replaceable">fsname</em>.
        <em class="replaceable">clustername</em> must match that in the
        cluster configuration file, <code class="filename">/etc/corosync/corosync.conf</code>. Only members of this
        cluster are permitted to use this file system.
        <em class="replaceable">fsname</em> is a unique file system name used
        to distinguish this GFS2 file system from others created (1 to 16
        characters).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        Number of Journals (<code class="option">-j</code>)
       </p>
      </td><td>
       <p>
        The number of journals for gfs2_mkfs to create. You need at least
        one journal per machine that will mount the file system. If this
        option is not specified, one journal will be created.
       </p>
      </td></tr></tbody></table></div></div><div class="procedure" id="pro-gfs2-volume" data-id-title="Creating and Formatting a GFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.2: </span><span class="title-name">Creating and Formatting a GFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-volume">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    Execute the following steps only on <span class="emphasis"><em>one</em></span> of the
    cluster nodes.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Create and format the volume using the <code class="command">mkfs.gfs2</code>
     utility. For information about the syntax for this command, refer to
     the <code class="command">mkfs.gfs2</code> man page.
    </p><p>
     For example, to create a new GFS2 file system
     that supports up to 32 cluster nodes,
     use the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.gfs2 -t hacluster:mygfs2 -p lock_dlm -j 32 /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div><p>
     The <code class="systemitem">hacluster</code> name relates to
     the entry <code class="option">cluster_name</code> in the file
     <code class="filename">/etc/corosync/corosync.conf</code> (this is the default).
    </p><p>
      Always use a stable device name (for example:
      <code class="filename">/dev/disk/by-id/scsi-ST2000DM001-0123456_Wabcdefg</code>).
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-gfs2-mount" data-id-title="Mounting GFS2 Volumes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">21.4 </span><span class="title-name">Mounting GFS2 Volumes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-gfs2-mount">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   You can either mount a GFS2 volume manually or with the cluster manager,
   as described in <a class="xref" href="#pro-gfs2-mount-cluster" title="Mounting a GFS2 Volume with the Cluster Manager">Procedure 21.4, “Mounting a GFS2 Volume with the Cluster Manager”</a>.
  </p><div class="procedure" id="pro-gfs2-mount-manual" data-id-title="Manually Mounting a GFS2 Volume"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.3: </span><span class="title-name">Manually Mounting a GFS2 Volume </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-mount-manual">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open a terminal window and log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Check if the cluster is online with the command <code class="command">crm
     status</code>.
    </p></li><li class="step"><p>
     Mount the volume from the command line, using the
     <code class="command">mount</code> command.
    </p></li></ol></div></div><div id="id-1.4.5.5.7.4" data-id-title="Manually Mounted GFS2 Devices" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Manually Mounted GFS2 Devices</div><p>
    If you mount the GFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it by means of cluster
    resources.
   </p></div><div class="procedure" id="pro-gfs2-mount-cluster" data-id-title="Mounting a GFS2 Volume with the Cluster Manager"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 21.4: </span><span class="title-name">Mounting a GFS2 Volume with the Cluster Manager </span></span><a title="Permalink" class="permalink" href="#pro-gfs2-mount-cluster">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_gfs2.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    To mount a GFS2 volume with the High Availability software, configure an OCF file
    system resource in the cluster. The following procedure uses the
    <code class="command">crm</code> shell to configure the cluster resources.
    Alternatively, you can also use Hawk2 to configure the resources.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and log in as <code class="systemitem">root</code> or equivalent.
    </p></li><li class="step"><p>
     Run <code class="command">crm</code> <code class="option">configure</code>.
    </p></li><li class="step"><p>
     Configure Pacemaker to mount the GFS2 file system on every node in the
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive gfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em>" directory="/mnt/shared" fstype="gfs2" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Stopped"</code></pre></div></li><li class="step"><p>
     Add the <code class="literal">gfs2-1</code> primitive
     to the <code class="literal">g-storage</code> group you created in
     <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 19.1, “Configuring a Base Group for DLM”</a>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-storage add gfs2-1</pre></div><p>
     Because of the base group's internal colocation and ordering,
     the <code class="systemitem">gfs2-1</code> resource will only start
     on nodes that also have a <code class="literal">dlm</code> resource already running.
    </p></li><li class="step"><p>
     Review your changes with <code class="command">show</code>.
    </p></li><li class="step"><p>
     If everything is correct, submit your changes with
     <code class="command">commit</code> and leave the crm live configuration with
     <code class="command">quit</code>.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-drbd" data-id-title="DRBD"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">22 </span><span class="title-name">DRBD</span></span> <a title="Permalink" class="permalink" href="#cha-ha-drbd">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    The <span class="emphasis"><em>distributed replicated block device</em></span> (DRBD*)
    allows you to create a mirror of two block devices that are located at
    two different sites across an IP network. When used with Corosync,
    DRBD supports distributed high-availability Linux clusters. This chapter
    shows you how to install and set up DRBD.
   </p></div></div></div></div><section class="sect1" id="sec-ha-drbd-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   DRBD replicates data on the primary device to the secondary device in a
   way that ensures that both copies of the data remain identical. Think of
   it as a networked RAID 1. It mirrors data in real-time, so its
   replication occurs continuously. Applications do not need to know that in
   fact their data is stored on different disks.

  </p><p>
   DRBD is a Linux Kernel module and sits between the I/O scheduler at the
   lower end and the file system at the upper end, see
   <a class="xref" href="#fig-ha-drbd-concept" title="Position of DRBD within Linux">Figure 22.1, “Position of DRBD within Linux”</a>. To communicate with DRBD, users
   use the high-level command <code class="command">drbdadm</code>. For maximum
   flexibility DRBD comes with the low-level tool
   <code class="command">drbdsetup</code>.
  </p><div class="figure" id="fig-ha-drbd-concept"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_drbd.png"><img src="images/ha_drbd.png" width="80%" alt="Position of DRBD within Linux" title="Position of DRBD within Linux"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 22.1: </span><span class="title-name">Position of DRBD within Linux </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-concept">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><div id="id-1.4.5.6.3.5" data-id-title="Unencrypted Data" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Unencrypted Data</div><p>
    The data traffic between mirrors is not encrypted. For secure data
    exchange, you should deploy a Virtual Private Network (VPN) solution for
    the connection.
   </p></div><p>
   DRBD allows you to use any block device supported by Linux, usually:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     partition or complete hard disk
    </p></li><li class="listitem"><p>
     software RAID
    </p></li><li class="listitem"><p>
     Logical Volume Manager (LVM)
    </p></li><li class="listitem"><p>
     Enterprise Volume Management System (EVMS)
    </p></li></ul></div><p>
   By default, DRBD uses the TCP ports <code class="literal">7788</code> and higher
   for communication between DRBD nodes. Make sure that your firewall does
   not prevent communication on the used ports.
  </p><p>
   You must set up the DRBD devices before creating file systems on them.
   Everything pertaining to user data should be done solely via the
   <code class="filename">/dev/drbd<em class="replaceable">N</em></code> device and
   not on the raw device, as DRBD uses the last part of the raw device for
   metadata. Using the raw device will cause inconsistent data.
  </p><p>
   With udev integration, you will also get symbolic links in the form
   <code class="filename">/dev/drbd/by-res/<em class="replaceable">RESOURCES</em></code>
   which are easier to use and provide safety against remembering the wrong
   minor number of the device.
  </p><p>
   For example, if the raw device is 1024 MB in size, the DRBD
   device has only 1023 MB available for data, with about
   70 KB hidden and reserved for the metadata. Any attempt to access
   the remaining kilobytes via raw disks fails because
   it is not available for user data.
   
  </p></section><section class="sect1" id="sec-ha-drbd-install" data-id-title="Installing DRBD Services"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.2 </span><span class="title-name">Installing DRBD Services</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Install the High Availability pattern on both SUSE Linux Enterprise Server machines in your networked
   cluster as described in <a class="xref" href="#part-install" title="Part I. Installation and Setup">Part I, “Installation and Setup”</a>. Installing
   the pattern also installs the DRBD program files.
  </p><p>
   If you do not need the complete cluster stack but only want to use DRBD,
   install the packages <span class="package">drbd</span>,
    <span class="package">drbd-kmp-<em class="replaceable">FLAVOR</em></span>,
    <span class="package">drbd-utils</span>, and <span class="package">yast2-drbd</span>.
  </p></section><section class="sect1" id="sec-ha-drbd-configure" data-id-title="Setting Up DRBD Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.3 </span><span class="title-name">Setting Up DRBD Service</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.6.5.2" data-id-title="Adjustments Needed" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Adjustments Needed</div><p>
    The following procedure uses the server names alice and bob,
    and the DRBD resource name <code class="literal">r0</code>. It sets up
    alice as the primary node and <code class="filename">/dev/disk/by-id/example-disk1</code> for
    storage. Make sure to modify the instructions to use your own nodes and
    file names.
   </p></div><p>
   The following sections assumes you have two nodes, alice
   and bob, and that they should use the TCP port <code class="literal">7788</code>.
   Make sure this port is open in your firewall.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Prepare your system:</p><ol type="a" class="substeps"><li class="step"><p>Make sure the block devices in your Linux nodes are ready
            and partitioned (if needed).</p></li><li class="step"><p>
            If your disk already contains a file system that you do not need
            anymore, destroy the file system structure with the following
            command:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">dd</code> if=/dev/zero of=<em class="replaceable">YOUR_DEVICE</em> count=16 bs=1M</pre></div><p>If you have more file systems to destroy, repeat this step on
           all devices you want to include into your DRBD setup.</p></li><li class="step"><p>If the cluster is already using DRBD, put your cluster
              in maintenance mode: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=true</pre></div><p> If you skip this step when your cluster uses already
              DRBD, a syntax error in the live configuration will lead
              to a service shutdown. </p><p>As an alternative, you can also use
                <code class="command">drbdadm</code>
              <code class="option">-c <em class="replaceable">FILE</em></code> to
              test a configuration file.</p></li></ol></li><li class="step"><p>Configure DRBD by choosing your method:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="#sec-ha-drbd-configure-manually" title="22.3.1. Configuring DRBD Manually">Section 22.3.1, “Configuring DRBD Manually”</a></p></li><li class="listitem"><p><a class="xref" href="#sec-ha-drbd-configure-yast" title="22.3.2. Configuring DRBD with YaST">Section 22.3.2, “Configuring DRBD with YaST”</a></p></li></ul></div></li><li class="step"><p>
        If you have configured Csync2 (which should be the default), the
        DRBD configuration files are already included in the list of files that
        need to be synchronized. To synchronize them, run the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> -xv</pre></div><p>
        If you do not have Csync2 (or do not want to use it), copy the DRBD
        configuration files manually to the other node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.conf bob:/etc/
<code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.d/*  bob:/etc/drbd.d/</pre></div></li><li class="step"><p>Perform the initial synchronization (see <a class="xref" href="#sec-ha-drbd-configure-init" title="22.3.3. Initializing and Formatting DRBD Resource">Section 22.3.3, “Initializing and Formatting DRBD Resource”</a>).</p></li><li class="step"><p>
        Reset the cluster's maintenance mode flag:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure property maintenance-mode=false</pre></div></li></ol></div></div><section class="sect2" id="sec-ha-drbd-configure-manually" data-id-title="Configuring DRBD Manually"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.1 </span><span class="title-name">Configuring DRBD Manually</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-manually">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.6.5.6.2" data-id-title="Restricted Support of Auto Promote Feature" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restricted Support of <span class="quote">“<span class="quote">Auto Promote</span>”</span> Feature</div><p>
      The DRBD9 feature <span class="quote">“<span class="quote">auto-promote</span>”</span> can automatically promote a
      resource to the primary role when one of its devices is mounted or opened
      for writing.
     </p><p>
      The auto promote feature has currently restricted support.
      With DRBD 9, SUSE supports the same use cases that were also supported
 with DRBD 8. Use cases beyond that, such as setups with more than two
 nodes, are not supported.
     </p></div><p>
     To set up DRBD manually, proceed as follows:
    </p><div class="procedure" id="pro-drbd-configure" data-id-title="Manually Configuring DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 22.1: </span><span class="title-name">Manually Configuring DRBD </span></span><a title="Permalink" class="permalink" href="#pro-drbd-configure">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>Beginning with DRBD version 8.3, the former configuration file is
      split into separate files, located under the directory
      <code class="filename">/etc/drbd.d/</code>.</p><ol class="procedure" type="1"><li class="step"><p>
       Open the file <code class="filename">/etc/drbd.d/global_common.conf</code>. It
       contains already some global, pre-defined values. Go to the
       <code class="literal">startup</code> section and insert these lines:
      </p><div class="verbatim-wrap"><pre class="screen">startup {
    # wfc-timeout degr-wfc-timeout outdated-wfc-timeout
    # wait-after-sb;
    wfc-timeout 100;
    degr-wfc-timeout 120;
}</pre></div><p>
       These options are used to reduce the timeouts when booting, see
       <a class="link" href="https://docs.linbit.com/docs/users-guide-9.0/#ch-configure" target="_blank">https://docs.linbit.com/docs/users-guide-9.0/#ch-configure</a>
       for more details.
      </p></li><li class="step"><p>
       Create the file <code class="filename">/etc/drbd.d/r0.res</code>. Change the
       lines according to your situation and save it:
      </p><div class="verbatim-wrap"><pre class="screen">resource r0 { <span class="callout" id="co-drbd-config-r0">1</span>
  device /dev/drbd0; <span class="callout" id="co-drbd-config-device">2</span>
  disk /dev/disk/by-id/example-disk1; <span class="callout" id="co-drbd-config-disk">3</span>
  meta-disk internal; <span class="callout" id="co-drbd-config-meta-disk">4</span>
  on alice { <span class="callout" id="co-drbd-config-resname">5</span>
    address  192.168.1.10:7788; <span class="callout" id="co-drbd-config-address">6</span>
    node-id 0; <span class="callout" id="co-drbd-config-node-id">7</span>
  }
  on bob { <a class="xref" href="#co-drbd-config-resname"><span class="callout">5</span></a>
    address 192.168.1.11:7788; <a class="xref" href="#co-drbd-config-address"><span class="callout">6</span></a>
    node-id 1; <a class="xref" href="#co-drbd-config-node-id"><span class="callout">7</span></a>
  }
  disk {
    resync-rate 10M; <span class="callout" id="co-drbd-config-syncer-rate">8</span>
  }
  connection-mesh { <span class="callout" id="co-drbd-config-connection-mesh">9</span>
    hosts alice bob;
  }
}</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-r0"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         DRBD resource name that allows some association to the service that needs them.
         For example, <code class="systemitem">nfs</code>,
         <code class="systemitem">http</code>, <code class="systemitem">mysql_0</code>,
         <code class="systemitem">postgres_wal</code>, etc.
         Here a more general name <code class="literal">r0</code> is used.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-device"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The device name for DRBD and its minor number.
        </p><p>
         In the example above, the minor number 0 is used for DRBD. The udev
         integration scripts will give you a symbolic link
         <code class="filename">/dev/drbd/by-res/nfs/0</code>. Alternatively, omit
         the device node name in the configuration and use the following
         line instead:
        </p><p>
         <code class="literal">drbd0 minor 0</code> (<code class="literal">/dev/</code> is
         optional) or <code class="literal">/dev/drbd0</code>
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-disk"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The raw device that is replicated between nodes. Note, in this
         example the devices are the <span class="emphasis"><em>same</em></span> on both nodes.
         If you need different devices, move the <code class="literal">disk</code>
          parameter into the <code class="literal">on</code> host.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-meta-disk"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The meta-disk parameter usually contains the value
         <code class="literal">internal</code>, but it is possible to specify an
         explicit device to hold the meta data. See
         <a class="link" href="https://docs.linbit.com/docs/users-guide-9.0/#s-metadata" target="_blank">https://docs.linbit.com/docs/users-guide-9.0/#s-metadata</a>
         for more information.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-resname"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The <code class="literal">on</code> section states which host this
         configuration statement applies to.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-address"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The IP address and port number of the respective node. Each
         resource needs an individual port, usually starting with
         <code class="literal">7788</code>. Both ports must be the same for a
          DRBD resource.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-node-id"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The node ID is required when configuring more than two nodes. It
        is a unique, non-negative integer to distinguish the different
        nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-syncer-rate"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
         The synchronization rate. Set it to one third of the lower of the
         disk- and network bandwidth. It only limits the resynchronization,
         not the replication.
        </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-drbd-config-connection-mesh"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Defines all nodes of a mesh.
       The <code class="option">hosts</code> parameter contains all host names that
       share the same DRBD setup.
    </p></td></tr></table></div></li><li class="step"><p>
      Check the syntax of your configuration file(s). If the following
      command returns an error, verify your files:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> dump all</pre></div></li><li class="step"><p>Continue with <a class="xref" href="#sec-ha-drbd-configure-init" title="22.3.3. Initializing and Formatting DRBD Resource">Section 22.3.3, “Initializing and Formatting DRBD Resource”</a>.</p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-configure-yast" data-id-title="Configuring DRBD with YaST"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.2 </span><span class="title-name">Configuring DRBD with YaST</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-yast">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>YaST can be used to start with an initial setup of DRBD.
        After you have created your DRBD setup, you can fine-tune the
        generated files manually.
    </p><p>
        However, when you have changed the configuration files,
        do not use the YaST DRBD module anymore. The DRBD module supports
        only a limited set of basic configuration. If you use it again,
        it is very likely that the module will not show your changes.
    </p><p>
      To set up DRBD with YaST, proceed as follows:</p><div class="procedure" id="pro-drbd-configure-yast" data-id-title="Using YaST to Configure DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 22.2: </span><span class="title-name">Using YaST to Configure DRBD </span></span><a title="Permalink" class="permalink" href="#pro-drbd-configure-yast">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start YaST and select the configuration module <span class="guimenu">High Availability</span> › <span class="guimenu">DRBD</span>. If you already have a DRBD configuration, YaST
     warns you. YaST will change your configuration and will save your
     old DRBD configuration files as <code class="filename">*.YaSTsave</code>.
    </p></li><li class="step"><p>
     Leave the booting flag in <span class="guimenu">Start-up
     Configuration</span> › <span class="guimenu">Booting</span> as it is
     (by default it is <code class="literal">off</code>); do not change that as
     Pacemaker manages this service.
    </p></li><li class="step"><p>If you have a firewall running, enable <span class="guimenu">Open Port in
      Firewall</span>.</p></li><li class="step"><p>Go to the <span class="guimenu">Resource Configuration</span> entry.
      Press <span class="guimenu">Add</span> to create a new resource (see
      <a class="xref" href="#fig-ha-drbd-yast-resconfig" title="Resource Configuration">Figure 22.2, “Resource Configuration”</a>).
    </p><div class="figure" id="fig-ha-drbd-yast-resconfig"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_drbd-resconfig.png"><img src="images/yast_drbd-resconfig.png" width="90%" alt="Resource Configuration" title="Resource Configuration"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 22.2: </span><span class="title-name">Resource Configuration </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-yast-resconfig">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><p>
      The following parameters need to be set:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.6.5.7.5.5.4.1"><span class="term"><span class="guimenu">Resource Name</span></span></dt><dd><p>The name of the DRBD resource (mandatory)</p></dd><dt id="id-1.4.5.6.5.7.5.5.4.2"><span class="term"><span class="guimenu">Name</span></span></dt><dd><p>The host name of the relevant node</p></dd><dt id="id-1.4.5.6.5.7.5.5.4.3"><span class="term"><span class="guimenu">Address:Port</span></span></dt><dd><p>
          The IP address and port number (default
           <code class="systemitem">7788</code>) for the respective
          node
         </p></dd><dt id="id-1.4.5.6.5.7.5.5.4.4"><span class="term"><span class="guimenu">Device</span></span></dt><dd><p>
          The block device path that is used to access the replicated data.
          If the device contains a minor number, the associated block device
          is usually named <code class="filename">/dev/drbdX</code>, where
          <em class="replaceable">X</em> is the device minor number. If the
          device does not contain a minor number, make sure to add
          <code class="literal">minor 0</code> after the device name.
         </p></dd><dt id="id-1.4.5.6.5.7.5.5.4.5"><span class="term"><span class="guimenu">Disk</span></span></dt><dd><p>
          The raw device that is replicated between both nodes. If you use
          LVM, insert your LVM device name.
         </p></dd><dt id="id-1.4.5.6.5.7.5.5.4.6"><span class="term"><span class="guimenu">Meta-disk</span></span></dt><dd><p>
          The <span class="guimenu">Meta-disk</span> is either set to the value
          <code class="literal">internal</code> or specifies an explicit device
          extended by an index to hold the meta data needed by DRBD.
         </p><p>
          A real device may also be used for multiple DRBD resources. For
          example, if your <span class="guimenu">Meta-Disk</span> is
          <code class="filename">/dev/disk/by-id/example-disk6[0]</code> for the first resource, you may
          use <code class="filename">/dev/disk/by-id/example-disk6[1]</code> for the second resource.
          However, there must be at least 128 MB space for each resource
          available on this disk. The fixed metadata size limits the maximum
          data size that you can replicate.
         </p></dd></dl></div><p>
     All of these options are explained in the examples in the
     <code class="filename">/usr/share/doc/packages/drbd/drbd.conf</code> file and in
     the man page of <code class="command">drbd.conf(5)</code>.
    </p></li><li class="step"><p>Click <span class="guimenu">Save</span>.</p></li><li class="step"><p>Click <span class="guimenu">Add</span> to enter the second DRBD resource
     and finish with <span class="guimenu">Save</span>.
    </p></li><li class="step"><p>Close the resource configuration with <span class="guimenu">Ok</span>
      and <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>If you use LVM with DRBD, it is necessary to change some options
       in the LVM configuration file (see the <span class="guimenu">LVM
       Configuration</span> entry). This change can be done by the
       YaST DRBD module automatically.
     </p><p>
       The disk name of localhost for the DRBD resource and the default filter
       will be rejected in the LVM filter. Only <code class="filename">/dev/drbd</code>
       can be scanned for an LVM device.</p><p>
       For example, if <code class="filename">/dev/disk/by-id/example-disk1</code> is used as a DRBD disk,
       the device name will be inserted as the first entry in the LVM filter.
       To change the filter manually, click the
       <span class="guimenu">Modify LVM Device Filter Automatically</span> check box.
     </p></li><li class="step"><p>Save your changes with <span class="guimenu">Finish</span>.</p></li><li class="step"><p>Continue with <a class="xref" href="#sec-ha-drbd-configure-init" title="22.3.3. Initializing and Formatting DRBD Resource">Section 22.3.3, “Initializing and Formatting DRBD Resource”</a>.</p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-configure-init" data-id-title="Initializing and Formatting DRBD Resource"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.3 </span><span class="title-name">Initializing and Formatting DRBD Resource</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-init">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>After you have prepared your system and configured DRBD,
      initialize your disk for the first time:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
           On <span class="emphasis"><em>both</em></span> nodes (alice and bob),
           initialize the meta data storage:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> create-md r0
<code class="prompt root"># </code><code class="command">drbdadm</code> up r0</pre></div></li><li class="step"><p>
          To shorten the initial resynchronization of your DRBD resource
          check the following:
         </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            If the DRBD devices on all nodes have the same data (for example,
            by destroying the file system structure with the
            <code class="command">dd</code> command as shown in
            <a class="xref" href="#sec-ha-drbd-configure" title="22.3. Setting Up DRBD Service">Section 22.3, “Setting Up DRBD Service”</a>), then skip the initial
            resynchronization with the following command (on both nodes):
           </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> new-current-uuid --clear-bitmap r0/0</pre></div><p>The state will be <code class="literal">Secondary/Secondary UpToDate/UpToDate</code></p></li><li class="listitem"><p>
            Otherwise, proceed with the next step.
           </p></li></ul></div></li><li class="step"><p>
          On the primary node alice, start the resynchronization process:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary --force r0</pre></div></li><li class="step"><p> Check the status with: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0
r0 role:Primary
  disk:UpToDate
  bob role:Secondary
  peer-disk:UpToDate</pre></div></li><li class="step"><p> Create your file system on top of your DRBD device, for
            example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ext3</code> /dev/drbd0</pre></div></li><li class="step"><p> Mount the file system and use it: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> /dev/drbd0 /mnt/</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-configure-cluster-resource" data-id-title="Creating Cluster Resources for DRBD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.3.4 </span><span class="title-name">Creating Cluster Resources for DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-configure-cluster-resource">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    After you have initialized your DRBD device, create a cluster resource to manage
    the DRBD device, and a promotable clone to allow this resource to run on both nodes:
   </p><div class="procedure" id="pro-ha-drbd-configure-cluster-resource" data-id-title="Creating cluster resources for DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 22.3: </span><span class="title-name">Creating cluster resources for DRBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-drbd-configure-cluster-resource">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start the <code class="command">crm</code> interactive shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure</code></pre></div></li><li class="step"><p>
      Create a primitive for the DRBD resource <code class="literal">r0</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive drbd-r0 ocf:linbit:drbd \
  params drbd_resource="r0" \
  op monitor interval=15 role=Master \
  op monitor interval=30 role=Slave</code></pre></div></li><li class="step"><p>
      Create a promotable clone for the <code class="literal">drbd-r0</code> primitive:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">ms ms-drbd-r0 drbd-r0 \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true" interleave=true</code></pre></div></li><li class="step"><p>
      Commit this configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-ha-drbd-migrate" data-id-title="Migrating from DRBD 8 to DRBD 9"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.4 </span><span class="title-name">Migrating from DRBD 8 to DRBD 9</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-migrate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Between DRBD 8 (shipped with SUSE Linux Enterprise High Availability 12 SP1) and
   DRBD 9 (shipped with SUSE Linux Enterprise High Availability12 SP2), the metadata format
   has changed. DRBD 9 does not automatically convert previous metadata
   files to the new format.
  </p><p>
   After migrating to 12 SP2 and before starting DRBD, convert the DRBD
   metadata to the version 9 format manually. To do so, use
   <code class="command">drbdadm</code> <code class="option">create-md</code>. No configuration
   needs to be changed.
  </p><div id="id-1.4.5.6.6.4" data-id-title="Restricted Support" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Restricted Support</div><p>With DRBD 9, SUSE supports the same use cases that were also supported
 with DRBD 8. Use cases beyond that, such as setups with more than two
 nodes, are not supported.</p></div><p>
   DRBD 9 will fall back to be compatible with version 8.
   For three nodes and more, you need to re-create the metadata
   to use DRBD version 9 specific options.
  </p><p>
   If you have a stacked DRBD resource, refer also to <a class="xref" href="#sec-ha-drbd-resource-stacking" title="22.5. Creating a Stacked DRBD Device">Section 22.5, “Creating a Stacked DRBD Device”</a> for more information.
  </p><p>
   To keep your data and allow to add new nodes without re-creating new
   resources, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set one node in standby mode.
    </p></li><li class="step"><p>
     Update all the DRBD packages on all of your nodes, see <a class="xref" href="#sec-ha-drbd-install" title="22.2. Installing DRBD Services">Section 22.2, “Installing DRBD Services”</a>.
    </p></li><li class="step"><p>Add the new node information to your resource configuration:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><em class="parameter">node-id</em> on every <code class="literal">on</code>
       section.
      </p></li><li class="listitem"><p>
       <em class="parameter">connection-mesh</em> section contains all host names
       in the <em class="parameter">hosts</em> parameter.
      </p></li></ul></div><p>
     See the example configuration in <a class="xref" href="#pro-drbd-configure" title="Manually Configuring DRBD">Procedure 22.1, “Manually Configuring DRBD”</a>.
    </p></li><li class="step"><p>
     Enlarge the space of your DRBD disks when using <code class="literal">internal</code>
     as <code class="literal">meta-disk</code> key. Use a device that supports enlarging
     the space like LVM.
     As an alternative, change to an external disk for metadata
     and use <code class="literal">meta-disk <em class="replaceable">DEVICE</em>;</code>.
    </p></li><li class="step"><p>
     Re-create the metadata based on the new configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> create-md <em class="replaceable">RESOURCE</em></pre></div></li><li class="step"><p>
     Cancel the standby mode.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-drbd-resource-stacking" data-id-title="Creating a Stacked DRBD Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.5 </span><span class="title-name">Creating a Stacked DRBD Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-resource-stacking">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   A stacked DRBD device contains two other devices of which at least one
   device is also a DRBD resource. In other words, DRBD adds an additional
   node on top of an already existing DRBD resource (see <a class="xref" href="#fig-ha-drbd-resource-stacking" title="Resource Stacking">Figure 22.3, “Resource Stacking”</a>). Such a replication setup
   can be used for backup and disaster recovery purposes.
  </p><div class="figure" id="fig-ha-drbd-resource-stacking"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_stacked_drbd.png"><img src="images/ha_stacked_drbd.png" width="50%" alt="Resource Stacking" title="Resource Stacking"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 22.3: </span><span class="title-name">Resource Stacking </span></span><a title="Permalink" class="permalink" href="#fig-ha-drbd-resource-stacking">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><p>
   Three-way replication uses asynchronous (DRBD protocol A) and
   synchronous replication (DRBD protocol C). The asynchronous part is used for
   the stacked resource whereas the synchronous part is used for the backup.
  </p><p>
   Your production environment uses the stacked device. For example,
   if you have a DRBD device <code class="filename">/dev/drbd0</code> and a stacked
   device <code class="filename">/dev/drbd10</code> on top, the file system will
   be created on <code class="filename">/dev/drbd10</code>, see <a class="xref" href="#exa-ha-drbd-stacked-drbd" title="Configuration of a Three-Node Stacked DRBD Resource">Example 22.1, “Configuration of a Three-Node Stacked DRBD Resource”</a> for more details.
  </p><div class="example" id="exa-ha-drbd-stacked-drbd" data-id-title="Configuration of a Three-Node Stacked DRBD Resource"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 22.1: </span><span class="title-name">Configuration of a Three-Node Stacked DRBD Resource </span></span><a title="Permalink" class="permalink" href="#exa-ha-drbd-stacked-drbd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># /etc/drbd.d/r0.res
resource r0 {
  protocol C;
  device    /dev/drbd0;
  disk      /dev/disk/by-id/example-disk1;
  meta-disk internal;

  on amsterdam-alice {
    address    192.168.1.1:7900;
  }

  on amsterdam-bob {
    address    192.168.1.2:7900;
  }
}

resource r0-U {
  protocol A;
  device     /dev/drbd10;

  stacked-on-top-of r0 {
    address    192.168.2.1:7910;
  }

  on berlin-charlie {
    disk       /dev/disk/by-id/example-disk10;
    address    192.168.2.2:7910; # Public IP of the backup node
    meta-disk  internal;
  }
}</pre></div></div></div></section><section class="sect1" id="sec-ha-drbd-fencing" data-id-title="Using Resource-Level Fencing with STONITH"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.6 </span><span class="title-name">Using Resource-Level Fencing with STONITH</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-fencing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When a DRBD replication link becomes interrupted, Pacemaker tries to promote
   the DRBD resource to another node. To prevent Pacemaker from starting a service
   with outdated data, enable resource-level fencing in the DRBD configuration
   file.
  </p><p>
    The fencing policy can have different values (see man page <code class="command">drbdsetup</code> and the
     <code class="option">--fencing</code> option).
    As a SUSE Linux Enterprise High Availability cluster is normally used with a STONITH device, the value
    <code class="constant">resource-and-stonith</code> is used in
    <a class="xref" href="#ex-ha-drbd-fencing" title="Configuration of DRBD with Resource-Level Fencing Using the Cluster Information Base (CIB)">Example 22.2, “Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB)”</a>.
  </p><div class="example" id="ex-ha-drbd-fencing" data-id-title="Configuration of DRBD with Resource-Level Fencing Using the Cluster Information Base (CIB)"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 22.2: </span><span class="title-name">Configuration of DRBD with Resource-Level Fencing Using the Cluster
    Information Base (CIB) </span></span><a title="Permalink" class="permalink" href="#ex-ha-drbd-fencing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">resource <em class="replaceable">RESOURCE</em> {
  net {
    fencing resource-and-stonith;
    # ...
  }
  handlers {
    fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
    after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
    # ...
  }
  ...
}</pre></div></div></div><p>If the DRBD replication link becomes disconnected, DRBD does the
  following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>DRBD calls the <code class="command">crm-fence-peer.9.sh</code> script.</p></li><li class="listitem"><p>The script contacts the cluster manager.
    </p></li><li class="listitem"><p>The script determines the Pacemaker resource associated with this
     DRBD resource.</p></li><li class="listitem"><p>The script ensures that the DRBD resource no longer gets
    promoted to any other node. It stays on the currently active one.</p></li><li class="listitem"><p>If the replication link becomes connected again and DRBD
    completes its synchronization process, then the constraint is removed.
    The cluster manager is now free to promote the resource.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-drbd-test" data-id-title="Testing the DRBD Service"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.7 </span><span class="title-name">Testing the DRBD Service</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-test">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   If the install and configuration procedures worked as expected, you are
   ready to run a basic test of the DRBD functionality. This test also helps
   with understanding how the software works.
  </p><div class="procedure" id="pro-drbd-test"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Test the DRBD service on alice.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a terminal console, then log in as
       <code class="systemitem">root</code>.
      </p></li><li class="step"><p>
       Create a mount point on alice, such as
       <code class="filename">/srv/r0</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> -p /srv/r0</pre></div></li><li class="step"><p>
       Mount the <code class="command">drbd</code> device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -o rw /dev/drbd0 /srv/r0</pre></div></li><li class="step"><p>
       Create a file from the primary node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">touch</code> /srv/r0/from_alice</pre></div></li><li class="step"><p>
       Unmount the disk on alice:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">umount</code> /srv/r0</pre></div></li><li class="step"><p>
       Downgrade the DRBD service on alice by typing the following
       command on alice:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary r0</pre></div></li></ol></li><li class="step"><p>
     Test the DRBD service on bob.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Open a terminal console, then log in as <code class="systemitem">root</code>
       on bob.
      </p></li><li class="step"><p>
       On bob, promote the DRBD service to primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
       On bob, check to see if bob is primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li><li class="step"><p> On bob, create a mount point such as
        <code class="filename">/srv/r0</code>: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> /srv/r0</pre></div></li><li class="step"><p>
       On bob, mount the DRBD device:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -o rw /dev/drbd0 /srv/r0</pre></div></li><li class="step"><p>
       Verify that the file you created on alice exists:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">ls</code> /srv/r0/from_alice</pre></div><p> The <code class="filename">/srv/r0/from_alice</code> file should be
       listed. </p></li></ol></li><li class="step"><p>
     If the service is working on both nodes, the DRBD setup is complete.
    </p></li><li class="step"><p>
     Set up alice as the primary again.
    </p><ol type="a" class="substeps"><li class="step"><p>
       Dismount the disk on bob by typing the following command on
       bob:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">umount</code> /srv/r0</pre></div></li><li class="step"><p>
       Downgrade the DRBD service on bob by typing the following
       command on bob:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary r0</pre></div></li><li class="step"><p>
       On alice, promote the DRBD service to primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
       On alice, check to see if alice is primary:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li></ol></li><li class="step"><p>
     To get the service to automatically start and fail over if the server
     has a problem, you can set up DRBD as a high availability service with
     Pacemaker/Corosync. For information about installing and
     configuring for SUSE Linux Enterprise 15 SP2 see
     <a class="xref" href="#part-config" title="Part II. Configuration and Administration">Part II, “Configuration and Administration”</a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-drbd-monitor" data-id-title="Monitoring DRBD Devices"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.8 </span><span class="title-name">Monitoring DRBD Devices</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-monitor">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>DRBD comes with the utility <code class="command">drbdmon</code> which offers
   real time monitoring. It shows all the configured resources and their
   problems.
  </p><div class="figure" id="id-1.4.5.6.10.3"><div class="figure-contents"><div class="mediaobject"><a href="images/drbd-drbdmon-ok.png"><img src="images/drbd-drbdmon-ok.png" width="70%" alt="Showing a Good Connection by drbdmon" title="Showing a Good Connection by drbdmon"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 22.4: </span><span class="title-name">Showing a Good Connection by <code class="command">drbdmon</code> </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.6.10.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div><p>In case of problems, <code class="command">drbdadm</code> shows an error message:
  </p><div class="figure" id="id-1.4.5.6.10.5"><div class="figure-contents"><div class="mediaobject"><a href="images/drbd-drbdmon-bad.png"><img src="images/drbd-drbdmon-bad.png" width="70%" alt="Showing a Bad Connection by drbdmon" title="Showing a Bad Connection by drbdmon"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 22.5: </span><span class="title-name">Showing a Bad Connection by <code class="command">drbdmon</code> </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.6.10.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></section><section class="sect1" id="sec-ha-drbd-tuning" data-id-title="Tuning DRBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.9 </span><span class="title-name">Tuning DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-tuning">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   There are several ways to tune DRBD:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Use an external disk for your metadata. This might help, at the cost of
     maintenance ease.
    </p></li><li class="listitem"><p>
     Tune your network connection, by changing the receive and send buffer
     settings via <code class="command">sysctl</code>.
    </p></li><li class="listitem"><p>
     Change the <code class="systemitem">max-buffers</code>,
     <code class="systemitem">max-epoch-size</code> or both in the DRBD
     configuration.
    </p></li><li class="listitem"><p>
     Increase the <code class="systemitem">al-extents</code> value, depending on
     your IO patterns.
    </p></li><li class="listitem"><p>
     If you have a hardware RAID controller with a BBU (<span class="emphasis"><em>Battery
     Backup Unit</em></span>), you might benefit from setting
     <code class="systemitem">no-disk-flushes</code>,
     <code class="systemitem">no-disk-barrier</code> and/or
     <code class="systemitem">no-md-flushes</code>.
    </p></li><li class="listitem"><p>
     Enable read-balancing depending on your workload. See
     <a class="link" href="https://www.linbit.com/en/read-balancing/" target="_blank">https://www.linbit.com/en/read-balancing/</a> for
     more details.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-drbd-trouble" data-id-title="Troubleshooting DRBD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.10 </span><span class="title-name">Troubleshooting DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The DRBD setup involves many components and problems may arise
   from different sources. The following sections cover several common
   scenarios and recommend various solutions.
  </p><section class="sect2" id="sec-ha-drbd-trouble-config" data-id-title="Configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.10.1 </span><span class="title-name">Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If the initial DRBD setup does not work as expected, there is probably
    something wrong with your configuration.
   </p><p>
    To get information about the configuration:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a terminal console, then log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Test the configuration file by running <code class="command">drbdadm</code> with
      the <code class="command">-d</code> option. Enter the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> -d adjust r0</pre></div><p>
      In a dry run of the <code class="command">adjust</code> option,
      <code class="command">drbdadm</code> compares the actual configuration of the
      DRBD resource with your DRBD configuration file, but it does not
      execute the calls. Review the output to make sure you know the source
      and cause of any errors.
     </p></li><li class="step"><p>
      If there are errors in the <code class="filename">/etc/drbd.d/*</code> and
      <code class="filename">drbd.conf</code> files, correct them before continuing.
     </p></li><li class="step"><p>
      If the partitions and settings are correct, run
      <code class="command">drbdadm</code> again without the <code class="command">-d</code>
      option.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> adjust r0</pre></div><p>
      This applies the configuration file to the DRBD resource.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-drbd-hostnames" data-id-title="Host Names"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.10.2 </span><span class="title-name">Host Names</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-hostnames">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For DRBD, host names are case-sensitive (<code class="systemitem">Node0</code>
    would be a different host than <code class="systemitem">node0</code>), and
    compared to the host name as stored in the Kernel (see the
    <code class="command">uname -n</code> output).
   </p><p>
    If you have several network devices and want to use a dedicated network
    device, the host name will likely not resolve to the used IP address. In
    this case, use the parameter <code class="literal">disable-ip-verification</code>.
   </p></section><section class="sect2" id="sec-ha-drbd-port" data-id-title="TCP Port 7788"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.10.3 </span><span class="title-name">TCP Port 7788</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-port">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If your system cannot connect to the peer, this might be a problem with
    your local firewall. By default, DRBD uses the TCP port
    <code class="literal">7788</code> to access the other node. Make sure that this
    port is accessible on both nodes.
   </p></section><section class="sect2" id="sec-ha-drbd-trouble-broken" data-id-title="DRBD Devices Broken after Reboot"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">22.10.4 </span><span class="title-name">DRBD Devices Broken after Reboot</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-trouble-broken">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    In cases when DRBD does not know which of the real devices holds the
    latest data, it changes to a split brain condition. In this case, the
    respective DRBD subsystems come up as secondary and do not connect to
    each other. In this case, the following message can be found in the
    logging data:
   </p><div class="verbatim-wrap"><pre class="screen">Split-Brain detected, dropping connection!</pre></div><p>
    To resolve this situation, enter the following commands on the node which has
    data to be discarded:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> secondary r0</pre></div><p>
    If the state is in <code class="literal">WFconnection</code>, disconnect first:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> disconnect r0</pre></div><p>
    On the node which has the latest data enter the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> connect  --discard-my-data r0</pre></div><p>
    That resolves the issue by overwriting one node's data with the peer's
    data, therefore getting a consistent view on both nodes.
   </p></section></section><section class="sect1" id="sec-ha-drbd-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">22.11 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-drbd-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_drbd.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following open source resources are available for DRBD:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The project home page <a class="link" href="http://www.drbd.org" target="_blank">http://www.drbd.org</a>.
    </p></li><li class="listitem"><p>
     See <span class="intraxref">Article “Highly Available NFS Storage with DRBD and Pacemaker”</span>.
    </p></li><li class="listitem"><p>
     <a class="link" href="http://clusterlabs.org/wiki/DRBD_HowTo_1.0" target="_blank">http://clusterlabs.org/wiki/DRBD_HowTo_1.0</a> by the
     Linux Pacemaker Cluster Stack Project.
    </p></li><li class="listitem"><p>
     The following man pages for DRBD are available in the distribution:
     <code class="command">drbd(8)</code>, <code class="command">drbdmeta(8)</code>,
     <code class="command">drbdsetup(8)</code>, <code class="command">drbdadm(8)</code>,
     <code class="command">drbd.conf(5)</code>.
    </p></li><li class="listitem"><p>
     Find a commented example configuration for DRBD at
     <code class="filename">/usr/share/doc/packages/drbd-utils/drbd.conf.example</code>.
    </p></li><li class="listitem"><p>
     Furthermore, for easier storage administration across your cluster, see
     the recent announcement about the <em class="citetitle">DRBD-Manager</em>
     at <a class="link" href="https://www.linbit.com/en/drbd-manager/" target="_blank">https://www.linbit.com/en/drbd-manager/</a>.
    </p></li></ul></div></section></section><section class="chapter" id="cha-ha-clvm" data-id-title="Cluster Logical Volume Manager (Cluster LVM)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">23 </span><span class="title-name">Cluster Logical Volume Manager (Cluster LVM)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-clvm">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    When managing shared storage on a cluster, every node must be informed
    about changes to the storage subsystem. Logical Volume
    Manager (LVM) supports transparent management of volume groups
    across the whole cluster. Volume groups shared among multiple hosts
    can be managed using the same commands as local storage.
   </p></div></div></div></div><section class="sect1" id="sec-ha-clvm-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Cluster LVM is coordinated with different tools:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.7.3.3.1"><span class="term">Distributed Lock Manager (DLM)</span></dt><dd><p> Coordinates access to shared resources among multiple hosts through
      cluster-wide locking.</p></dd><dt id="id-1.4.5.7.3.3.2"><span class="term">Logical Volume Manager (LVM)</span></dt><dd><p>
      LVM provides a virtual pool of disk space and enables flexible distribution of
      one logical volume over several disks.
     </p></dd><dt id="id-1.4.5.7.3.3.3"><span class="term">Cluster Logical Volume Manager (Cluster LVM)</span></dt><dd><p>
      The term <code class="literal">Cluster LVM</code> indicates that LVM is being used
      in a cluster environment. This needs some configuration adjustments
      to protect the LVM metadata on shared storage. From SUSE Linux Enterprise 15 onward, the
      cluster extension uses lvmlockd, which replaces
      clvmd. For more information about lvmlockd, see the man page of the
      <code class="command">lvmlockd</code> command (<code class="command">man 8
      lvmlockd</code>).
     </p></dd><dt id="id-1.4.5.7.3.3.4"><span class="term">Volume Group and Logical Volume</span></dt><dd><p>
      Volume groups (VGs) and logical volumes (LVs) are basic concepts of LVM.
      A volume group is a storage pool of multiple physical
      disks. A logical volume belongs to a volume group, and can be seen as an
      elastic volume on which you can create a file system. In a cluster environment,
      there is a concept of shared VGs, which consist of shared storage and can
      be used concurrently by multiple hosts.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-clvm-config" data-id-title="Configuration of Cluster LVM"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.2 </span><span class="title-name">Configuration of Cluster LVM</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Make sure the following requirements are fulfilled:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A shared storage device is available, provided by a Fibre
     Channel, FCoE, SCSI, iSCSI SAN, or DRBD*, for example.
    </p></li><li class="listitem"><p>
     Make sure the following packages have been installed: <code class="systemitem">lvm2</code> and <code class="systemitem">lvm2-lockd</code>.
    </p></li><li class="listitem"><p>
     From SUSE Linux Enterprise 15 onward, the cluster extension uses lvmlockd, which replaces
      clvmd. Make sure the clvmd daemon is not running,
     otherwise lvmlockd will fail to start.
    </p></li></ul></div><section class="sect2" id="sec-ha-clvm-config-resources" data-id-title="Creating the Cluster Resources"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.2.1 </span><span class="title-name">Creating the Cluster Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-config-resources">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    Perform the following basic steps on one node to configure a shared VG in
    the cluster:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-rsc-dlm" title="Creating a DLM Resource">Creating a DLM Resource</a>
     </p></li><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-rsc-lvmlockd" title="Creating an lvmlockd Resource">Creating an lvmlockd Resource</a></p></li><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-rsc-vg-lv" title="Creating a Shared VG and LV">Creating a Shared VG and LV</a></p></li><li class="listitem"><p>
      <a class="xref" href="#pro-ha-clvm-rsc-lvm-activate" title="Creating an LVM-activate Resource">Creating an LVM-activate Resource</a>
     </p></li></ul></div><div class="procedure" id="pro-ha-clvm-rsc-dlm" data-id-title="Creating a DLM Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.1: </span><span class="title-name">Creating a DLM Resource </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-rsc-dlm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Check the current configuration of the cluster resources:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure show</pre></div></li><li class="step"><p>
      If you have already configured a DLM resource (and a corresponding
      base group and base clone), continue with <a class="xref" href="#pro-ha-clvm-rsc-lvmlockd" title="Creating an lvmlockd Resource">Procedure 23.2, “Creating an lvmlockd Resource”</a>.
     </p><p>
      Otherwise, configure a DLM resource and a corresponding base group and
      base clone as described in <a class="xref" href="#pro-dlm-resources" title="Configuring a Base Group for DLM">Procedure 19.1, “Configuring a Base Group for DLM”</a>.
     </p></li></ol></div></div><div class="procedure" id="pro-ha-clvm-rsc-lvmlockd" data-id-title="Creating an lvmlockd Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.2: </span><span class="title-name">Creating an lvmlockd Resource </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-rsc-lvmlockd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Run the following command to see the usage of this resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure ra info lvmlockd</pre></div></li><li class="step"><p>
      Configure a <code class="systemitem">lvmlockd</code> resource as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure primitive lvmlockd lvmlockd \
  op start timeout="90" \
  op stop timeout="100" \
  op monitor interval="30" timeout="90"</pre></div></li><li class="step"><p>
      To ensure the <code class="systemitem">lvmlockd</code> resource is started on every node, add the primitive resource
      to the base group for storage you have created in <a class="xref" href="#pro-ha-clvm-rsc-dlm" title="Creating a DLM Resource">Procedure 23.1, “Creating a DLM Resource”</a>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure modgroup g-storage add lvmlockd</pre></div></li><li class="step"><p>
      Review your changes:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure show</pre></div></li><li class="step"><p>Check if the resources are running well:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status full</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-clvm-rsc-vg-lv" data-id-title="Creating a Shared VG and LV"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.3: </span><span class="title-name">Creating a Shared VG and LV </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-rsc-vg-lv">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
     Assuming you already have two shared disks, create a shared VG with them:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate --shared vg1 /dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em></code></pre></div></li><li class="step"><p>
      Create an LV and do not activate it initially:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>lvcreate -an -L10G -n lv1 vg1</pre></div></li></ol></div></div><div class="procedure" id="pro-ha-clvm-rsc-lvm-activate" data-id-title="Creating an LVM-activate Resource"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.4: </span><span class="title-name">Creating an LVM-activate Resource </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-rsc-lvm-activate">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Start a shell and log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Run the following command to see the usage of this resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure ra info LVM-activate</pre></div><p>
      This resource manages the activation of a VG. In a shared VG, LV activation
      has two different modes: exclusive and shared mode. The exclusive mode is
      the default and should be used normally, when a local file system like <code class="systemitem">ext4</code>
      uses the LV. The shared mode should only be used for cluster file systems
      like OCFS2.
     </p></li><li class="step"><p>
      Configure a resource to manage the activation of your VG. Choose one of the
      following options according to your scenario:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Use exclusive activation mode for local file system usage:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure primitive vg1 LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s</pre></div></li><li class="listitem"><p>
        Use shared activation mode for OCFS2 and add it to the cloned
        <code class="literal">g-storage</code> group:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm configure primitive vg1 LVM-activate \
  params vgname=vg1 vg_access_mode=lvmlockd activation_mode=shared \
  op start timeout=90s interval=0 \
  op stop timeout=90s interval=0 \
  op monitor interval=30s timeout=90s
<code class="prompt root"># </code>crm configure modgroup g-storage add vg1</pre></div></li></ul></div></li><li class="step"><p>
      Check if the resources are running well:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm status full</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ha-clvm-scenario-iscsi" data-id-title="Scenario: Cluster LVM with iSCSI on SANs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.2.2 </span><span class="title-name">Scenario: Cluster LVM with iSCSI on SANs</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-scenario-iscsi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenario uses two SAN boxes which export their iSCSI
    targets to several clients. The general idea is displayed in
    <a class="xref" href="#fig-ha-clvm-scenario-iscsi" title="Setup of a Shared Disk with Cluster LVM">Figure 23.1, “Setup of a Shared Disk with Cluster LVM”</a>.
   </p><div class="figure" id="fig-ha-clvm-scenario-iscsi"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_clvm.png"><img src="images/ha_clvm.png" width="45%" alt="Setup of a Shared Disk with Cluster LVM" title="Setup of a Shared Disk with Cluster LVM"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 23.1: </span><span class="title-name">Setup of a Shared Disk with Cluster LVM </span></span><a title="Permalink" class="permalink" href="#fig-ha-clvm-scenario-iscsi">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div><div id="id-1.4.5.7.4.5.4" data-id-title="Data Loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Data Loss</div><p>
     The following procedures will destroy any data on your disks!
    </p></div><p>
    Configure only one SAN box first. Each SAN box needs to export its own
    iSCSI target. Proceed as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenario-iscsi-targets" data-id-title="Configuring iSCSI Targets (SAN)"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.5: </span><span class="title-name">Configuring iSCSI Targets (SAN) </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenario-iscsi-targets">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI LIO Target</span> to
      start the iSCSI Server module.
     </p></li><li class="step"><p>
      If you want to start the iSCSI target whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise choose
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      If you have a firewall running, enable <span class="guimenu">Open Port in
      Firewall</span>.
     </p></li><li class="step"><p>
      Switch to the <span class="guimenu">Global</span> tab. If you need
      authentication, enable incoming or outgoing authentication or both. In
      this example, we select <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Add a new iSCSI target:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Switch to the <span class="guimenu">Targets</span> tab.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step" id="st-ha-clvm-iscsi-iqn"><p>
        Enter a target name. The name needs to be formatted like this:
       </p><div class="verbatim-wrap"><pre class="screen">iqn.<em class="replaceable">DATE</em>.<em class="replaceable">DOMAIN</em></pre></div><p>
        For more information about the format, refer to <em class="citetitle">Section
        3.2.6.3.1. Type "iqn." (iSCSI Qualified Name) </em> at
        <a class="link" href="http://www.ietf.org/rfc/rfc3720.txt" target="_blank">http://www.ietf.org/rfc/rfc3720.txt</a>.
       </p></li><li class="step"><p>
        If you want a more descriptive name, you can change it as long as
        your identifier is unique for your different targets.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Add</span>.
       </p></li><li class="step"><p>
        Enter the device name in <span class="guimenu">Path</span> and use a
        <span class="guimenu">Scsiid</span>.
       </p></li><li class="step"><p>
        Click <span class="guimenu">Next</span> twice.
       </p></li></ol></li><li class="step"><p>
      Confirm the warning box with <span class="guimenu">Yes</span>.
     </p></li><li class="step"><p>
      Open the configuration file <code class="filename">/etc/iscsi/iscsid.conf</code>
      and change the parameter <code class="literal">node.startup</code> to
      <code class="literal">automatic</code>.
     </p></li></ol></div></div><p>
    Now set up your iSCSI initiators as follows:
   </p><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-initiator" data-id-title="Configuring iSCSI Initiators"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.6: </span><span class="title-name">Configuring iSCSI Initiators </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenarios-iscsi-initiator">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run YaST and click <span class="guimenu">Network
      Services</span> › <span class="guimenu">iSCSI Initiator</span>.
     </p></li><li class="step"><p>
      If you want to start the iSCSI initiator whenever your computer is
      booted, choose <span class="guimenu">When Booting</span>, otherwise set
      <span class="guimenu">Manually</span>.
     </p></li><li class="step"><p>
      Change to the <span class="guimenu">Discovery</span> tab and click the
      <span class="guimenu">Discovery</span> button.
     </p></li><li class="step"><p>
      Add the IP address and the port of your iSCSI target (see
      <a class="xref" href="#pro-ha-clvm-scenario-iscsi-targets" title="Configuring iSCSI Targets (SAN)">Procedure 23.5, “Configuring iSCSI Targets (SAN)”</a>). Normally, you
      can leave the port as it is and use the default value.
     </p></li><li class="step"><p>
      If you use authentication, insert the incoming and outgoing user name
      and password, otherwise activate <span class="guimenu">No Authentication</span>.
     </p></li><li class="step"><p>
      Select <span class="guimenu">Next</span>. The found connections are displayed in
      the list.
     </p></li><li class="step"><p>
      Proceed with <span class="guimenu">Finish</span>.
     </p></li><li class="step"><p>
      Open a shell, log in as <code class="systemitem">root</code>.
     </p></li><li class="step"><p>
      Test if the iSCSI initiator has been started successfully:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m discovery -t st -p 192.168.3.100
192.168.3.100:3260,1 iqn.2010-03.de.jupiter:san1</pre></div></li><li class="step"><p>
      Establish a session:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">iscsiadm</code> -m node -l -p 192.168.3.100 -T iqn.2010-03.de.jupiter:san1
Logging in to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]
Login to [iface: default, target: iqn.2010-03.de.jupiter:san1, portal: 192.168.3.100,3260]: successful</pre></div><p>
      See the device names with <code class="command">lsscsi</code>:
     </p><div class="verbatim-wrap"><pre class="screen">...
[4:0:0:2]    disk    IET      ...     0     /dev/sdd
[5:0:0:1]    disk    IET      ...     0     /dev/sde</pre></div><p>
      Look for entries with <code class="literal">IET</code> in their third column. In
      this case, the devices are <code class="filename">/dev/sdd</code> and
      <code class="filename">/dev/sde</code>.
     </p></li></ol></div></div><div class="procedure" id="pro-ha-clvm-scenarios-iscsi-lvm" data-id-title="Creating the Shared Volume Groups"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.7: </span><span class="title-name">Creating the Shared Volume Groups </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-scenarios-iscsi-lvm">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open a <code class="systemitem">root</code> shell on one of the nodes you have run the iSCSI
      initiator from
      <a class="xref" href="#pro-ha-clvm-scenarios-iscsi-initiator" title="Configuring iSCSI Initiators">Procedure 23.6, “Configuring iSCSI Initiators”</a>.
     </p></li><li class="step"><p>
     Create the shared volume group on disks <code class="filename">/dev/sdd</code> and
     <code class="filename">/dev/sde</code>, using their stable device names (for example, in
      <code class="filename">/dev/disk/by-id/</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate --shared testvg /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div></li><li class="step"><p>
      Create logical volumes as needed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> --name lv1 --size 500M testvg</pre></div></li><li class="step"><p>
      Check the volume group with <code class="command">vgdisplay</code>:
     </p><div class="verbatim-wrap"><pre class="screen">  --- Volume group ---
      VG Name               testvg
      System ID
      Format                lvm2
      Metadata Areas        2
      Metadata Sequence No  1
      VG Access             read/write
      VG Status             resizable
      MAX LV                0
      Cur LV                0
      Open LV               0
      Max PV                0
      Cur PV                2
      Act PV                2
      VG Size               1016,00 MB
      PE Size               4,00 MB
      Total PE              254
      Alloc PE / Size       0 / 0
      Free  PE / Size       254 / 1016,00 MB
      VG UUID               UCyWw8-2jqV-enuT-KH4d-NXQI-JhH3-J24anD</pre></div></li><li class="step"><p>
      Check the shared state of the volume group with the command <code class="command">vgs</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgs</code>
  VG       #PV #LV #SN Attr   VSize     VFree
  vgshared   1   1   0 wz--ns 1016.00m  1016.00m</pre></div><p>
      The <code class="literal">Attr</code> column shows the volume attributes. In this example,
      the volume group is writable (<code class="literal">w</code>),
      resizeable (<code class="literal">z</code>), the allocation policy is normal (<code class="literal">n</code>),
      and it is a shared resource (<code class="literal">s</code>).
      See the man page of <code class="command">vgs</code> for details.</p></li></ol></div></div><p>
    After you have created the volumes and started your resources you should have new device
    names under <code class="filename">/dev/testvg</code>, for example <code class="filename">/dev/testvg/lv1</code>.
    This indicates the LV has been activated for use.
   </p></section><section class="sect2" id="sec-ha-clvm-scenario-drbd" data-id-title="Scenario: Cluster LVM with DRBD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.2.3 </span><span class="title-name">Scenario: Cluster LVM with DRBD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-scenario-drbd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following scenarios can be used if you have data centers located in
    different parts of your city, country, or continent.
   </p><div class="procedure" id="pro-ha-clvm-withdrbd" data-id-title="Creating a Cluster-Aware Volume Group With DRBD"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 23.8: </span><span class="title-name">Creating a Cluster-Aware Volume Group With DRBD </span></span><a title="Permalink" class="permalink" href="#pro-ha-clvm-withdrbd">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a primary/primary DRBD resource:
     </p><ol type="a" class="substeps"><li class="step"><p>
        First, set up a DRBD device as primary/secondary as described in
        <a class="xref" href="#pro-drbd-configure" title="Manually Configuring DRBD">Procedure 22.1, “Manually Configuring DRBD”</a>. Make sure the disk state is
        <code class="literal">up-to-date</code> on both nodes. Check this with
        <code class="command">drbdadm status</code>.
       </p></li><li class="step"><p>
        Add the following options to your configuration file (usually
        something like <code class="filename">/etc/drbd.d/r0.res</code>):
       </p><div class="verbatim-wrap"><pre class="screen">resource r0 {
  net {
     allow-two-primaries;
  }
  ...
}</pre></div></li><li class="step"><p>
        Copy the changed configuration file to the other node, for example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">scp</code> /etc/drbd.d/r0.res venus:/etc/drbd.d/</pre></div></li><li class="step"><p>
        Run the following commands on <span class="emphasis"><em>both</em></span> nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> disconnect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> connect r0
<code class="prompt root"># </code><code class="command">drbdadm</code> primary r0</pre></div></li><li class="step"><p>
        Check the status of your nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">drbdadm</code> status r0</pre></div></li></ol></li><li class="step"><p>
      Include the lvmlockd resource as a clone in the pacemaker configuration,
      and make it depend on the DLM clone resource. See
      <a class="xref" href="#pro-ha-clvm-rsc-dlm" title="Creating a DLM Resource">Procedure 23.1, “Creating a DLM Resource”</a> for detailed instructions.
      Before proceeding, confirm that these resources have started
      successfully on your cluster. Use <code class="command">crm status</code>
      or the Web interface to check the running services.
     </p></li><li class="step"><p>
      Prepare the physical volume for LVM with the command
      <code class="command">pvcreate</code>. For example, on the device
      <code class="filename">/dev/drbd_r0</code> the command would look like this:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">pvcreate</code> /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create a shared volume group:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">vgcreate</code> --shared testvg /dev/drbd_r0</pre></div></li><li class="step"><p>
      Create logical volumes as needed. You probably want to change the
      size of the logical volume. For example, create a 4 GB logical
      volume with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lvcreate</code> --name lv1 -L 4G testvg</pre></div></li><li class="step"><p>
      The logical volumes within the VG are now available as file system
      mounts for raw usage. Ensure that services using them have proper
      dependencies to collocate them with and order them after the VG has
      been activated.
     </p></li></ol></div></div><p>
    After finishing these configuration steps, the LVM configuration can be
    done like on any stand-alone workstation.
   </p></section></section><section class="sect1" id="sec-ha-clvm-drbd" data-id-title="Configuring Eligible LVM Devices Explicitly"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.3 </span><span class="title-name">Configuring Eligible LVM Devices Explicitly</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-drbd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or DRBD), we recommend to
   explicitly configure the devices which LVM scans for PVs.
  </p><p>
   For example, if the command <code class="command">vgcreate</code> uses the physical
   device instead of using the mirrored block device, DRBD will be confused.
   This may result in a split brain condition for DRBD.
  </p><p>
   To deactivate a single device for LVM, do the following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit the file <code class="filename">/etc/lvm/lvm.conf</code> and search for the
     line starting with <code class="literal">filter</code>.
    </p></li><li class="step"><p>
     The patterns there are handled as regular expressions. A leading
     <span class="quote">“<span class="quote">a</span>”</span> means to accept a device pattern to the scan, a
     leading <span class="quote">“<span class="quote">r</span>”</span> rejects the devices that follow the device
     pattern.
    </p></li><li class="step"><p>
     To remove a device named <code class="filename">/dev/sdb1</code>, add the
     following expression to the filter rule:
    </p><div class="verbatim-wrap"><pre class="screen">"r|^/dev/sdb1$|"</pre></div><p>
     The complete filter line will look like the following:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|", "r|/dev/.*/by-id/.*|", "a/.*/" ]</pre></div><p>
     A filter line that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </p><div class="verbatim-wrap"><pre class="screen">filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/" ]</pre></div></li><li class="step"><p>
     Write the configuration file and copy it to all cluster nodes.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-clvm-migrate" data-id-title="Online Migration from Mirror LV to Cluster MD"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">23.4 </span><span class="title-name">Online Migration from Mirror LV to Cluster MD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-migrate">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Starting with SUSE Linux Enterprise High Availability 15, <code class="systemitem">cmirrord</code> in Cluster LVM is deprecated. We highly
   recommend to migrate the mirror logical volumes in your cluster to cluster MD.
   Cluster MD stands for cluster multi-device and is a software-based
   RAID storage solution for a cluster.
  </p><section class="sect2" id="sec-ha-clvm-migrate-setup-before" data-id-title="Example Setup Before Migration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.4.1 </span><span class="title-name">Example Setup Before Migration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-migrate-setup-before">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
     Let us assume you have the following example setup:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    You have a two-node cluster consisting of the nodes <code class="literal">alice</code>
    and <code class="literal">bob</code>.
   </p></li><li class="listitem"><p>
     A mirror logical volume named <code class="literal">test-lv</code> was
     created from a volume group named <code class="literal">cluster-vg2</code>.
    </p></li><li class="listitem"><p>
     The volume group <code class="literal">cluster-vg2</code> is composed of the
     disks <code class="filename">/dev/vdb</code> and <code class="filename">/dev/vdc</code>.
   </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">lsblk</code>
NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                                   253:0    0   40G  0 disk
├─vda1                                253:1    0    4G  0 part [SWAP]
└─vda2                                253:2    0   36G  0 part /
vdb                                   253:16   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_0 254:0    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_0      254:3    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm
vdc                                   253:32   0   20G  0 disk
├─cluster--vg2-test--lv_mlog_mimage_1 254:1    0    4M  0 lvm
│ └─cluster--vg2-test--lv_mlog        254:2    0    4M  0 lvm
│   └─cluster--vg2-test--lv           254:5    0   12G  0 lvm
└─cluster--vg2-test--lv_mimage_1      254:4    0   12G  0 lvm
  └─cluster--vg2-test--lv             254:5    0   12G  0 lvm</pre></div><div id="adm-migration-fail" data-id-title="Avoiding Migration Failures" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Avoiding Migration Failures</div><p>
   Before you start the migration procedure, check the capacity and degree
   of utilization of your logical and physical volumes. If the logical volume
   uses 100% of the physical volume capacity, the migration might fail with an
   <code class="literal">insufficient free space</code> error on the target volume.
   How to prevent this migration failure depends on the options used for
   mirror log:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Is the mirror log itself mirrored (<code class="option">mirrored</code>
       option) and allocated on the same device as the mirror leg?</span> (For example, this might be the case if you have created the
       logical volume for a <code class="systemitem">cmirrord</code> setup on SUSE Linux Enterprise High Availability 11 or 12 as
       described in the <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/html/SLE-HA-all/cha-ha-clvm.html#sec-ha-clvm-config-cmirrord" target="_blank">
        Administration Guide for those versions</a>.)</p><p>
      By default, <code class="command">mdadm</code> reserves a certain amount of space
      between the start of a device and the start of array data. During migration,
      you can check for the unused padding space and reduce it with the
      <code class="option">data-offset</code> option as shown in <a class="xref" href="#step-data-offset" title="Step 1.d">Step 1.d</a>
      and following.
     </p><p>
      The <code class="option">data-offset</code> must leave enough space on the device
      for cluster MD to write its metadata to it. On the other hand, the offset
      must be small enough for the remaining capacity of the device to accommodate
      all physical volume extents of the migrated volume. Because the volume may
      have spanned the complete device minus the mirror log, the offset must be
      smaller than the size of the mirror log.
     </p><p>
      We recommend to set the <code class="option">data-offset</code> to 128 kB.
      If no value is specified for the offset, its default value is 1 kB
      (1024 bytes).
     </p></li><li class="listitem"><p><span class="formalpara-title">
      Is the mirror log written to a different device (<code class="option">disk</code>
      option) or kept in memory (<code class="option">core</code> option)?</span>
      Before starting the migration, either enlarge the size of the physical
      volume or reduce the size of the logical volume (to free more space for
      the physical volume).
     </p></li></ul></div></div></section><section class="sect2" id="sec-ha-clvm-migrate-lv2clustermd" data-id-title="Migrating a Mirror LV to Cluster MD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.4.2 </span><span class="title-name">Migrating a Mirror LV to Cluster MD</span></span> <a title="Permalink" class="permalink" href="#sec-ha-clvm-migrate-lv2clustermd">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following procedure is based on <a class="xref" href="#sec-ha-clvm-migrate-setup-before" title="23.4.1. Example Setup Before Migration">Section 23.4.1, “Example Setup Before Migration”</a>.
    Adjust the instructions to match your setup and replace the names for the
    LVs, VGs, disks and the cluster MD device accordingly.
  </p><p>
  The migration does not involve any downtime. The file system can
  still be mounted during the migration procedure.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On node <code class="literal">alice</code>, execute the following steps:
    </p><ol type="a" class="substeps"><li class="step"><p>
     Convert the mirror logical volume <code class="literal">test-lv</code>
     to a linear logical volume:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>lvconvert -m0 cluster-vg2/test-lv /dev/vdc</pre></div></li><li class="step"><p>
      Remove the physical volume <code class="filename">/dev/vdc</code> from the volume
      group <code class="literal">cluster-vg2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgreduce cluster-vg2 /dev/vdc</pre></div></li><li class="step"><p>
      Remove this physical volume from LVM:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>pvremove /dev/vdc</pre></div><p>When you run <code class="command">lsblk</code> now, you get:</p><div class="verbatim-wrap"><pre class="screen">NAME                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda                     253:0    0   40G  0 disk
├─vda1                  253:1    0    4G  0 part [SWAP]
└─vda2                  253:2    0   36G  0 part /
vdb                     253:16   0   20G  0 disk
└─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                     253:32   0   20G  0 disk</pre></div></li><li class="step" id="step-data-offset"><p>
      Create a cluster MD device <code class="filename">/dev/md0</code> with the disk
     <code class="filename">/dev/vdc</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mdadm --create /dev/md0 --bitmap=clustered \
--metadata=1.2 --raid-devices=1 --force --level=mirror \
/dev/vdc --data-offset=128</pre></div><p>
     For details on why to use the <code class="option">data-offset</code> option,
     see <a class="xref" href="#adm-migration-fail" title="Important: Avoiding Migration Failures">Important: Avoiding Migration Failures</a>.
    </p></li></ol></li><li class="step"><p>
     On node <code class="literal">bob</code>, assemble this MD device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mdadm --assemble md0 /dev/vdc</pre></div><p>
     If your cluster consists of more than two nodes, execute this step on all
     remaining nodes in your cluster.
    </p></li><li class="step"><p>Back on node <code class="literal">alice</code>:
   </p><ol type="a" class="substeps"><li class="step"><p>
     Initialize the MD device <code class="filename">/dev/md0</code> as physical volume
     for use with LVM:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>pvcreate /dev/md0</pre></div></li><li class="step"><p>
     Add the MD device <code class="filename">/dev/md0</code> to the volume group
     <code class="literal">cluster-vg2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgextend cluster-vg2 /dev/md0</pre></div></li><li class="step"><p>
     Move the data from the disk <code class="filename">/dev/vdb</code> to the
     <code class="filename">/dev/md0</code> device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>pvmove /dev/vdb /dev/md0</pre></div></li><li class="step"><p>
     Remove the physical volume <code class="filename">/dev/vdb</code> from the volume
     <code class="literal">group cluster-vg2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>vgreduce cluster-vg2 /dev/vdb</pre></div></li><li class="step"><p>
     Remove the label from the device so that LVM no longer recognizes it as
     physical volume:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>pvremove /dev/vdb</pre></div></li><li class="step"><p>
     Add <code class="filename">/dev/vdb</code> to the MD device <code class="filename">/dev/md0</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mdadm --grow /dev/md0 --raid-devices=2 --add /dev/vdb</pre></div></li></ol></li></ol></div></div></section><section class="sect2" id="ex-ha-clvm-migrate-setup-after" data-id-title="Example Setup After Migration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">23.4.3 </span><span class="title-name">Example Setup After Migration</span></span> <a title="Permalink" class="permalink" href="#ex-ha-clvm-migrate-setup-after">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_lvm.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When you run <code class="command">lsblk</code> now, you get:
  </p><div class="verbatim-wrap"><pre class="screen">NAME                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
vda                       253:0    0   40G  0 disk
├─vda1                    253:1    0    4G  0 part  [SWAP]
└─vda2                    253:2    0   36G  0 part  /
vdb                       253:16   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm
vdc                       253:32   0   20G  0 disk
└─md0                       9:0    0   20G  0 raid1
  └─cluster--vg2-test--lv 254:5    0   12G  0 lvm</pre></div></section></section></section><section class="chapter" id="cha-ha-cluster-md" data-id-title="Cluster Multi-device (Cluster MD)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">24 </span><span class="title-name">Cluster Multi-device (Cluster MD)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-cluster-md">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>The cluster multi-device (Cluster MD) is a software based RAID
   storage solution for a cluster. Currently, Cluster MD provides the redundancy of
   RAID1 mirroring to the cluster. With SUSE Linux Enterprise High Availability 15 SP2, RAID10 is
   included as a technology preview. If you want to try RAID10, replace <code class="literal">mirror</code>
   with <code class="literal">10</code> in the related <code class="command">mdadm</code> command.
   This chapter shows you how to create and use Cluster MD.
   </p></div></div></div></div><section class="sect1" id="sec-ha-cluster-md-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>The Cluster MD provides support for use of RAID1 across a cluster
   environment. The disks or devices used by Cluster MD are accessed by each node.
   If one device of the Cluster MD fails, it can be
   replaced at runtime by another device and it is re-synced to provide
   the same amount of redundancy. The Cluster MD requires Corosync
   and Distributed Lock Manager (DLM) for co-ordination and messaging.
  </p><p>
   A Cluster MD device is not automatically started on boot like the rest of
   the regular MD devices. A clustered device needs to be started using
   resource agents to ensure the DLM resource has been started.
  </p></section><section class="sect1" id="sec-ha-cluster-md-create" data-id-title="Creating a Clustered MD RAID Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.2 </span><span class="title-name">Creating a Clustered MD RAID Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-create">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><div class="title-container"><div class="itemizedlist-title-wrap"><div class="itemizedlist-title"><span class="title-number-name"><span class="title-name">Requirements </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.8.4.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div><ul class="itemizedlist"><li class="listitem"><p>A running cluster with pacemaker.</p></li><li class="listitem"><p>A resource agent for DLM (see <a class="xref" href="#sec-ha-storage-generic-dlm-config" title="19.2. Configuring DLM Cluster Resources">Section 19.2, “Configuring DLM Cluster Resources”</a>).</p></li><li class="listitem"><p>At least two shared disk devices. You can use an additional device as
    a spare which will fail over automatically in case of device failure.</p></li><li class="listitem"><p>An installed package <span class="package">cluster-md-kmp-default</span>.</p></li></ul></div><p>
   For better stability, use persistent device names
   such as <code class="literal">/dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code>
   instead of <code class="literal">/dev/sd<em class="replaceable">X</em></code> names.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the DLM resource is up and running on every node of the cluster
     and check the resource status with the command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_resource</code> -r dlm -W</pre></div></li><li class="step"><p>Create the Cluster MD device:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If you do not have an existing normal RAID device, create the Cluster MD
       device on the node running the DLM resource with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --create /dev/md0 --bitmap=clustered \
--metadata=1.2 --raid-devices=2 --level=mirror \
/dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em></code></pre></div><p>
       As Cluster MD only works with version 1.2 of the metadata, it is
       recommended to specify the version using the <code class="option">--metadata</code>
       option.
       For other useful options, refer to the man page of
        <code class="command">mdadm</code>. Monitor the progress of the re-sync in
        <code class="filename">/proc/mdstat</code>. </p></li><li class="listitem"><p>
       If you already have an existing normal RAID, first clear the existing
       bitmap and then create the clustered bitmap:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --grow /dev/md<em class="replaceable">X</em> --bitmap=none
<code class="prompt root"># </code><code class="command">mdadm</code> --grow /dev/md<em class="replaceable">X</em> --bitmap=clustered</pre></div></li><li class="listitem"><p>Optionally, to create a Cluster MD device with a
       spare device for automatic failover, run the following command on one
       cluster node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --create /dev/md0 --bitmap=clustered --raid-devices=2 \
--level=mirror --spare-devices=1 --metadata=1.2 \
/dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID3</em></code></pre></div></li></ul></div></li><li class="step"><p>Get the UUID and the related md path:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm</code> --detail --scan</pre></div><p>The UUID must match the UUID stored in the superblock. For details on
      the UUID, refer to the <code class="command">mdadm.conf</code> man page.
     </p></li><li class="step"><p>Open <code class="filename">/etc/mdadm.conf</code> and add the md device name
     and the devices associated with it. Use the UUID from the previous step: </p><div class="verbatim-wrap"><pre class="screen">DEVICE /dev/disk/by-id/<em class="replaceable">DEVICE_ID1</em> /dev/disk/by-id/<em class="replaceable">DEVICE_ID2</em>
ARRAY /dev/md0 UUID=1d70f103:49740ef1:af2afce5:fcf6a489</pre></div></li><li class="step"><p>Open Csync2's configuration file <code class="filename">/etc/csync2/csync2.cfg</code>
     and add <code class="filename">/etc/mdadm.conf</code>:</p><div class="verbatim-wrap"><pre class="screen">group ha_group
{
   # ... list of files pruned ...
   include /etc/mdadm.conf
}</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-ra" data-id-title="Configuring a Resource Agent"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.3 </span><span class="title-name">Configuring a Resource Agent</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-ra">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Configure a CRM resource as follows:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Create a <code class="systemitem">Raid1</code> primitive:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> raider Raid1 \
  params raidconf="/etc/mdadm.conf" raiddev=/dev/md0 \
  force_clones=true \
  op monitor timeout=20s interval=10 \
  op start timeout=20s interval=0 \
  op stop timeout=20s interval=0</pre></div></li><li class="step"><p>Add the <code class="systemitem">raider</code> resource to the base group for storage that you have created for
     DLM:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">modgroup</code> g-storage add raider</pre></div><p>The <code class="command">add</code> sub-command appends the new group
     member by default. </p><p>
    If not already done, clone the <code class="literal">g-storage</code> group so that it runs on all nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-storage g-storage \
    meta interleave=true target-role=Started</pre></div></li><li class="step"><p>Review your changes with
     <code class="command">show</code>.</p></li><li class="step"><p>If everything seems correct, submit your changes with
     <code class="command">commit</code>.</p></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-dev-add" data-id-title="Adding a Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.4 </span><span class="title-name">Adding a Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-add">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>To add a device to an existing, active Cluster MD device, first ensure that
   the device is <span class="quote">“<span class="quote">visible</span>”</span> on each node with the command
   <code class="command">cat /proc/mdstat</code>.
   If the device is not visible, the command will fail.
  </p><p>
   Use the following command on one cluster node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --manage /dev/md0 --add /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div><p>The behavior of the new device added depends on the state of the Cluster
   MD device:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If only one of the mirrored devices is active, the new device becomes
     the second device of the mirrored devices and a recovery is initiated.</p></li><li class="listitem"><p>If both devices of the Cluster MD device are active, the new
     added device becomes a spare device.</p></li></ul></div></section><section class="sect1" id="sec-ha-cluster-md-dev-readd" data-id-title="Re-adding a Temporarily Failed Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.5 </span><span class="title-name">Re-adding a Temporarily Failed Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-readd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Quite often the failures are transient and limited to a single node. If
   any of the nodes encounters a failure during an I/O operation, the device will
   be marked as failed for the entire cluster.
  </p><p> This could happen, for example, because of a cable failure on one of the
   nodes. After correcting the problem, you can re-add the device. Only the
   outdated parts will be synchronized as opposed to synchronizing the entire device by
   adding a new one.
  </p><p>
   To re-add the device, run the following command on one cluster node: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --manage /dev/md0 --re-add /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div></section><section class="sect1" id="sec-ha-cluster-md-dev-remove" data-id-title="Removing a Device"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.6 </span><span class="title-name">Removing a Device</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-dev-remove">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>Before removing a device at runtime for replacement, do the following:</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Make sure the device is failed by introspecting <code class="filename">/proc/mdstat</code>.
    Look for an <code class="literal">(F)</code> before the device.</p></li><li class="step"><p>Run the following command on one cluster node to make a device fail:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --manage /dev/md0 --fail /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div></li><li class="step"><p>Remove the failed device using the command on one cluster node:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mdadm --manage /dev/md0 --remove /dev/disk/by-id/<em class="replaceable">DEVICE_ID</em></code></pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-cluster-md-convert-raid" data-id-title="Assembling Cluster MD as normal RAID at the disaster recovery site"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">24.7 </span><span class="title-name">Assembling Cluster MD as normal RAID at the disaster recovery site</span></span> <a title="Permalink" class="permalink" href="#sec-ha-cluster-md-convert-raid">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_cluster_md.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the event of disaster recovery, you might face the situation that you do not have
   a Pacemaker cluster stack in the infrastructure on the disaster recovery site, but
   applications still need to access the data on the existing Cluster MD disks,
   or from the backups.
  </p><p>
   You can convert a Cluster MD RAID to a normal RAID by using the <code class="option">--assemble</code>
   operation with the <code class="option">-U no-bitmap</code> option to change the metadata
   of the RAID disks accordingly.
   </p><p>
    Find an example below of how to assemble all arrays on the data recovery site:
   </p><div class="verbatim-wrap"><pre class="screen">while read i; do
   NAME=`echo $i | sed 's/.*name=//'|awk '{print $1}'|sed 's/.*://'`
   UUID=`echo $i | sed 's/.*UUID=//'|awk '{print $1}'`
   mdadm -AR "/dev/md/$NAME" -u $UUID -U no-bitmap
   echo "NAME =" $NAME ", UUID =" $UUID ", assembled."
done &lt; &lt;(mdadm -Es)</pre></div></section></section><section class="chapter" id="cha-ha-samba" data-id-title="Samba Clustering"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">25 </span><span class="title-name">Samba Clustering</span></span> <a title="Permalink" class="permalink" href="#cha-ha-samba">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    A clustered Samba server provides a High Availability solution in your
    heterogeneous networks. This chapter explains some background
    information and how to set up a clustered Samba server.
   </p></div></div></div></div><section class="sect1" id="sec-ha-samba-overview" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">25.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Trivial Database (TDB) has been used by Samba for many years. It allows
   multiple applications to write simultaneously. To make sure all write
   operations are successfully performed and do not collide with each other,
   TDB uses an internal locking mechanism.
  </p><p>
   Cluster Trivial Database (CTDB) is a small extension of the existing TDB.
   CTDB is described by the project as a <span class="quote">“<span class="quote">cluster implementation of
   the TDB database used by Samba and other projects to store temporary
   data</span>”</span>.
  </p><p>
   Each cluster node runs a local CTDB daemon. Samba communicates with its
   local CTDB daemon instead of writing directly to its TDB. The daemons
   exchange metadata over the network, but actual write and read operations
   are done on a local copy with fast storage. The concept of CTDB is
   displayed in <a class="xref" href="#fig-ha-samba-overview" title="Structure of a CTDB Cluster">Figure 25.1, “Structure of a CTDB Cluster”</a>.
  </p><div id="id-1.4.5.9.3.5" data-id-title="CTDB For Samba Only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: CTDB For Samba Only</div><p>
    The current implementation of the CTDB Resource Agent configures CTDB to
    only manage Samba. Everything else, including IP failover, should be
    configured with Pacemaker.
   </p><p>
    CTDB is only supported for completely homogeneous clusters. For example,
    all nodes in the cluster need to have the same architecture. You cannot
    mix x86 with AMD64.
   </p></div><div class="figure" id="fig-ha-samba-overview"><div class="figure-contents"><div class="mediaobject"><a href="images/ha_samba.png"><img src="images/ha_samba.png" width="80%" alt="Structure of a CTDB Cluster" title="Structure of a CTDB Cluster"/></a></div></div><div class="title-container"><div class="figure-title-wrap"><div class="figure-title"><span class="title-number-name"><span class="title-number">Figure 25.1: </span><span class="title-name">Structure of a CTDB Cluster </span></span><a title="Permalink" class="permalink" href="#fig-ha-samba-overview">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div><p>
   A clustered Samba server must share certain data:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Mapping table that associates Unix user and group IDs to Windows users
     and groups.
    </p></li><li class="listitem"><p>
     The user database must be synchronized between all nodes.
    </p></li><li class="listitem"><p>
     Join information for a member server in a Windows domain must be
     available on all nodes.
    </p></li><li class="listitem"><p>
     Metadata needs to be available on all nodes, like active SMB sessions,
     share connections, and various locks.
    </p></li></ul></div><p>
   The goal is that a clustered Samba server with N+1 nodes is faster than
   with only N nodes. One node is not slower than an unclustered Samba
   server.
  </p></section><section class="sect1" id="sec-ha-samba-basicconf" data-id-title="Basic Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">25.2 </span><span class="title-name">Basic Configuration</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-basicconf">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.5.9.4.3" data-id-title="Changed Configuration Files" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Changed Configuration Files</div><p>
    The CTDB Resource Agent automatically changes
    <code class="filename">/etc/sysconfig/ctdb</code>. Use <code class="command">crm
    ra</code> <code class="option">info CTDB</code> to list all parameters
    that can be specified for the CTDB resource.
   </p></div><p>
   To set up a clustered Samba server, proceed as follows:
  </p><div class="procedure" id="pro-ha-samba-basicconf" data-id-title="Setting Up a Basic Clustered Samba Server"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 25.1: </span><span class="title-name">Setting Up a Basic Clustered Samba Server </span></span><a title="Permalink" class="permalink" href="#pro-ha-samba-basicconf">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Prepare your cluster:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Make sure the following packages are installed before you proceed:
       <span class="package">ctdb</span>,
       <span class="package">tdb-tools</span>, and
       <span class="package">samba</span> (needed for
       <code class="literal">smb</code> and <code class="literal">nmb</code> resources).
      </p></li><li class="step"><p>
       Configure your cluster (Pacemaker, OCFS2) as described in this guide
       in <a class="xref" href="#part-config" title="Part II. Configuration and Administration">Part II, “Configuration and Administration”</a>.
      </p></li><li class="step"><p>
       Configure a shared file system, like OCFS2, and mount it, for
       example, on <code class="filename">/srv/clusterfs</code>.
        See <a class="xref" href="#cha-ha-ocfs2" title="Chapter 20. OCFS2">Chapter 20, <em>OCFS2</em></a> for more information.
      </p></li><li class="step"><p>
       If you want to turn on POSIX ACLs, enable it:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         For a new OCFS2 file system use:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkfs.ocfs2</code> --fs-features=xattr ...</pre></div></li><li class="listitem"><p>
         For an existing OCFS2 file system use:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">tunefs.ocfs2</code> --fs-feature=xattr <em class="replaceable">DEVICE</em></pre></div><p>
         Make sure the <code class="option">acl</code> option is specified in the file
         system resource. Use the <code class="command">crm</code> shell as follows:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ocfs2-3 ocf:heartbeat:Filesystem params options="acl" ...</pre></div></li></ul></div></li><li class="step"><p>
       Make sure the services <code class="systemitem">ctdb</code>,
       <code class="systemitem">smb</code>, and
       <code class="systemitem">nmb</code> are disabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable ctdb
<code class="prompt root"># </code><code class="command">systemctl</code> disable smb
<code class="prompt root"># </code><code class="command">systemctl</code> disable nmb</pre></div></li><li class="step"><p>
       Open port <code class="literal">4379</code> of your firewall on all nodes. This
       is needed for CTDB to communicate with other cluster nodes.
      </p></li></ol></li><li class="step"><p>
     Create a directory for the CTDB lock on the shared file system:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mkdir</code> -p /srv/clusterfs/samba/</pre></div></li><li class="step"><p>
     In <code class="filename">/etc/ctdb/nodes</code> insert all nodes which contain
     all private IP addresses of each node in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen">192.168.1.10
192.168.1.11</pre></div></li><li class="step"><p>
     Configure Samba. Add the following lines in the
     <code class="literal">[global]</code> section of
     <code class="filename">/etc/samba/smb.conf</code>. Use the host name of your
     choice in place of "CTDB-SERVER" (all nodes in the cluster will appear
     as one big node with this name, effectively):
    </p><div class="verbatim-wrap"><pre class="screen">[global]
    # ...
    # settings applicable for all CTDB deployments
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket
    # settings necessary for CTDB on OCFS2
    fileid:algorithm = fsid
    vfs objects = fileid
    # ...</pre></div></li><li class="step"><p>
     Copy the configuration file to all of your nodes by using
     <code class="command">csync2</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p>
     For more information, see
     <a class="xref" href="#pro-ha-installation-setup-csync2-start" title="Synchronizing the configuration files with Csync2">Procedure 4.9, “Synchronizing the configuration files with Csync2”</a>.
    </p></li><li class="step"><p>
     Add a CTDB resource to the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ctdb CTDB params \
    ctdb_manages_winbind="false" \ 
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="/srv/clusterfs/samba/ctdb.lock" \
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \ 
      op monitor interval="10" timeout="20" \
      op start interval="0" timeout="90" \
      op stop interval="0" timeout="100"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">group</code> g-ctdb ctdb nmb smb
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ctdb g-ctdb meta interleave="true"
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-ctdb-with-clusterfs inf: cl-ctdb cl-clusterfs
<code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-clusterfs-then-ctdb Mandatory: cl-clusterfs cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Add a clustered IP address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> ip IPaddr2 params ip=192.168.2.222 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<code class="prompt custom">crm(live)configure# </code><code class="command">clone</code> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<code class="prompt custom">crm(live)configure# </code><code class="command">colocation</code> col-ip-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">order</code> o-ip-then-ctdb 0: cl-ip cl-ctdb
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
     If <code class="literal">unique_clone_address</code> is set to
     <code class="literal">true</code>, the IPaddr2 resource agent adds a clone ID to
     the specified address, leading to three different IP addresses. These
     are usually not needed, but help with load balancing. For further
     information about this topic, see <a class="xref" href="#sec-ha-lb-lvs" title="17.2. Configuring Load Balancing with Linux Virtual Server">Section 17.2, “Configuring Load Balancing with Linux Virtual Server”</a>.
    </p></li><li class="step"><p>
     Commit your change:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Check the result:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> status
Clone Set: cl-storage [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
Clone Set: cl-clusterfs [clusterfs]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</pre></div></li><li class="step"><p>
     Test from a client machine. On a Linux client, run the following
     command to see if you can copy files from and to the system:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">smbclient</code> <code class="option">//192.168.2.222/myshare</code></pre></div></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-ad" data-id-title="Joining an Active Directory Domain"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">25.3 </span><span class="title-name">Joining an Active Directory Domain</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-ad">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Active Directory (AD) is a directory service for Windows server systems.
  </p><p>
   The following instructions outline how to join a CTDB cluster to an
   Active Directory domain:
  </p><div class="procedure" id="pro-ha-samba-ad-join"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a CTDB resource as described in
     <a class="xref" href="#pro-ha-samba-basicconf" title="Setting Up a Basic Clustered Samba Server">Procedure 25.1, “Setting Up a Basic Clustered Samba Server”</a>.
    </p></li><li class="step"><p>
     Install the <span class="package">samba-winbind</span>
     package.
    </p></li><li class="step"><p>
     Disable the <code class="systemitem">winbind</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> disable winbind</pre></div></li><li class="step"><p>
     Define a winbind cluster resource:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> configure
<code class="prompt custom">crm(live)configure# </code><code class="command">primitive</code> winbind systemd:winbind \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<code class="prompt custom">crm(live)configure# </code><code class="command">commit</code></pre></div></li><li class="step"><p>
     Edit the <code class="literal">g-ctdb</code> group and insert
     <code class="literal">winbind</code> between the <code class="literal">nmb</code> and
     <code class="literal">smb</code> resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)configure# </code><code class="command">edit</code> g-ctdb</pre></div><p>
     Save and close the editor with <span class="keycap">:</span><span class="key-connector">–</span><span class="keycap">w</span> (<code class="command">vim</code>).
    </p></li><li class="step"><p>
     Consult your Windows Server documentation for instructions on how to
     set up an Active Directory domain. In this example, we use the
     following parameters:
    </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          AD and DNS server
         </p>
        </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">win2k3.2k3test.example.com</pre></div>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          AD domain
         </p>
        </td><td style="border-bottom: 1px solid ; ">
<div class="verbatim-wrap"><pre class="screen">2k3test.example.com</pre></div>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Cluster AD member NetBIOS name
         </p>
        </td><td>
<div class="verbatim-wrap"><pre class="screen">CTDB-SERVER</pre></div>
        </td></tr></tbody></table></div></li><li class="step"><p>
     <a class="xref" href="#pro-ha-samba-config-join-ad" title="Joining Active Directory">Procedure 25.2, “Joining Active Directory”</a>
    </p></li></ol></div></div><p>
   Finally, join your cluster to the Active Directory server:
  </p><div class="procedure" id="pro-ha-samba-config-join-ad" data-id-title="Joining Active Directory"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 25.2: </span><span class="title-name">Joining Active Directory </span></span><a title="Permalink" class="permalink" href="#pro-ha-samba-config-join-ad">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure the following files are included in Csync2's
     configuration to become installed on all cluster hosts:
    </p><div class="verbatim-wrap"><pre class="screen">/etc/samba/smb.conf
/etc/security/pam_winbind.conf
/etc/krb5.conf
/etc/nsswitch.conf
/etc/security/pam_mount.conf.xml
/etc/pam.d/common-session</pre></div><p>
     You can also use YaST's <span class="guimenu">Configure Csync2</span>
     module for this task, see
     <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
    </p></li><li class="step"><p>
     Run YaST and open the <span class="guimenu">Windows Domain Membership</span>
     module from the <span class="guimenu">Network Services</span> entry.
    </p></li><li class="step"><p>
     Enter your domain or workgroup settings and finish with
     <span class="guimenu">Ok</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-testing" data-id-title="Debugging and Testing Clustered Samba"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">25.4 </span><span class="title-name">Debugging and Testing Clustered Samba</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-testing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To debug your clustered Samba server, the following tools which operate
   on different levels are available:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.9.6.3.1"><span class="term"><code class="command">ctdb_diagnostics</code>
    </span></dt><dd><p>
      Run this tool to diagnose your clustered Samba server. Detailed debug
      messages should help you track down any problems you might have.
     </p><p>
      The <code class="command">ctdb_diagnostics</code> command searches for the
      following files which must be available on all nodes:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/krb5.conf
/etc/hosts
/etc/ctdb/nodes
/etc/sysconfig/ctdb
/etc/resolv.conf
/etc/nsswitch.conf
/etc/sysctl.conf
/etc/samba/smb.conf
/etc/fstab
/etc/multipath.conf
/etc/pam.d/system-auth
/etc/sysconfig/nfs
/etc/exports
/etc/vsftpd/vsftpd.conf</pre></div><p>
      If the files <code class="filename">/etc/ctdb/public_addresses</code> and
      <code class="filename">/etc/ctdb/static-routes</code> exist, they will be
      checked as well.
     </p></dd><dt id="id-1.4.5.9.6.3.2"><span class="term"><code class="command">ping_pong</code>
    </span></dt><dd><p>
      Check whether your file system is suitable for CTDB with
      <code class="command">ping_pong</code>. It performs certain tests of your
      cluster file system like coherence and performance (see
      <a class="link" href="http://wiki.samba.org/index.php/Ping_pong" target="_blank">http://wiki.samba.org/index.php/Ping_pong</a>) and
      gives some indication how your cluster may behave under high load.
     </p></dd><dt id="id-1.4.5.9.6.3.3"><span class="term"><code class="command">send_arp</code> Tool and <code class="systemitem">SendArp</code> Resource Agent</span></dt><dd><p>
      The <code class="systemitem">SendArp</code> resource agent
      is located in <code class="filename">/usr/lib/heartbeat/send_arp</code> (or
      <code class="filename">/usr/lib64/heartbeat/send_arp</code>). The
      <code class="command">send_arp</code> tool sends out a gratuitous ARP (Address
      Resolution Protocol) packet and can be used for updating other
      machines' ARP tables. It can help to identify communication problems
      after a failover process. If you cannot connect to a node or ping it
      although it shows the clustered IP address for Samba, use the
      <code class="command">send_arp</code> command to test if the nodes only need an
      ARP table update.
     </p><p>
      For more information, refer to
      <a class="link" href="http://wiki.wireshark.org/Gratuitous_ARP" target="_blank">http://wiki.wireshark.org/Gratuitous_ARP</a>.
     </p></dd></dl></div><p>
   To test certain aspects of your cluster file system proceed as follows:
  </p><div class="procedure" id="id-1.4.5.9.6.5" data-id-title="Test Coherence and Performance of Your Cluster File System"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 25.3: </span><span class="title-name">Test Coherence and Performance of Your Cluster File System </span></span><a title="Permalink" class="permalink" href="#id-1.4.5.9.6.5">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start the command <code class="command">ping_pong</code> on one node and replace
     the placeholder <em class="replaceable">N</em> with the amount of nodes
     plus one. The file
     <code class="filename"><em class="replaceable">ABSPATH</em>/data.txt</code> is
     available in your shared storage and is therefore accessible on all
     nodes (<em class="replaceable">ABSPATH </em> indicates an absolute path):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="command">ping_pong</code> <em class="replaceable">ABSPATH</em>/data.txt <em class="replaceable">N</em></pre></div><p>
     Expect a very high locking rate as you are running only one node. If
     the program does not print a locking rate, replace your cluster file
     system.
    </p></li><li class="step"><p>
     Start a second copy of <code class="command">ping_pong</code> on another node
     with the same parameters.
    </p><p>
     Expect to see a dramatic drop in the locking rate. If any of the
     following applies to your cluster file system, replace it:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="command">ping_pong</code> does not print a locking rate per
       second,
      </p></li><li class="listitem"><p>
       the locking rates in the two instances are not almost equal,
      </p></li><li class="listitem"><p>
       the locking rate did not drop after you started the second instance.
      </p></li></ul></div></li><li class="step"><p>
     Start a third copy of <code class="command">ping_pong</code>. Add another node
     and note how the locking rates change.
    </p></li><li class="step"><p>
     Kill the <code class="command">ping_pong</code> commands one after the other. You
     should observe an increase of the locking rate until you get back to
     the single node case. If you did not get the expected behavior, find
     more information in <a class="xref" href="#cha-ha-ocfs2" title="Chapter 20. OCFS2">Chapter 20, <em>OCFS2</em></a>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-samba-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">25.5 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-samba-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_samba.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="http://wiki.samba.org/index.php/CTDB_Setup" target="_blank">http://wiki.samba.org/index.php/CTDB_Setup</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://ctdb.samba.org" target="_blank">http://ctdb.samba.org</a>
    </p></li><li class="listitem"><p>
     <a class="link" href="http://wiki.samba.org/index.php/Samba_%26_Clustering" target="_blank">http://wiki.samba.org/index.php/Samba_%26_Clustering</a>
    </p></li></ul></div></section></section><section class="chapter" id="cha-ha-rear" data-id-title="Disaster Recovery with ReaR (Relax-and-Recover)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">26 </span><span class="title-name">Disaster Recovery with ReaR (Relax-and-Recover)</span></span> <a title="Permalink" class="permalink" href="#cha-ha-rear">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Relax-and-Recover (<span class="quote">“<span class="quote">ReaR</span>”</span>) is a disaster recovery framework for use by
    system administrators. It is a collection of Bash scripts that need to
    be adjusted to the specific production environment that is to be
    protected in case of disaster.
   </p><p>
    No disaster recovery solution will  work out-of-the-box. Therefore
    it is essential to take preparations <span class="emphasis"><em>before</em></span> any
    disaster happens.
   </p></div></div></div></div><section class="sect1" id="sec-ha-rear-concept" data-id-title="Conceptual Overview"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.1 </span><span class="title-name">Conceptual Overview</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The following sections describe the general disaster recovery concept and
   the basic steps you need to execute for successful recovery with
   ReaR. They also provide some guidance on ReaR requirements,

   some limitations to be aware of, and scenarios and backup tools.
  </p><section class="sect2" id="sec-ha-rear-concept-drp" data-id-title="Creating a Disaster Recovery Plan"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.1 </span><span class="title-name">Creating a Disaster Recovery Plan</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-drp">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    <span class="emphasis"><em>Before</em></span> the worst scenario happens, take action:
    analyze your IT infrastructure for any substantial risks, evaluate your
    budget, and create a disaster recovery plan. If you do not already have
    a disaster recovery plan at hand, find some information on each step
    below:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Risk Analysis. </span>
       Conduct a solid risk analysis of your infrastructure. List all the
       possible threats and evaluate how serious they are. Determine how
       likely these threats are and prioritize them. It is recommended to
       use a simple categorization: probability and impact.
      </p></li><li class="listitem"><p><span class="formalpara-title">Budget Planning. </span>
       The outcome of the analysis is an overview, which risks can be
       tolerated and which are critical for your business. Ask yourself how
       you can minimize risks and how much will it cost. Depending on how
       big your company is, spend two to fifteen percent of the overall IT
       budget on disaster recovery.
      </p></li><li class="listitem"><p><span class="formalpara-title">Disaster Recovery Plan Development. </span>
       Make checklists, test procedures, establish and assign priorities,
       and inventory your IT infrastructure. Define how to deal with a
       problem when some services in your infrastructure fail.
      </p></li><li class="listitem"><p><span class="formalpara-title">Test. </span>
       After defining an elaborate plan, test it. Test it at least once a
       year. Use the same testing hardware as your main IT infrastructure.
      </p></li></ul></div></section><section class="sect2" id="sec-ha-rear-concept-recovery" data-id-title="What Does Disaster Recovery Mean?"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.2 </span><span class="title-name">What Does Disaster Recovery Mean?</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-recovery">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    If a system in a production environment has been destroyed (for whatever
    reasons—be it broken hardware, a misconfiguration or software
    problems), you need to re-create the system. The recreation can be done
    either on the same hardware or on compatible replacement hardware.
    Re-creating a system means more than restoring files from a backup.
    It also includes preparing the system's storage (with regard to partitioning, file
    systems, and mount points), and reinstalling the boot
    loader.
   </p></section><section class="sect2" id="sec-ha-rear-concept-basics" data-id-title="How Does Disaster Recovery With ReaR Work?"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.3 </span><span class="title-name">How Does Disaster Recovery With ReaR Work?</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-basics">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    While the system is up and running, create a backup of the files and
    create a recovery system on a recovery medium. The recovery system
    contains a recovery installer.
   </p><p>
    In case the system has been destroyed, replace broken hardware (if
    needed), boot the recovery system from the recovery medium and launch
    the recovery installer. The recovery installer re-creates the system:
    First, it prepares the storage (partitioning, file systems, mount
    points), then it restores the files from the backup. Finally, it
    reinstalls the boot loader.
   </p></section><section class="sect2" id="sec-ha-rear-concept-requirements" data-id-title="ReaR Requirements"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.4 </span><span class="title-name">ReaR Requirements</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-requirements">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To use ReaR you need at least two identical systems: the machine
    that runs your production environment and an identical test machine.
    <span class="quote">“<span class="quote">Identical</span>”</span> in this context means that you can, for
    example, replace a network card with another one using the same Kernel
    driver.
   </p><div id="id-1.4.5.10.3.6.3" data-id-title="Identical Drivers Required" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Identical Drivers Required</div><p>
     If a hardware component does not use the same driver as the one in
     your production environment, it is not considered identical by
     ReaR.
    </p></div></section><section class="sect2" id="sec-ha-rear-concept-versions" data-id-title="ReaR Version Updates"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.5 </span><span class="title-name">ReaR Version Updates</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-versions">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    SUSE Linux Enterprise High Availability 15 SP2 ships with ReaR version 2.3, provided by the
    package <span class="package">rear23a</span>.
   </p><div id="id-1.4.5.10.3.7.3" data-id-title="Find Important Information in Changelogs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Find Important Information in Changelogs</div><p>
     Any information about bug fixes, incompatibilities, and other issues can
     be found in the changelogs of the packages. It is recommended to review
     also later package versions of ReaR in case you need to re-validate
     your disaster recovery procedure.
    </p></div><p>
     Be aware of the following issues with ReaR:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      To allow disaster recovery on UEFI systems, you need at least ReaR
      version 1.18.a and the package <span class="package">ebiso</span>.
      Only this version supports the new helper tool
      <code class="filename">/usr/bin/ebiso</code>. This helper tool is
      used to create a UEFI-bootable ReaR system ISO image.
     </p></li><li class="listitem"><p>
      If you have a tested and fully functional disaster recovery procedure
      with one ReaR version, do not update ReaR. Keep the ReaR package
      and do not change your disaster recovery method!
     </p></li><li class="listitem"><p>
      Version updates for ReaR are provided as separate packages that
      intentionally conflict with each other to prevent your installed version
      getting accidentally replaced with another version.
     </p></li></ul></div><p>
    In the following cases you need to completely re-validate your existing
    disaster recovery procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      For each ReaR version update.
     </p></li><li class="listitem"><p>When you update ReaR manually.</p></li><li class="listitem"><p>
      For each software that is used by ReaR.
     </p></li><li class="listitem"><p>
      If you update low-level system components such as <code class="command">parted</code>,
       <code class="command">btrfs</code> and similar.
     </p></li></ul></div></section><section class="sect2" id="sec-ha-rear-concept-limit" data-id-title="Limitations with Btrfs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.6 </span><span class="title-name">Limitations with Btrfs</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-limit">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    The following limitations apply if you use Btrfs.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.10.3.8.3.1"><span class="term">Your System Includes Subvolumes, but No Snapshots Subvolumes</span></dt><dd><p>
       At least ReaR version 1.17.2.a is required. This version supports re-creating
       <span class="quote">“<span class="quote">normal</span>”</span> Btrfs subvolume structure (no snapshot
       subvolumes).
      </p></dd><dt id="id-1.4.5.10.3.8.3.2"><span class="term">Your System Includes Snapshot Subvolumes</span></dt><dd><div id="id-1.4.5.10.3.8.3.2.2.1" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
        Btrfs snapshot subvolumes <span class="emphasis"><em>cannot</em></span> be backed up
        and restored as usual with file-based backup software.
       </p></div><p>
       While recent snapshot subvolumes on Btrfs file systems need
       almost no disk space (because of Btrfs's copy-on-write functionality),
       those files would be backed up as complete files when using
       file-based backup software. They would end up twice in the backup
       with their original file size. Therefore, it is impossible to
       restore the snapshots as they have been before on the original
       system.
      </p></dd><dt id="id-1.4.5.10.3.8.3.3"><span class="term">Your SLE System Needs Matching ReaR Configuration</span></dt><dd><p>
       For example, the setup in SLE12 GA, SLE12 SP1, and SLE12 SP2 have several
       different incompatible Btrfs default structures. As such, it is
       crucial to use a matching ReaR configuration file. See the example
       files <code class="filename">/usr/share/rear/conf/examples/SLE12*-btrfs-example.conf</code>.
       
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-rear-concept-scenarios" data-id-title="Scenarios and Backup Tools"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.7 </span><span class="title-name">Scenarios and Backup Tools</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    ReaR can create a disaster recovery system (including a system-specific
    recovery installer) that can be booted from a local medium (like a hard
    disk, a flash disk, a DVD/CD-R) or via PXE. The backup data can be
    stored on a network file system, for example NFS, as described in
    <a class="xref" href="#ex-ha-rear-nfs-server-backup" title="Using an NFS Server to Store the File Backup">Example 26.1</a>.
   </p><p>
    ReaR does not replace a file backup, but complements it. By
    default, ReaR supports the generic <code class="command">tar</code> command,
    and several third-party backup tools (such as Tivoli Storage Manager,
    QNetix Galaxy, Symantec NetBackup, EMC NetWorker, or HP DataProtector).
    Refer to
    <a class="xref" href="#ex-ha-rear-config-EMC" title="Using Third-Party Backup Tools Like EMC NetWorker">Example 26.2</a> for an
    example configuration of using ReaR with EMC NetWorker as backup
    tool.
   </p></section><section class="sect2" id="sec-ha-rear-concept-overview" data-id-title="Basic Steps"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">26.1.8 </span><span class="title-name">Basic Steps</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-concept-overview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    For a successful recovery with ReaR in case of disaster, you need
    to execute the following basic steps:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.10.3.10.3.1"><span class="term"><a class="xref" href="#sec-ha-rear-config" title="26.2. Setting Up ReaR and Your Backup Solution">Setting Up ReaR and Your Backup Solution</a>
     </span></dt><dd><p>
       This includes tasks like editing the ReaR configuration file,
       adjusting the Bash scripts, and configuring the backup solution that
       you want to use.
      </p></dd><dt id="id-1.4.5.10.3.10.3.2"><span class="term"><a class="xref" href="#sec-ha-rear-mkbackup" title="26.3. Creating the Recovery Installation System">Creating the Recovery Installation System</a>
     </span></dt><dd><p>
       While the system to be protected is up and running use the
       <code class="command">rear mkbackup</code> command to create a file backup and
       to generate a recovery system that contains a system-specific
       ReaR recovery installer.
      </p></dd><dt id="id-1.4.5.10.3.10.3.3"><span class="term"><a class="xref" href="#sec-ha-rear-testing" title="26.4. Testing the Recovery Process">Testing the Recovery Process</a>
     </span></dt><dd><p>
       Whenever you have created a disaster recovery medium with ReaR,
       test the disaster recovery process thoroughly. It is essential to use
       a test machine that has <span class="emphasis"><em>identical</em></span> hardware like
       the one that is part of your production environment. For details,
       refer to <a class="xref" href="#sec-ha-rear-concept-requirements" title="26.1.4. ReaR Requirements">Section 26.1.4, “ReaR Requirements”</a>.
      </p></dd><dt id="id-1.4.5.10.3.10.3.4"><span class="term"><a class="xref" href="#sec-ha-rear-recover" title="26.5. Recovering from Disaster">Recovering from Disaster</a>
     </span></dt><dd><p>
       After a disaster has occurred, replace any broken hardware (if
       necessary). Then boot the ReaR recovery system and start the
       recovery installer with the <code class="command">rear recover</code> command.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-ha-rear-config" data-id-title="Setting Up ReaR and Your Backup Solution"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.2 </span><span class="title-name">Setting Up ReaR and Your Backup Solution</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-config">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   To set up ReaR, you need to edit at least the ReaR
   configuration file <code class="filename">/etc/rear/local.conf</code> and, if
   needed, the Bash scripts that are part of the ReaR framework.
  </p><p>
   In particular, you need to define the following tasks that ReaR
   should do:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">When your system is booted with UEFI. </span>
      If your system boots with a UEFI boot loader, install the package
      <span class="package">ebiso</span> and add the following line to
      <code class="filename">/etc/rear/local.conf</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ISO_MKISOFS_BIN="/usr/bin/ebiso"</pre></div><p>
     If your system boots with UEFI Secure Boot, you must also add the following line:
    </p><div class="verbatim-wrap"><pre class="screen">SECURE_BOOT_BOOTLOADER="/boot/efi/EFI/BOOT/shim.efi"</pre></div><p>
     For more information about ReaR configuration variables for UEFI, see the
     <code class="filename">/usr/share/rear/conf/default.conf</code> file.
    </p></li><li class="listitem"><p><span class="formalpara-title">How to back up files and how to create and store the disaster recovery system. </span>
      This needs to be configured in
      <code class="filename">/etc/rear/local.conf</code>.
     </p></li><li class="listitem"><p><span class="formalpara-title">What to re-create exactly (partitioning, file systems, mount points, etc.). </span>
      This can be defined in <code class="filename">/etc/rear/local.conf</code> (for
      example, what to exclude). To re-create non-standard systems, you may
      need to enhance the Bash scripts.
     </p></li><li class="listitem"><p><span class="formalpara-title">How the recovery process works. </span>
      To change how ReaR generates the recovery installer, or to adapt
      what the ReaR recovery installer does, you need to edit the Bash
      scripts.
     </p></li></ul></div><p>
   To configure ReaR, add your options to the
   <code class="filename">/etc/rear/local.conf</code> configuration file. (The former
   configuration file <code class="filename">/etc/rear/sites.conf</code> has been
   removed from the package. However, if you have such a file from your last
   setup, ReaR will still use it.)
  </p><p>
   All ReaR configuration variables and their default values are set in
   <code class="filename">/usr/share/rear/conf/default.conf</code>.
   Some example files (<code class="filename">*example.conf</code>) for user configurations
   (for example, what is set in <code class="filename">/etc/rear/local.conf</code>)
   are available in the <code class="filename">examples</code> subdirectory.
   Find more information in the ReaR man page.
  </p><p>
   You should start with a matching example configuration file as template
   and adapt it as needed to create your particular configuration file.
   Copy various options from several example configuration files and paste them
   into your specific <code class="filename">/etc/rear/local.conf</code> file that
   matches your particular system.
   Do not use original example configuration files, because they provide an
   overview of variables that can be used for specific setups.
  </p><div class="complex-example"><div class="example" id="ex-ha-rear-nfs-server-backup" data-id-title="Using an NFS Server to Store the File Backup"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 26.1: </span><span class="title-name">Using an NFS Server to Store the File Backup </span></span><a title="Permalink" class="permalink" href="#ex-ha-rear-nfs-server-backup">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    ReaR can be used in different scenarios. The following example uses
    an NFS server as storage for the file backup.
   </p></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up an NFS server with YaST as described in the
     <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-nfs.html" target="_blank">
      Administration Guide for SUSE Linux Enterprise Server 15 SP2</a>.
    </p></li><li class="step"><p>
     Define the configuration for your NFS server in the
     <code class="filename">/etc/exports</code> file. Make sure the directory on the
     NFS server (where you want the backup data to be available), has the
     right mount options. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">/srv/nfs</em> *([...],rw,no_root_squash,[...])</pre></div><p>
     Replace <code class="filename">/srv/nfs</code> with the path to your
     backup data on the NFS server and adjust the mount options. You might
     need <code class="literal">no_root_squash</code> as the
     <code class="command">rear mkbackup</code> command runs as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     Adjust the various <code class="varname">BACKUP</code> parameters in the
     configuration file <code class="filename">/etc/rear/local.conf</code> to make
     ReaR store the file backup on the respective NFS server. Find
     examples in your installed system under
     <code class="filename">/usr/share/rear/conf/examples/SLE*-example.conf</code>.
    </p></li></ol></div></div><div class="complex-example"><div class="example" id="ex-ha-rear-config-EMC" data-id-title="Using Third-Party Backup Tools Like EMC NetWorker"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example 26.2: </span><span class="title-name">Using Third-Party Backup Tools Like EMC NetWorker </span></span><a title="Permalink" class="permalink" href="#ex-ha-rear-config-EMC">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="example-contents"><p>
    Using third-party backup tools instead of <code class="command">tar</code>
    requires appropriate settings in the ReaR configuration file.
   </p><p>
    The following is an example configuration for EMC NetWorker. Add this
    configuration snippet to <code class="filename">/etc/rear/local.conf</code> and
    adjust it according to your setup:
   </p><div class="verbatim-wrap"><pre class="screen">BACKUP=NSR
    OUTPUT=ISO
    BACKUP_URL=nfs://<em class="replaceable">host.example.com/path/to/rear/backup</em>
    OUTPUT_URL=nfs://<em class="replaceable">host.example.com/path/to/rear/backup</em>
    NSRSERVER=<em class="replaceable">backupserver.example.com</em>
    RETENTION_TIME="Month"</pre></div></div></div></div></section><section class="sect1" id="sec-ha-rear-mkbackup" data-id-title="Creating the Recovery Installation System"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.3 </span><span class="title-name">Creating the Recovery Installation System</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-mkbackup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After you have configured ReaR as described in
   <a class="xref" href="#sec-ha-rear-config" title="26.2. Setting Up ReaR and Your Backup Solution">Section 26.2</a>, create the
   recovery installation system (including the ReaR recovery installer)
   plus the file backup with the following command:
  </p><div class="verbatim-wrap"><pre class="screen">rear -d -D mkbackup</pre></div><p>
   It executes the following steps:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Analyzing the target system and gathering information, in particular
     about the disk layout (partitioning, file systems, mount points) and
     about the boot loader.
    </p></li><li class="listitem"><p>
     Creating a bootable recovery system with the information gathered in
     the first step. The resulting ReaR recovery installer is
     <span class="emphasis"><em>specific</em></span> to the system that you want to protect
     from disaster. It can only be used to re-create this specific system.
    </p></li><li class="listitem"><p>
     Calling the configured backup tool to back up system and user files.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-rear-testing" data-id-title="Testing the Recovery Process"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.4 </span><span class="title-name">Testing the Recovery Process</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-testing">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   After having created the recovery system, test the recovery process on a
   test machine with identical hardware. See also
   <a class="xref" href="#sec-ha-rear-concept-requirements" title="26.1.4. ReaR Requirements">Section 26.1.4, “ReaR Requirements”</a>. Make sure the test
   machine is correctly set up and can serve as a replacement for your main
   machine.
  </p><div id="id-1.4.5.10.6.3" data-id-title="Extensive Testing on Identical Hardware" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Extensive Testing on Identical Hardware</div><p>
    Thorough testing of the disaster recovery process on machines is
    required. Test the recovery procedure on a regular basis to ensure
    everything works as expected.
   </p></div><div class="procedure" id="pro-ha-rear-testing" data-id-title="Performing a Disaster Recovery on a Test Machine"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 26.1: </span><span class="title-name">Performing a Disaster Recovery on a Test Machine </span></span><a title="Permalink" class="permalink" href="#pro-ha-rear-testing">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a recovery medium by burning the recovery system that you have
     created in
     <a class="xref" href="#sec-ha-rear-mkbackup" title="26.3. Creating the Recovery Installation System">Section 26.3</a> to a
     DVD or CD. Alternatively, you can use a network boot via PXE.
    </p></li><li class="step"><p>
     Boot the test machine from the recovery medium.
    </p></li><li class="step"><p>
     From the menu, select <span class="guimenu">Recover</span>.
    </p></li><li class="step"><p>
     Log in as <code class="systemitem">root</code> (no password needed).
    </p></li><li class="step"><p>
     Enter the following command to start the recovery installer:
    </p><div class="verbatim-wrap"><pre class="screen">rear -d -D recover</pre></div><p>
     For details about the steps that ReaR takes during the process,
     see <a class="xref" href="#ol-ha-rear-recovers-steps" title="Recovery Process">Recovery Process</a>.
    </p></li><li class="step"><p>
     After the recovery process has finished, check whether the system has been
     successfully re-created and can serve as a replacement for your original
     system in the production environment.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-rear-recover" data-id-title="Recovering from Disaster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.5 </span><span class="title-name">Recovering from Disaster</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-recover">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In case a disaster has occurred, replace any broken hardware if
   necessary. Then proceed as described in
   <a class="xref" href="#pro-ha-rear-testing" title="Performing a Disaster Recovery on a Test Machine">Procedure 26.1</a>, using
   either the repaired machine (or a tested, identical machine that can
   serve as a replacement for your original system).
  </p><p>
   The <code class="command">rear recover</code> command will execute the following
   steps:
  </p><div class="orderedlist" id="ol-ha-rear-recovers-steps" data-id-title="Recovery Process"><div class="title-container"><div class="orderedlist-title-wrap"><div class="orderedlist-title"><span class="title-number-name"><span class="title-name">Recovery Process </span></span><a title="Permalink" class="permalink" href="#ol-ha-rear-recovers-steps">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div><ol class="orderedlist" type="1"><li class="listitem"><p>
     Restoring the disk layout (partitions, file systems, and mount points).
    </p></li><li class="listitem"><p>
     Restoring the system and user files from the backup.
    </p></li><li class="listitem"><p>
     Restoring the boot loader.
    </p></li></ol></div></section><section class="sect1" id="sec-ha-rear-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">26.6 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-rear-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_rear.xml" title="Edit source document"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="link" href="http://en.opensuse.org/SDB:Disaster_Recovery" target="_blank">http://en.opensuse.org/SDB:Disaster_Recovery</a>
    </p></li><li class="listitem"><p>
     <code class="command">rear</code> man page
    </p></li><li class="listitem"><p>
     <code class="filename">/usr/share/doc/packages/rear/</code>
    </p></li></ul></div></section></section></div><div class="part" id="part-maintenance" data-id-title="Maintenance and Upgrade"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part IV </span><span class="title-name">Maintenance and Upgrade </span></span><a title="Permalink" class="permalink" href="#part-maintenance">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ha-maintenance"><span class="title-number">27 </span><span class="title-name">Executing Maintenance Tasks</span></a></span></li><dd class="toc-abstract"><p>
    To perform maintenance tasks on the cluster nodes, you might need to stop
    the resources running on that node, to move them, or to shut down or reboot
    the node. It might also be necessary to temporarily take over the control of
    resources from the cluster, or even to stop the cluster service while resources
    remain running.
   </p><p>
    This chapter explains how to manually take down a cluster node without
    negative side-effects. It also gives an overview of different options the
    cluster stack provides for executing maintenance tasks.
   </p></dd><li><span class="chapter"><a href="#cha-ha-migration"><span class="title-number">28 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></a></span></li><dd class="toc-abstract"><p>
    This chapter covers two different scenarios: upgrading a cluster to
    another version of SUSE Linux Enterprise High Availability (either a major release or a service
    pack) as opposed to updating individual packages on cluster nodes. See
    <a class="xref" href="#sec-ha-migration-upgrade" title="28.2. Upgrading your Cluster to the Latest Product Version">Section 28.2, “Upgrading your Cluster to the Latest Product Version”</a> versus
    <a class="xref" href="#sec-ha-migration-update" title="28.3. Updating Software Packages on Cluster Nodes">Section 28.3, “Updating Software Packages on Cluster Nodes”</a>.
   </p><p>
    If you want to upgrade your cluster, check
    <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a> and
    <a class="xref" href="#sec-ha-migration-upgrade-require" title="28.2.2. Required Preparations Before Upgrading">Section 28.2.2, “Required Preparations Before Upgrading”</a> before starting
    to upgrade.
   </p></dd></ul></div><section class="chapter" id="cha-ha-maintenance" data-id-title="Executing Maintenance Tasks"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">27 </span><span class="title-name">Executing Maintenance Tasks</span></span> <a title="Permalink" class="permalink" href="#cha-ha-maintenance">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    To perform maintenance tasks on the cluster nodes, you might need to stop
    the resources running on that node, to move them, or to shut down or reboot
    the node. It might also be necessary to temporarily take over the control of
    resources from the cluster, or even to stop the cluster service while resources
    remain running.
   </p><p>
    This chapter explains how to manually take down a cluster node without
    negative side-effects. It also gives an overview of different options the
    cluster stack provides for executing maintenance tasks.
   </p></div></div></div></div><section class="sect1" id="sec-ha-maint-outline" data-id-title="Preparing and Finishing Maintenance Work"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.1 </span><span class="title-name">Preparing and Finishing Maintenance Work</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-outline">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Use the following commands to start, stop, or view the status of the cluster:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.3.3.3.1"><span class="term"><code class="command">crm cluster start [--all]</code></span></dt><dd><p>Start the cluster services on one node or all nodes</p></dd><dt id="id-1.4.6.3.3.3.2"><span class="term"><code class="command">crm cluster stop [--all]</code></span></dt><dd><p>Stop the cluster services on one node or all nodes</p></dd><dt id="id-1.4.6.3.3.3.3"><span class="term"><code class="command">crm cluster restart [--all]</code></span></dt><dd><p>Restart the cluster services on one node or all nodes</p></dd><dt id="id-1.4.6.3.3.3.4"><span class="term"><code class="command">crm cluster status</code></span></dt><dd><p>View the status of the cluster stack</p></dd></dl></div><p>
   Execute the above commands as user <code class="systemitem">root</code>, or as a user with the required privileges.
  </p><p>
   When you shut down or reboot a cluster node (or stop the cluster services on a
   node), the following processes will be triggered:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The resources that are running on the node will be stopped or moved off
     the node.
    </p></li><li class="listitem"><p>
     If stopping a resource fails or times out, the STONITH mechanism
     will fence the node and shut it down.
    </p></li></ul></div><div id="id-1.4.6.3.3.7" data-id-title="Risk of data loss" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Risk of data loss</div><p>
    If you need to do testing or maintenance work, follow the general steps
    below.
   </p><p>
    Otherwise, you risk unwanted side effects, like resources not starting in an
    orderly fashion, unsynchronized CIBs across the cluster nodes, or even data loss.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Before you start, choose the appropriate option from <a class="xref" href="#sec-ha-maint-overview" title="27.2. Different Options for Maintenance Tasks">Section 27.2, “Different Options for Maintenance Tasks”</a>.
    </p></li><li class="step"><p>
     Apply this option with Hawk2 or crmsh.
    </p></li><li class="step"><p>
     Execute your maintenance task or tests.
    </p></li><li class="step"><p>
     After you have finished, put the resource, node or cluster back to
     <span class="quote">“<span class="quote">normal</span>”</span> operation.
    </p></li></ol></div></div></div></section><section class="sect1" id="sec-ha-maint-overview" data-id-title="Different Options for Maintenance Tasks"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.2 </span><span class="title-name">Different Options for Maintenance Tasks</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-overview">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Pacemaker offers the following options for performing system maintenance:
  </p><div class="variablelist"><dl class="variablelist"><dt id="vle-ha-maint-mode-cluster"><span class="term"><a class="xref" href="#sec-ha-maint-mode-cluster" title="27.3. Putting the Cluster into Maintenance Mode">Putting the Cluster into Maintenance Mode</a></span></dt><dd><p>
      The global cluster property <code class="literal">maintenance-mode</code> puts all
      resources into maintenance state at once. The cluster stops monitoring them
      and becomes oblivious to their status. Note that only the resource management
      by Pacemaker is disabled. Corosync and SBD are still functional. Use maintenance
      mode for any tasks involving cluster resources. For any tasks involving
      infrastructure such as storage or networking, the safest method is to stop
      the cluster services completely. See <a class="xref" href="#sec-ha-maint-shutdown-node" title="27.6. Stopping the Cluster Services on a Node">Section 27.6, “Stopping the Cluster Services on a Node”</a>.
     </p></dd><dt id="vle-ha-maint-mode-node"><span class="term"><a class="xref" href="#sec-ha-maint-mode-node" title="27.4. Putting a Node into Maintenance Mode">Putting a Node into Maintenance Mode</a></span></dt><dd><p>
      This option allows you to put all resources running on a specific node into
      maintenance state at once. The cluster will cease monitoring them and thus
      become oblivious to their status.
     </p></dd><dt id="vle-ha-maint-node-standby"><span class="term"><a class="xref" href="#sec-ha-maint-node-standby" title="27.5. Putting a Node into Standby Mode">Putting a Node into Standby Mode</a></span></dt><dd><p>
      A node that is in standby mode can no longer run resources. Any resources
      running on the node will be moved away or stopped (if no other node
      is eligible to run the resource). Also, all monitoring operations will be
      stopped on the node (except for those with
      <code class="literal">role="Stopped"</code>).
     </p><p>
      You can use this option if you need to stop a node in a cluster while
      continuing to provide the services running on another node.
     </p></dd><dt id="vle-ha-maint-shutdown-node"><span class="term"><a class="xref" href="#sec-ha-maint-shutdown-node" title="27.6. Stopping the Cluster Services on a Node">Stopping the Cluster Services on a Node</a></span></dt><dd><p>
      This option stops all of the cluster services on a single node. Any resources
      running on the node will be moved away or stopped (if no other node is
      eligible to run the resource). If stopping a resource fails or times out,
      the node will be fenced.
     </p></dd><dt id="vle-ha-maint-mode-rsc"><span class="term"><a class="xref" href="#sec-ha-maint-mode-rsc" title="27.7. Putting a Resource into Maintenance Mode">Putting a Resource into Maintenance Mode</a></span></dt><dd><p>
      When this mode is enabled for a resource, no monitoring operations will be
      triggered for the resource.
     </p><p>
      Use this option if you need to manually touch the service that is managed
      by this resource and do not want the cluster to run any monitoring
      operations for the resource during that time.
     </p></dd><dt id="vle-ha-maint-rsc-unmanaged"><span class="term"><a class="xref" href="#sec-ha-maint-rsc-unmanaged" title="27.8. Putting a Resource into Unmanaged Mode">Putting a Resource into Unmanaged Mode</a></span></dt><dd><p>
      The <code class="option">is-managed</code> meta attribute allows you to temporarily
      <span class="quote">“<span class="quote">release</span>”</span> a resource from being managed by the cluster
      stack. This means you can manually touch the service that is managed by
      this resource (for example, to adjust any components). However, the
      cluster will continue to <span class="emphasis"><em>monitor</em></span> the resource and to
      report any failures.
     </p><p>
      If you want the cluster to also cease <span class="emphasis"><em>monitoring</em></span> the
      resource, use the per-resource maintenance mode instead (see
      <a class="xref" href="#vle-ha-maint-mode-rsc">Putting a Resource into Maintenance Mode</a>).
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-maint-mode-cluster" data-id-title="Putting the Cluster into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.3 </span><span class="title-name">Putting the Cluster into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-cluster">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.6.3.5.2" data-id-title="Maintenance mode only disables Pacemaker" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Maintenance mode only disables Pacemaker</div><p>
    When putting a cluster into maintenance mode, only the resource management
    by Pacemaker is disabled. Corosync and SBD are still functional. Depending
    on your maintenance tasks, this might lead to fence operations.
   </p><p>
    Use maintenance mode for any tasks involving cluster resources. For any tasks
    involving infrastructure such as storage or networking, the safest method is to stop
    the cluster services completely. See <a class="xref" href="#sec-ha-maint-shutdown-node" title="27.6. Stopping the Cluster Services on a Node">Section 27.6, “Stopping the Cluster Services on a Node”</a>.
   </p></div><p>
    To put the cluster into maintenance mode on the crm shell, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure property maintenance-mode=true</code></pre></div><p>
    To put the cluster back to normal mode after your maintenance work is done, use
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm configure property maintenance-mode=false</code></pre></div><div class="procedure" id="pro-ha-maint-mode-cluster-hawk2" data-id-title="Putting the Cluster into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.1: </span><span class="title-name">Putting the Cluster into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-cluster-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="5.4.2. Logging In">Section 5.4.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster
     Configuration</span>.
    </p></li><li class="step"><p>
     In the <span class="guimenu">CRM Configuration</span> group, select the
     <span class="guimenu">maintenance-mode</span> attribute from the empty drop-down box
     and click the plus icon to add it.
    </p></li><li class="step"><p>
     To set <code class="literal">maintenance-mode=true</code>, activate the check box
     next to <code class="literal">maintenance-mode</code> and confirm your changes.
    </p></li><li class="step"><p>
     After you have finished the maintenance task for the whole cluster,
     deactivate the check box next to the <code class="literal">maintenance-mode</code>
     attribute.
    </p><p>
     From this point on, High Availability will take over cluster management again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-mode-node" data-id-title="Putting a Node into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.4 </span><span class="title-name">Putting a Node into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a node into maintenance mode on the crm shell, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm node maintenance <em class="replaceable">NODENAME</em></code></pre></div><p>
    To put the node back to normal mode after your maintenance work is done, use
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm node ready <em class="replaceable">NODENAME</em></code></pre></div><div class="procedure" id="pro-ha-maint-mode-nodes-hawk2" data-id-title="Putting a Node into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.2: </span><span class="title-name">Putting a Node into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-nodes-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="5.4.2. Logging In">Section 5.4.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Status</span>.
    </p></li><li class="step"><p>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <span class="guimenu">Maintenance</span>.
    </p></li><li class="step"><p>
      After you have finished your maintenance task, click the wrench icon next to the node
     and select <span class="guimenu">Ready</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-node-standby" data-id-title="Putting a Node into Standby Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.5 </span><span class="title-name">Putting a Node into Standby Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-node-standby">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a node into standby mode on the crm shell, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm node standby <em class="replaceable">NODENAME</em></code></pre></div><p>
    To bring the node back online after your maintenance work is done, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm node online <em class="replaceable">NODENAME</em></code></pre></div><div class="procedure" id="pro-ha-maint-node-standby-hawk2" data-id-title="Putting a Node into Standby Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.3: </span><span class="title-name">Putting a Node into Standby Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-node-standby-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="5.4.2. Logging In">Section 5.4.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Cluster Status</span>.
    </p></li><li class="step"><p>
     In one of the individual nodes' views, click the wrench icon next to the
     node and select <span class="guimenu">Standby</span>.
    </p></li><li class="step"><p>
     Finish the maintenance task for the node.
    </p></li><li class="step"><p>
     To deactivate the standby mode, click the wrench icon next to the node
     and select <span class="guimenu">Ready</span>.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-shutdown-node" data-id-title="Stopping the Cluster Services on a Node"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.6 </span><span class="title-name">Stopping the Cluster Services on a Node</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-shutdown-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  To move the services off the node in an orderly fashion
  before shutting down or rebooting the node, proceed as follows:
 </p><div class="procedure" id="pro-ha-maint-shutdown-node" data-id-title="Manually rebooting a cluster node"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.4: </span><span class="title-name">Manually rebooting a cluster node </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-shutdown-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    On the node you want to reboot or shut down, log in as <code class="systemitem">root</code> or
    equivalent.
   </p></li><li class="step"><p>
    Put the node into <code class="literal">standby</code> mode:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm -w node standby</code></pre></div><p>
    That way, services can migrate off the node without being limited by the
    shutdown timeout of the cluster services.
   </p></li><li class="step"><p>
    Check the cluster status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm status</code></pre></div><p>
    It shows the respective node in <code class="literal">standby</code> mode:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
Node <em class="replaceable">bob</em>: standby
[...]</pre></div></li><li class="step"><p>
    Stop the cluster services on that node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster stop</code></pre></div></li><li class="step"><p>
    Reboot the node.
   </p></li></ol></div></div><p>
  To check if the node joins the cluster again:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Log in to the node as <code class="systemitem">root</code> or equivalent.
   </p></li><li class="step"><p>
    Check if the cluster services have started:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster status</code></pre></div><p>
    If not, start them:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster start</code></pre></div></li><li class="step"><p>
    Check the cluster status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm status</code></pre></div><p>
    It should show the node coming online again.
   </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-mode-rsc" data-id-title="Putting a Resource into Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.7 </span><span class="title-name">Putting a Resource into Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-mode-rsc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a resource into maintenance mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource maintenance <em class="replaceable">RESOURCE_ID</em> true</code></pre></div><p>
    To put the resource back into normal mode after your maintenance work is done, use
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource maintenance <em class="replaceable">RESOURCE_ID</em> false</code></pre></div><div class="procedure" id="pro-ha-maint-mode-rsc-hawk2" data-id-title="Putting a Resource into Maintenance Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.5: </span><span class="title-name">Putting a Resource into Maintenance Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-mode-rsc-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="5.4.2. Logging In">Section 5.4.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     In the left navigation bar, select <span class="guimenu">Resources</span>.
    </p></li><li class="step"><p>
     Select the resource you want to put in maintenance mode or unmanaged mode,
     click the wrench icon next to the resource and select <span class="guimenu">Edit
     Resource</span>.
    </p></li><li class="step"><p>
     Open the <span class="guimenu">Meta Attributes</span> category.
    </p></li><li class="step"><p>
     From the empty drop-down list, select the <span class="guimenu">maintenance</span>
     attribute and click the plus icon to add it.
    </p></li><li class="step"><p>
     Activate the check box next to <code class="literal">maintenance</code> to set the
     maintenance attribute to <code class="literal">yes</code>.
    </p></li><li class="step"><p>
     Confirm your changes.
    </p></li><li class="step"><p>
     After you have finished the maintenance task for that resource, deactivate
     the check box next to the <code class="literal">maintenance</code> attribute for
     that resource.
    </p><p>
     From this point on, the resource will be managed by the High Availability software
     again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-rsc-unmanaged" data-id-title="Putting a Resource into Unmanaged Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.8 </span><span class="title-name">Putting a Resource into Unmanaged Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-rsc-unmanaged">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    To put a resource into unmanaged mode on the crm shell, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource unmanage <em class="replaceable">RESOURCE_ID</em></code></pre></div><p>
    To put it into managed mode again after your maintenance work is done, use the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm resource manage <em class="replaceable">RESOURCE_ID</em></code></pre></div><div class="procedure" id="pro-ha-maint-rsc-unmanaged-hawk2" data-id-title="Putting a Resource into Unmanaged Mode with Hawk2"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.6: </span><span class="title-name">Putting a Resource into Unmanaged Mode with Hawk2 </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-rsc-unmanaged-hawk2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a Web browser and log in to the cluster as described in
     <a class="xref" href="#sec-conf-hawk2-login" title="5.4.2. Logging In">Section 5.4.2, “Logging In”</a>.
    </p></li><li class="step"><p>
     From the left navigation bar, select <span class="guimenu">Status</span> and go to
     the <span class="guimenu">Resources</span> list.
    </p></li><li class="step"><p>
     In the <span class="guimenu">Operations</span> column, click the arrow down icon
     next to the resource you want to modify and select
     <span class="guimenu">Edit</span>.
    </p><p>
     The resource configuration screen opens.
    </p></li><li class="step"><p>
     Below <span class="guimenu">Meta Attributes</span>, select the
     <span class="guimenu">is-managed</span> entry from the empty drop-down box.
    </p></li><li class="step"><p>
     Set its value to <code class="literal">No</code> and click <span class="guimenu">Apply</span>.
    </p></li><li class="step"><p>
     After you have finished your maintenance task, set
     <span class="guimenu">is-managed</span> to <code class="literal">Yes</code> (which is the
     default value) and apply your changes.
    </p><p>
     From this point on, the resource will be managed by the High Availability software
     again.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-maint-shutdown-node-maint-mode" data-id-title="Rebooting a Cluster Node While in Maintenance Mode"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">27.9 </span><span class="title-name">Rebooting a Cluster Node While in Maintenance Mode</span></span> <a title="Permalink" class="permalink" href="#sec-ha-maint-shutdown-node-maint-mode">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.6.3.11.2" data-id-title="Implications" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Implications</div><p>
       If the cluster or a node is in maintenance mode, you can use tools external
       to the cluster stack (for example, <code class="command">systemctl</code>) to manually
       operate the components that are managed by the cluster as resources.
       The High Availability software will not monitor them or attempt to restart them.
      </p><p>
       If you stop the cluster services on a node, all daemons and processes
       (originally started as Pacemaker-managed cluster resources) will continue
       to run.
      </p><p>
       If you attempt to start cluster services on a node while the cluster or
       node is in maintenance mode, Pacemaker will initiate a single one-shot monitor
       operation (a <span class="quote">“<span class="quote">probe</span>”</span>) for every resource to evaluate which
       resources are currently running on that node. However, it will take no
       further action other than determining the resources' status.
      </p></div><div class="procedure" id="pro-ha-maint-reboot-node" data-id-title="Rebooting a cluster node while the cluster or node is in maintenance mode"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 27.7: </span><span class="title-name">Rebooting a cluster node while the cluster or node is in maintenance mode </span></span><a title="Permalink" class="permalink" href="#pro-ha-maint-reboot-node">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_maintenance.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     On the node you want to reboot or shut down, log in as <code class="systemitem">root</code> or
     equivalent.
    </p></li><li class="step"><p>
     If you have a DLM resource (or other resources depending on DLM), make
     sure to explicitly stop those resources before stopping the cluster services:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)resource# </code><code class="command">stop <em class="replaceable">RESOURCE_ID</em></code></pre></div><p>
     The reason is that stopping Pacemaker also stops the Corosync service on
     whose membership and messaging services DLM depends. If Corosync stops,
     the DLM resource will assume a split brain scenario and trigger a fencing
     operation.
    </p></li><li class="step"><p>
     Stop the cluster services on that node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster stop</code></pre></div></li><li class="step"><p>
     Shut down or reboot the node.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-ha-migration" data-id-title="Upgrading Your Cluster and Updating Software Packages"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">28 </span><span class="title-name">Upgrading Your Cluster and Updating Software Packages</span></span> <a title="Permalink" class="permalink" href="#cha-ha-migration">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    This chapter covers two different scenarios: upgrading a cluster to
    another version of SUSE Linux Enterprise High Availability (either a major release or a service
    pack) as opposed to updating individual packages on cluster nodes. See
    <a class="xref" href="#sec-ha-migration-upgrade" title="28.2. Upgrading your Cluster to the Latest Product Version">Section 28.2, “Upgrading your Cluster to the Latest Product Version”</a> versus
    <a class="xref" href="#sec-ha-migration-update" title="28.3. Updating Software Packages on Cluster Nodes">Section 28.3, “Updating Software Packages on Cluster Nodes”</a>.
   </p><p>
    If you want to upgrade your cluster, check
    <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a> and
    <a class="xref" href="#sec-ha-migration-upgrade-require" title="28.2.2. Required Preparations Before Upgrading">Section 28.2.2, “Required Preparations Before Upgrading”</a> before starting
    to upgrade.
   </p></div></div></div></div><section class="sect1" id="sec-ha-migration-terminology" data-id-title="Terminology"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">28.1 </span><span class="title-name">Terminology</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-terminology">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the following, find definitions of the most important terms used in
   this chapter:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.4.3.3.1"><span class="term">Major Release, </span><span class="term">General Availability (GA) Version</span></dt><dd><p>
      A major release is a new product version that brings new features and
      tools, and decommissions previously deprecated components. It comes with
      backward incompatible changes.
     </p></dd><dt id="vle-upgrade-offline"><span class="term">Cluster Offline Upgrade</span></dt><dd><p>
      If a new product version includes major changes that are backward
      incompatible, the cluster needs to be upgraded by a cluster offline
      upgrade. You need to take all nodes offline and upgrade the cluster as a whole,
      before you can bring all nodes back online.
     </p></dd><dt id="vle-upgrade-rolling"><span class="term">Cluster Rolling Upgrade</span></dt><dd><p>
      In a cluster rolling upgrade one cluster node at a time is upgraded while the
      rest of the cluster is still running. You take the first node offline,
      upgrade it and bring it back online to join the cluster. Then you
      continue one by one until all cluster nodes are upgraded to a major
      version.
     </p></dd><dt id="id-1.4.6.4.3.3.4"><span class="term">Service Pack (SP)</span></dt><dd><p>
      Combines several patches into a form that is easy to install or
      deploy. Service packs are numbered and usually contain security fixes,
      updates, upgrades, or enhancements of programs.
     </p></dd><dt id="id-1.4.6.4.3.3.5"><span class="term">Update</span></dt><dd><p>
      Installation of a newer <span class="emphasis"><em>minor</em></span> version of a
      package, which usually contains security fixes and other important fixes.
     </p></dd><dt id="id-1.4.6.4.3.3.6"><span class="term">Upgrade</span></dt><dd><p>
      Installation of a newer <span class="emphasis"><em>major</em></span> version of a
      package or distribution, which brings <span class="emphasis"><em>new
      features</em></span>. See also <a class="xref" href="#vle-upgrade-offline">Cluster Offline Upgrade</a>
      versus <a class="xref" href="#vle-upgrade-rolling">Cluster Rolling Upgrade</a>.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-migration-upgrade" data-id-title="Upgrading your Cluster to the Latest Product Version"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">28.2 </span><span class="title-name">Upgrading your Cluster to the Latest Product Version</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Which upgrade path is supported, and how to perform the upgrade, depends
   on the current product version as well as on the target version you want
   to migrate to.
  </p><p>
   SUSE Linux Enterprise High Availability has the same supported upgrade paths as the underlying base system. For a complete
   overview, see the section <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-paths.html#sec-upgrade-paths-supported" target="_blank"><em class="citetitle">Supported Upgrade Paths to SUSE Linux Enterprise Server 15 SP2</em></a> in the
   SUSE Linux Enterprise Server Upgrade Guide.
  </p><p>
   In addition, the following rules apply, as the High Availability cluster stack offers two methods for
   upgrading the cluster:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title"><a class="xref" href="#vle-upgrade-rolling">Cluster Rolling Upgrade</a>. </span>A cluster rolling upgrade is only supported within the same major release
      (from one service pack to the next, or from the GA version of a product to SP1).</p></li><li class="listitem"><p><span class="formalpara-title"><a class="xref" href="#vle-upgrade-offline">Cluster Offline Upgrade</a>. </span>A cluster offline upgrade is required to upgrade from one major release to
      the next (for example, from SLE HA 12 to
      SLE HA 15) or from a service pack within one major
      release to the next major release (for example, from
      SLE HA 12 SP3 to SLE HA 15).
     </p></li></ul></div><p>
   <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1</a>
   list the supported upgrade paths and methods for SLE HA (Geo),
   moving from one version to the next. The column <em class="citetitle">For Details</em> lists
   the specific upgrade documentation you should refer to (including also the base
   system and Geo Clustering for SUSE Linux Enterprise High Availability). This documentation is available from:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <a class="link" href="https://documentation.suse.com/sles" target="_blank">https://documentation.suse.com/sles</a>
     </p></li><li class="listitem"><p>
      <a class="link" href="https://documentation.suse.com/sle-ha" target="_blank">https://documentation.suse.com/sle-ha</a>
     </p></li></ul></div><div id="id-1.4.6.4.4.8" data-id-title="No Support for Mixed Clusters and Reversion After Upgrade" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: No Support for Mixed Clusters and Reversion After Upgrade</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Mixed clusters running on SUSE Linux Enterprise High Availability 12/SUSE Linux Enterprise High Availability 15 are
      <span class="emphasis"><em>not</em></span> supported.
     </p></li><li class="listitem"><p>
      After the upgrade process to product version 15, reverting back to
      product version 12 is <span class="emphasis"><em>not</em></span> supported.
     </p></li></ul></div></div><section class="sect2" id="sec-ha-migration-upgrade-oview" data-id-title="Supported Upgrade Paths for SLE HA and SLE HA Geo"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">28.2.1 </span><span class="title-name">Supported Upgrade Paths for SLE HA and SLE HA Geo</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-oview">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="1"/><col class="2"/><col class="3"/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Upgrade From ... To
        </p>
       </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Upgrade Path
        </p>
       </th><th style="border-bottom: 1px solid ; ">
        <p>
         For Details
        </p>
       </th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA 11 SP3 to
        </p>
       <p>SLE HA (Geo) 12</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Offline Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12, part
           <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Quick Start for
           SLE HA 12, section <em class="citetitle">Upgrading from
             SLE HA (Geo) 11 SP3 to
             SLE HA Geo 12</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA (Geo) 11 SP4 to
         SLE HA (Geo) 12 SP1
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Offline Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP1,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Quick Start for
           SLE HA 12 SP1,
           section <em class="citetitle">Upgrading to the Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         SLE HA (Geo) 12 to
         SLE HA (Geo) 12 SP1
        </p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>
         Cluster Rolling Upgrade
        </p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP1,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Quick Start for
           SLE HA 12 SP1, section <em class="citetitle">Upgrading to the
           Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP1 to
         SLE HA (Geo) 12 SP2</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP2,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Quick Start for
           SLE HA 12 SP2, section <em class="citetitle">Upgrading to the
           Latest Product Version</em>
          </p></li><li class="listitem"><p>DRBD 8 to DRBD 9: <a class="xref" href="#sec-ha-drbd-migrate" title="22.4. Migrating from DRBD 8 to DRBD 9">Migrating from DRBD 8 to DRBD 9</a></p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP2 to
         SLE HA (Geo) 12 SP3</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP3,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Guide for
           SLE HA 12 SP3, section <em class="citetitle">Upgrading to the
           Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP3 to
         SLE HA (Geo) 12 SP4</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP4,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: Geo Clustering Guide for
           SLE HA 12 SP4, section <em class="citetitle">Upgrading to the
           Latest Product Version</em>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP3 to
         SLE HA 15</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Offline Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Upgrade Guide for SLES 15
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Upgrading from Product Version 12 to 15: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li><li class="listitem"><p>Cluster LVM: <a class="xref" href="#sec-ha-clvm-migrate" title="23.4. Online Migration from Mirror LV to Cluster MD">Online Migration from Mirror LV to Cluster MD</a></p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP4 to
         SLE HA (Geo) 12 SP5</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Deployment Guide for SLES 12 SP5,
           part <em class="citetitle">Updating and Upgrading SUSE Linux Enterprise</em>
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP4 to
         SLE HA 15 SP1</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Offline Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Upgrade Guide for SLES 15 SP1
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Upgrading from Product Version 12 to 15: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li><li class="listitem"><p>Cluster LVM: <a class="xref" href="#sec-ha-clvm-migrate" title="23.4. Online Migration from Mirror LV to Cluster MD">Online Migration from Mirror LV to Cluster MD</a></p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA (Geo) 12 SP5 to
         SLE HA 15 SP2</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Offline Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Upgrade Guide for SLES 15 SP2
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Upgrading from Product Version 12 to 15: Cluster Offline Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li><li class="listitem"><p>Cluster LVM: <a class="xref" href="#sec-ha-clvm-migrate" title="23.4. Online Migration from Mirror LV to Cluster MD">Online Migration from Mirror LV to Cluster MD</a></p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>SLE HA 15 to
         SLE HA 15 SP1</p>
       </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td style="border-bottom: 1px solid ; ">
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Upgrade Guide for
           SLES 15 SP1
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li></ul></div>
       </td></tr><tr><td style="border-right: 1px solid ; ">
        <p>SLE HA 15 SP1 to
         SLE HA 15 SP2</p>
       </td><td style="border-right: 1px solid ; ">
        <p>Cluster Rolling Upgrade</p>
       </td><td>
        <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           Base System: Upgrade Guide for
           SLES 15 SP2
          </p></li><li class="listitem"><p>
           SLE HA:
           <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Performing a Cluster Rolling Upgrade</a>
          </p></li><li class="listitem"><p>
           SLE HA Geo: <span class="intraxref">Book “Geo Clustering Guide”, Chapter 10 “Upgrading to the Latest Product Version”</span>
          </p></li></ul></div>
       </td></tr></tbody></table></div><div id="id-1.4.6.4.4.9.3" data-id-title="Skipping Service Packs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Skipping Service Packs</div><p>
     The easiest upgrade path is consecutively installing all service packs.
     For the SUSE Linux Enterprise 15 product line (GA and the subsequent service packs) it is
     also supported to skip one service pack when upgrading. For example, upgrading from
     SLE HA 15 GA to 15 SP2 or from SLE HA 15 SP1
     to 15 SP3 is supported.
    </p></div></section><section class="sect2" id="sec-ha-migration-upgrade-require" data-id-title="Required Preparations Before Upgrading"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">28.2.2 </span><span class="title-name">Required Preparations Before Upgrading</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-require">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.4.4.10.2.1"><span class="term">Backup</span></dt><dd><p>
       Ensure that your system backup is up to date and restorable.
      </p></dd><dt id="id-1.4.6.4.4.10.2.2"><span class="term">Testing</span></dt><dd><p>
       Test the upgrade procedure on a staging instance of your cluster
       setup first, before performing it in a production environment.
       This gives you an estimation of the time frame required for the
       maintenance window. It also helps to detect and solve any unexpected
       problems that might arise.
      </p></dd></dl></div></section><section class="sect2" id="sec-ha-migration-upgrade-offline" data-id-title="Cluster Offline Upgrade"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">28.2.3 </span><span class="title-name">Cluster Offline Upgrade</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-offline">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section applies to the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Upgrading from SLE HA 11 SP3 to
      SLE HA 12—for details see
      <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Procedure 28.1, “Upgrading from Product Version 11 to 12: Cluster Offline Upgrade”</a>.</p></li><li class="listitem"><p>
      Upgrading from SLE HA 11 SP4 to
      SLE HA 12 SP1—for details see
      <a class="xref" href="#pro-ha-migration-offline" title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade">Procedure 28.1, “Upgrading from Product Version 11 to 12: Cluster Offline Upgrade”</a>.
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP3 to
      SLE HA 15—for details see
      <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Procedure 28.2, “Upgrading from Product Version 12 to 15: Cluster Offline Upgrade”</a>.
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP4 to
      SLE HA 15 SP1—for details see
      <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Procedure 28.2, “Upgrading from Product Version 12 to 15: Cluster Offline Upgrade”</a>.
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP5 to
      SLE HA 15 SP2—for details see
      <a class="xref" href="#pro-ha-migration-offline-12-15" title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade">Procedure 28.2, “Upgrading from Product Version 12 to 15: Cluster Offline Upgrade”</a>.
     </p></li></ul></div><p>
    If your cluster is still based on an older product version than the ones
    listed above, first upgrade it to a version of SLES and SLE HA
    that can be used as a source for upgrading to the desired target
    version.
   </p><div class="procedure" id="pro-ha-migration-offline" data-id-title="Upgrading from Product Version 11 to 12: Cluster Offline Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 28.1: </span><span class="title-name">Upgrading from Product Version 11 to 12: Cluster Offline Upgrade </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-offline">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
     SUSE Linux Enterprise High Availability 12 cluster stack comes with major changes in various
     components (for example, <code class="filename">/etc/corosync/corosync.conf</code>, disk formats of OCFS2).
     Therefore, a <code class="literal">cluster rolling upgrade</code> from any SUSE Linux Enterprise High Availability
     11 version is not supported. Instead, all cluster nodes must be offline
     and the cluster needs to be upgraded as a whole as described below.
    </p><ol class="procedure" type="1"><li class="step"><p>
      Log in to each cluster node and stop the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">rcopenais</code> stop</pre></div></li><li class="step"><p>
      For each cluster node, perform an upgrade to the desired target
      version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability—see <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a>.
     </p></li><li class="step"><p>
      After the upgrade process has finished, reboot each node with the
      upgraded version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability.
     </p></li><li class="step"><p>
      If you use OCFS2 in your cluster setup, update the on-device structure
      by executing the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">o2cluster</code> --update <em class="replaceable">PATH_TO_DEVICE</em></pre></div><p>
      It adds additional parameters to the disk. They are needed for the
      updated OCFS2 version that is shipped with SUSE Linux Enterprise High Availability 12 and
      12 SPx.
     </p></li><li class="step"><p>
      To update <code class="filename">/etc/corosync/corosync.conf</code> for Corosync version 2:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Log in to one node and start the YaST cluster module.
       </p></li><li class="step"><p>
        Switch to the <span class="guimenu">Communication Channels</span> category and
        enter values for the following new parameters: <span class="guimenu">Cluster
        Name</span> and <span class="guimenu">Expected Votes</span>. For details,
        see <a class="xref" href="#pro-ha-installation-setup-channel1-udp" title="Defining the First Communication Channel (Multicast)">Procedure 4.1, “Defining the First Communication Channel (Multicast)”</a>
        or <a class="xref" href="#pro-ha-installation-setup-channel1-udpu" title="Defining the First Communication Channel (Unicast)">Procedure 4.2, “Defining the First Communication Channel (Unicast)”</a>,
        respectively.
       </p><p>
        If YaST should detect any other options that are invalid or
        missing according to Corosync version 2, it will prompt you to
        change them.
       </p></li><li class="step"><p>
        Confirm your changes in YaST. YaST will write them to
        <code class="filename">/etc/corosync/corosync.conf</code>.
       </p></li><li class="step"><p>
        If Csync2 is configured for your cluster, use the following command
        to push the updated Corosync configuration to the other cluster
        nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">csync2</code> <code class="option">-xv</code></pre></div><p>
        For details on Csync2, see
        <a class="xref" href="#sec-ha-installation-setup-csync2" title="4.7. Transferring the configuration to all nodes">Section 4.7, “Transferring the configuration to all nodes”</a>.
       </p><p>
        Alternatively, synchronize the updated Corosync configuration by
        manually copying <code class="filename">/etc/corosync/corosync.conf</code> to all cluster nodes.
       </p></li></ol></li><li class="step"><p>
      Log in to each node and start the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      Check the cluster status with <code class="command">crm status</code> or with
      Hawk2.
     </p></li><li class="step"><p>Configure the following services to start at boot time:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl enable pacemaker
<code class="prompt root"># </code>systemctl enable hawk
<code class="prompt root"># </code>systemctl enable sbd</pre></div></li></ol></div></div><div id="note-ha-cib-upgrade" data-id-title="Upgrading the CIB Syntax Version" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Upgrading the CIB Syntax Version</div><p>
     Sometimes new features are only available with the latest CIB syntax version.
     When you upgrade to a new product version, your CIB syntax version
     will <span class="emphasis"><em>not</em></span> be upgraded by default.
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Check your version with:</p><div class="verbatim-wrap"><pre class="screen">cibadmin -Q | grep validate-with</pre></div></li><li class="step"><p>Upgrade to the latest CIB syntax version with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">cibadmin</code> --upgrade --force</pre></div></li></ol></div></div></div><div class="procedure" id="pro-ha-migration-offline-12-15" data-id-title="Upgrading from Product Version 12 to 15: Cluster Offline Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 28.2: </span><span class="title-name">Upgrading from Product Version 12 to 15: Cluster Offline Upgrade </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-offline-12-15">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><div id="id-1.4.6.4.4.11.7.2" data-id-title="Installation from Scratch" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Installation from Scratch</div><p>If you decide to install the cluster nodes from scratch (instead
      of upgrading them), see <a class="xref" href="#sec-ha-requirements-sw" title="2.2. Software Requirements">Section 2.2, “Software Requirements”</a> for the
      list of modules required for SUSE Linux Enterprise High Availability 15 SP2. Find more
      information about modules, extensions and related products in the release
      notes for SUSE Linux Enterprise Server 15. They are available at <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
     </p></div><ol class="procedure" type="1"><li class="step"><p>
      Before starting the offline upgrade to SUSE Linux Enterprise High Availability 15, manually
      upgrade the CIB syntax in your current cluster as described in
      <a class="xref" href="#note-ha-cib-upgrade" title="Note: Upgrading the CIB Syntax Version">Note: Upgrading the CIB Syntax Version</a>.
     </p></li><li class="step"><p>
      Log in to each cluster node and stop the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="step"><p>
      For each cluster node, perform an upgrade to the desired target
      version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability—see <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a>.
     </p></li><li class="step"><p>
      After the upgrade process has finished, log in to each node and boot it with the
      upgraded version of SUSE Linux Enterprise Server and SUSE Linux Enterprise High Availability.
     </p></li><li class="step"><p>If you use Cluster LVM, you need to migrate from clvmd to lvmlockd.
      See the man page of <code class="command">lvmlockd</code>, section
      <em class="citetitle">changing a clvm VG to a lockd VG</em> and <a class="xref" href="#sec-ha-clvm-migrate" title="23.4. Online Migration from Mirror LV to Cluster MD">Section 23.4, “Online Migration from Mirror LV to Cluster MD”</a>.
     </p></li><li class="step"><p>
      Start the cluster stack with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      Check the cluster status with <code class="command">crm status</code> or with
      Hawk2.
     </p></li></ol></div></div></section><section class="sect2" id="sec-ha-migration-upgrade-rolling" data-id-title="Cluster Rolling Upgrade"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">28.2.4 </span><span class="title-name">Cluster Rolling Upgrade</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-upgrade-rolling">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This section applies to the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Upgrading from SLE HA 12 to SLE HA 12 SP1
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP1 to
      SLE HA 12 SP2
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP2 to
      SLE HA 12 SP3
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP3 to
      SLE HA 12 SP4
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 12 SP4 to
      SLE HA 12 SP5
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 15 to
      SLE HA 15 SP1
     </p></li><li class="listitem"><p>
      Upgrading from SLE HA 15 SP1 to
      SLE HA 15 SP2
     </p></li></ul></div><p>Use one of the following procedures for your scenario:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      For a more general rolling upgrade, refer to <a class="xref" href="#pro-ha-migration-rolling-upgrade" title="Performing a Cluster Rolling Upgrade">Procedure 28.3</a>.
     </p></li><li class="listitem"><p>
      For a specific rolling upgrade, refer to <a class="xref" href="#pro-ha-migration-rolling-upgrade-newsp-freshinstall" title="Performing a Cluster-wide Fresh Installation of a New Service Pack">Procedure 28.4</a>.
     </p></li></ul></div><div id="id-1.4.6.4.4.12.6" data-id-title="Active Cluster Stack" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Active Cluster Stack</div><p>
     Before starting an upgrade for a node, <span class="emphasis"><em>stop</em></span> the
     cluster stack <span class="emphasis"><em>on that node</em></span>.
    </p><p>
     If the cluster resource manager on a node is active during the software
     update, this can lead to results such as fencing of active
     nodes.
    </p></div><div id="id-1.4.6.4.4.12.7" data-id-title="Time Limit for Cluster Rolling Upgrade" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Time Limit for Cluster Rolling Upgrade</div><p>
     The new features shipped with the latest product version will only be
     available after <span class="emphasis"><em>all</em></span> cluster nodes have been
     upgraded to the latest product version. Mixed version clusters are only
     supported for a short time frame during the cluster rolling upgrade. Complete
     the cluster rolling upgrade within one week.
    </p><p>
     Once all the online nodes are running the upgraded version, it is not
     possible for any other nodes with the old version to (re-)join without
     having been upgraded.
    </p></div><div class="procedure" id="pro-ha-migration-rolling-upgrade" data-id-title="Performing a Cluster Rolling Upgrade"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 28.3: </span><span class="title-name">Performing a Cluster Rolling Upgrade </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-rolling-upgrade">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Log in as <code class="systemitem">root</code> on the node that you want to upgrade and stop the
      cluster stack:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="step" id="step-ha-migration-upgrade-tolatest"><p>
      Perform an upgrade to the desired target version of SUSE Linux Enterprise Server and
      SUSE Linux Enterprise High Availability. To find the details for the individual upgrade
      processes, see
      <a class="xref" href="#sec-ha-migration-upgrade-oview" title="28.2.1. Supported Upgrade Paths for SLE HA and SLE HA Geo">Section 28.2.1, “Supported Upgrade Paths for SLE HA and SLE HA Geo”</a>.
     </p></li><li class="step" id="step-ha-migration-upgrade-ais"><p>
      Start the cluster stack on the upgraded node to make the node rejoin
      the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>
      Take the next node offline and repeat the procedure for that node.
     </p></li><li class="step"><p>
      Check the cluster status with <code class="command">crm status</code> or with
      Hawk2.
     </p><p>
      The Hawk2 <span class="guimenu">Status</span> screen also shows a warning if
      different CRM versions are detected for your cluster nodes.
     </p></li></ol></div></div><div id="id-1.4.6.4.4.12.9" data-id-title="Time Limit for Rolling Upgrade" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Time Limit for Rolling Upgrade</div><p>
     The new features shipped with the latest product version will only be
     available after <span class="emphasis"><em>all</em></span> cluster nodes have been
     upgraded to the latest product version. Mixed version clusters are only
     supported for a short time frame during the rolling upgrade. Complete
     the rolling upgrade within one week.
    </p></div><p>The Hawk2 <span class="guimenu">Status</span> screen also shows a warning if
    different CRM versions are detected for your cluster nodes.</p><p>Beside an in-place upgrade, many customers prefer a fresh installation
    even for moving to the next service pack. The following procedure shows a
    scenario where a two-node cluster with the nodes alice and bob is
    upgraded to the next service pack (SP):
   </p><div class="procedure" id="pro-ha-migration-rolling-upgrade-newsp-freshinstall" data-id-title="Performing a Cluster-wide Fresh Installation of a New Service Pack"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 28.4: </span><span class="title-name">Performing a Cluster-wide Fresh Installation of a New Service Pack </span></span><a title="Permalink" class="permalink" href="#pro-ha-migration-rolling-upgrade-newsp-freshinstall">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="st-ha-migration-rolling-up-sp-backup"><p>Make a backup of your cluster configuration. A minimum set of files
      are shown in the following list:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/corosync/corosync.conf
/etc/corosync/authkey
/etc/sysconfig/sbd
/etc/modules-load.d/watchdog.conf
/etc/hosts
/etc/ntp.conf</pre></div><p>Depending on your resources, you may also need the following files:</p><div class="verbatim-wrap"><pre class="screen">/etc/services
/etc/passwd
/etc/shadow
/etc/groups
/etc/drbd/*
/etc/lvm/lvm.conf
/etc/mdadm.conf
/etc/mdadm.SID.conf</pre></div></li><li class="step" id="st-ha-migration-rolling-alice"><p>Start with node alice.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Put the node into standby node. That way, resources can move off the node:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> --wait node standby alice reboot</pre></div><p>
        With the option <code class="option">--wait</code>, the command returns only when
        the cluster finishes the transition and becomes idle.
        The <code class="option">reboot</code> option has the effect that the node will
        be already out of standby mode when it is online again.
        Despite its name, the <code class="option">reboot</code> option works as long
        as the node goes offline and online.
       </p></li><li class="step"><p>Stop the cluster services on node alice:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="step"><p>
        At this point, alice does not have running resources anymore.
        Upgrade the node alice and reboot it afterward.
        Cluster services are assumed not to start on boot.
       </p></li><li class="step"><p>
        Copy your backup files from <a class="xref" href="#st-ha-migration-rolling-up-sp-backup" title="Step 1">Step 1</a>
        to the original places.
       </p></li><li class="step"><p>Bring back node alice into cluster:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="step"><p>Check that resources are fine.</p></li></ol></li><li class="step"><p>Repeat <a class="xref" href="#st-ha-migration-rolling-alice" title="Step 2">Step 2</a> for node
      bob.
     </p></li></ol></div></div></section></section><section class="sect1" id="sec-ha-migration-update" data-id-title="Updating Software Packages on Cluster Nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">28.3 </span><span class="title-name">Updating Software Packages on Cluster Nodes</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-update">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.4.6.4.5.2" data-id-title="Active Cluster Stack" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Active Cluster Stack</div><p>
    Before starting a package update for a node, either <span class="emphasis"><em>stop</em></span>
    the cluster stack <span class="emphasis"><em>on that node</em></span> or put the
    <span class="emphasis"><em>node into maintenance mode</em></span>, depending on whether the
    cluster stack is affected or not. See <a class="xref" href="#step-update-check" title="Step 1">Step 1</a>
    for details.
   </p><p>
    If the cluster resource manager on a node is active during the software
    update, this can lead to results such as fencing of active
    nodes.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step" id="step-update-check"><p>
     Before installing any package updates on a node, check the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Does the update affect any packages belonging to SUSE Linux Enterprise High Availability?
       If <code class="literal">yes</code>: Stop the cluster stack on
       the node before starting the software update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="listitem"><p>
       Does the package update require a reboot? If <code class="literal">yes</code>:
       Stop the cluster stack on the node before starting the software
       update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster stop</pre></div></li><li class="listitem"><p>
       If none of the situations above apply, you do not need to stop the
       cluster stack. In that case, put the node into maintenance mode
       before starting the software update:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node maintenance <em class="replaceable">NODE_NAME</em></pre></div><p>
       For more details on maintenance mode, see
       <a class="xref" href="#sec-ha-maint-overview" title="27.2. Different Options for Maintenance Tasks">Section 27.2, “Different Options for Maintenance Tasks”</a>.
      </p></li></ul></div></li><li class="step"><p>
     Install the package update using either YaST or Zypper.
    </p></li><li class="step"><p>
     After the update has been successfully installed:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Either start the cluster stack on the respective node (if you
       stopped it in <a class="xref" href="#step-update-check" title="Step 1">Step 1</a>):
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></li><li class="listitem"><p>
       or remove the maintenance flag to bring the node back to
       normal mode:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> node ready <em class="replaceable">NODE_NAME</em></pre></div></li></ul></div></li><li class="step"><p>
     Check the cluster status with <code class="command">crm status</code> or with
     Hawk2.
    </p></li></ol></div></div></section><section class="sect1" id="sec-ha-migration-more" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">28.4 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-migration-more">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_migration.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For detailed information about any changes and new features of the
   product you are upgrading to, refer to its release notes. They are
   available from <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section></section></div><div class="part" id="part-appendix" data-id-title="Appendix"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">Part V </span><span class="title-name">Appendix </span></span><a title="Permalink" class="permalink" href="#part-appendix">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/book_sle_ha_guide.xml" title="Edit source document"> </a></div></div></div></div></div><div class="toc"><ul><li><span class="appendix"><a href="#app-ha-troubleshooting"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></a></span></li><dd class="toc-abstract"><p>
    Strange problems may occur that are not easy to understand, especially
    when starting to experiment with High Availability. However, there are several
    utilities that allow you to take a closer look at the High Availability internal
    processes. This chapter recommends various solutions.
   </p></dd><li><span class="appendix"><a href="#app-naming"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></a></span></li><dd class="toc-abstract"><p>
  This guide uses the following naming conventions for cluster nodes and
  names, cluster resources, and constraints.
 </p></dd><li><span class="appendix"><a href="#app-ha-management"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></a></span></li><dd class="toc-abstract"><p>SUSE Linux Enterprise High Availability ships with a comprehensive set of tools to assists you in managing your cluster from the command line. This chapter introduces the tools needed for managing the cluster configuration in the CIB and the cluster resources. Other command line tools for managing r…</p></dd><li><span class="appendix"><a href="#app-crmreport-nonroot"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></a></span></li><dd class="toc-abstract"><p>
  All cluster nodes must be able to access each other via SSH. Tools like
  <code class="command">crm report</code> (for
  troubleshooting) and Hawk2's <span class="guimenu">History Explorer</span> require
  passwordless SSH access between the nodes, otherwise they can only collect
  data from the current node.
 </p></dd></ul></div><section class="appendix" id="app-ha-troubleshooting" data-id-title="Troubleshooting"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">Troubleshooting</span></span> <a title="Permalink" class="permalink" href="#app-ha-troubleshooting">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div><div><div class="abstract"><p>
    Strange problems may occur that are not easy to understand, especially
    when starting to experiment with High Availability. However, there are several
    utilities that allow you to take a closer look at the High Availability internal
    processes. This chapter recommends various solutions.
   </p></div></div></div></div><section class="sect1" id="sec-ha-troubleshooting-install" data-id-title="Installation and First Steps"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.1 </span><span class="title-name">Installation and First Steps</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-install">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Troubleshooting difficulties when installing the packages or bringing the
   cluster online.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.3.3.1"><span class="term">Are the HA packages installed?</span></dt><dd><p>
      The packages needed for configuring and managing a cluster are
      included in the <code class="literal">High Availability</code> installation
      pattern, available with SUSE Linux Enterprise High Availability.
     </p><p>
      Check if SUSE Linux Enterprise High Availability is installed on each of the cluster nodes and if the
      <span class="guimenu">High Availability</span> pattern is installed on each of
      the machines as described in the Installation and Setup Quick Start.
     </p></dd><dt id="id-1.4.7.2.3.3.2"><span class="term">Is the initial configuration the same for all cluster nodes?</span></dt><dd><p>
      To communicate with each other, all nodes belonging to the same
      cluster need to use the same <code class="literal">bindnetaddr</code>,
      <code class="literal">mcastaddr</code> and <code class="literal">mcastport</code> as
      described in <a class="xref" href="#cha-ha-ycluster" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a>.
     </p><p>
      Check if the communication channels and options configured in
      <code class="filename">/etc/corosync/corosync.conf</code> are the same for all
      cluster nodes.
     </p><p>
      In case you use encrypted communication, check if the
      <code class="filename">/etc/corosync/authkey</code> file is available on all
      cluster nodes.
     </p><p>
      All <code class="filename">corosync.conf</code> settings except for
      <code class="literal">nodeid</code> must be the same;
      <code class="filename">authkey</code> files on all nodes must be identical.
     </p></dd><dt id="id-1.4.7.2.3.3.3"><span class="term">Does the firewall allow communication via the
            <code class="literal">mcastport</code>?</span></dt><dd><p>
      If the mcastport used for communication between the cluster nodes is
      blocked by the firewall, the nodes cannot see each other. When
      doing the initial setup with YaST or the bootstrap scripts
      (as described in <a class="xref" href="#cha-ha-ycluster" title="Chapter 4. Using the YaST Cluster Module">Chapter 4, <em>Using the YaST Cluster Module</em></a> or
      the <span class="intraxref">Article “Installation and Setup Quick Start”</span>, respectively), the firewall
      settings are usually automatically adjusted.
     </p><p>
      To make sure the mcastport is not blocked by the firewall, check the
      firewall settings on each  node.
     </p></dd><dt id="id-1.4.7.2.3.3.4"><span class="term">Are Pacemaker and Corosync started on each cluster node?</span></dt><dd><p>
      Usually, starting Pacemaker also starts the Corosync service. To
      check if both services are running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster status</pre></div><p>
      In case they are not running, start them by executing the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> cluster start</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-log" data-id-title="Logging"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.2 </span><span class="title-name">Logging</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-log">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.4.2.1"><span class="term">Where to find the log files? </span></dt><dd><p>
      Pacemaker writes its log files into the <code class="filename">/var/log/pacemaker</code>
      directory. The main Pacemaker log file is
      <code class="filename">/var/log/pacemaker/pacemaker.log</code>. In case you cannot
      find the log files, check the
      logging settings in <code class="filename">/etc/sysconfig/pacemaker</code>,
      Pacemaker's own configuration file. If <code class="literal">PCMK_logfile</code> is
      configured there, Pacemaker will
      use the path that is defined by this parameter.
     </p><p>
      If you need a cluster-wide report showing all relevant log files, see
      <a class="xref" href="#vle-ha-crmreport">How can I create a report with an analysis of all my cluster nodes?</a> for more information.
     </p></dd><dt id="id-1.4.7.2.4.2.2"><span class="term">I enabled monitoring but there is no trace of monitoring operations in
          the log files?</span></dt><dd><p>
      The <code class="systemitem">pacemaker-execd</code> daemon does not log
      recurring monitor operations unless an error occurred. Logging all
      recurring operations would produce too much noise. Therefore recurring
      monitor operations are logged only once an hour.
     </p></dd><dt id="id-1.4.7.2.4.2.3"><span class="term">I only get a <code class="literal">failed</code> message. Is it possible to get more
          information?</span></dt><dd><p>
      Add the <code class="literal">--verbose</code> parameter to your commands. If
      you do that multiple times, the debug output becomes quite verbose.
      See the logging data (<code class="command">sudo journalctl -n</code>) for
      useful hints.
     </p></dd><dt id="id-1.4.7.2.4.2.4"><span class="term">How can I get an overview of all my nodes and resources?</span></dt><dd><p>
      Use the <code class="command">crm_mon</code> command. The following displays the
      resource operation history (option <code class="option">-o</code>) and inactive
      resources (<code class="option">-r</code>):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm_mon</code> -o -r</pre></div><p>
      The display is refreshed when the status changes (to cancel this press
      <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>). An example may look like:
     </p><div class="example" id="id-1.4.7.2.4.2.4.2.4" data-id-title="Stopped Resources"><div class="title-container"><div class="example-title-wrap"><div class="example-title"><span class="title-number-name"><span class="title-number">Example A.1: </span><span class="title-name">Stopped Resources </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.2.4.2.4.2.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">Last updated: Fri Aug 15 10:42:08 2014
Last change: Fri Aug 15 10:32:19 2014
 Stack: corosync
Current DC: bob (175704619) - partition with quorum
Version: 1.1.12-ad083a8
2 Nodes configured
3 Resources configured
       
Online: [ alice bob ]
       
Full list of resources:
       
my_ipaddress    (ocf:heartbeat:Dummy): Started bob
my_filesystem   (ocf:heartbeat:Dummy): Stopped
my_webserver    (ocf:heartbeat:Dummy): Stopped
       
Operations:
* Node bob: 
    my_ipaddress: migration-threshold=3
      + (14) start: rc=0 (ok)
      + (15) monitor: interval=10000ms rc=0 (ok)
      * Node alice:</pre></div></div></div><p>
      The  <em class="citetitle">Pacemaker Explained</em> PDF, available at <a class="link" href="http://www.clusterlabs.org/pacemaker/doc/" target="_blank">http://www.clusterlabs.org/pacemaker/doc/</a>, covers three
      different recovery types in the <em class="citetitle">How are OCF Return Codes
      Interpreted?</em> section.
     </p></dd><dt id="id-1.4.7.2.4.2.5"><span class="term">How to view logs?</span></dt><dd><p>For a more detailed view of what is happening in your
            cluster, use the following command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history log [<em class="replaceable">NODE</em>]</pre></div><p>Replace <em class="replaceable">NODE</em> with the node you
            want to examine, or leave it empty. See <a class="xref" href="#sec-ha-troubleshooting-history" title="A.5. History">Section A.5, “History”</a> for further
            information.</p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-resource" data-id-title="Resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.3 </span><span class="title-name">Resources</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-resource">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.5.2.1"><span class="term">How can I clean up my resources?</span></dt><dd><p>
      Use the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource list
crm resource cleanup <em class="replaceable">rscid</em> [<em class="replaceable">node</em>]</pre></div><p>
      If you leave out the node, the resource is cleaned on all nodes. More
      information can be found in
      <a class="xref" href="#sec-ha-manual-config-cleanup" title="8.5.2. Cleaning Up Cluster Resources with crmsh">Section 8.5.2, “Cleaning Up Cluster Resources with crmsh”</a>.
     </p></dd><dt id="id-1.4.7.2.5.2.2"><span class="term">How can I list my currently known resources?</span></dt><dd><p>
      Use the command <code class="command">crm resource list</code> to display your
      current resources.
     </p></dd><dt id="id-1.4.7.2.5.2.3"><span class="term">I configured a resource, but it always fails. Why?</span></dt><dd><p>
      To check an OCF script use <code class="command">ocf-tester</code>, for
      instance:
     </p><div class="verbatim-wrap"><pre class="screen">ocf-tester -n ip1 -o ip=<em class="replaceable">YOUR_IP_ADDRESS</em> \
  /usr/lib/ocf/resource.d/heartbeat/IPaddr</pre></div><p>
      Use <code class="option">-o</code> multiple times for more parameters. The list
      of required and optional parameters can be obtained by running
      <code class="command">crm</code> <code class="option">ra</code> <code class="option">info</code>
      <em class="replaceable">AGENT</em>, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> ra info ocf:heartbeat:IPaddr</pre></div><p>
      Before running ocf-tester, make sure the resource is not managed by
      the cluster.
     </p></dd><dt id="id-1.4.7.2.5.2.4"><span class="term">Why do resources not fail over and why are there no errors?</span></dt><dd><p>
      The terminated node might be considered unclean.
      Then it is necessary to fence it. If the STONITH resource is not
      operational or does not exist, the remaining node will waiting for the
      fencing to happen. The fencing timeouts are typically high, so it may
      take quite a while to see any obvious sign of problems (if ever).
     </p><p>
      Yet another possible explanation is that a resource is simply not
      allowed to run on this node. That may be because of a failure which
      happened in the past and which was not <span class="quote">“<span class="quote">cleaned</span>”</span>. Or it
      may be because of an earlier administrative action, that is a location
      constraint with a negative score. Such a location constraint is
      inserted by the <code class="command">crm resource move</code> command,
      for example.
     </p></dd><dt id="id-1.4.7.2.5.2.5"><span class="term">Why can I never tell where my resource will run?</span></dt><dd><p>
      If there are no location constraints for a resource, its placement is
      subject to an (almost) random node choice. You are well advised to
      always express a preferred node for resources. That does not mean that
      you need to specify location preferences for <span class="emphasis"><em>all</em></span>
      resources. One preference suffices for a set of related (colocated)
      resources. A node preference looks like this:
     </p><div class="verbatim-wrap"><pre class="screen">location rsc-prefers-alice rsc 100: alice</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-stonith" data-id-title="STONITH and Fencing"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.4 </span><span class="title-name">STONITH and Fencing</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-stonith">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.6.2.1"><span class="term">Why does my STONITH resource not start?</span></dt><dd><p>
      Start (or enable) operation includes checking the status of the
      device. If the device is not ready, the STONITH resource will
      fail to start.
     </p><p>
      At the same time the STONITH plugin will be asked to produce a
      host list. If this list is empty, there is no point in running a
      STONITH resource which cannot shoot anything. The name of the
      host on which STONITH is running is filtered from the list, since
      the node cannot shoot itself.
     </p><p>
      If you want to use single-host management devices such as lights-out
      devices, make sure that the STONITH resource is
      <span class="emphasis"><em>not</em></span> allowed to run on the node which it is
      supposed to fence. Use an infinitely negative location node preference
      (constraint). The cluster will move the STONITH resource to
      another place where it can start, but not before informing you.
     </p></dd><dt id="id-1.4.7.2.6.2.2"><span class="term">Why does fencing not happen, although I have the STONITH resource?</span></dt><dd><p>
      Each STONITH resource must provide a host list. This list may be
      inserted by hand in the STONITH resource configuration or
      retrieved from the device itself from outlet names, for example. That
      depends on the nature of the STONITH plugin.
      <code class="systemitem">pacemaker-fenced</code> uses the list to find out which
      STONITH resource can fence the target node. Only if the node
      appears in the list can the STONITH resource shoot (fence) the
      node.
     </p><p>
      If <code class="systemitem">pacemaker-fenced</code> does not find the node in any of
      the host lists provided by running STONITH resources, it will ask
      <code class="systemitem">pacemaker-fenced</code> instances on other nodes. If the
      target node does not show up in the host lists of other
      <code class="systemitem">pacemaker-fenced</code> instances, the fencing request ends
      in a timeout at the originating node.
     </p></dd><dt id="id-1.4.7.2.6.2.3"><span class="term">Why does my STONITH resource fail occasionally?</span></dt><dd><p>
      Power management devices may give up if there is too much broadcast
      traffic. Space out the monitor operations. Given that fencing is
      necessary only once in a while (and hopefully never), checking the
      device status once a few hours is more than enough.
     </p><p>
      Also, some of these devices may refuse to talk to more than one party
      at the same time. This may be a problem if you keep a terminal or
      browser session open while the cluster tries to test the status.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-history" data-id-title="History"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.5 </span><span class="title-name">History</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-history">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.7.2.1"><span class="term">How to retrieve status information or a log from a failed resource?</span></dt><dd><p>Use the <code class="command">history</code> command and its subcommand
         <code class="command">resource</code>:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history resource <em class="replaceable">NAME1</em></pre></div><p>This gives you a full transition log for the given resource only.
          However, it is possible to investigate more than one resource. Append
          the resource names after the first.
         </p><p>If you followed some naming conventions (see <a class="xref" href="#app-naming" title="Appendix B. Naming Conventions">Appendix B, <em>Naming Conventions</em></a>), the
         <code class="command">resource</code> command makes it easier to investigate
          a group of resources. For example, this command investigates all
          primitives starting with <code class="literal">db</code>:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history resource db*</pre></div><p>View the log file in
           <code class="filename">/var/cache/crm/history/live/alice/ha-log.txt</code>.</p></dd><dt id="id-1.4.7.2.7.2.2"><span class="term">How can I reduce the history output?</span></dt><dd><p>There are two options for the <code class="command">history</code> command:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Use <code class="command">exclude</code></p></li><li class="listitem"><p>Use <code class="command">timeframe</code></p></li></ul></div><p>The <code class="command">exclude</code> command let you set an
            additive regular expression that excludes certain patterns
            from the log. For example, the following command excludes
            all SSH, <code class="systemitem">systemd</code>, and kernel messages: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history exclude ssh|systemd|kernel.</pre></div><p>With the <code class="command">timeframe</code> command you limit
            the output to a certain range. For example, the following
            command shows all the events on August 23rd from 12:00 to
            12:30:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> history timeframe "Aug 23 12:00" "Aug 23 12:30"</pre></div></dd><dt id="id-1.4.7.2.7.2.3"><span class="term">How can I store a <span class="quote">“<span class="quote">session</span>”</span> for later inspection?</span></dt><dd><p>When you encounter a bug or an event that needs further
            examination, it is useful to store all the current settings.
            This file can be sent to support or viewed with
              <code class="command">bzless</code>. For example:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)history# </code><code class="command">timeframe</code> "Oct 13 15:00" "Oct 13 16:00"
<code class="prompt custom">crm(live)history# </code><code class="command">session</code> save tux-test
<code class="prompt custom">crm(live)history# </code><code class="command">session</code> pack
Report saved in '/root/tux-test.tar.bz2'</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-hawk2" data-id-title="Hawk2"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.6 </span><span class="title-name">Hawk2</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-hawk2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="vle-trouble-hawk2-cert"><span class="term">Replacing the Self-Signed Certificate</span></dt><dd><p> To avoid the warning about the self-signed certificate on first
      Hawk2 start-up, replace the automatically created certificate with
      your own certificate (or a certificate that was signed by an official
      Certificate Authority, CA):</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>Replace <code class="filename">/etc/hawk/hawk.key</code> with the private
        key.</p></li><li class="step"><p>Replace <code class="filename">/etc/hawk/hawk.pem</code> with the
        certificate that Hawk2 should present.</p></li><li class="step"><p>
        Restart the Hawk2 services to reload the new certificate:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">systemctl</code> restart hawk-backend hawk</pre></div></li></ol></div></div><p>
      Change ownership of the files to <code class="literal">root:haclient</code>
      and make the files accessible to the group:</p><div class="verbatim-wrap"><pre class="screen">chown root:haclient /etc/hawk/hawk.key /etc/hawk/hawk.pem
chmod 640 /etc/hawk/hawk.key /etc/hawk/hawk.pem</pre></div></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-misc" data-id-title="Miscellaneous"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.7 </span><span class="title-name">Miscellaneous</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-misc">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.9.2.1"><span class="term">How can I run commands on all cluster nodes?</span></dt><dd><p>
      Use the command <code class="command">crm cluster run</code> for this task. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "ls -l /etc/corosync/*.conf"</code>
INFO: [alice]
-rw-r--r-- 1 root root 812 Oct 27 15:42 /etc/corosync/corosync.conf
INFO: [bob]
-rw-r--r-- 1 root root 812 Oct 27 15:42 /etc/corosync/corosync.conf
INFO: [charlie]
-rw-r--r-- 1 root root 812 Oct 27 15:42 /etc/corosync/corosync.conf</pre></div><p>
      By default, the specified command runs on all nodes in the cluster.
      Alternatively, you can run the command on a specific node or group of nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm cluster run "ls -l /etc/corosync/*.conf" alice bob</code></pre></div></dd><dt id="id-1.4.7.2.9.2.2"><span class="term">What is the state of my cluster?</span></dt><dd><p>
      To check the current state of your cluster, use one of the programs
      <code class="literal">crm_mon</code> or <code class="command">crm</code>
      <code class="option">status</code>. This displays the current DC and all the
      nodes and resources known by the current node.
     </p></dd><dt id="id-1.4.7.2.9.2.3"><span class="term">Why can several nodes of my cluster not see each other?</span></dt><dd><p>
      There could be several reasons:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Look first in the configuration file
        <code class="filename">/etc/corosync/corosync.conf</code>. Check if the
        multicast or unicast address is the same for every node in the
        cluster (look in the <code class="literal">interface</code> section with the
        key <code class="literal">mcastaddr</code>).
       </p></li><li class="listitem"><p>
        Check your firewall settings.
       </p></li><li class="listitem"><p>
        Check if your switch supports multicast or unicast addresses.
       </p></li><li class="listitem"><p>
        Check if the connection between your nodes is broken. Most often,
        this is the result of a badly configured firewall. This also may be
        the reason for a <span class="emphasis"><em>split brain</em></span> condition, where
        the cluster is partitioned.
       </p></li></ul></div></dd><dt id="id-1.4.7.2.9.2.4"><span class="term">Why can an OCFS2 device not be mounted?</span></dt><dd><p>
      Check the log messages (<code class="command">sudo journalctl -n</code>) for the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">Jan 12 09:58:55 alice pacemaker-execd: [3487]: info: RA output: [...]
  ERROR: Could not load ocfs2_stackglue
Jan 12 16:04:22 alice modprobe: FATAL: Module ocfs2_stackglue not found.</pre></div><p>
      In this case the Kernel module <code class="filename">ocfs2_stackglue.ko</code>
      is missing. Install the package
      <code class="filename">ocfs2-kmp-default</code>,
      <code class="filename">ocfs2-kmp-pae</code> or
      <code class="filename">ocfs2-kmp-xen</code>, depending on the installed Kernel.
     </p></dd><dt id="vle-ha-crmreport"><span class="term">How can I create a report with an analysis of all my cluster nodes?</span></dt><dd><p> On the crm shell, use <code class="command">crm report</code> to
            create a report. This tool compiles: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Cluster-wide log files,
       </p></li><li class="listitem"><p>
        Package states,
       </p></li><li class="listitem"><p>
        DLM/OCFS2 states,
       </p></li><li class="listitem"><p>
        System information,
       </p></li><li class="listitem"><p>
        CIB history,
       </p></li><li class="listitem"><p>
        Parsing of core dump reports, if a debuginfo package is installed.
       </p></li></ul></div><p>
      Usually run <code class="command">crm report</code> with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm report</code> -f 0:00 -n alice -n bob</pre></div><p>
      The command extracts all information since 0am on the hosts alice
      and bob and creates a <code class="literal">*.tar.bz2</code> archive named
      <code class="filename">crm_report-<em class="replaceable">DATE</em>.tar.bz2</code>
      in the current directory, for example,
      <code class="filename">crm_report-Wed-03-Mar-2012</code>. If you are only
      interested in a specific time frame, add the end time with the
      <code class="option">-t</code> option.
     </p><div id="id-1.4.7.2.9.2.5.2.6" data-id-title="Remove Sensitive Information" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Remove Sensitive Information</div><p>
       The <code class="command">crm report</code> tool tries to remove any sensitive
       information from the CIB and the PE input files, however, it cannot know
       everything. If you have more sensitive information, supply additional
       patterns with the <code class="option">-p</code> option (see man page).
       The log files and the <code class="command">crm_mon</code>,
       <code class="command">ccm_tool</code>, and <code class="command">crm_verify</code> output
       are <span class="emphasis"><em>not</em></span> sanitized.
      </p><p>
       Before sharing your data in any way, check the archive and remove all
       information you do not want to expose.
      </p></div><p>
      Customize the command execution with further options. For example, if
      you have a Pacemaker cluster, you certainly want to add the option
      <code class="option">-A</code>. In case you have another user who has permissions
      to the cluster, use the <code class="option">-u</code> option and specify this
      user (in addition to <code class="systemitem">root</code> and
      <code class="systemitem">hacluster</code>). In case you have
      a non-standard SSH port, use the <code class="option">-X</code> option to add the
      port (for example, with the port 3479, use <code class="literal">-X "-p
      3479"</code>). Further options can be found in the man page of
      <code class="command">crm report</code>.
     </p><p>
      After <code class="command">crm report</code> has analyzed all the relevant log
      files and created the directory (or archive), check the log files for
      an uppercase <code class="literal">ERROR</code> string. The most important files
      in the top level directory of the report are:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.2.9.2.5.2.9.1"><span class="term"><code class="filename">analysis.txt</code>
       </span></dt><dd><p>
         Compares files that should be identical on all nodes.
        </p></dd><dt id="id-1.4.7.2.9.2.5.2.9.2"><span class="term"><code class="filename">corosync.txt</code>
         </span></dt><dd><p>
             Contains a copy of the Corosync configuration file.
           </p></dd><dt id="id-1.4.7.2.9.2.5.2.9.3"><span class="term"><code class="filename">crm_mon.txt</code>
       </span></dt><dd><p>
         Contains the output of the <code class="command">crm_mon</code> command.
        </p></dd><dt id="id-1.4.7.2.9.2.5.2.9.4"><span class="term"><code class="filename">description.txt</code>
       </span></dt><dd><p>
         Contains all cluster package versions on your nodes. There is also
         the <code class="filename">sysinfo.txt</code> file which is node specific.
         It is linked to the top directory.
        </p><p>This file can be used as a template to describe the issue
        you encountered and post it to <a class="link" href="https://github.com/ClusterLabs/crmsh/issues" target="_blank">https://github.com/ClusterLabs/crmsh/issues</a>.</p></dd><dt id="id-1.4.7.2.9.2.5.2.9.5"><span class="term"><code class="filename">members.txt</code></span></dt><dd><p>A list of all nodes</p></dd><dt id="id-1.4.7.2.9.2.5.2.9.6"><span class="term"><code class="filename">sysinfo.txt</code></span></dt><dd><p>Contains a list of all relevant package names and their
            versions. Additionally, there is also a list of configuration
            files which are different from the original RPM package.</p></dd></dl></div><p>
      Node-specific files are stored in a subdirectory named by the node's
      name. It contains a copy of the directory <code class="filename">/etc</code>
      of the respective node.
     </p><p>
      In case you need to simplify the arguments, set your default values
      in the configuration file <code class="filename">/etc/crm/crm.conf</code>,
      section <code class="literal">report</code>. Further information is
      written in the man page <code class="command">man 8 crmsh_hb_report</code>.
     </p></dd></dl></div></section><section class="sect1" id="sec-ha-troubleshooting-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">A.8 </span><span class="title-name">For More Information</span></span> <a title="Permalink" class="permalink" href="#sec-ha-troubleshooting-moreinfo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_troubleshooting.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   For additional information about high availability on Linux, including
   configuring cluster resources and managing and customizing a High Availability
   cluster, see
   <a class="link" href="http://clusterlabs.org/wiki/Documentation" target="_blank">http://clusterlabs.org/wiki/Documentation</a>.
  </p></section></section><section class="appendix" id="app-naming" data-id-title="Naming Conventions"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">B </span><span class="title-name">Naming Conventions</span></span> <a title="Permalink" class="permalink" href="#app-naming">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_naming.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This guide uses the following naming conventions for cluster nodes and
  names, cluster resources, and constraints.
 </p><div class="variablelist" id="vl-naming-cluster-names-nodes"><dl class="variablelist"><dt id="id-1.4.7.3.4.1"><span class="term">Cluster Nodes</span></dt><dd><p>
     Cluster nodes use first names:
    </p><p>
     alice, bob, charlie, doro, and eris
    </p></dd><dt id="id-1.4.7.3.4.2"><span class="term">Cluster Site Names</span></dt><dd><p>
     Cluster sites are named after cities:
    </p><p>
     amsterdam, berlin, canberra, dublin,
     fukuoka, gizeh, hanoi, and istanbul
    </p></dd><dt id="id-1.4.7.3.4.3"><span class="term">Cluster Resources</span></dt><dd><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Primitives
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          No prefix
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Groups
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">g-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Clones
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">cl-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Promotable Clones
         </p>
        </td><td>
         <p>
          (formerly known as multi-state resources)
         </p>
         <p>
          Prefix <code class="literal">ms-</code>
         </p>
        </td></tr></tbody></table></div></dd><dt id="id-1.4.7.3.4.4"><span class="term">Constraints</span></dt><dd><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Order constraints
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">o-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
         <p>
          Location constraints
         </p>
        </td><td style="border-bottom: 1px solid ; ">
         <p>
          Prefix <code class="literal">loc-</code>
         </p>
        </td></tr><tr><td style="border-right: 1px solid ; ">
         <p>
          Colocation constraints
         </p>
        </td><td>
         <p>
          Prefix <code class="literal">col-</code>
         </p>
        </td></tr></tbody></table></div></dd></dl></div></section><section class="appendix" id="app-ha-management" data-id-title="Cluster Management Tools (Command Line)"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">C </span><span class="title-name">Cluster Management Tools (Command Line)</span></span> <a title="Permalink" class="permalink" href="#app-ha-management">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_management.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  SUSE Linux Enterprise High Availability ships with a comprehensive set of tools to assists you in
  managing your cluster from the command line. This chapter introduces the
  tools needed for managing the cluster configuration in the CIB and the
  cluster resources. Other command line tools for managing resource agents
  or tools used for debugging (and troubleshooting) your setup are covered
  in <a class="xref" href="#app-ha-troubleshooting" title="Appendix A. Troubleshooting">Appendix A, <em>Troubleshooting</em></a>.
 </p><div id="id-1.4.7.4.4" data-id-title="Use crmsh" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Use crmsh</div><p>
   This tool is for experts only. Usually the crm shell (crmsh) is
   the recommended way of managing your cluster.
  </p></div><p>
  The following list presents several tasks related to cluster management
  and briefly introduces the tools to use to accomplish these tasks:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.7.4.6.1"><span class="term">Monitoring the Cluster's Status</span></dt><dd><p>
     The <code class="command">crm_mon</code> command allows you to monitor your
     cluster's status and configuration. Its output includes the number of
     nodes, uname, UUID, status, the resources configured in your cluster,
     and the current status of each. The output of
     <code class="command">crm_mon</code> can be displayed at the console or printed
     into an HTML file. When provided with a cluster configuration file
     without the status section, <code class="command">crm_mon</code> creates an
     overview of nodes and resources as specified in the file. See the
     <code class="command">crm_mon</code> man page for a detailed introduction to this
     tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.2"><span class="term">Managing the CIB</span></dt><dd><p>
     The <code class="command">cibadmin</code> command is the low-level administrative
     command for manipulating the CIB. It can be used to dump all or part of
     the CIB, update all or part of it, modify all or part of it, delete the
     entire CIB, or perform miscellaneous CIB administrative operations. See
     the <code class="command">cibadmin</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.3"><span class="term">Managing Configuration Changes</span></dt><dd><p>
     The <code class="command">crm_diff</code> command assists you in creating and
     applying XML patches. This can be useful for visualizing the changes
     between two versions of the cluster configuration or saving changes so
     they can be applied at a later time using <code class="command">cibadmin</code>.
     See the <code class="command">crm_diff</code> man page for a detailed
     introduction to this tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.4"><span class="term">Manipulating CIB Attributes</span></dt><dd><p>
     The <code class="command">crm_attribute</code> command lets you query and
     manipulate node attributes and cluster configuration options that are
     used in the CIB. See the <code class="command">crm_attribute</code> man page for
     a detailed introduction to this tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.5"><span class="term">Validating the Cluster Configuration</span></dt><dd><p>
     The <code class="command">crm_verify</code> command checks the configuration
     database (CIB) for consistency and other problems. It can check a file
     containing the configuration or connect to a running cluster. It
     reports two classes of problems. Errors must be fixed before the
     High Availability software can work properly while warning resolution is up to the
     administrator. <code class="command">crm_verify</code> assists in creating new or
     modified configurations. You can take a local copy of a CIB in the
     running cluster, edit it, validate it using
     <code class="command">crm_verify</code>, then put the new configuration into
     effect using <code class="command">cibadmin</code>. See the
     <code class="command">crm_verify</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.6"><span class="term">Managing Resource Configurations</span></dt><dd><p>
     The <code class="command">crm_resource</code> command performs various
     resource-related actions on the cluster. It lets you modify the
     definition of configured resources, start and stop resources, or delete
     and migrate resources between nodes. See the
     <code class="command">crm_resource</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd><dt id="id-1.4.7.4.6.7"><span class="term">Managing Resource Fail Counts</span></dt><dd><p>
     The <code class="command">crm_failcount</code> command queries the number of
     failures per resource on a given node. This tool can also be used to
     reset the fail count, allowing the resource to again run on nodes where
     it had failed too often. See the <code class="command">crm_failcount</code> man
     page for a detailed introduction to this tool's usage and command
     syntax.
    </p></dd><dt id="id-1.4.7.4.6.8"><span class="term">Managing a Node's Standby Status</span></dt><dd><p>
     The <code class="command">crm_standby</code> command can manipulate a node's
     standby attribute. Any node in standby mode is no longer eligible to
     host resources and any resources that are there must be moved. Standby
     mode can be useful for performing maintenance tasks, such as Kernel
     updates. Remove the standby attribute from the node for it to become a
     fully active member of the cluster again. See the
     <code class="command">crm_standby</code> man page for a detailed introduction to
     this tool's usage and command syntax.
    </p></dd></dl></div></section><section class="appendix" id="app-crmreport-nonroot" data-id-title="Running Cluster Reports Without root Access"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">D </span><span class="title-name">Running Cluster Reports Without <code class="systemitem">root</code> Access</span></span> <a title="Permalink" class="permalink" href="#app-crmreport-nonroot">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  All cluster nodes must be able to access each other via SSH. Tools like
  <code class="command">crm report</code> (for
  troubleshooting) and Hawk2's <span class="guimenu">History Explorer</span> require
  passwordless SSH access between the nodes, otherwise they can only collect
  data from the current node.
 </p><p>
  If passwordless SSH <code class="systemitem">root</code> access does not comply with regulatory
  requirements, you can use a work-around for running cluster reports. It
  consists of the following basic steps:
 </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
    Creating a dedicated local user account (for running
    <code class="command">crm report</code>).
   </p></li><li class="listitem"><p>
    Configuring passwordless SSH access for that user account, ideally by
    using a non-standard SSH port.
   </p></li><li class="listitem"><p>
    Configuring <code class="command">sudo</code> for that user.
   </p></li><li class="listitem"><p>
    Running <code class="command">crm report</code> as
    that user.
   </p></li></ol></div><p>
  By default when <code class="command">crm report</code> is run, it attempts to
  log in to
  remote nodes first as <code class="systemitem">root</code>, then as user
  <code class="systemitem">hacluster</code>. However, if your
  local security policy prevents <code class="systemitem">root</code> login using SSH, the script
  execution will fail on all remote nodes. Even attempting to run the script
  as user <code class="systemitem">hacluster</code> will fail
  because this is a service account, and its shell is set to
  <code class="filename">/bin/false</code>, which prevents login. Creating a
  dedicated local user is the only option to successfully run the
  <code class="command">crm report</code> script on
  all nodes in the High Availability cluster.
 </p><section class="sect1" id="sec-crmreport-nonroot-user" data-id-title="Creating a Local User Account"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.1 </span><span class="title-name">Creating a Local User Account</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-user">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   In the following example, we will create a local user named
   <code class="systemitem">hareport</code> from command line. The
   password can be anything that meets your security requirements.
   Alternatively, you can create the user account and set the password with
   YaST.
  </p><div class="procedure" id="id-1.4.7.5.7.3" data-id-title="Creating a Dedicated User Account for Running Cluster Reports"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.1: </span><span class="title-name">Creating a Dedicated User Account for Running Cluster Reports </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.5.7.3">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Start a shell and create a user
     <code class="systemitem">hareport</code> with a home
     directory <code class="filename">/home/hareport </code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">useradd</code> -m -d /home/hareport -c "HA Report" hareport</pre></div></li><li class="step"><p>
     Set a password for the user:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>passwd hareport</pre></div></li><li class="step"><p>
     When prompted, enter and re-enter a password for the user.
    </p></li></ol></div></div><div id="id-1.4.7.5.7.4" data-id-title="Same User Is Required On Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same User Is Required On Each Cluster Node</div><p>
    To create the same user account on all nodes, repeat the steps above on
    each cluster node.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-ssh" data-id-title="Configuring a Passwordless SSH Account"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.2 </span><span class="title-name">Configuring a Passwordless SSH Account</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-ssh">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><div class="procedure" id="id-1.4.7.5.8.2" data-id-title="Configuring the SSH Daemon for a Non-Standard Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.2: </span><span class="title-name">Configuring the SSH Daemon for a Non-Standard Port </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.5.8.2">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    By default, the SSH daemon and the SSH client talk and listen on port
    <code class="literal">22</code>. If your network security guidelines require the
    default SSH port to be changed to an alternate high numbered port, you
    need to modify the daemon's configuration file
    <code class="filename">/etc/ssh/sshd_config</code>.
   </p><ol class="procedure" type="1"><li class="step"><p>
     To modify the default port, search the file for the
     <code class="literal">Port</code> line, uncomment it and edit it according to
     your wishes. For example, set it to:
    </p><div class="verbatim-wrap"><pre class="screen">Port 5022</pre></div></li><li class="step"><p>
     If your organization does not permit the <code class="systemitem">root</code> user to access
     other servers, search the file for the
     <code class="literal">PermitRootLogin</code> entry, uncomment it and set it to
     <code class="literal">no</code>:
    </p><div class="verbatim-wrap"><pre class="screen">PermitRootLogin no</pre></div></li><li class="step"><p>
     Alternatively, add the respective lines to the end of the file by
     executing the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>echo “PermitRootLogin no” &gt;&gt; /etc/ssh/sshd_config
<code class="prompt root"># </code>echo “Port 5022” &gt;&gt; /etc/ssh/sshd_config</pre></div></li><li class="step"><p>
     After modifying <code class="filename">/etc/ssh/sshd_config</code>, restart the
     SSH daemon to make the new settings take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart sshd</pre></div></li></ol></div></div><div id="id-1.4.7.5.8.3" data-id-title="Same Settings Are Required On Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same Settings Are Required On Each Cluster Node</div><p>
    Repeat the SSH daemon configuration above on each cluster node.
   </p></div><div class="procedure" id="id-1.4.7.5.8.4" data-id-title="Configuring the SSH Client for a Non-Standard Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.3: </span><span class="title-name">Configuring the SSH Client for a Non-Standard Port </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.5.8.4">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    If the SSH port change is going to be made on all nodes in the cluster,
    it is useful to modify the SSH configuration file,
    <code class="filename">/etc/ssh/sshd_config</code>.
    </p><ol class="procedure" type="1"><li class="step"><p>
     To modify the default port, search the file for the
     <code class="literal">Port</code> line, uncomment it and edit it according to
     your wishes. For example, set it to:
    </p><div class="verbatim-wrap"><pre class="screen">Port 5022</pre></div></li><li class="step"><p>
     Alternatively, add the respective line to the end of the file by
     executing the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>echo “Port 5022” &gt;&gt; /etc/ssh/ssh_config</pre></div></li></ol></div></div><div id="id-1.4.7.5.8.5" data-id-title="Settings Only Required on One Node" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings Only Required on One Node</div><p>
    The SSH client configuration above is only needed on the node on which
    you want to run the cluster report.</p><p>Alternatively, you can use the <code class="option">-X</code> option to run the
    <code class="command">crm report</code> with a custom SSH port or even make
    <code class="command">crm report</code> use your custom SSH port by default. For
    details, see <a class="xref" href="#pro-crmreport-custom-ssh" title="Generating a Cluster Report Using a Custom SSH Port">Procedure D.5, “Generating a Cluster Report Using a Custom SSH Port”</a>.</p></div><div class="procedure" id="id-1.4.7.5.8.6" data-id-title="Configuring Shared SSH Keys"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.4: </span><span class="title-name">Configuring Shared SSH Keys </span></span><a title="Permalink" class="permalink" href="#id-1.4.7.5.8.6">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><p>
    You can access other servers using SSH and not be asked for a
    password. While this may appear insecure at first sight, it is actually
    a very secure access method since the users can only access servers that
    their public key has been shared with. The shared key must be created as
    the user that will use the key.
   </p><ol class="procedure" type="1"><li class="step"><p>
     Log in to one of the nodes with the user account that you have created
     for running cluster reports (in our example above, the user account was
     <code class="systemitem">hareport</code>).
    </p></li><li class="step"><p>
     Generate a new key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh-keygen –t rsa</pre></div><p>
     This command will generate a 2048 bit key by default. The default
     location for the key is <code class="filename">~/.ssh/</code>. You are asked to
     set a passphrase on the key. However, do not enter a passphrase because
     for passwordless login there must not be a passphrase on the key.
     </p></li><li class="step"><p>
     After the keys have been generated, copy the public key to
     <span class="emphasis"><em>each</em></span> of the other nodes
     (<span class="emphasis"><em>including</em></span> the node where you created the key):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh-copy-id -i ~/.ssh/id_rsa.pub <em class="replaceable">HOSTNAME_OR_IP</em></pre></div><p>
     In the command, you can either use the DNS name for each server, an
     alias, or the IP address. During the copy process you will be asked to
     accept the host key for each node, and you will need to provide the
     password for the <code class="systemitem">hareport</code>
     user account (this will be the only time you need to enter it).
    </p></li><li class="step"><p>
     After the key is shared to all cluster nodes, test if you can log in as
     user <code class="systemitem">hareport</code> to the other
     nodes by using passwordless SSH:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code>ssh <em class="replaceable">HOSTNAME_OR_IP</em></pre></div><p>
     You should be automatically connected to the remote server without
     being asked to accept a certificate or enter a password.
    </p></li></ol></div></div><div id="id-1.4.7.5.8.7" data-id-title="Settings Only Required on One Node" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Settings Only Required on One Node</div><p>
    If you intend to run the cluster report from the same node each time, it
    is sufficient to execute the procedure above on this node only.
    Otherwise repeat the procedure on each node.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-sudo" data-id-title="Configuring sudo"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.3 </span><span class="title-name">Configuring <code class="command">sudo</code></span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-sudo">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   The <code class="command">sudo</code> command allows a regular user to quickly
   become <code class="systemitem">root</code> and issue a command, with or without providing a
   password. Sudo access can be given to all root-level commands or to
   specific commands only. Sudo typically uses aliases to define the entire
   command string.
  </p><p>
   To configure sudo either use <code class="command">visudo</code>
   (<span class="emphasis"><em>not</em></span> vi) or YaST.
  </p><div id="id-1.4.7.5.9.4" data-id-title="Do Not Use vi" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Do Not Use vi</div><p>
    For sudo configuration from command line, you must edit the sudoers file
    as <code class="systemitem">root</code> with <code class="command">visudo</code>. Using any other editor may
    result in syntax or file permission errors that prevent sudo from
    running.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Log in as <code class="systemitem">root</code>.
    </p></li><li class="step"><p>
     To open the <code class="filename">/etc/sudoers</code> file, enter
     <code class="command">visudo</code>.
    </p></li><li class="step"><p> Look for the following categories: <code class="literal">Host alias
      specification</code>,<code class="literal">User alias specification</code>,
      <code class="literal">Cmnd alias specification</code>, and <code class="literal">Runas alias
      specification</code>. </p></li><li class="step"><p>
     Add the following entries to the respective categories in
     <code class="filename">/etc/sudoers</code>:
    </p><div class="verbatim-wrap"><pre class="screen">Host_Alias	CLUSTER = alice,bob,charlie <span class="callout" id="ha-sudoers-host-alias">1</span>
User_Alias HA = hareport <span class="callout" id="ha-sudoers-user-alias">2</span>
Cmnd_Alias HA_ALLOWED = /bin/su, /usr/sbin/crm report *<span class="callout" id="ha-sudoers-cmd-alias">3</span>
Runas_Alias R = root <span class="callout" id="ha-sudoers-runas-alias">4</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-host-alias"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The host alias defines on which server (or range of servers) the sudo
       user has rights to issue commands. In the host alias you can use DNS
       names, or IP addresses, or specify an entire network range (for
       example, <code class="literal">172.17.12.0/24</code>). To limit the scope of
       access you should specify the host names for the cluster nodes only.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-user-alias"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The user alias allows you to add multiple local user accounts to a
       single alias. However, in this case you could avoid creating an alias since
       only one account is being used. In the example above, we added the
       <code class="systemitem">hareport</code> user which we have
       created for running cluster reports.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-cmd-alias"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The command alias defines which commands can be executed by the user.
       This is useful if you want to limit what the non-root user can access
       when using <code class="command">sudo</code>. In this case the
       <code class="systemitem">hareport</code>
       user account will need access to the commands <code class="command">crm report</code>
       and <code class="command">su</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#ha-sudoers-runas-alias"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The <code class="literal">runas</code> alias specifies the account that the command will be run
       as. In this case <code class="systemitem">root</code>.
      </p></td></tr></table></div></li><li class="step"><p>Search for the following two lines:</p><div class="verbatim-wrap"><pre class="screen">Defaults targetpw
ALL     ALL=(ALL) ALL</pre></div><p>As they would conflict with the setup we want to create, disable them:</p><div class="verbatim-wrap"><pre class="screen">#Defaults targetpw
#ALL     ALL=(ALL) ALL</pre></div></li><li class="step"><p>Look for the <code class="literal">User privilege specification</code> category.
     After having defined the aliases above, you can now add the following
     rule there:</p><div class="verbatim-wrap"><pre class="screen">HA	CLUSTER = (R) NOPASSWD:HA_ALLOWED</pre></div><p>The <code class="literal">NOPASSWORD</code> option ensures that the user
      <code class="systemitem">hareport</code> can execute the cluster
     report without providing a password.</p></li><li class="step"><p><span class="step-optional">(Optional)</span> 
     To allow the user <code class="systemitem">hareport</code> to run cluster reports using your local SSH keys, add the following line to the <code class="literal">Defaults specification</code> category. This preserves the <code class="literal">SSH_AUTH_SOCK</code> environment
     variable, which is required for SSH agent forwarding.
    </p><div class="verbatim-wrap"><pre class="screen">Defaults!HA_ALLOWED env_keep+=SSH_AUTH_SOCK</pre></div><p>
     When you log into a node as the user <code class="systemitem">hareport</code> via <code class="command">ssh -A</code>, and use <code class="command">sudo</code>
     to run <code class="command">crm report</code>, your local SSH keys are passed to the node for
     authentication.
    </p></li></ol></div></div><div id="id-1.4.7.5.9.6" data-id-title="Same sudo Configuration Is Required on Each Cluster Node" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Same sudo Configuration Is Required on Each Cluster Node</div><p>
    This sudo configuration must be made on all nodes in the cluster. No
    other changes are needed for sudo and no services need to be restarted.
   </p></div></section><section class="sect1" id="sec-crmreport-nonroot-execute" data-id-title="Generating a Cluster Report"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">D.4 </span><span class="title-name">Generating a Cluster Report</span></span> <a title="Permalink" class="permalink" href="#sec-crmreport-nonroot-execute">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div></div></div></div><p>To run cluster reports with the settings you have configured above, you need to be logged
   in to one of the nodes as user <code class="systemitem">hareport</code>.
   To start a cluster report, use the <code class="command">crm report</code> command.
   For example: </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">hareport &gt; </code><code class="command">sudo crm report -u hareport -f 0:00 -n "alice bob charlie"</code></pre></div><p>This command will extract all information since <code class="literal">0 am</code> on the named nodes
   and create a <code class="literal">*.tar.bz2</code> archive named
     <code class="filename">pcmk-<em class="replaceable">DATE</em>.tar.bz2</code> in
     the current directory.</p><div class="procedure" id="pro-crmreport-custom-ssh" data-id-title="Generating a Cluster Report Using a Custom SSH Port"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure D.5: </span><span class="title-name">Generating a Cluster Report Using a Custom SSH Port </span></span><a title="Permalink" class="permalink" href="#pro-crmreport-custom-ssh">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_crmreport_passl.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>When using a custom SSH port, use the <code class="option">-X</code> with
     <code class="command">crm report</code> to modify the client's SSH port. For example,
     if your custom SSH port is <code class="literal">5022</code>, use the following
     command:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>crm report -X "-p 5022" [...]</pre></div></li><li class="step"><p>To set your custom SSH port permanently for
     <code class="command">crm report</code>, start the interactive crm shell:</p><div class="verbatim-wrap"><pre class="screen">crm options</pre></div></li><li class="step"><p>
     Enter the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt custom">crm(live)options# </code> set core.report_tool_options "-X -oPort=5022"</pre></div></li></ol></div></div></section></section></div><section class="glossary"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number"> </span><span class="title-name">Glossary</span></span> <a title="Permalink" class="permalink" href="#gl-heartb">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/ha_glossary.xml" title="Edit source document"> </a></div></div></div></div></div><div class="line"/><dl><dt id="id-1.4.8.3"><span><span class="glossterm">active/active, active/passive</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3">#</a></span></dt><dd class="glossdef"><p>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time. For
    example, it has <span class="emphasis"><em>some</em></span> services running, but can take
    over other services from the other node. Compare with primary/secondary
    and dual-primary in DRBD speak.
   </p></dd><dt id="id-1.4.8.4"><span><span class="glossterm">arbitrator</span> <a title="Permalink" class="permalink" href="#id-1.4.8.4">#</a></span></dt><dd class="glossdef"><p>
    Additional instance in a Geo cluster that helps to reach consensus
    about decisions such as failover of resources across sites. Arbitrators
    are single machines that run one or more booth instances in a special
    mode.
   </p></dd><dt id="id-1.4.8.5"><span><span class="glossterm">AutoYaST</span> <a title="Permalink" class="permalink" href="#id-1.4.8.5">#</a></span></dt><dd class="glossdef"><p>
    AutoYaST is a system for installing one or more SUSE Linux Enterprise systems automatically
    and without user intervention. 
   </p></dd><dt id="id-1.4.8.6"><span><span class="glossterm">bindnetaddr (bind network address)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.6">#</a></span></dt><dd class="glossdef"><p>
    The network address the Corosync executive should bind to. 
   </p></dd><dt id="glos-booth"><span><span class="glossterm">booth</span> <a title="Permalink" class="permalink" href="#glos-booth">#</a></span></dt><dd class="glossdef"><p>
    The instance that manages the failover process between the sites of a
    Geo cluster. It aims to get multi-site resources active on one and
    only one site. This is achieved by using so-called tickets that are
    treated as failover domain between cluster sites, in case a site should
    be down.
   </p></dd><dt id="id-1.4.8.8"><span><span class="glossterm">boothd (booth
  daemon)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.8">#</a></span></dt><dd class="glossdef"><p>
    Each of the participating clusters and arbitrators in a Geo cluster
    runs a service, the <code class="systemitem">boothd</code>. It
    connects to the booth daemons running at the other sites and exchanges
    connectivity details.
   </p></dd><dt id="id-1.4.8.9"><span><span class="glossterm">CCM (consensus cluster membership)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.9">#</a></span></dt><dd class="glossdef"><p>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </p></dd><dt id="id-1.4.8.10"><span><span class="glossterm">CIB (cluster information base)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.10">#</a></span></dt><dd class="glossdef"><p>
    A representation of the whole cluster configuration and status (cluster
    options, nodes, resources, constraints and the relationship to each
    other). It is written in XML and resides in memory. A primary CIB is kept
    and maintained on the
    <a class="xref" href="#glos-dc" title="DC (designated coordinator)">DC (designated coordinator)</a> and replicated to
    the other nodes. Normal read and write operations on the CIB are
    serialized through the primary CIB.
   </p></dd><dt id="id-1.4.8.11"><span><span class="glossterm">cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.11">#</a></span></dt><dd class="glossdef"><p>
    A <span class="emphasis"><em>high-performance</em></span> cluster is a group of computers
    (real or virtual) sharing the application load to achieve faster
    results. A <span class="emphasis"><em>high-availability</em></span> cluster is designed
    primarily to secure the highest possible availability of services.
   </p></dd><dt id="id-1.4.8.12"><span><span class="glossterm">cluster partition</span> <a title="Permalink" class="permalink" href="#id-1.4.8.12">#</a></span></dt><dd class="glossdef"><p>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster are
    split into partitions but still active. They can only communicate with
    nodes in the same partition and are unaware of the separated nodes. As
    the loss of the nodes on the other partition cannot be confirmed, a
    split brain scenario develops (see also
    <a class="xref" href="#glos-splitbrain" title="split brain">split brain</a>).
   </p></dd><dt id="id-1.4.8.13"><span><span class="glossterm">cluster site</span> <a title="Permalink" class="permalink" href="#id-1.4.8.13">#</a></span></dt><dd class="glossdef"><p>
    In Geo clustering, a cluster site (or just <span class="quote">“<span class="quote">site</span>”</span>) is a group of
    nodes in the same physical location, managed by <a class="xref" href="#glos-booth" title="booth">booth</a>.
   </p></dd><dt id="id-1.4.8.14"><span><span class="glossterm">cluster stack</span> <a title="Permalink" class="permalink" href="#id-1.4.8.14">#</a></span></dt><dd class="glossdef"><p>
    The ensemble of software technologies and components which compose
    a cluster.
   </p></dd><dt id="id-1.4.8.15"><span><span class="glossterm">concurrency violation</span> <a title="Permalink" class="permalink" href="#id-1.4.8.15">#</a></span></dt><dd class="glossdef"><p>
    A resource that should be running on only one node in the cluster is
    running on several nodes.
   </p></dd><dt id="id-1.4.8.16"><span><span class="glossterm">conntrack tools</span> <a title="Permalink" class="permalink" href="#id-1.4.8.16">#</a></span></dt><dd class="glossdef"><p>
    Allow interaction with the in-kernel connection tracking system for
    enabling <span class="emphasis"><em>stateful</em></span> packet
    inspection for iptables. Used by SUSE Linux Enterprise High Availability to synchronize the connection
    status between cluster nodes.
   </p></dd><dt id="id-1.4.8.17"><span><span class="glossterm">CRM (cluster resource manager)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.17">#</a></span></dt><dd class="glossdef"><p>
    The management entity responsible for coordinating all non-local
    interactions in a High Availability cluster. SUSE Linux Enterprise High Availability uses Pacemaker as CRM.
    The CRM is implemented as <code class="systemitem">pacemaker-controld</code>. It interacts with several
    components: local resource managers, both on its own node and on the other nodes,
    non-local CRMs, administrative commands, the fencing functionality, and the membership
    layer.
   </p></dd><dt id="id-1.4.8.18"><span><span class="glossterm">crmsh</span> <a title="Permalink" class="permalink" href="#id-1.4.8.18">#</a></span></dt><dd class="glossdef"><p>
    The command line utility crmsh manages your cluster, nodes, and
    resources.
   </p><p>
    See <a class="xref" href="#cha-ha-manual-config" title="5.5. Introduction to crmsh">Section 5.5, “Introduction to crmsh”</a> for more information.
   </p></dd><dt id="id-1.4.8.19"><span><span class="glossterm">Csync2</span> <a title="Permalink" class="permalink" href="#id-1.4.8.19">#</a></span></dt><dd class="glossdef"><p>
    A synchronization tool that can be used to replicate configuration files
    across all nodes in the cluster, and even across Geo clusters.
   </p></dd><dt id="glos-dc"><span><span class="glossterm">DC (designated coordinator)</span> <a title="Permalink" class="permalink" href="#glos-dc">#</a></span></dt><dd class="glossdef"><p>
    The DC is elected from all nodes in the cluster. This happens if there
    is no DC yet or if the current DC leaves the cluster for any reason.
    The DC is the only entity
    in the cluster that can decide that a cluster-wide change needs to
    be performed, such as fencing a node or moving resources around. All
    other nodes get their configuration and resource allocation
    information from the current DC.
   </p></dd><dt id="id-1.4.8.21"><span><span class="glossterm">Disaster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.21">#</a></span></dt><dd class="glossdef"><p>
    Unexpected interruption of critical infrastructure induced by nature,
    humans, hardware failure, or software bugs.
   </p></dd><dt id="id-1.4.8.23"><span><span class="glossterm">Disaster Recover Plan</span> <a title="Permalink" class="permalink" href="#id-1.4.8.23">#</a></span></dt><dd class="glossdef"><p>
    A strategy to recover from a disaster with minimum impact on IT
    infrastructure.
   </p></dd><dt id="id-1.4.8.22"><span><span class="glossterm">Disaster Recovery</span> <a title="Permalink" class="permalink" href="#id-1.4.8.22">#</a></span></dt><dd class="glossdef"><p>
    Disaster recovery is the process by which a business function is
    restored to the normal, steady state after a disaster.
   </p></dd><dt id="id-1.4.8.24"><span><span class="glossterm">DLM (distributed lock manager)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.24">#</a></span></dt><dd class="glossdef"><p>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </p></dd><dt id="id-1.4.8.25"><span><span class="glossterm">DRBD</span> <a title="Permalink" class="permalink" href="#id-1.4.8.25">#</a></span></dt><dd class="glossdef"><p>
    <span class="trademark">DRBD</span>® is a block device
    designed for building high availability clusters. The whole block device
    is mirrored via a dedicated network and is seen as a network RAID-1.
   </p></dd><dt id="id-1.4.8.26"><span><span class="glossterm">existing cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.26">#</a></span></dt><dd class="glossdef"><p>
      The term <span class="quote">“<span class="quote">existing
    cluster</span>”</span> is used to refer to any
    cluster that consists of at least one node. Existing clusters have a basic
    Corosync configuration that defines the communication channels, but
    they do not necessarily have resource configuration yet.
   </p></dd><dt id="glo-failover"><span><span class="glossterm">failover</span> <a title="Permalink" class="permalink" href="#glo-failover">#</a></span></dt><dd class="glossdef"><p>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </p></dd><dt id="id-1.4.8.28"><span><span class="glossterm">failover domain</span> <a title="Permalink" class="permalink" href="#id-1.4.8.28">#</a></span></dt><dd class="glossdef"><p>
    A named subset of cluster nodes that are eligible to run a cluster
    service if a node fails.
   </p></dd><dt id="id-1.4.8.29"><span><span class="glossterm">fencing</span> <a title="Permalink" class="permalink" href="#id-1.4.8.29">#</a></span></dt><dd class="glossdef"><p>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. There are two classes of fencing:
    resource level fencing and node level fencing. Resource level fencing ensures
    exclusive access to a given resource. Node level fencing prevents a failed
    node from accessing shared resources entirely and prevents resources from running
    on a node whose status is uncertain. This is usually done in a simple and
    abrupt way: reset or power off the node.
   </p></dd><dt id="id-1.4.8.30"><span><span class="glossterm">Geo cluster (geographically dispersed cluster)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.30">#</a></span></dt><dd class="glossdef"><p>
    Consists of multiple, geographically dispersed sites with a local cluster
    each. The sites communicate via IP. Failover across the sites is
    coordinated by a higher-level entity, the booth. Geo clusters need
    to cope with limited network bandwidth and high latency. Storage is
    replicated asynchronously.
   </p></dd><dt id="glos-lb"><span><span class="glossterm">load balancing</span> <a title="Permalink" class="permalink" href="#glos-lb">#</a></span></dt><dd class="glossdef"><p>
    The ability to make several servers participate in the same service and
    do the same work.
   </p></dd><dt id="id-1.4.8.32"><span><span class="glossterm">local cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.32">#</a></span></dt><dd class="glossdef"><p>
    A single cluster in one location (for example, all nodes are located in
    one data center). Network latency can be neglected. Storage is typically
    accessed synchronously by all nodes.
   </p></dd><dt id="id-1.4.8.33"><span><span class="glossterm">location</span> <a title="Permalink" class="permalink" href="#id-1.4.8.33">#</a></span></dt><dd class="glossdef"><p>
    In the context of a <span class="emphasis"><em>location constraint</em></span>, <span class="quote">“<span class="quote">location</span>”</span>
    refers to the nodes on which a resource can or cannot run.
   </p></dd><dt id="id-1.4.8.34"><span><span class="glossterm">LRM (local resource manager)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.34">#</a></span></dt><dd class="glossdef"><p>
    The local resource manager is located between the Pacemaker layer and the
    resources layer on each node. It is implemented as <code class="systemitem">pacemaker-execd</code> daemon. Through this daemon,
    Pacemaker can start, stop, and monitor resources.
   </p></dd><dt id="id-1.4.8.35"><span><span class="glossterm">mcastaddr (multicast address)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.35">#</a></span></dt><dd class="glossdef"><p>
      IP address to be used for multicasting by the Corosync executive. The IP
   address can either be IPv4 or IPv6. 
   </p></dd><dt id="id-1.4.8.36"><span><span class="glossterm">mcastport (multicast port)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.36">#</a></span></dt><dd class="glossdef"><p>
       The port to use for cluster communication.
   </p></dd><dt id="id-1.4.8.37"><span><span class="glossterm">metro cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.37">#</a></span></dt><dd class="glossdef"><p>
    A single cluster that can stretch over multiple buildings or data
    centers, with all sites connected by fibre channel. Network latency is
    usually low (&lt;5 ms for distances of approximately
    20 miles). Storage is frequently replicated (mirroring or
    synchronous replication).
   </p></dd><dt id="id-1.4.8.38"><span><span class="glossterm">multicast</span> <a title="Permalink" class="permalink" href="#id-1.4.8.38">#</a></span></dt><dd class="glossdef"><p>
      A technology used for a one-to-many communication within a network that
    can be used for cluster communication. Corosync supports both
    multicast and unicast.
   </p></dd><dt id="id-1.4.8.39"><span><span class="glossterm">node</span> <a title="Permalink" class="permalink" href="#id-1.4.8.39">#</a></span></dt><dd class="glossdef"><p>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </p></dd><dt id="id-1.4.8.40"><span><span class="glossterm">pacemaker-controld (cluster controller daemon)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.40">#</a></span></dt><dd class="glossdef"><p>
    The CRM is implemented as daemon, pacemaker-controld. It has an instance on each
    cluster node. All cluster decision-making is centralized by electing one
    of the pacemaker-controld instances to act as a primary. If the elected pacemaker-controld process
    fails (or the node it ran on), a new one is established.
   </p></dd><dt id="id-1.4.8.41"><span><span class="glossterm">PE (policy engine)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.41">#</a></span></dt><dd class="glossdef"><p>
    The policy engine is implemented as
    <code class="systemitem">pacemaker-schedulerd</code> daemon.
    When a cluster transition is needed, based on the current state and
    configuration, <code class="systemitem">pacemaker-schedulerd</code>
    calculates the expected next state of the cluster. It determines what
    actions need to be scheduled to achieve the next state.
   </p></dd><dt id="gloss-quorum"><span><span class="glossterm">quorum</span> <a title="Permalink" class="permalink" href="#gloss-quorum">#</a></span></dt><dd class="glossdef"><p>
    In a cluster, a cluster partition is defined to have quorum (be
    <span class="quote">“<span class="quote">quorate</span>”</span>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </p></dd><dt id="id-1.4.8.43"><span><span class="glossterm">RA (resource agent)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.43">#</a></span></dt><dd class="glossdef"><p>
    A script acting as a proxy to manage a resource (for example, to start,
    stop, or monitor a resource). SUSE Linux Enterprise High Availability supports different
    kinds of resource agents. For details, see
    <a class="xref" href="#sec-ha-config-basics-raclasses" title="6.2. Supported Resource Agent Classes">Section 6.2, “Supported Resource Agent Classes”</a>.
   </p></dd><dt id="id-1.4.8.44"><span><span class="glossterm">ReaR (Relax and Recover)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.44">#</a></span></dt><dd class="glossdef"><p>
    An administrator tool set for creating disaster recovery images.
   </p></dd><dt id="id-1.4.8.45"><span><span class="glossterm">resource</span> <a title="Permalink" class="permalink" href="#id-1.4.8.45">#</a></span></dt><dd class="glossdef"><p>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </p><p>
    The term <span class="quote">“<span class="quote">resource</span>”</span> is also used for DRBD, where it names a
    set of block devices that are using a common connection for replication.
   </p></dd><dt id="id-1.4.8.46"><span><span class="glossterm">RRP (redundant ring protocol)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.46">#</a></span></dt><dd class="glossdef"><p>
     Allows the  use of multiple redundant local area networks for resilience
   against partial or total network faults. This way, cluster communication can
   still be kept up as long as a single network is operational.
   Corosync supports the Totem Redundant Ring Protocol.
   </p></dd><dt id="id-1.4.8.47"><span><span class="glossterm">SBD (STONITH Block Device)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.47">#</a></span></dt><dd class="glossdef"><p>
    Provides a node fencing mechanism through the exchange of messages via shared
    block storage (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode.
    Needs a hardware or software watchdog on each node to ensure that misbehaving
    nodes are really stopped.
   </p></dd><dt id="id-1.4.8.48"><span><span class="glossterm">SFEX (shared disk file exclusiveness)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.48">#</a></span></dt><dd class="glossdef"><p>
    SFEX provides storage protection over SAN.
   </p></dd><dt id="glos-splitbrain"><span><span class="glossterm">split brain</span> <a title="Permalink" class="permalink" href="#glos-splitbrain">#</a></span></dt><dd class="glossdef"><p>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). STONITH prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <span class="quote">“<span class="quote">partitioned
    cluster</span>”</span> scenario.
   </p><p>
    The term split brain is also used in DRBD but means that the two nodes
    contain different data.
   </p></dd><dt id="id-1.4.8.50"><span><span class="glossterm">SPOF (single point of failure)</span> <a title="Permalink" class="permalink" href="#id-1.4.8.50">#</a></span></dt><dd class="glossdef"><p>
    Any component of a cluster that, should it fail, triggers the failure of
    the entire cluster.
   </p></dd><dt id="glo-stonith"><span><span class="glossterm">STONITH</span> <a title="Permalink" class="permalink" href="#glo-stonith">#</a></span></dt><dd class="glossdef"><p>
    The acronym for <span class="quote">“<span class="quote">Shoot the other node in the head</span>”</span>. It refers
    to the fencing mechanism that shuts down a misbehaving node to prevent it
    from causing trouble in a cluster. In a Pacemaker cluster, the implementation
    of node level fencing is STONITH. For this, Pacemaker comes with a fencing
    subsystem, <code class="systemitem">pacemaker-fenced</code>.
   </p></dd><dt id="id-1.4.8.52"><span><span class="glossterm">switchover</span> <a title="Permalink" class="permalink" href="#id-1.4.8.52">#</a></span></dt><dd class="glossdef"><p>
    Planned, on-demand moving of services to other nodes in a cluster. See
    <a class="xref" href="#glo-failover" title="failover">failover</a>.
   </p></dd><dt id="id-1.4.8.53"><span><span class="glossterm">ticket</span> <a title="Permalink" class="permalink" href="#id-1.4.8.53">#</a></span></dt><dd class="glossdef"><p>
    A component used in Geo clusters. A ticket grants the right to run
    certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by
    dependencies. Only if the defined ticket is available at a site, the
    respective resources are started. Vice versa, if the ticket is removed,
    the resources depending on that ticket are automatically stopped.
   </p></dd><dt id="id-1.4.8.54"><span><span class="glossterm">unicast</span> <a title="Permalink" class="permalink" href="#id-1.4.8.54">#</a></span></dt><dd class="glossdef"><p>
    A technology for sending messages to a single network destination.
    Corosync supports both multicast and unicast. In Corosync,
    unicast is implemented as UDP-unicast (UDPU).
   </p></dd></dl></section><div class="legal-section"><section class="appendix" id="id-1.4.9" data-id-title="GNU licenses"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">E </span><span class="title-name">GNU licenses</span></span> <a title="Permalink" class="permalink" href="#id-1.4.9">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_legal.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  This appendix contains the GNU Free Documentation License version 1.2.
 </p><section class="sect1" id="id-1.4.9.4" data-id-title="GNU Free Documentation License"><div class="titlepage"><div><div><div class="title-container"><h2 class="title legal"><span class="title-number-name"><span class="title-name">GNU Free Documentation License</span></span> <a title="Permalink" class="permalink" href="#id-1.4.9.4">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-sleha/edit/maintenance/SLEHA15SP2/xml/common_license_gfdl1.2.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.4"><span class="name">
    0. PREAMBLE
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.4">#</a></h5></div><p>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </p><p>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </p><p>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.8"><span class="name">
    1. APPLICABILITY AND DEFINITIONS
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.8">#</a></h5></div><p>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </p><p>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </p><p>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </p><p>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </p><p>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </p><p>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </p><p>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </p><p>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </p><p>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </p><p>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.19"><span class="name">
    2. VERBATIM COPYING
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.19">#</a></h5></div><p>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </p><p>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.22"><span class="name">
    3. COPYING IN QUANTITY
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.22">#</a></h5></div><p>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </p><p>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </p><p>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </p><p>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.27"><span class="name">
    4. MODIFICATIONS
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.27">#</a></h5></div><p>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </p></li><li class="listitem"><p>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </p></li><li class="listitem"><p>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </p></li><li class="listitem"><p>
    Preserve all the copyright notices of the Document.
   </p></li><li class="listitem"><p>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </p></li><li class="listitem"><p>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </p></li><li class="listitem"><p>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </p></li><li class="listitem"><p>
    Include an unaltered copy of this License.
   </p></li><li class="listitem"><p>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </p></li><li class="listitem"><p>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </p></li><li class="listitem"><p>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </p></li><li class="listitem"><p>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </p></li><li class="listitem"><p>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </p></li><li class="listitem"><p>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </p></li><li class="listitem"><p>
    Preserve any Warranty Disclaimers.
   </p></li></ol></div><p>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </p><p>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </p><p>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </p><p>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.34"><span class="name">
    5. COMBINING DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.34">#</a></h5></div><p>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </p><p>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </p><p>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.38"><span class="name">
    6. COLLECTIONS OF DOCUMENTS
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.38">#</a></h5></div><p>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </p><p>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.41"><span class="name">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.41">#</a></h5></div><p>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </p><p>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.44"><span class="name">
    8. TRANSLATION
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.44">#</a></h5></div><p>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </p><p>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.47"><span class="name">
    9. TERMINATION
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.47">#</a></h5></div><p>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.49"><span class="name">
    10. FUTURE REVISIONS OF THIS LICENSE
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.49">#</a></h5></div><p>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <a class="link" href="https://www.gnu.org/copyleft/" target="_blank">https://www.gnu.org/copyleft/</a>.
 </p><p>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </p><div class="sect4 bridgehead"><h5 class="title legal" id="id-1.4.9.4.52"><span class="name">
    ADDENDUM: How to use this License for your documents
  </span><a title="Permalink" class="permalink" href="#id-1.4.9.4.52">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</pre></div><p>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </p><p>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
 </p></section></section></div></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>